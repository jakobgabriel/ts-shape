{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"ts-shape | Timeseries Shaper","text":"<p>ts-shape is a lightweight, composable toolkit to load, shape, and analyze time series data. It embraces a simple DataFrame-in \u2192 DataFrame-out philosophy across loaders, transforms, feature extractors, and event detectors.</p> <p>Key ideas:</p> <ul> <li>Unified DataFrame workflow: Load timeseries + metadata, join on <code>uuid</code>, and process.</li> <li>Modular building blocks: Use only what you need; components are decoupled and easy to extend.</li> <li>Performance aware: Vectorized ops, chunked DB reads, and concurrent I/O for remote storage.</li> </ul>"},{"location":"#install","title":"Install","text":"<pre><code>pip install ts-shape\n# Parquet engine (recommended)\npip install pyarrow  # or: pip install fastparquet\n</code></pre> <p>Optional integrations:</p> <ul> <li>Azure Blob Storage: <code>pip install azure-storage-blob</code></li> <li>Azure AAD + management (optional): <code>pip install azure-identity azure-mgmt-storage</code></li> <li>S3 proxy access: already included via <code>s3fs</code></li> <li>TimescaleDB: <code>pip install sqlalchemy psycopg2-binary</code></li> </ul>"},{"location":"#whats-inside","title":"What\u2019s Inside","text":"<ul> <li>Loaders (timeseries):</li> <li>Parquet folders (local)</li> <li>S3 proxy parquet via <code>s3fs</code></li> <li>Azure Blob parquet (hourly layout, UUID filters, time range)</li> <li>TimescaleDB (chunked reads, parquet export by hour)</li> <li>Loaders (metadata):</li> <li>JSON metadata loader (robust input shapes, flattens config)</li> <li>Transformations:</li> <li>Filters (numeric/string/boolean/datetime), generic functions, time functions, calculators</li> <li>Features:</li> <li>Descriptive stats, time stats, cycles utilities</li> <li>Events:</li> <li>Quality (outlier detection, SPC, tolerance deviation), production/maintenance patterns</li> </ul> <p>See the extended concept overview in <code>docs/concept.md</code>.</p>"},{"location":"#license","title":"License","text":"<p>MIT \u2014 see <code>LICENSE.txt</code>.</p>"},{"location":"CYCLE_EXTRACTOR_ENHANCEMENTS/","title":"CycleExtractor Enhancements Documentation","text":"<p>This document describes the enhancements made to the <code>CycleExtractor</code> class in <code>/home/user/ts-shape/src/ts_shape/features/cycles/cycles_extractor.py</code>.</p>"},{"location":"CYCLE_EXTRACTOR_ENHANCEMENTS/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Overview</li> <li>New Features</li> <li>API Reference</li> <li>Usage Examples</li> <li>Backward Compatibility</li> </ol>"},{"location":"CYCLE_EXTRACTOR_ENHANCEMENTS/#overview","title":"Overview","text":"<p>The <code>CycleExtractor</code> class has been enhanced with several new features to improve cycle extraction, validation, and analysis capabilities while maintaining full backward compatibility with existing code.</p>"},{"location":"CYCLE_EXTRACTOR_ENHANCEMENTS/#key-improvements","title":"Key Improvements","text":"<ul> <li>Incomplete Cycle Tracking: All cycles now include an <code>is_complete</code> flag</li> <li>Cycle Validation: Validate cycles based on duration constraints</li> <li>Overlap Detection: Identify and optionally resolve overlapping cycles</li> <li>Method Suggestions: Get AI-powered recommendations for the best extraction method</li> <li>Extraction Statistics: Detailed statistics about the extraction process</li> <li>Value Change Threshold: Configure significance threshold for numeric value changes</li> <li>Better Error Handling: Incomplete cycles are now tracked instead of silently lost</li> </ul>"},{"location":"CYCLE_EXTRACTOR_ENHANCEMENTS/#new-features","title":"New Features","text":""},{"location":"CYCLE_EXTRACTOR_ENHANCEMENTS/#1-incomplete-cycle-handling","title":"1. Incomplete Cycle Handling","text":"<p>What it does: Tracks cycles that have a start time but no matching end time, marking them as incomplete instead of silently dropping them.</p> <p>Output changes: - All extracted cycle DataFrames now include an <code>is_complete</code> boolean column - Incomplete cycles have <code>is_complete=False</code> and <code>cycle_end=NaT</code> (Not a Time)</p> <p>Benefits: - No silent data loss - Better understanding of extraction quality - Ability to investigate incomplete cycles</p>"},{"location":"CYCLE_EXTRACTOR_ENHANCEMENTS/#2-cycle-validation","title":"2. Cycle Validation","text":"<p>What it does: Validates extracted cycles against duration constraints and completeness criteria.</p> <p>New method: <code>validate_cycles(cycle_df, min_duration='1s', max_duration='1h', warn=True)</code></p> <p>Parameters: - <code>cycle_df</code>: DataFrame with extracted cycles - <code>min_duration</code>: Minimum acceptable duration (e.g., '1s', '5m', '1h') - <code>max_duration</code>: Maximum acceptable duration - <code>warn</code>: Whether to log warnings for invalid cycles</p> <p>Output columns added: - <code>cycle_duration</code>: Calculated duration of each cycle - <code>is_valid</code>: Boolean indicating if cycle passes validation - <code>validation_issue</code>: String describing any validation issues</p> <p>Validation issues: - <code>incomplete_cycle</code>: Cycle has no end time - <code>too_short</code>: Cycle duration is below minimum - <code>too_long</code>: Cycle duration exceeds maximum</p>"},{"location":"CYCLE_EXTRACTOR_ENHANCEMENTS/#3-overlapping-cycle-detection","title":"3. Overlapping Cycle Detection","text":"<p>What it does: Identifies cycles where one cycle's time range overlaps with another's.</p> <p>New method: <code>detect_overlapping_cycles(cycle_df, resolve='flag')</code></p> <p>Parameters: - <code>cycle_df</code>: DataFrame with extracted cycles - <code>resolve</code>: How to handle overlaps   - <code>'flag'</code>: Only mark overlapping cycles (default)   - <code>'keep_first'</code>: Remove later overlapping cycles   - <code>'keep_last'</code>: Remove earlier overlapping cycles   - <code>'keep_longest'</code>: Keep the cycle with longest duration</p> <p>Output column added: - <code>has_overlap</code>: Boolean indicating if cycle overlaps with another</p> <p>Use cases: - Data quality checks - Identifying extraction method issues - Cleaning cycle data for analysis</p>"},{"location":"CYCLE_EXTRACTOR_ENHANCEMENTS/#4-method-selection-helper","title":"4. Method Selection Helper","text":"<p>What it does: Analyzes input data characteristics and recommends the most appropriate cycle extraction method(s).</p> <p>New method: <code>suggest_method()</code></p> <p>Returns: Dictionary with: - <code>recommended_methods</code>: List of method names in priority order - <code>reasoning</code>: List of explanations for each recommendation - <code>data_characteristics</code>: Analysis of input data</p> <p>Data characteristics analyzed: - Presence of boolean, integer, double, and string values - Boolean transition patterns - Integer value diversity (for step sequences) - Separate start/end UUID configuration</p> <p>Example output: <pre><code>{\n    'recommended_methods': ['process_persistent_cycle', 'process_trigger_cycle'],\n    'reasoning': [\n        'Boolean transitions detected, suitable for persistent cycles',\n        'Boolean data present, can use trigger-based extraction'\n    ],\n    'data_characteristics': {\n        'has_boolean_values': True,\n        'has_integer_values': True,\n        'has_double_values': False,\n        'has_string_values': False,\n        'row_count': 1000,\n        'separate_start_end': False\n    }\n}\n</code></pre></p>"},{"location":"CYCLE_EXTRACTOR_ENHANCEMENTS/#5-cycle-extraction-statistics","title":"5. Cycle Extraction Statistics","text":"<p>What it does: Provides detailed statistics about the most recent cycle extraction.</p> <p>New methods: - <code>get_extraction_stats()</code>: Returns statistics dictionary - <code>reset_stats()</code>: Resets statistics counters</p> <p>Statistics tracked: - <code>total_cycles</code>: Total number of cycles extracted - <code>complete_cycles</code>: Number of complete cycles - <code>incomplete_cycles</code>: Number of incomplete cycles - <code>unmatched_starts</code>: Cycle starts with no matching end - <code>unmatched_ends</code>: Cycle ends with no matching start - <code>overlapping_cycles</code>: Number of overlapping cycle pairs detected - <code>success_rate</code>: Ratio of complete to total cycles - <code>warnings</code>: List of warning messages generated - <code>configuration</code>: Extraction configuration used</p> <p>Use cases: - Quality assurance - Debugging extraction issues - Monitoring extraction performance - Automated testing and validation</p>"},{"location":"CYCLE_EXTRACTOR_ENHANCEMENTS/#6-improved-iterator-handling","title":"6. Improved Iterator Handling","text":"<p>What changed: The internal <code>_generate_cycle_dataframe()</code> method now continues processing all cycle starts even when cycle ends run out, marking remaining cycles as incomplete.</p> <p>Previous behavior: Silently stopped processing when cycle ends were exhausted.</p> <p>New behavior: - Continues processing all starts - Marks incomplete cycles with <code>is_complete=False</code> - Logs warnings for each incomplete cycle - Maintains statistics about unmatched starts</p>"},{"location":"CYCLE_EXTRACTOR_ENHANCEMENTS/#7-value-change-significance-threshold","title":"7. Value Change Significance Threshold","text":"<p>What it does: Allows configuration of a threshold for determining when numeric value changes are significant enough to trigger a new cycle.</p> <p>New parameter: <code>value_change_threshold</code> in <code>__init__()</code></p> <p>Default: <code>0.0</code> (any change is significant)</p> <p>How it works: - For <code>value_double</code> and <code>value_integer</code> columns - Change must exceed threshold: <code>abs(diff) &gt; threshold</code> - Boolean and string changes always considered significant - Applied in <code>process_value_change_cycle()</code> method</p> <p>Use cases: - Filtering out noise in sensor data - Ignoring minor fluctuations - Focusing on significant state transitions</p>"},{"location":"CYCLE_EXTRACTOR_ENHANCEMENTS/#api-reference","title":"API Reference","text":""},{"location":"CYCLE_EXTRACTOR_ENHANCEMENTS/#constructor","title":"Constructor","text":"<pre><code>CycleExtractor(\n    dataframe: pd.DataFrame,\n    start_uuid: str,\n    end_uuid: Optional[str] = None,\n    value_change_threshold: float = 0.0\n)\n</code></pre> <p>New parameter: - <code>value_change_threshold</code>: Minimum threshold for numeric value changes (default: 0.0)</p>"},{"location":"CYCLE_EXTRACTOR_ENHANCEMENTS/#new-methods","title":"New Methods","text":""},{"location":"CYCLE_EXTRACTOR_ENHANCEMENTS/#validate_cycles","title":"validate_cycles()","text":"<pre><code>validate_cycles(\n    cycle_df: pd.DataFrame,\n    min_duration: str = '1s',\n    max_duration: str = '1h',\n    warn: bool = True\n) -&gt; pd.DataFrame\n</code></pre> <p>Validates cycles based on duration constraints.</p> <p>Parameters: - <code>cycle_df</code>: DataFrame with cycle data (output from process_* methods) - <code>min_duration</code>: Minimum acceptable cycle duration (default: '1s') - <code>max_duration</code>: Maximum acceptable cycle duration (default: '1h') - <code>warn</code>: Whether to log warnings for invalid cycles (default: True)</p> <p>Returns: DataFrame with added columns: - <code>cycle_duration</code>: Duration of each cycle - <code>is_valid</code>: Whether cycle passes validation - <code>validation_issue</code>: Description of validation issues</p> <p>Duration format: Number followed by unit: - <code>s</code>: seconds (e.g., '30s') - <code>m</code>: minutes (e.g., '5m') - <code>h</code>: hours (e.g., '2h') - <code>d</code>: days (e.g., '1d')</p>"},{"location":"CYCLE_EXTRACTOR_ENHANCEMENTS/#detect_overlapping_cycles","title":"detect_overlapping_cycles()","text":"<pre><code>detect_overlapping_cycles(\n    cycle_df: pd.DataFrame,\n    resolve: str = 'flag'\n) -&gt; pd.DataFrame\n</code></pre> <p>Detects and optionally resolves overlapping cycles.</p> <p>Parameters: - <code>cycle_df</code>: DataFrame with cycle data - <code>resolve</code>: How to handle overlaps   - <code>'flag'</code>: Only mark overlapping cycles (default)   - <code>'keep_first'</code>: Remove later overlapping cycles   - <code>'keep_last'</code>: Remove earlier overlapping cycles   - <code>'keep_longest'</code>: Keep cycles with longest duration</p> <p>Returns: DataFrame with <code>has_overlap</code> column added (and potentially filtered rows)</p>"},{"location":"CYCLE_EXTRACTOR_ENHANCEMENTS/#suggest_method","title":"suggest_method()","text":"<pre><code>suggest_method() -&gt; Dict[str, Any]\n</code></pre> <p>Suggests the best cycle extraction method based on data characteristics.</p> <p>Returns: Dictionary with: - <code>recommended_methods</code>: List of recommended method names - <code>reasoning</code>: List of explanations for each recommendation - <code>data_characteristics</code>: Dictionary of analyzed data properties</p>"},{"location":"CYCLE_EXTRACTOR_ENHANCEMENTS/#get_extraction_stats","title":"get_extraction_stats()","text":"<pre><code>get_extraction_stats() -&gt; Dict[str, Any]\n</code></pre> <p>Gets statistics about the last cycle extraction.</p> <p>Returns: Dictionary with extraction statistics: - <code>total_cycles</code>: Total cycles extracted - <code>complete_cycles</code>: Number of complete cycles - <code>incomplete_cycles</code>: Number of incomplete cycles - <code>unmatched_starts</code>: Starts without matching ends - <code>unmatched_ends</code>: Ends without matching starts - <code>overlapping_cycles</code>: Number of overlapping pairs - <code>success_rate</code>: Ratio of complete to total cycles - <code>warnings</code>: List of warning messages - <code>configuration</code>: Extraction configuration</p>"},{"location":"CYCLE_EXTRACTOR_ENHANCEMENTS/#reset_stats","title":"reset_stats()","text":"<pre><code>reset_stats() -&gt; None\n</code></pre> <p>Resets extraction statistics to initial values.</p>"},{"location":"CYCLE_EXTRACTOR_ENHANCEMENTS/#modified-methods","title":"Modified Methods","text":"<p>All existing <code>process_*</code> methods now return DataFrames with an additional <code>is_complete</code> column:</p> <ul> <li><code>process_persistent_cycle()</code></li> <li><code>process_trigger_cycle()</code></li> <li><code>process_separate_start_end_cycle()</code></li> <li><code>process_step_sequence(start_step, end_step)</code></li> <li><code>process_state_change_cycle()</code></li> <li><code>process_value_change_cycle()</code></li> </ul>"},{"location":"CYCLE_EXTRACTOR_ENHANCEMENTS/#usage-examples","title":"Usage Examples","text":""},{"location":"CYCLE_EXTRACTOR_ENHANCEMENTS/#example-1-basic-usage-with-incomplete-cycle-tracking","title":"Example 1: Basic Usage with Incomplete Cycle Tracking","text":"<pre><code>from ts_shape.features.cycles.cycles_extractor import CycleExtractor\nimport pandas as pd\n\n# Create extractor\ndf = pd.DataFrame(...)  # Your data\nextractor = CycleExtractor(df, start_uuid='machine-status')\n\n# Extract cycles\ncycles = extractor.process_persistent_cycle()\n\n# Check for incomplete cycles\nprint(f\"Total cycles: {len(cycles)}\")\nprint(f\"Complete: {cycles['is_complete'].sum()}\")\nprint(f\"Incomplete: {(~cycles['is_complete']).sum()}\")\n\n# Filter out incomplete cycles if needed\ncomplete_cycles = cycles[cycles['is_complete']]\n</code></pre>"},{"location":"CYCLE_EXTRACTOR_ENHANCEMENTS/#example-2-validation-workflow","title":"Example 2: Validation Workflow","text":"<pre><code># Extract cycles\ncycles = extractor.process_persistent_cycle()\n\n# Validate with duration constraints\nvalidated = extractor.validate_cycles(\n    cycles,\n    min_duration='10s',\n    max_duration='5m',\n    warn=True\n)\n\n# Get only valid cycles\nvalid_cycles = validated[validated['is_valid']]\n\n# Investigate invalid cycles\ninvalid_cycles = validated[~validated['is_valid']]\nprint(invalid_cycles[['cycle_start', 'validation_issue']])\n</code></pre>"},{"location":"CYCLE_EXTRACTOR_ENHANCEMENTS/#example-3-overlap-detection-and-resolution","title":"Example 3: Overlap Detection and Resolution","text":"<pre><code># Extract cycles\ncycles = extractor.process_persistent_cycle()\n\n# Detect overlaps\ncycles_with_flags = extractor.detect_overlapping_cycles(cycles, resolve='flag')\nprint(f\"Overlapping cycles: {cycles_with_flags['has_overlap'].sum()}\")\n\n# Resolve by keeping first occurrence\nclean_cycles = extractor.detect_overlapping_cycles(cycles, resolve='keep_first')\n\n# Or resolve by keeping longest cycles\nclean_cycles = extractor.detect_overlapping_cycles(cycles, resolve='keep_longest')\n</code></pre>"},{"location":"CYCLE_EXTRACTOR_ENHANCEMENTS/#example-4-method-selection","title":"Example 4: Method Selection","text":"<pre><code># Get method suggestions before extraction\nsuggestions = extractor.suggest_method()\n\nprint(\"Recommended methods:\")\nfor method, reason in zip(suggestions['recommended_methods'], suggestions['reasoning']):\n    print(f\"- {method}: {reason}\")\n\n# Use the first recommended method\nrecommended_method = suggestions['recommended_methods'][0]\nif recommended_method == 'process_persistent_cycle':\n    cycles = extractor.process_persistent_cycle()\nelif recommended_method == 'process_trigger_cycle':\n    cycles = extractor.process_trigger_cycle()\n# ... etc\n</code></pre>"},{"location":"CYCLE_EXTRACTOR_ENHANCEMENTS/#example-5-statistics-and-monitoring","title":"Example 5: Statistics and Monitoring","text":"<pre><code># Extract cycles\ncycles = extractor.process_persistent_cycle()\n\n# Get statistics\nstats = extractor.get_extraction_stats()\n\nprint(f\"Success rate: {stats['success_rate']:.2%}\")\nprint(f\"Complete cycles: {stats['complete_cycles']}\")\nprint(f\"Incomplete cycles: {stats['incomplete_cycles']}\")\nprint(f\"Warnings: {len(stats['warnings'])}\")\n\n# Check if extraction quality is acceptable\nif stats['success_rate'] &lt; 0.95:\n    print(\"Warning: Low success rate detected!\")\n    print(\"Warnings:\", stats['warnings'])\n</code></pre>"},{"location":"CYCLE_EXTRACTOR_ENHANCEMENTS/#example-6-value-change-threshold","title":"Example 6: Value Change Threshold","text":"<pre><code># Create extractor with threshold for significant changes\nextractor = CycleExtractor(\n    df,\n    start_uuid='temperature-sensor',\n    value_change_threshold=2.5  # Ignore changes &lt; 2.5 units\n)\n\n# Extract cycles (only significant value changes trigger new cycles)\ncycles = extractor.process_value_change_cycle()\n</code></pre>"},{"location":"CYCLE_EXTRACTOR_ENHANCEMENTS/#example-7-complete-workflow","title":"Example 7: Complete Workflow","text":"<pre><code># 1. Initialize with threshold\nextractor = CycleExtractor(df, start_uuid='process-id', value_change_threshold=1.0)\n\n# 2. Get method suggestions\nsuggestions = extractor.suggest_method()\nprint(f\"Using: {suggestions['recommended_methods'][0]}\")\n\n# 3. Extract cycles\ncycles = extractor.process_persistent_cycle()\n\n# 4. Validate\nvalidated = extractor.validate_cycles(cycles, min_duration='5s', max_duration='2m')\n\n# 5. Check for overlaps\nclean_cycles = extractor.detect_overlapping_cycles(\n    validated[validated['is_valid']],\n    resolve='keep_longest'\n)\n\n# 6. Review statistics\nstats = extractor.get_extraction_stats()\nprint(f\"Final: {len(clean_cycles)} cycles, {stats['success_rate']:.1%} success rate\")\n\n# 7. Use clean cycles for analysis\nfinal_cycles = clean_cycles[clean_cycles['is_complete'] &amp; clean_cycles['is_valid']]\n</code></pre>"},{"location":"CYCLE_EXTRACTOR_ENHANCEMENTS/#backward-compatibility","title":"Backward Compatibility","text":"<p>All enhancements are fully backward compatible with existing code:</p>"},{"location":"CYCLE_EXTRACTOR_ENHANCEMENTS/#what-stays-the-same","title":"What Stays the Same","text":"<ol> <li>All existing methods work unchanged:</li> <li><code>process_persistent_cycle()</code></li> <li><code>process_trigger_cycle()</code></li> <li><code>process_separate_start_end_cycle()</code></li> <li><code>process_step_sequence()</code></li> <li><code>process_state_change_cycle()</code></li> <li> <p><code>process_value_change_cycle()</code></p> </li> <li> <p>Existing return columns preserved:</p> </li> <li><code>cycle_start</code></li> <li><code>cycle_end</code></li> <li> <p><code>cycle_uuid</code></p> </li> <li> <p>Constructor still works with original parameters:    <pre><code># Old code continues to work\nextractor = CycleExtractor(df, start_uuid='my-uuid')\nextractor = CycleExtractor(df, start_uuid='start-uuid', end_uuid='end-uuid')\n</code></pre></p> </li> </ol>"},{"location":"CYCLE_EXTRACTOR_ENHANCEMENTS/#whats-new-opt-in","title":"What's New (Opt-in)","text":"<ol> <li>New column added: <code>is_complete</code> (can be ignored if not needed)</li> <li>New optional parameter: <code>value_change_threshold</code> (defaults to 0.0 for original behavior)</li> <li>New methods: All new methods are additions and don't affect existing functionality</li> </ol>"},{"location":"CYCLE_EXTRACTOR_ENHANCEMENTS/#migration-notes","title":"Migration Notes","text":"<p>No migration needed! Existing code will continue to work without changes. To use new features:</p> <ol> <li>For incomplete cycle tracking: Simply use the new <code>is_complete</code> column</li> <li>For validation: Call <code>validate_cycles()</code> on extracted cycles</li> <li>For overlap detection: Call <code>detect_overlapping_cycles()</code> on extracted cycles</li> <li>For method suggestions: Call <code>suggest_method()</code> before extraction</li> <li>For statistics: Call <code>get_extraction_stats()</code> after extraction</li> </ol>"},{"location":"CYCLE_EXTRACTOR_ENHANCEMENTS/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>Validation: O(n) where n is the number of cycles</li> <li>Overlap detection: O(n\u00b2) worst case, but typically much faster as it breaks early</li> <li>Overlap resolution: O(n) additional overhead</li> <li>Method suggestion: O(m) where m is the number of data rows</li> <li>Statistics: Negligible overhead (computed during extraction)</li> </ul>"},{"location":"CYCLE_EXTRACTOR_ENHANCEMENTS/#testing","title":"Testing","text":"<p>A comprehensive demonstration script is available at: <code>/home/user/ts-shape/examples/cycle_extractor_enhancements_demo.py</code></p> <p>Run it to see all features in action: <pre><code>python examples/cycle_extractor_enhancements_demo.py\n</code></pre></p>"},{"location":"CYCLE_EXTRACTOR_ENHANCEMENTS/#future-enhancements","title":"Future Enhancements","text":"<p>Possible future additions (not yet implemented):</p> <ol> <li>Custom validation rules</li> <li>Cycle gap analysis</li> <li>Automatic overlap resolution strategy selection</li> <li>Parallel cycle extraction for large datasets</li> <li>Cycle quality scoring</li> <li>Interactive visualization of cycles and issues</li> </ol>"},{"location":"CYCLE_EXTRACTOR_ENHANCEMENTS/#support","title":"Support","text":"<p>For issues or questions about these enhancements, please refer to: - Source code: <code>/home/user/ts-shape/src/ts_shape/features/cycles/cycles_extractor.py</code> - Demo script: <code>/home/user/ts-shape/examples/cycle_extractor_enhancements_demo.py</code> - This documentation: <code>/home/user/ts-shape/docs/CYCLE_EXTRACTOR_ENHANCEMENTS.md</code></p>"},{"location":"changelog/","title":"Sep 09, 2025","text":"<p>add: Azure blob storage container loader add: Metadata JSON loader added changed: development guide</p>"},{"location":"changelog/#apr-06-2025","title":"Apr 06, 2025","text":"<p>add: quickstart guide added to docs</p>"},{"location":"changelog/#dec-26-2024","title":"Dec 26, 2024","text":"<p>add: mkdocs material deployment with gh actions</p>"},{"location":"changelog/#dec-25-2024","title":"Dec 25, 2024","text":"<p>add: pdoc docs exchanged with mkdocs material autodoc</p>"},{"location":"changelog/#dec-23-2024","title":"Dec 23, 2024","text":"<p>add: library rename add: library structure change. loader, transform, feature, context, events</p>"},{"location":"changelog/#dec-20-2024","title":"Dec 20, 2024","text":"<p>fix: closes #7</p>"},{"location":"changelog/#nov-16-2024","title":"Nov 16, 2024","text":"<p>add: dev state docs for combine/integrator.py add: dev state for combine/integrator.py add: metadata loader improved</p>"},{"location":"changelog/#nov-4-2024","title":"Nov 4, 2024","text":"<p>add: stats classes improved + feature table class added</p>"},{"location":"changelog/#oct-29-2024","title":"Oct 29, 2024","text":"<p>add: timescaledb loader adjusted</p>"},{"location":"changelog/#oct-28-2024","title":"Oct 28, 2024","text":"<p>add: loader classes adjusted add: classes for timestamp_converter added add: classes for s3, timescaledb, timezone_shift added</p>"},{"location":"changelog/#oct-27-2024","title":"Oct 27, 2024","text":"<p>add: docs for time_stats added add: time_stats for numeric value columns added</p>"},{"location":"changelog/#sep-11-2024","title":"Sep 11, 2024","text":"<p>add: methods refactored to execute without an instance</p>"},{"location":"changelog/#sep-9-2024","title":"Sep 9, 2024","text":"<p>add: cycle methods refined and splitted to cycle processor and extractor</p>"},{"location":"changelog/#sep-8-2024","title":"Sep 8, 2024","text":"<p>add: cycle method and metadata loader including tests for numeric filters add: additional methods added</p>"},{"location":"changelog/#aug-25-2024","title":"Aug 25, 2024","text":"<p>fix: docs re-created fix: docs add: parquet loader, timestamp, string, numeric and boolean stats methods</p>"},{"location":"changelog/#may-4-2024","title":"May 4, 2024","text":"<p>change: typings added, first test added change: package folder moved init: pypi package structure for timeseries-shaper / internal project for my master thesis</p>"},{"location":"concept/","title":"Concept","text":"<p>ts-shape is a lightweight, composable toolkit for shaping time series data into analysis-ready DataFrames. It focuses on three pillars: loading, transforming, and extracting higher-level features/events \u2014 with a consistent, Pandas-first interface.</p>"},{"location":"concept/#architecture-overview","title":"Architecture Overview","text":"<pre><code>flowchart TD\n    A[Loaders: Timeseries + Metadata] --&gt; B[Combine: join on uuid]\n    B --&gt; C[Transform: Filters / Functions / Time Functions / Calculator]\n    C --&gt; D[Features: Stats / Time Stats / Cycles]\n    D --&gt; E[Events: Quality / Maintenance / Production / Engineering]</code></pre> <p>Core ideas:</p> <ul> <li>DataFrame-in, DataFrame-out: Every stage accepts and returns Pandas DataFrames for easy composition.</li> <li>Simple schema: Timeseries frames use a compact set of typed columns; metadata/enrichment joins on a stable <code>uuid</code> key.</li> <li>Modular blocks: Use only what you need \u2014 loaders, transforms, features, and events are decoupled.</li> </ul>"},{"location":"concept/#data-model","title":"Data Model","text":"<p>Timeseries DataFrame (typical columns):</p> <ul> <li>uuid: string identifier for a signal/series</li> <li>systime: timestamp (tz-aware recommended)</li> <li>value_double, value_integer, value_string, value_bool: value channels (one or more may be present)</li> <li>is_delta: boolean flag indicating delta semantics (optional)</li> </ul> <p>Metadata DataFrame:</p> <ul> <li>Indexed by uuid or has a <code>uuid</code> column</li> <li>Arbitrary columns describing the signal (label, unit, config.*)</li> </ul> <p>Conventions:</p> <ul> <li>Join key is <code>uuid</code> by default.</li> <li>Keep values narrow: prefer one type-specific value column where possible.</li> </ul>"},{"location":"concept/#loaders","title":"Loaders","text":"<p>Timeseries:</p> <ul> <li>Parquet folder loader: Recursively reads parquet files from local/remote mounts.</li> <li>S3 proxy parquet loader: Streams parquet via S3-compatible endpoints.</li> <li>Azure Blob parquet loader: Loads parquet files from containers; supports time-based folder structure (parquet/YYYY/MM/DD/HH) and UUID filters.</li> <li>TimescaleDB loader: Streams rows by UUID and time range; can emit parquet partitioned by hour.</li> </ul> <p>Metadata:</p> <ul> <li>JSON metadata loader: Robustly ingests JSON in multiple shapes (list-of-records, dicts of lists/dicts), flattens <code>config</code> into columns, and indexes by <code>uuid</code>.</li> </ul> <p>All loaders expose either a DataFrame-returning method (e.g., <code>fetch_data_as_dataframe</code>, <code>to_df</code>) or a parquet materialization method when desired.</p>"},{"location":"concept/#combination-layer","title":"Combination Layer","text":"<p>Use <code>DataIntegratorHybrid.combine_data(...)</code> to merge timeseries and metadata sources into one frame:</p> <ul> <li>Accepts DataFrames or source objects (with <code>fetch_data_as_dataframe</code>/<code>fetch_metadata</code>).</li> <li>Merges on <code>uuid</code> (configurable), supporting different join strategies (<code>left</code>, <code>inner</code>, ...).</li> </ul> <p>Example: <pre><code>from ts_shape.loader.combine.integrator import DataIntegratorHybrid\n\ncombined = DataIntegratorHybrid.combine_data(\n    timeseries_sources=[ts_df_or_loader],\n    metadata_sources=[meta_df_or_loader],\n    uuids=[\"id-1\", \"id-2\"],\n    join_key=\"uuid\",\n    merge_how=\"left\",\n)\n</code></pre></p>"},{"location":"concept/#transform","title":"Transform","text":"<p>Reusable blocks to reshape and clean data:</p> <ul> <li>Filters: datatype-specific predicates (numeric/string/boolean/datetime) to subset rows or fix values.</li> <li>Functions: arbitrary lambda-like transformations for columns.</li> <li>Time Functions: timestamp operations (timezone shift, conversion, resampling helpers).</li> <li>Calculator: numeric calculators to derive engineered columns.</li> </ul> <p>All transformations accept/return DataFrames to compose pipelines like small, testable steps.</p>"},{"location":"concept/#features","title":"Features","text":"<p>Feature extractors summarize series into compact descriptors:</p> <ul> <li>Stats: per-type descriptive stats (min/max/mean/std for numeric, frequency for strings, etc.).</li> <li>Time Stats: timestamp-specific stats (first/last timestamp, counts per window, coverage).</li> <li>Cycles: utilities to identify and process cycles in signals.</li> </ul> <p><code>DescriptiveFeatures.compute(...)</code> can emit a nested dict or a flat DataFrame for easy downstream analysis.</p>"},{"location":"concept/#events","title":"Events","text":"<p>Event detectors derive categorical flags and ranges from raw signals:</p> <ul> <li>Quality: outlier detection, SPC rules, tolerance deviations.</li> <li>Maintenance: downtime and other operational events.</li> <li>Production/Engineering: domain patterns extractable from the shaped series.</li> </ul> <p>Each detector takes a DataFrame and returns either annotated frames or event tables.</p>"},{"location":"concept/#typical-pipeline","title":"Typical Pipeline","text":"<ol> <li>Load</li> <li>Read timeseries (e.g., parquet or DB) into a DataFrame with <code>uuid</code>, <code>systime</code>, and values.</li> <li> <p>Load metadata JSON and convert to a <code>uuid</code>-indexed DataFrame.</p> </li> <li> <p>Combine</p> </li> <li> <p>Join timeseries with metadata on <code>uuid</code> to enrich context.</p> </li> <li> <p>Transform</p> </li> <li> <p>Apply filters/functions/time operations; compute engineered columns.</p> </li> <li> <p>Features &amp; Events</p> </li> <li> <p>Compute stats and time stats; identify domain events.</p> </li> <li> <p>Output</p> </li> <li>Keep as a DataFrame, write parquet/CSV, or feed to a model/BI tool.</li> </ol>"},{"location":"concept/#design-principles","title":"Design Principles","text":"<ul> <li>Minimal assumptions: Works with partial columns; you choose the value channel(s) in play.</li> <li>Composability: Small building blocks; pure DataFrame IO.</li> <li>Performance-aware: Vectorized Pandas ops; chunked DB reads; concurrent IO for remote storage.</li> <li>Extensible: Add new loaders, transforms, features, or events with simple, documented interfaces.</li> </ul>"},{"location":"concept/#extending-ts-shape","title":"Extending ts-shape","text":"<ul> <li>New loader: implement a class with <code>fetch_data_as_dataframe()</code> or an explicit <code>to_parquet()</code> flow.</li> <li>New transform: write a function that takes/returns a DataFrame; place under <code>transform/*</code>.</li> <li>New feature/event: follow existing patterns; accept a DataFrame and return a summary/event frame.</li> </ul>"},{"location":"concept/#when-to-use-ts-shape","title":"When to Use ts-shape","text":"<ul> <li>You need a quick, pythonic path from raw timeseries + context to analysis-ready tables.</li> <li>You want modular building blocks instead of a monolithic framework.</li> <li>You operate across storage backends (parquet, S3/Azure, SQL) and prefer a unified DataFrame API.</li> </ul>"},{"location":"license/","title":"License","text":"<p>MIT License</p> <p>Copyright (c) [2024] Jakob Gabriel</p> <p>Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:</p> <p>The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.</p> <p>THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</p>"},{"location":"insiders/development/","title":"Development Guide","text":""},{"location":"insiders/development/#1-install-package-locally","title":"1. Install Package Locally","text":"<pre><code>pip install -e .\n</code></pre> <p>Installs your package in editable mode for local development.</p>"},{"location":"insiders/development/#2-run-tests","title":"2. Run Tests","text":"<pre><code>pytest ./tests\n</code></pre> <p>Executes all tests to ensure your code is working as expected.</p>"},{"location":"insiders/development/#3-build-distribution-packages","title":"3. Build Distribution Packages","text":"<pre><code>python setup.py sdist bdist_wheel\n</code></pre> <p>Creates source and wheel distributions in the <code>dist/</code> directory.</p>"},{"location":"insiders/development/#4-publish-to-pypi","title":"4. Publish to PyPI","text":"<pre><code>twine upload dist/* --verbose --skip-existing\n</code></pre> <p>Uploads your package to PyPI. Tip: Ensure your credentials are set up in <code>~/.pypirc</code>.</p>"},{"location":"insiders/development/#5-automatic-version-bumping-and-publishing-ci","title":"5. Automatic Version Bumping and Publishing (CI)","text":"<p>This repo is configured to auto-bump the version in <code>setup.py</code>, create a Git tag, and publish to PyPI on pushes to <code>main</code>.</p> <ul> <li>Workflow: <code>.github/workflows/auto_bump_version.yml</code></li> <li>Publish on tags: <code>.github/workflows/pypi-packaging.yml</code> (triggers on <code>v*</code> tags)</li> </ul> <p>Keep the version declaration in <code>setup.py</code> in this exact form (the trailing comma is fine):</p> <pre><code>setuptools.setup(\n    name=\"ts_shape\",\n    version = \"0.0.0.24\",\n    # ...\n)\n</code></pre> <p>On every push to <code>main</code>, the auto-bump workflow reads that line and increments it based on the latest commit message:</p> <ul> <li>Major: include <code>BREAKING CHANGE</code>, <code>#major</code>, or the short <code>!:</code> in the subject</li> <li>Minor: start the subject with <code>feat</code> or include <code>#minor</code></li> <li>Patch: default for all other commits</li> </ul> <p>Examples:</p> <pre><code>feat: add new SPC rule 9\n# =&gt; bumps 0.0.0.24 -&gt; 0.1.0 and tags v0.1.0\n\nfix: handle NaNs in StringFilter\n# =&gt; bumps 0.1.0 -&gt; 0.1.1 and tags v0.1.1\n\nrefactor!: remove deprecated API (BREAKING CHANGE)\n# =&gt; bumps 0.1.1 -&gt; 1.0.0 and tags v1.0.0\n</code></pre> <p>The workflow then: - Commits the updated <code>setup.py</code> with <code>[skip ci]</code> to avoid loops - Creates and pushes a tag <code>vX.Y.Z</code> - The packaging workflow sees the tag and publishes the built artifacts to PyPI</p> <p>One\u2011time repo setting required: enable \u201cRead and write permissions\u201d for GitHub Actions under Settings \u2192 Actions \u2192 General \u2192 Workflow permissions.</p>"},{"location":"insiders/development/#6-manage-requirements-piptools","title":"6. Manage Requirements (pip\u2011tools)","text":"<p>Keep only direct dependencies in <code>requirements.in</code> and compile pinned versions into <code>requirements.txt</code>.</p> <p>Setup (once per environment):</p> <pre><code>python -m pip install --upgrade pip-tools\n</code></pre> <p>Compile/update pins:</p> <pre><code># Compile requirements.in -&gt; requirements.txt\npython scripts/requirements.py compile\n\n# Upgrade all pins to latest compatible versions\npython scripts/requirements.py upgrade\n</code></pre> <p>Sync your virtualenv exactly to <code>requirements.txt</code> (adds/removes packages):</p> <pre><code>python scripts/requirements.py sync\n</code></pre> <p>Notes: - Edit direct deps in <code>requirements.in</code> (not <code>requirements.txt</code>). - <code>pip-sync</code> will uninstall anything not listed in <code>requirements.txt</code>.</p>"},{"location":"insiders/installation/","title":"Installation Guide","text":"<p>This guide helps you install ts-shape for common environments, and optionally enable cloud/data-source integrations.</p>"},{"location":"insiders/installation/#requirements","title":"Requirements","text":"<ul> <li>Python 3.10 or newer</li> <li>pip 22+ recommended</li> <li>A virtual environment (recommended): <code>python -m venv .venv &amp;&amp; source .venv/bin/activate</code> (PowerShell: <code>.venv\\Scripts\\Activate.ps1</code>)</li> </ul>"},{"location":"insiders/installation/#quick-start-pypi","title":"Quick Start (PyPI)","text":"<ul> <li>Install the package:</li> <li><code>pip install ts-shape</code></li> <li>Verify import and version:</li> <li><code>python -c \"import ts_shape, importlib.metadata as m; print('ts_shape imported, version', m.version('ts-shape'))\"</code></li> </ul>"},{"location":"insiders/installation/#optional-parquet-engines","title":"Optional Parquet Engines","text":"<p>ts-shape uses pandas to read/write Parquet. Install one engine:</p> <ul> <li><code>pip install pyarrow</code>  (recommended)</li> <li>or <code>pip install fastparquet</code></li> </ul> <p>Without one of these, <code>pd.read_parquet</code> will raise an error.</p>"},{"location":"insiders/installation/#optional-integrations","title":"Optional Integrations","text":"<ul> <li>S3 access (S3 proxy loader):</li> <li> <p>Already included: <code>s3fs</code></p> </li> <li> <p>Azure Blob Storage (Azure parquet loader):</p> </li> <li><code>pip install azure-storage-blob pyarrow</code></li> <li> <p>For AAD auth and management APIs:</p> <ul> <li><code>pip install azure-identity azure-mgmt-storage</code></li> </ul> </li> <li> <p>TimescaleDB (Timescale loader):</p> </li> <li><code>pip install sqlalchemy psycopg2-binary</code></li> </ul>"},{"location":"insiders/installation/#from-source-editable","title":"From Source (Editable)","text":"<p>If you\u2019re developing or want the latest:</p> <p>1) Clone the repo and enter it:    - <code>git clone https://github.com/jakobgabriel/ts-shape.git</code>    - <code>cd ts-shape</code></p> <p>2) Create and activate a venv (recommended):    - <code>python -m venv .venv</code>    - Linux/macOS: <code>source .venv/bin/activate</code>    - Windows (PowerShell): <code>.venv\\Scripts\\Activate.ps1</code></p> <p>3) Install base deps and package:    - <code>pip install -r requirements.txt</code>    - <code>pip install -e .</code></p> <p>4) Add optional integrations as needed (see above).</p>"},{"location":"insiders/installation/#sanity-check","title":"Sanity Check","text":"<ul> <li>Minimal check to ensure functionality:</li> </ul> <pre><code>python - &lt;&lt; 'PY'\nimport pandas as pd\nfrom ts_shape.loader.metadata.metadata_json_loader import MetadataJsonLoader\ndata = {\"uuid\": {\"0\": \"u1\"}, \"label\": {\"0\": \"sensor-1\"}, \"config\": {\"0\": {\"unit\": \"C\"}}}\ndf = MetadataJsonLoader(data).to_df()\nprint(df)\nPY\n</code></pre>"},{"location":"insiders/installation/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>Parquet engine missing: Install <code>pyarrow</code> (or <code>fastparquet</code>).</li> <li>ImportError for Azure classes: Install <code>azure-storage-blob</code> (and optionally <code>azure-identity</code>, <code>azure-mgmt-storage</code>).</li> <li>Timescale connection errors: Verify <code>sqlalchemy</code>/<code>psycopg2-binary</code> installed and DSN is correct.</li> <li>Version conflicts: <code>pip install --upgrade pip setuptools wheel</code> then reinstall.</li> </ul>"},{"location":"insiders/installation/#uninstall","title":"Uninstall","text":"<ul> <li><code>pip uninstall ts-shape</code></li> </ul>"},{"location":"reference/SUMMARY/","title":"Summary","text":"<ul> <li>ts_shape<ul> <li>context<ul> <li>value_mapping</li> </ul> </li> <li>events<ul> <li>engineering<ul> <li>setpoint_events</li> <li>startup_events</li> </ul> </li> <li>maintenance</li> <li>production<ul> <li>changeover</li> <li>downtime</li> <li>flow_constraints</li> <li>line_throughput</li> <li>machine_state</li> </ul> </li> <li>quality<ul> <li>outlier_detection</li> <li>statistical_process_control</li> <li>tolerance_deviation</li> </ul> </li> <li>supplychain</li> </ul> </li> <li>features<ul> <li>cycles<ul> <li>cycle_processor</li> <li>cycles_extractor</li> </ul> </li> <li>stats<ul> <li>boolean_stats</li> <li>feature_table</li> <li>numeric_stats</li> <li>string_stats</li> <li>timestamp_stats</li> </ul> </li> <li>time_stats<ul> <li>time_stats_numeric</li> </ul> </li> </ul> </li> <li>loader<ul> <li>combine<ul> <li>integrator</li> </ul> </li> <li>context</li> <li>metadata<ul> <li>metadata_api_loader</li> <li>metadata_db_loader</li> <li>metadata_json_loader</li> </ul> </li> <li>timeseries<ul> <li>azure_blob_loader</li> <li>energy_api_loader</li> <li>parquet_loader</li> <li>s3proxy_parquet_loader</li> <li>timescale_loader</li> </ul> </li> </ul> </li> <li>transform<ul> <li>calculator<ul> <li>numeric_calc</li> </ul> </li> <li>filter<ul> <li>boolean_filter</li> <li>custom_filter</li> <li>datetime_filter</li> <li>numeric_filter</li> <li>string_filter</li> </ul> </li> <li>functions<ul> <li>lambda_func</li> </ul> </li> <li>time_functions<ul> <li>timestamp_converter</li> <li>timezone_shift</li> </ul> </li> </ul> </li> <li>utils<ul> <li>base</li> </ul> </li> </ul> </li> </ul>"},{"location":"reference/ts_shape/__init__/","title":"init","text":""},{"location":"reference/ts_shape/__init__/#ts_shape","title":"ts_shape","text":"<p>Modules:</p> <ul> <li> <code>context</code>           \u2013            <p>Context</p> </li> <li> <code>events</code>           \u2013            <p>Events</p> </li> <li> <code>features</code>           \u2013            <p>Features</p> </li> <li> <code>loader</code>           \u2013            <p>Loaders</p> </li> <li> <code>transform</code>           \u2013            <p>Transform</p> </li> <li> <code>utils</code>           \u2013            <p>Utils</p> </li> </ul>"},{"location":"reference/ts_shape/context/__init__/","title":"init","text":""},{"location":"reference/ts_shape/context/__init__/#ts_shape.context","title":"ts_shape.context","text":"<p>Context</p> <p>Utilities for enriching DataFrames with contextual information and mappings.</p> <ul> <li>ValueMapper: Map categorical codes to readable values from external files.</li> <li>map_values: Merge and replace a target column using a CSV/JSON mapping table.</li> <li>_load_mapping_table: Load a mapping table from CSV or JSON.</li> </ul> <p>Modules:</p> <ul> <li> <code>value_mapping</code>           \u2013            </li> </ul>"},{"location":"reference/ts_shape/context/value_mapping/","title":"value_mapping","text":""},{"location":"reference/ts_shape/context/value_mapping/#ts_shape.context.value_mapping","title":"ts_shape.context.value_mapping","text":"<p>Classes:</p> <ul> <li> <code>ValueMapper</code>           \u2013            <p>A class to map values from specified columns of a DataFrame using a mapping table (CSV or JSON file),</p> </li> </ul>"},{"location":"reference/ts_shape/context/value_mapping/#ts_shape.context.value_mapping.ValueMapper","title":"ValueMapper","text":"<pre><code>ValueMapper(dataframe: DataFrame, mapping_file: str, map_column: str, mapping_key_column: str, mapping_value_column: str, file_type: str = 'csv', sep: str = ',', encoding: str = 'utf-8', column_name: str = 'systime')\n</code></pre> <p>               Bases: <code>Base</code></p> <p>A class to map values from specified columns of a DataFrame using a mapping table (CSV or JSON file), inheriting from the Base class.</p> <p>Parameters:</p> <p>Methods:</p> <ul> <li> <code>get_dataframe</code>             \u2013              <p>Returns the processed DataFrame.</p> </li> <li> <code>map_values</code>             \u2013              <p>Maps values in the specified DataFrame column based on the mapping table.</p> </li> </ul> Source code in <code>src/ts_shape/context/value_mapping.py</code> <pre><code>def __init__(\n    self, \n    dataframe: pd.DataFrame, \n    mapping_file: str, \n    map_column: str, \n    mapping_key_column: str, \n    mapping_value_column: str, \n    file_type: str = 'csv', \n    sep: str = ',', \n    encoding: str = 'utf-8', \n    column_name: str = 'systime'\n) -&gt; None:\n    \"\"\"\n    Initializes ValueMapper and the base DataFrame from the Base class.\n\n    Args:\n        dataframe (pd.DataFrame): The DataFrame to be processed and mapped.\n        mapping_file (str): The file path of the mapping table (CSV or JSON).\n        map_column (str): The name of the column in the DataFrame that needs to be mapped.\n        mapping_key_column (str): The column in the mapping table to match with values from the DataFrame.\n        mapping_value_column (str): The column in the mapping table containing the values to map to.\n        file_type (str): The type of the mapping file ('csv' or 'json'). Defaults to 'csv'.\n        sep (str): The separator for CSV files. Defaults to ','.\n        encoding (str): The encoding to use for reading the file. Defaults to 'utf-8'.\n        column_name (str): The name of the column to sort the DataFrame by in the base class. Defaults to 'systime'.\n    \"\"\"\n    # Initialize the Base class with the sorted DataFrame\n    super().__init__(dataframe, column_name)\n\n    # Additional attributes for ValueMapper\n    self.map_column: str = map_column\n    self.mapping_key_column: str = mapping_key_column\n    self.mapping_value_column: str = mapping_value_column\n    self.sep: str = sep\n    self.encoding: str = encoding\n\n    # Load the mapping table based on file type\n    self.mapping_table: pd.DataFrame = self._load_mapping_table(mapping_file, file_type)\n</code></pre>"},{"location":"reference/ts_shape/context/value_mapping/#ts_shape.context.value_mapping.ValueMapper(dataframe)","title":"<code>dataframe</code>","text":"(<code>DataFrame</code>)           \u2013            <p>The DataFrame to be processed and mapped.</p>"},{"location":"reference/ts_shape/context/value_mapping/#ts_shape.context.value_mapping.ValueMapper(mapping_file)","title":"<code>mapping_file</code>","text":"(<code>str</code>)           \u2013            <p>The file path of the mapping table (CSV or JSON).</p>"},{"location":"reference/ts_shape/context/value_mapping/#ts_shape.context.value_mapping.ValueMapper(map_column)","title":"<code>map_column</code>","text":"(<code>str</code>)           \u2013            <p>The name of the column in the DataFrame that needs to be mapped.</p>"},{"location":"reference/ts_shape/context/value_mapping/#ts_shape.context.value_mapping.ValueMapper(mapping_key_column)","title":"<code>mapping_key_column</code>","text":"(<code>str</code>)           \u2013            <p>The column in the mapping table to match with values from the DataFrame.</p>"},{"location":"reference/ts_shape/context/value_mapping/#ts_shape.context.value_mapping.ValueMapper(mapping_value_column)","title":"<code>mapping_value_column</code>","text":"(<code>str</code>)           \u2013            <p>The column in the mapping table containing the values to map to.</p>"},{"location":"reference/ts_shape/context/value_mapping/#ts_shape.context.value_mapping.ValueMapper(file_type)","title":"<code>file_type</code>","text":"(<code>str</code>, default:                   <code>'csv'</code> )           \u2013            <p>The type of the mapping file ('csv' or 'json'). Defaults to 'csv'.</p>"},{"location":"reference/ts_shape/context/value_mapping/#ts_shape.context.value_mapping.ValueMapper(sep)","title":"<code>sep</code>","text":"(<code>str</code>, default:                   <code>','</code> )           \u2013            <p>The separator for CSV files. Defaults to ','.</p>"},{"location":"reference/ts_shape/context/value_mapping/#ts_shape.context.value_mapping.ValueMapper(encoding)","title":"<code>encoding</code>","text":"(<code>str</code>, default:                   <code>'utf-8'</code> )           \u2013            <p>The encoding to use for reading the file. Defaults to 'utf-8'.</p>"},{"location":"reference/ts_shape/context/value_mapping/#ts_shape.context.value_mapping.ValueMapper(column_name)","title":"<code>column_name</code>","text":"(<code>str</code>, default:                   <code>'systime'</code> )           \u2013            <p>The name of the column to sort the DataFrame by in the base class. Defaults to 'systime'.</p>"},{"location":"reference/ts_shape/context/value_mapping/#ts_shape.context.value_mapping.ValueMapper.get_dataframe","title":"get_dataframe","text":"<pre><code>get_dataframe() -&gt; DataFrame\n</code></pre> <p>Returns the processed DataFrame.</p> Source code in <code>src/ts_shape/utils/base.py</code> <pre><code>def get_dataframe(self) -&gt; pd.DataFrame:\n    \"\"\"Returns the processed DataFrame.\"\"\"\n    return self.dataframe\n</code></pre>"},{"location":"reference/ts_shape/context/value_mapping/#ts_shape.context.value_mapping.ValueMapper.map_values","title":"map_values","text":"<pre><code>map_values() -&gt; DataFrame\n</code></pre> <p>Maps values in the specified DataFrame column based on the mapping table.</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>pd.DataFrame: A new DataFrame with the mapped values.</p> </li> </ul> Source code in <code>src/ts_shape/context/value_mapping.py</code> <pre><code>def map_values(self) -&gt; pd.DataFrame:\n    \"\"\"\n    Maps values in the specified DataFrame column based on the mapping table.\n\n    Returns:\n        pd.DataFrame: A new DataFrame with the mapped values.\n    \"\"\"\n    # Merge the mapping table with the DataFrame based on the map_column and mapping_key_column\n    mapped_df = self.dataframe.merge(\n        self.mapping_table[[self.mapping_key_column, self.mapping_value_column]],\n        left_on=self.map_column,\n        right_on=self.mapping_key_column,\n        how='left'\n    )\n\n    # Replace the original column with the mapped values\n    mapped_df[self.map_column] = mapped_df[self.mapping_value_column]\n\n    # Drop unnecessary columns\n    mapped_df = mapped_df.drop([self.mapping_key_column, self.mapping_value_column], axis=1)\n\n    return mapped_df\n</code></pre>"},{"location":"reference/ts_shape/events/__init__/","title":"init","text":""},{"location":"reference/ts_shape/events/__init__/#ts_shape.events","title":"ts_shape.events","text":"<p>Events</p> <p>Extract events from shaped timeseries across quality, maintenance, and production domains.</p> <ul> <li>OutlierDetectionEvents: Detect and group outlier events in a time series.</li> <li>detect_outliers_zscore: Detect outliers using Z-score thresholding and group nearby points.</li> <li> <p>detect_outliers_iqr: Detect outliers using IQR bounds and group nearby points.</p> </li> <li> <p>StatisticalProcessControlRuleBased: Apply Western Electric rules to flag   control-limit violations on actual values using tolerance context.</p> </li> <li>calculate_control_limits: Compute mean and \u00b11/\u00b12/\u00b13 standard-deviation bands from tolerance rows.</li> <li> <p>process: Apply selected rules and emit event rows for violations.</p> </li> <li> <p>ToleranceDeviationEvents: Flag intervals where actual values cross configured   tolerance and group them into start/end events.</p> </li> <li>process_and_group_data_with_events: Build grouped deviation events with event UUIDs.</li> </ul> <p>Modules:</p> <ul> <li> <code>engineering</code>           \u2013            <p>Engineering Events</p> </li> <li> <code>maintenance</code>           \u2013            <p>Maintenance Events</p> </li> <li> <code>production</code>           \u2013            <p>Production Events</p> </li> <li> <code>quality</code>           \u2013            <p>Quality Events</p> </li> <li> <code>supplychain</code>           \u2013            <p>Supply Chain Events</p> </li> </ul>"},{"location":"reference/ts_shape/events/engineering/__init__/","title":"init","text":""},{"location":"reference/ts_shape/events/engineering/__init__/#ts_shape.events.engineering","title":"ts_shape.events.engineering","text":"<p>Engineering Events</p> <p>Detectors for engineering-related patterns over shaped timeseries.</p> <ul> <li>SetpointChangeEvents: Detect setpoint changes and compute response KPIs.</li> <li>detect_setpoint_steps: Point events where |\u0394setpoint| \u2265 min_delta and holds for min_hold.</li> <li>detect_setpoint_ramps: Intervals where |dS/dt| \u2265 min_rate for at least min_duration.</li> <li>detect_setpoint_changes: Unified table of steps and ramps with standardized columns.</li> <li>time_to_settle: Time until |actual \u2212 setpoint| \u2264 tol for a hold duration within a window.</li> <li> <p>overshoot_metrics: Peak overshoot magnitude/percent and time-to-peak after a change.</p> </li> <li> <p>StartupDetectionEvents: Detect startup intervals from thresholds or slope.</p> </li> <li>detect_startup_by_threshold: Rising threshold crossing with minimum dwell above threshold.</li> <li>detect_startup_by_slope: Intervals with sustained positive slope \u2265 min_slope for min_duration.</li> </ul> <p>Modules:</p> <ul> <li> <code>setpoint_events</code>           \u2013            </li> <li> <code>startup_events</code>           \u2013            </li> </ul> <p>Classes:</p> <ul> <li> <code>SetpointChangeEvents</code>           \u2013            <p>Detect step/ramp changes on a setpoint signal and compute follow-up KPIs</p> </li> <li> <code>StartupDetectionEvents</code>           \u2013            <p>Detect equipment startup intervals based on threshold crossings or</p> </li> </ul>"},{"location":"reference/ts_shape/events/engineering/__init__/#ts_shape.events.engineering.SetpointChangeEvents","title":"SetpointChangeEvents","text":"<pre><code>SetpointChangeEvents(dataframe: DataFrame, setpoint_uuid: str, *, event_uuid: str = 'setpoint_change_event', value_column: str = 'value_double', time_column: str = 'systime')\n</code></pre> <p>               Bases: <code>Base</code></p> <p>Detect step/ramp changes on a setpoint signal and compute follow-up KPIs like time-to-settle and overshoot based on an actual (process) value.</p> <p>Schema assumptions (columns): - uuid, sequence_number, systime, plctime, is_delta - value_integer, value_string, value_double, value_bool, value_bytes</p> <p>Methods:</p> <ul> <li> <code>control_quality_metrics</code>             \u2013              <p>Comprehensive control quality metrics combining multiple performance indicators.</p> </li> <li> <code>decay_rate</code>             \u2013              <p>Estimate exponential decay rate of the settling behavior.</p> </li> <li> <code>detect_setpoint_changes</code>             \u2013              <p>Unified setpoint change table (steps + ramps) with standardized columns.</p> </li> <li> <code>detect_setpoint_ramps</code>             \u2013              <p>Interval events where |dS/dt| &gt;= min_rate for at least <code>min_duration</code>.</p> </li> <li> <code>detect_setpoint_steps</code>             \u2013              <p>Point events at times where the setpoint changes by &gt;= min_delta and the</p> </li> <li> <code>get_dataframe</code>             \u2013              <p>Returns the processed DataFrame.</p> </li> <li> <code>oscillation_frequency</code>             \u2013              <p>Estimate the frequency of oscillations during settling.</p> </li> <li> <code>overshoot_metrics</code>             \u2013              <p>For each change, compute peak overshoot, undershoot, and oscillation metrics</p> </li> <li> <code>rise_time</code>             \u2013              <p>Compute rise time: time for actual to go from start_pct to end_pct of the setpoint change.</p> </li> <li> <code>time_to_settle</code>             \u2013              <p>For each setpoint change (any change), compute time until the actual signal</p> </li> <li> <code>time_to_settle_derivative</code>             \u2013              <p>Detect settling based on rate of change (derivative) falling below threshold.</p> </li> </ul> Source code in <code>src/ts_shape/events/engineering/setpoint_events.py</code> <pre><code>def __init__(\n    self,\n    dataframe: pd.DataFrame,\n    setpoint_uuid: str,\n    *,\n    event_uuid: str = \"setpoint_change_event\",\n    value_column: str = \"value_double\",\n    time_column: str = \"systime\",\n) -&gt; None:\n    super().__init__(dataframe, column_name=time_column)\n    self.setpoint_uuid = setpoint_uuid\n    self.event_uuid = event_uuid\n    self.value_column = value_column\n    self.time_column = time_column\n\n    # isolate setpoint series and ensure proper dtypes/sort\n    self.sp = (\n        self.dataframe[self.dataframe[\"uuid\"] == self.setpoint_uuid]\n        .copy()\n        .sort_values(self.time_column)\n    )\n    self.sp[self.time_column] = pd.to_datetime(self.sp[self.time_column])\n\n    # Cache for performance optimization\n    self._actual_cache: Dict[str, pd.DataFrame] = {}\n</code></pre>"},{"location":"reference/ts_shape/events/engineering/__init__/#ts_shape.events.engineering.SetpointChangeEvents.control_quality_metrics","title":"control_quality_metrics","text":"<pre><code>control_quality_metrics(actual_uuid: str, *, tol: float = 0.0, settle_pct: Optional[float] = None, hold: str = '0s', lookahead: str = '10m', rate_threshold: float = 0.01) -&gt; DataFrame\n</code></pre> <p>Comprehensive control quality metrics combining multiple performance indicators.</p> <p>Computes all available metrics for each setpoint change and returns them in a single DataFrame. This includes: settling time, rise time, overshoot, undershoot, oscillations, and decay characteristics.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>DataFrame with comprehensive metrics including:</p> </li> <li> <code>DataFrame</code>           \u2013            <ul> <li>start, uuid, is_delta</li> </ul> </li> <li> <code>DataFrame</code>           \u2013            <ul> <li>t_settle_seconds, settled (from time_to_settle)</li> </ul> </li> <li> <code>DataFrame</code>           \u2013            <ul> <li>t_settle_derivative_seconds (from time_to_settle_derivative)</li> </ul> </li> <li> <code>DataFrame</code>           \u2013            <ul> <li>rise_time_seconds (from rise_time)</li> </ul> </li> <li> <code>DataFrame</code>           \u2013            <ul> <li>overshoot_abs, overshoot_pct (from overshoot_metrics)</li> </ul> </li> <li> <code>DataFrame</code>           \u2013            <ul> <li>undershoot_abs, undershoot_pct</li> </ul> </li> <li> <code>DataFrame</code>           \u2013            <ul> <li>oscillation_count, oscillation_amplitude, oscillation_freq_hz</li> </ul> </li> <li> <code>DataFrame</code>           \u2013            <ul> <li>decay_rate_lambda, fit_quality_r2</li> </ul> </li> <li> <code>DataFrame</code>           \u2013            <ul> <li>steady_state_error (final error in window)</li> </ul> </li> </ul> Source code in <code>src/ts_shape/events/engineering/setpoint_events.py</code> <pre><code>def control_quality_metrics(\n    self,\n    actual_uuid: str,\n    *,\n    tol: float = 0.0,\n    settle_pct: Optional[float] = None,\n    hold: str = \"0s\",\n    lookahead: str = \"10m\",\n    rate_threshold: float = 0.01,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Comprehensive control quality metrics combining multiple performance indicators.\n\n    Computes all available metrics for each setpoint change and returns them in a single DataFrame.\n    This includes: settling time, rise time, overshoot, undershoot, oscillations, and decay characteristics.\n\n    Args:\n        actual_uuid: UUID of the actual/process value signal\n        tol: Absolute tolerance for settling (used if settle_pct is None)\n        settle_pct: Percentage-based tolerance for settling\n        hold: Minimum duration to confirm settling\n        lookahead: Time window for all analyses\n        rate_threshold: Rate threshold for derivative-based settling\n\n    Returns:\n        DataFrame with comprehensive metrics including:\n        - start, uuid, is_delta\n        - t_settle_seconds, settled (from time_to_settle)\n        - t_settle_derivative_seconds (from time_to_settle_derivative)\n        - rise_time_seconds (from rise_time)\n        - overshoot_abs, overshoot_pct (from overshoot_metrics)\n        - undershoot_abs, undershoot_pct\n        - oscillation_count, oscillation_amplitude, oscillation_freq_hz\n        - decay_rate_lambda, fit_quality_r2\n        - steady_state_error (final error in window)\n    \"\"\"\n    if self.sp.empty:\n        return pd.DataFrame(\n            columns=[\n                \"start\",\n                \"uuid\",\n                \"is_delta\",\n                \"t_settle_seconds\",\n                \"settled\",\n                \"t_settle_derivative_seconds\",\n                \"rise_time_seconds\",\n                \"overshoot_abs\",\n                \"overshoot_pct\",\n                \"undershoot_abs\",\n                \"undershoot_pct\",\n                \"oscillation_count\",\n                \"oscillation_amplitude\",\n                \"oscillation_freq_hz\",\n                \"decay_rate_lambda\",\n                \"fit_quality_r2\",\n                \"steady_state_error\",\n            ]\n        )\n\n    # Compute all individual metrics using cached actual data\n    settle_df = self.time_to_settle(\n        actual_uuid, tol=tol, settle_pct=settle_pct, hold=hold, lookahead=lookahead\n    )\n    settle_deriv_df = self.time_to_settle_derivative(\n        actual_uuid, rate_threshold=rate_threshold, lookahead=lookahead, hold=hold\n    )\n    rise_df = self.rise_time(actual_uuid, lookahead=lookahead)\n    overshoot_df = self.overshoot_metrics(actual_uuid, window=lookahead)\n    decay_df = self.decay_rate(actual_uuid, lookahead=lookahead)\n    freq_df = self.oscillation_frequency(actual_uuid, window=lookahead)\n\n    # Compute steady-state error\n    actual = self._get_actual(actual_uuid)\n    look_td = pd.to_timedelta(lookahead)\n\n    sp = self.sp[[self.time_column, self.value_column]].copy()\n    sp[\"prev\"] = sp[self.value_column].shift(1)\n    sp[\"delta\"] = sp[self.value_column] - sp[\"prev\"]\n    changes = sp.loc[sp[\"delta\"].abs() &gt; 0, [self.time_column, self.value_column]].reset_index(drop=True)\n\n    ss_error_rows: List[Dict[str, Any]] = []\n    for _, c in changes.iterrows():\n        t0 = c[self.time_column]\n        s_new = float(c[self.value_column])\n        win = actual[(actual[self.time_column] &gt;= t0) &amp; (actual[self.time_column] &lt;= t0 + look_td)]\n\n        if win.empty:\n            ss_error = None\n        else:\n            # Use last 10% of window for steady-state\n            n_points = len(win)\n            last_10pct = win.iloc[int(n_points * 0.9):]\n            if len(last_10pct) &gt; 0:\n                ss_error = float((last_10pct[self.value_column] - s_new).abs().mean())\n            else:\n                ss_error = None\n\n        ss_error_rows.append({\"start\": t0, \"steady_state_error\": ss_error})\n\n    ss_error_df = pd.DataFrame(ss_error_rows)\n\n    # Merge all metrics on 'start'\n    result = settle_df.copy()\n\n    # Merge settle_deriv\n    result = result.merge(\n        settle_deriv_df[[\"start\", \"t_settle_seconds\"]].rename(\n            columns={\"t_settle_seconds\": \"t_settle_derivative_seconds\"}\n        ),\n        on=\"start\",\n        how=\"left\",\n    )\n\n    # Merge rise time\n    result = result.merge(\n        rise_df[[\"start\", \"rise_time_seconds\"]],\n        on=\"start\",\n        how=\"left\",\n    )\n\n    # Merge overshoot metrics\n    result = result.merge(\n        overshoot_df[\n            [\n                \"start\",\n                \"overshoot_abs\",\n                \"overshoot_pct\",\n                \"undershoot_abs\",\n                \"undershoot_pct\",\n                \"oscillation_count\",\n                \"oscillation_amplitude\",\n            ]\n        ],\n        on=\"start\",\n        how=\"left\",\n    )\n\n    # Merge frequency\n    result = result.merge(\n        freq_df[[\"start\", \"oscillation_freq_hz\"]],\n        on=\"start\",\n        how=\"left\",\n    )\n\n    # Merge decay\n    result = result.merge(\n        decay_df[[\"start\", \"decay_rate_lambda\", \"fit_quality_r2\"]],\n        on=\"start\",\n        how=\"left\",\n    )\n\n    # Merge steady-state error\n    result = result.merge(\n        ss_error_df[[\"start\", \"steady_state_error\"]],\n        on=\"start\",\n        how=\"left\",\n    )\n\n    return result\n</code></pre>"},{"location":"reference/ts_shape/events/engineering/__init__/#ts_shape.events.engineering.SetpointChangeEvents.control_quality_metrics(actual_uuid)","title":"<code>actual_uuid</code>","text":"(<code>str</code>)           \u2013            <p>UUID of the actual/process value signal</p>"},{"location":"reference/ts_shape/events/engineering/__init__/#ts_shape.events.engineering.SetpointChangeEvents.control_quality_metrics(tol)","title":"<code>tol</code>","text":"(<code>float</code>, default:                   <code>0.0</code> )           \u2013            <p>Absolute tolerance for settling (used if settle_pct is None)</p>"},{"location":"reference/ts_shape/events/engineering/__init__/#ts_shape.events.engineering.SetpointChangeEvents.control_quality_metrics(settle_pct)","title":"<code>settle_pct</code>","text":"(<code>Optional[float]</code>, default:                   <code>None</code> )           \u2013            <p>Percentage-based tolerance for settling</p>"},{"location":"reference/ts_shape/events/engineering/__init__/#ts_shape.events.engineering.SetpointChangeEvents.control_quality_metrics(hold)","title":"<code>hold</code>","text":"(<code>str</code>, default:                   <code>'0s'</code> )           \u2013            <p>Minimum duration to confirm settling</p>"},{"location":"reference/ts_shape/events/engineering/__init__/#ts_shape.events.engineering.SetpointChangeEvents.control_quality_metrics(lookahead)","title":"<code>lookahead</code>","text":"(<code>str</code>, default:                   <code>'10m'</code> )           \u2013            <p>Time window for all analyses</p>"},{"location":"reference/ts_shape/events/engineering/__init__/#ts_shape.events.engineering.SetpointChangeEvents.control_quality_metrics(rate_threshold)","title":"<code>rate_threshold</code>","text":"(<code>float</code>, default:                   <code>0.01</code> )           \u2013            <p>Rate threshold for derivative-based settling</p>"},{"location":"reference/ts_shape/events/engineering/__init__/#ts_shape.events.engineering.SetpointChangeEvents.decay_rate","title":"decay_rate","text":"<pre><code>decay_rate(actual_uuid: str, *, lookahead: str = '10m', min_points: int = 5) -&gt; DataFrame\n</code></pre> <p>Estimate exponential decay rate of the settling behavior. Fits error(t) = A * exp(-lambda * t) and returns lambda.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>DataFrame with columns: start, uuid, is_delta, decay_rate_lambda, fit_quality_r2.</p> </li> </ul> Source code in <code>src/ts_shape/events/engineering/setpoint_events.py</code> <pre><code>def decay_rate(\n    self,\n    actual_uuid: str,\n    *,\n    lookahead: str = \"10m\",\n    min_points: int = 5,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Estimate exponential decay rate of the settling behavior.\n    Fits error(t) = A * exp(-lambda * t) and returns lambda.\n\n    Args:\n        actual_uuid: UUID of the actual/process value signal\n        lookahead: Time window for analysis\n        min_points: Minimum number of points required for fitting\n\n    Returns:\n        DataFrame with columns: start, uuid, is_delta, decay_rate_lambda, fit_quality_r2.\n    \"\"\"\n    if self.sp.empty:\n        return pd.DataFrame(\n            columns=[\"start\", \"uuid\", \"is_delta\", \"decay_rate_lambda\", \"fit_quality_r2\"]\n        )\n\n    # Use cached actual data\n    actual = self._get_actual(actual_uuid)\n    look_td = pd.to_timedelta(lookahead)\n\n    sp = self.sp[[self.time_column, self.value_column]].copy()\n    sp[\"prev\"] = sp[self.value_column].shift(1)\n    sp[\"delta\"] = sp[self.value_column] - sp[\"prev\"]\n    changes = sp.loc[sp[\"delta\"].abs() &gt; 0, [self.time_column, self.value_column]].reset_index(drop=True)\n\n    rows: List[Dict[str, Any]] = []\n    for _, c in changes.iterrows():\n        t0 = c[self.time_column]\n        s_new = float(c[self.value_column])\n        window = actual[(actual[self.time_column] &gt;= t0) &amp; (actual[self.time_column] &lt;= t0 + look_td)]\n\n        if window.empty or len(window) &lt; min_points:\n            rows.append(\n                {\n                    \"start\": t0,\n                    \"uuid\": self.event_uuid,\n                    \"is_delta\": True,\n                    \"decay_rate_lambda\": None,\n                    \"fit_quality_r2\": None,\n                }\n            )\n            continue\n\n        # Calculate error and time since change\n        err = (window[self.value_column] - s_new).abs()\n        t_sec = (window[self.time_column] - t0).dt.total_seconds()\n\n        # Filter out zero or near-zero errors for log fit\n        valid_mask = err &gt; 1e-6\n        if valid_mask.sum() &lt; min_points:\n            rows.append(\n                {\n                    \"start\": t0,\n                    \"uuid\": self.event_uuid,\n                    \"is_delta\": True,\n                    \"decay_rate_lambda\": None,\n                    \"fit_quality_r2\": None,\n                }\n            )\n            continue\n\n        err_valid = err[valid_mask]\n        t_valid = t_sec[valid_mask]\n\n        try:\n            # Linear fit to log(error) vs time: log(err) = log(A) - lambda*t\n            log_err = np.log(err_valid)\n            coeffs = np.polyfit(t_valid, log_err, 1)\n            decay_lambda = -coeffs[0]  # negative slope\n\n            # Calculate R^2\n            log_err_pred = np.polyval(coeffs, t_valid)\n            ss_res = np.sum((log_err - log_err_pred) ** 2)\n            ss_tot = np.sum((log_err - log_err.mean()) ** 2)\n            r2 = 1 - (ss_res / ss_tot) if ss_tot &gt; 0 else 0.0\n\n            rows.append(\n                {\n                    \"start\": t0,\n                    \"uuid\": self.event_uuid,\n                    \"is_delta\": True,\n                    \"decay_rate_lambda\": float(decay_lambda) if decay_lambda &gt; 0 else None,\n                    \"fit_quality_r2\": float(r2),\n                }\n            )\n        except Exception:\n            rows.append(\n                {\n                    \"start\": t0,\n                    \"uuid\": self.event_uuid,\n                    \"is_delta\": True,\n                    \"decay_rate_lambda\": None,\n                    \"fit_quality_r2\": None,\n                }\n            )\n\n    return pd.DataFrame(rows)\n</code></pre>"},{"location":"reference/ts_shape/events/engineering/__init__/#ts_shape.events.engineering.SetpointChangeEvents.decay_rate(actual_uuid)","title":"<code>actual_uuid</code>","text":"(<code>str</code>)           \u2013            <p>UUID of the actual/process value signal</p>"},{"location":"reference/ts_shape/events/engineering/__init__/#ts_shape.events.engineering.SetpointChangeEvents.decay_rate(lookahead)","title":"<code>lookahead</code>","text":"(<code>str</code>, default:                   <code>'10m'</code> )           \u2013            <p>Time window for analysis</p>"},{"location":"reference/ts_shape/events/engineering/__init__/#ts_shape.events.engineering.SetpointChangeEvents.decay_rate(min_points)","title":"<code>min_points</code>","text":"(<code>int</code>, default:                   <code>5</code> )           \u2013            <p>Minimum number of points required for fitting</p>"},{"location":"reference/ts_shape/events/engineering/__init__/#ts_shape.events.engineering.SetpointChangeEvents.detect_setpoint_changes","title":"detect_setpoint_changes","text":"<pre><code>detect_setpoint_changes(*, min_delta: float = 0.0, min_rate: Optional[float] = None, min_hold: str = '0s', min_duration: str = '0s') -&gt; DataFrame\n</code></pre> <p>Unified setpoint change table (steps + ramps) with standardized columns.</p> Source code in <code>src/ts_shape/events/engineering/setpoint_events.py</code> <pre><code>def detect_setpoint_changes(\n    self,\n    *,\n    min_delta: float = 0.0,\n    min_rate: Optional[float] = None,\n    min_hold: str = \"0s\",\n    min_duration: str = \"0s\",\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Unified setpoint change table (steps + ramps) with standardized columns.\n    \"\"\"\n    steps = self.detect_setpoint_steps(min_delta=min_delta, min_hold=min_hold)\n    ramps = (\n        self.detect_setpoint_ramps(min_rate=min_rate, min_duration=min_duration)\n        if min_rate is not None\n        else pd.DataFrame(columns=[\"start\", \"end\", \"uuid\", \"is_delta\", \"change_type\", \"avg_rate\", \"delta\"])\n    )\n    # ensure uniform columns\n    if not steps.empty:\n        steps = steps.assign(avg_rate=None, delta=None)[\n            [\n                \"start\",\n                \"end\",\n                \"uuid\",\n                \"is_delta\",\n                \"change_type\",\n                \"magnitude\",\n                \"prev_level\",\n                \"new_level\",\n                \"avg_rate\",\n                \"delta\",\n            ]\n        ]\n    if not ramps.empty:\n        ramps = ramps.assign(magnitude=None, prev_level=None, new_level=None)[\n            [\n                \"start\",\n                \"end\",\n                \"uuid\",\n                \"is_delta\",\n                \"change_type\",\n                \"magnitude\",\n                \"prev_level\",\n                \"new_level\",\n                \"avg_rate\",\n                \"delta\",\n            ]\n        ]\n    frames = [df for df in (steps, ramps) if not df.empty]\n    combined = pd.concat(frames, ignore_index=True) if frames else pd.DataFrame(\n        columns=[\n            \"start\",\n            \"end\",\n            \"uuid\",\n            \"is_delta\",\n            \"change_type\",\n            \"magnitude\",\n            \"prev_level\",\n            \"new_level\",\n            \"avg_rate\",\n            \"delta\",\n        ]\n    )\n    return combined.sort_values([\"start\", \"end\"]) if not combined.empty else combined\n</code></pre>"},{"location":"reference/ts_shape/events/engineering/__init__/#ts_shape.events.engineering.SetpointChangeEvents.detect_setpoint_ramps","title":"detect_setpoint_ramps","text":"<pre><code>detect_setpoint_ramps(min_rate: float, min_duration: str = '0s') -&gt; DataFrame\n</code></pre> <p>Interval events where |dS/dt| &gt;= min_rate for at least <code>min_duration</code>.</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>DataFrame with columns: start, end, uuid, is_delta, change_type='ramp',</p> </li> <li> <code>DataFrame</code>           \u2013            <p>avg_rate, delta.</p> </li> </ul> Source code in <code>src/ts_shape/events/engineering/setpoint_events.py</code> <pre><code>def detect_setpoint_ramps(self, min_rate: float, min_duration: str = \"0s\") -&gt; pd.DataFrame:\n    \"\"\"\n    Interval events where |dS/dt| &gt;= min_rate for at least `min_duration`.\n\n    Returns:\n        DataFrame with columns: start, end, uuid, is_delta, change_type='ramp',\n        avg_rate, delta.\n    \"\"\"\n    if self.sp.empty:\n        return pd.DataFrame(\n            columns=[\"start\", \"end\", \"uuid\", \"is_delta\", \"change_type\", \"avg_rate\", \"delta\"]\n        )\n\n    sp = self.sp[[self.time_column, self.value_column]].copy()\n    sp[\"dt_s\"] = sp[self.time_column].diff().dt.total_seconds()\n    sp[\"dv\"] = sp[self.value_column].diff()\n    sp[\"rate\"] = sp[\"dv\"] / sp[\"dt_s\"]\n    rate_mask = sp[\"rate\"].abs() &gt;= float(min_rate)\n\n    # group contiguous True segments\n    group_id = (rate_mask != rate_mask.shift()).cumsum()\n    events: List[Dict[str, Any]] = []\n    min_d = pd.to_timedelta(min_duration)\n    for gid, seg in sp.groupby(group_id):\n        seg_mask_true = rate_mask.loc[seg.index]\n        if not seg_mask_true.any():\n            continue\n        # boundaries\n        start_time = seg.loc[seg_mask_true, self.time_column].iloc[0]\n        end_time = seg.loc[seg_mask_true, self.time_column].iloc[-1]\n        if (end_time - start_time) &lt; min_d:\n            continue\n        avg_rate = seg.loc[seg_mask_true, \"rate\"].mean()\n        delta = seg.loc[seg_mask_true, \"dv\"].sum()\n        events.append(\n            {\n                \"start\": start_time,\n                \"end\": end_time,\n                \"uuid\": self.event_uuid,\n                \"is_delta\": True,\n                \"change_type\": \"ramp\",\n                \"avg_rate\": float(avg_rate) if pd.notna(avg_rate) else None,\n                \"delta\": float(delta) if pd.notna(delta) else None,\n            }\n        )\n\n    return pd.DataFrame(events)\n</code></pre>"},{"location":"reference/ts_shape/events/engineering/__init__/#ts_shape.events.engineering.SetpointChangeEvents.detect_setpoint_steps","title":"detect_setpoint_steps","text":"<pre><code>detect_setpoint_steps(min_delta: float, min_hold: str = '0s', filter_noise: bool = False, noise_threshold: float = 0.01) -&gt; DataFrame\n</code></pre> <p>Point events at times where the setpoint changes by &gt;= min_delta and the new level holds for at least <code>min_hold</code> (no subsequent change within that time).</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>DataFrame with columns: start, end (== start), uuid, is_delta,</p> </li> <li> <code>DataFrame</code>           \u2013            <p>change_type='step', magnitude, prev_level, new_level.</p> </li> </ul> Source code in <code>src/ts_shape/events/engineering/setpoint_events.py</code> <pre><code>def detect_setpoint_steps(\n    self,\n    min_delta: float,\n    min_hold: str = \"0s\",\n    filter_noise: bool = False,\n    noise_threshold: float = 0.01,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Point events at times where the setpoint changes by &gt;= min_delta and the\n    new level holds for at least `min_hold` (no subsequent change within that time).\n\n    Args:\n        min_delta: Minimum magnitude of change to detect\n        min_hold: Minimum duration the new level must hold\n        filter_noise: If True, filter out changes smaller than noise_threshold\n        noise_threshold: Threshold for noise filtering (absolute value)\n\n    Returns:\n        DataFrame with columns: start, end (== start), uuid, is_delta,\n        change_type='step', magnitude, prev_level, new_level.\n    \"\"\"\n    if self.sp.empty:\n        return pd.DataFrame(\n            columns=[\n                \"start\",\n                \"end\",\n                \"uuid\",\n                \"is_delta\",\n                \"change_type\",\n                \"magnitude\",\n                \"prev_level\",\n                \"new_level\",\n            ]\n        )\n\n    sp = self.sp[[self.time_column, self.value_column]].copy()\n\n    # Apply noise filtering if requested\n    if filter_noise:\n        # Group consecutive values within noise_threshold and use mean\n        sp[\"filtered_value\"] = sp[self.value_column]\n        current_group = sp[\"filtered_value\"].iloc[0]\n        for i in range(1, len(sp)):\n            if abs(sp[\"filtered_value\"].iloc[i] - current_group) &lt;= noise_threshold:\n                sp.loc[sp.index[i], \"filtered_value\"] = current_group\n            else:\n                current_group = sp[\"filtered_value\"].iloc[i]\n        sp[self.value_column] = sp[\"filtered_value\"]\n        sp = sp.drop(columns=[\"filtered_value\"])\n\n    sp[\"prev\"] = sp[self.value_column].shift(1)\n    sp[\"delta\"] = sp[self.value_column] - sp[\"prev\"]\n    change_mask = sp[\"delta\"].abs() &gt;= float(min_delta)\n\n    # hold condition: next change must be after min_hold\n    change_times = sp.loc[change_mask, self.time_column]\n    min_hold_td = pd.to_timedelta(min_hold)\n    next_change_times = change_times.shift(-1)\n    hold_ok = (next_change_times - change_times &gt;= min_hold_td) | next_change_times.isna()\n    valid_change_times = change_times[hold_ok]\n\n    rows: List[Dict[str, Any]] = []\n    for t in valid_change_times:\n        row = sp.loc[sp[self.time_column] == t].iloc[0]\n        rows.append(\n            {\n                \"start\": t,\n                \"end\": t,\n                \"uuid\": self.event_uuid,\n                \"is_delta\": True,\n                \"change_type\": \"step\",\n                \"magnitude\": float(row[\"delta\"]),\n                \"prev_level\": float(row[\"prev\"]) if pd.notna(row[\"prev\"]) else None,\n                \"new_level\": float(row[self.value_column]),\n            }\n        )\n\n    return pd.DataFrame(rows)\n</code></pre>"},{"location":"reference/ts_shape/events/engineering/__init__/#ts_shape.events.engineering.SetpointChangeEvents.detect_setpoint_steps(min_delta)","title":"<code>min_delta</code>","text":"(<code>float</code>)           \u2013            <p>Minimum magnitude of change to detect</p>"},{"location":"reference/ts_shape/events/engineering/__init__/#ts_shape.events.engineering.SetpointChangeEvents.detect_setpoint_steps(min_hold)","title":"<code>min_hold</code>","text":"(<code>str</code>, default:                   <code>'0s'</code> )           \u2013            <p>Minimum duration the new level must hold</p>"},{"location":"reference/ts_shape/events/engineering/__init__/#ts_shape.events.engineering.SetpointChangeEvents.detect_setpoint_steps(filter_noise)","title":"<code>filter_noise</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, filter out changes smaller than noise_threshold</p>"},{"location":"reference/ts_shape/events/engineering/__init__/#ts_shape.events.engineering.SetpointChangeEvents.detect_setpoint_steps(noise_threshold)","title":"<code>noise_threshold</code>","text":"(<code>float</code>, default:                   <code>0.01</code> )           \u2013            <p>Threshold for noise filtering (absolute value)</p>"},{"location":"reference/ts_shape/events/engineering/__init__/#ts_shape.events.engineering.SetpointChangeEvents.get_dataframe","title":"get_dataframe","text":"<pre><code>get_dataframe() -&gt; DataFrame\n</code></pre> <p>Returns the processed DataFrame.</p> Source code in <code>src/ts_shape/utils/base.py</code> <pre><code>def get_dataframe(self) -&gt; pd.DataFrame:\n    \"\"\"Returns the processed DataFrame.\"\"\"\n    return self.dataframe\n</code></pre>"},{"location":"reference/ts_shape/events/engineering/__init__/#ts_shape.events.engineering.SetpointChangeEvents.oscillation_frequency","title":"oscillation_frequency","text":"<pre><code>oscillation_frequency(actual_uuid: str, *, window: str = '10m', min_oscillations: int = 2) -&gt; DataFrame\n</code></pre> <p>Estimate the frequency of oscillations during settling. Counts zero crossings and estimates period.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>DataFrame with columns: start, uuid, is_delta, oscillation_freq_hz, period_seconds.</p> </li> </ul> Source code in <code>src/ts_shape/events/engineering/setpoint_events.py</code> <pre><code>def oscillation_frequency(\n    self,\n    actual_uuid: str,\n    *,\n    window: str = \"10m\",\n    min_oscillations: int = 2,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Estimate the frequency of oscillations during settling.\n    Counts zero crossings and estimates period.\n\n    Args:\n        actual_uuid: UUID of the actual/process value signal\n        window: Time window for analysis\n        min_oscillations: Minimum number of oscillations to compute frequency\n\n    Returns:\n        DataFrame with columns: start, uuid, is_delta, oscillation_freq_hz, period_seconds.\n    \"\"\"\n    if self.sp.empty:\n        return pd.DataFrame(\n            columns=[\"start\", \"uuid\", \"is_delta\", \"oscillation_freq_hz\", \"period_seconds\"]\n        )\n\n    # Use cached actual data\n    actual = self._get_actual(actual_uuid)\n    look_td = pd.to_timedelta(window)\n\n    sp = self.sp[[self.time_column, self.value_column]].copy()\n    sp[\"prev\"] = sp[self.value_column].shift(1)\n    sp[\"delta\"] = sp[self.value_column] - sp[\"prev\"]\n    changes = sp.loc[sp[\"delta\"].abs() &gt; 0, [self.time_column, self.value_column]].reset_index(drop=True)\n\n    rows: List[Dict[str, Any]] = []\n    for _, c in changes.iterrows():\n        t0 = c[self.time_column]\n        s_new = float(c[self.value_column])\n        win = actual[(actual[self.time_column] &gt;= t0) &amp; (actual[self.time_column] &lt;= t0 + look_td)]\n\n        if win.empty:\n            rows.append(\n                {\n                    \"start\": t0,\n                    \"uuid\": self.event_uuid,\n                    \"is_delta\": True,\n                    \"oscillation_freq_hz\": None,\n                    \"period_seconds\": None,\n                }\n            )\n            continue\n\n        # Calculate error\n        err = win[self.value_column] - s_new\n\n        # Detect zero crossings\n        err_sign = np.sign(err)\n        sign_changes = np.where(np.diff(err_sign) != 0)[0]\n        num_crossings = len(sign_changes)\n\n        if num_crossings &gt;= min_oscillations * 2:  # each oscillation has 2 crossings\n            # Calculate time between crossings\n            crossing_times = win[self.time_column].iloc[sign_changes]\n            time_diffs = crossing_times.diff().dt.total_seconds().dropna()\n\n            if len(time_diffs) &gt; 0:\n                # Average period (2 crossings = 1 period)\n                avg_half_period = time_diffs.mean()\n                period = avg_half_period * 2\n                freq = 1.0 / period if period &gt; 0 else None\n\n                rows.append(\n                    {\n                        \"start\": t0,\n                        \"uuid\": self.event_uuid,\n                        \"is_delta\": True,\n                        \"oscillation_freq_hz\": float(freq) if freq is not None else None,\n                        \"period_seconds\": float(period),\n                    }\n                )\n            else:\n                rows.append(\n                    {\n                        \"start\": t0,\n                        \"uuid\": self.event_uuid,\n                        \"is_delta\": True,\n                        \"oscillation_freq_hz\": None,\n                        \"period_seconds\": None,\n                    }\n                )\n        else:\n            rows.append(\n                {\n                    \"start\": t0,\n                    \"uuid\": self.event_uuid,\n                    \"is_delta\": True,\n                    \"oscillation_freq_hz\": None,\n                    \"period_seconds\": None,\n                }\n            )\n\n    return pd.DataFrame(rows)\n</code></pre>"},{"location":"reference/ts_shape/events/engineering/__init__/#ts_shape.events.engineering.SetpointChangeEvents.oscillation_frequency(actual_uuid)","title":"<code>actual_uuid</code>","text":"(<code>str</code>)           \u2013            <p>UUID of the actual/process value signal</p>"},{"location":"reference/ts_shape/events/engineering/__init__/#ts_shape.events.engineering.SetpointChangeEvents.oscillation_frequency(window)","title":"<code>window</code>","text":"(<code>str</code>, default:                   <code>'10m'</code> )           \u2013            <p>Time window for analysis</p>"},{"location":"reference/ts_shape/events/engineering/__init__/#ts_shape.events.engineering.SetpointChangeEvents.oscillation_frequency(min_oscillations)","title":"<code>min_oscillations</code>","text":"(<code>int</code>, default:                   <code>2</code> )           \u2013            <p>Minimum number of oscillations to compute frequency</p>"},{"location":"reference/ts_shape/events/engineering/__init__/#ts_shape.events.engineering.SetpointChangeEvents.overshoot_metrics","title":"overshoot_metrics","text":"<pre><code>overshoot_metrics(actual_uuid: str, *, window: str = '10m') -&gt; DataFrame\n</code></pre> <p>For each change, compute peak overshoot, undershoot, and oscillation metrics relative to the new setpoint within a lookahead window.</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>DataFrame with columns: start, uuid, is_delta, overshoot_abs, overshoot_pct,</p> </li> <li> <code>DataFrame</code>           \u2013            <p>t_peak_seconds, undershoot_abs, undershoot_pct, t_undershoot_seconds,</p> </li> <li> <code>DataFrame</code>           \u2013            <p>oscillation_count, oscillation_amplitude.</p> </li> </ul> Source code in <code>src/ts_shape/events/engineering/setpoint_events.py</code> <pre><code>def overshoot_metrics(\n    self,\n    actual_uuid: str,\n    *,\n    window: str = \"10m\",\n) -&gt; pd.DataFrame:\n    \"\"\"\n    For each change, compute peak overshoot, undershoot, and oscillation metrics\n    relative to the new setpoint within a lookahead window.\n\n    Returns:\n        DataFrame with columns: start, uuid, is_delta, overshoot_abs, overshoot_pct,\n        t_peak_seconds, undershoot_abs, undershoot_pct, t_undershoot_seconds,\n        oscillation_count, oscillation_amplitude.\n    \"\"\"\n    if self.sp.empty:\n        return pd.DataFrame(\n            columns=[\n                \"start\",\n                \"uuid\",\n                \"is_delta\",\n                \"overshoot_abs\",\n                \"overshoot_pct\",\n                \"t_peak_seconds\",\n                \"undershoot_abs\",\n                \"undershoot_pct\",\n                \"t_undershoot_seconds\",\n                \"oscillation_count\",\n                \"oscillation_amplitude\",\n            ]\n        )\n\n    # Use cached actual data\n    actual = self._get_actual(actual_uuid)\n    look_td = pd.to_timedelta(window)\n\n    sp = self.sp[[self.time_column, self.value_column]].copy()\n    sp[\"prev\"] = sp[self.value_column].shift(1)\n    sp[\"delta\"] = sp[self.value_column] - sp[\"prev\"]\n    changes = sp.loc[sp[\"delta\"].abs() &gt; 0, [self.time_column, self.value_column, \"delta\", \"prev\"]]\n\n    out_rows: List[Dict[str, Any]] = []\n    for _, r in changes.iterrows():\n        t0 = r[self.time_column]\n        s_new = float(r[self.value_column])\n        s_prev = float(r[\"prev\"]) if pd.notna(r[\"prev\"]) else s_new\n        delta = float(r[\"delta\"]) if pd.notna(r[\"delta\"]) else 0.0\n        win = actual[(actual[self.time_column] &gt;= t0) &amp; (actual[self.time_column] &lt;= t0 + look_td)]\n\n        if win.empty:\n            out_rows.append(\n                {\n                    \"start\": t0,\n                    \"uuid\": self.event_uuid,\n                    \"is_delta\": True,\n                    \"overshoot_abs\": None,\n                    \"overshoot_pct\": None,\n                    \"t_peak_seconds\": None,\n                    \"undershoot_abs\": None,\n                    \"undershoot_pct\": None,\n                    \"t_undershoot_seconds\": None,\n                    \"oscillation_count\": None,\n                    \"oscillation_amplitude\": None,\n                }\n            )\n            continue\n\n        err = win[self.value_column] - s_new\n\n        # Overshoot (in direction of change)\n        if delta &gt;= 0:\n            peak = err.max()\n            t_peak = win.loc[err.idxmax(), self.time_column] if peak &gt; 0 else None\n            # Undershoot (opposite direction)\n            undershoot_val = err.min()\n            t_undershoot = win.loc[err.idxmin(), self.time_column] if undershoot_val &lt; 0 else None\n        else:\n            peak = -err.min()  # magnitude for downward step\n            t_peak = win.loc[err.idxmin(), self.time_column] if err.min() &lt; 0 else None\n            # Undershoot (opposite direction)\n            undershoot_val = -err.max()\n            t_undershoot = win.loc[err.idxmax(), self.time_column] if err.max() &gt; 0 else None\n\n        overshoot_abs = float(peak) if pd.notna(peak) and peak &gt; 0 else 0.0\n        overshoot_pct = (overshoot_abs / abs(delta)) if (delta != 0 and overshoot_abs is not None) else None\n\n        undershoot_abs = float(abs(undershoot_val)) if pd.notna(undershoot_val) and abs(undershoot_val) &gt; 0 else 0.0\n        undershoot_pct = (undershoot_abs / abs(delta)) if (delta != 0 and undershoot_abs is not None) else None\n\n        # Oscillation detection: count zero crossings\n        err_sign = np.sign(err)\n        sign_changes = (err_sign.diff() != 0).sum() - 1  # -1 to exclude initial transition\n        oscillation_count = max(0, int(sign_changes))\n\n        # Oscillation amplitude: average of peak deviations\n        oscillation_amplitude = float(err.abs().mean()) if len(err) &gt; 0 else None\n\n        out_rows.append(\n            {\n                \"start\": t0,\n                \"uuid\": self.event_uuid,\n                \"is_delta\": True,\n                \"overshoot_abs\": overshoot_abs if overshoot_abs &gt; 0 else None,\n                \"overshoot_pct\": float(overshoot_pct) if overshoot_pct is not None and overshoot_abs &gt; 0 else None,\n                \"t_peak_seconds\": (t_peak - t0).total_seconds() if t_peak is not None else None,\n                \"undershoot_abs\": undershoot_abs if undershoot_abs &gt; 0 else None,\n                \"undershoot_pct\": float(undershoot_pct) if undershoot_pct is not None and undershoot_abs &gt; 0 else None,\n                \"t_undershoot_seconds\": (t_undershoot - t0).total_seconds() if t_undershoot is not None else None,\n                \"oscillation_count\": oscillation_count if oscillation_count &gt; 0 else 0,\n                \"oscillation_amplitude\": oscillation_amplitude,\n            }\n        )\n\n    return pd.DataFrame(out_rows)\n</code></pre>"},{"location":"reference/ts_shape/events/engineering/__init__/#ts_shape.events.engineering.SetpointChangeEvents.rise_time","title":"rise_time","text":"<pre><code>rise_time(actual_uuid: str, *, start_pct: float = 0.1, end_pct: float = 0.9, lookahead: str = '10m') -&gt; DataFrame\n</code></pre> <p>Compute rise time: time for actual to go from start_pct to end_pct of the setpoint change. Typically measured from 10% to 90% of the final value.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>DataFrame with columns: start, uuid, is_delta, rise_time_seconds, reached_end.</p> </li> </ul> Source code in <code>src/ts_shape/events/engineering/setpoint_events.py</code> <pre><code>def rise_time(\n    self,\n    actual_uuid: str,\n    *,\n    start_pct: float = 0.1,\n    end_pct: float = 0.9,\n    lookahead: str = \"10m\",\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Compute rise time: time for actual to go from start_pct to end_pct of the setpoint change.\n    Typically measured from 10% to 90% of the final value.\n\n    Args:\n        actual_uuid: UUID of the actual/process value signal\n        start_pct: Starting percentage of change (e.g., 0.1 for 10%)\n        end_pct: Ending percentage of change (e.g., 0.9 for 90%)\n        lookahead: Maximum time window to search\n\n    Returns:\n        DataFrame with columns: start, uuid, is_delta, rise_time_seconds, reached_end.\n    \"\"\"\n    if self.sp.empty:\n        return pd.DataFrame(\n            columns=[\"start\", \"uuid\", \"is_delta\", \"rise_time_seconds\", \"reached_end\"]\n        )\n\n    # Use cached actual data\n    actual = self._get_actual(actual_uuid)\n    look_td = pd.to_timedelta(lookahead)\n\n    sp = self.sp[[self.time_column, self.value_column]].copy()\n    sp[\"prev\"] = sp[self.value_column].shift(1)\n    sp[\"delta\"] = sp[self.value_column] - sp[\"prev\"]\n    changes = sp.loc[sp[\"delta\"].abs() &gt; 0, [self.time_column, self.value_column, \"delta\", \"prev\"]].reset_index(drop=True)\n\n    rows: List[Dict[str, Any]] = []\n    for _, c in changes.iterrows():\n        t0 = c[self.time_column]\n        s_new = float(c[self.value_column])\n        s_prev = float(c[\"prev\"]) if pd.notna(c[\"prev\"]) else s_new\n        delta = float(c[\"delta\"]) if pd.notna(c[\"delta\"]) else 0.0\n\n        window = actual[(actual[self.time_column] &gt;= t0) &amp; (actual[self.time_column] &lt;= t0 + look_td)]\n\n        if window.empty or delta == 0:\n            rows.append(\n                {\n                    \"start\": t0,\n                    \"uuid\": self.event_uuid,\n                    \"is_delta\": True,\n                    \"rise_time_seconds\": None,\n                    \"reached_end\": False,\n                }\n            )\n            continue\n\n        # Calculate target levels\n        start_level = s_prev + delta * start_pct\n        end_level = s_prev + delta * end_pct\n\n        # Find crossing times\n        t_start = None\n        t_end = None\n\n        if delta &gt; 0:\n            # Upward step\n            start_crossed = window[self.value_column] &gt;= start_level\n            end_crossed = window[self.value_column] &gt;= end_level\n        else:\n            # Downward step\n            start_crossed = window[self.value_column] &lt;= start_level\n            end_crossed = window[self.value_column] &lt;= end_level\n\n        if start_crossed.any():\n            t_start = window.loc[start_crossed.idxmax(), self.time_column]\n        if end_crossed.any():\n            t_end = window.loc[end_crossed.idxmax(), self.time_column]\n\n        if t_start is not None and t_end is not None:\n            rise_time_sec = (t_end - t_start).total_seconds()\n            reached_end = True\n        else:\n            rise_time_sec = None\n            reached_end = False\n\n        rows.append(\n            {\n                \"start\": t0,\n                \"uuid\": self.event_uuid,\n                \"is_delta\": True,\n                \"rise_time_seconds\": rise_time_sec,\n                \"reached_end\": reached_end,\n            }\n        )\n\n    return pd.DataFrame(rows)\n</code></pre>"},{"location":"reference/ts_shape/events/engineering/__init__/#ts_shape.events.engineering.SetpointChangeEvents.rise_time(actual_uuid)","title":"<code>actual_uuid</code>","text":"(<code>str</code>)           \u2013            <p>UUID of the actual/process value signal</p>"},{"location":"reference/ts_shape/events/engineering/__init__/#ts_shape.events.engineering.SetpointChangeEvents.rise_time(start_pct)","title":"<code>start_pct</code>","text":"(<code>float</code>, default:                   <code>0.1</code> )           \u2013            <p>Starting percentage of change (e.g., 0.1 for 10%)</p>"},{"location":"reference/ts_shape/events/engineering/__init__/#ts_shape.events.engineering.SetpointChangeEvents.rise_time(end_pct)","title":"<code>end_pct</code>","text":"(<code>float</code>, default:                   <code>0.9</code> )           \u2013            <p>Ending percentage of change (e.g., 0.9 for 90%)</p>"},{"location":"reference/ts_shape/events/engineering/__init__/#ts_shape.events.engineering.SetpointChangeEvents.rise_time(lookahead)","title":"<code>lookahead</code>","text":"(<code>str</code>, default:                   <code>'10m'</code> )           \u2013            <p>Maximum time window to search</p>"},{"location":"reference/ts_shape/events/engineering/__init__/#ts_shape.events.engineering.SetpointChangeEvents.time_to_settle","title":"time_to_settle","text":"<pre><code>time_to_settle(actual_uuid: str, *, tol: float = 0.0, settle_pct: Optional[float] = None, hold: str = '0s', lookahead: str = '10m') -&gt; DataFrame\n</code></pre> <p>For each setpoint change (any change), compute time until the actual signal is within \u00b1<code>tol</code> of the new setpoint for a continuous duration of <code>hold</code>.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>DataFrame with columns: start, uuid, is_delta, t_settle_seconds, settled.</p> </li> </ul> Source code in <code>src/ts_shape/events/engineering/setpoint_events.py</code> <pre><code>def time_to_settle(\n    self,\n    actual_uuid: str,\n    *,\n    tol: float = 0.0,\n    settle_pct: Optional[float] = None,\n    hold: str = \"0s\",\n    lookahead: str = \"10m\",\n) -&gt; pd.DataFrame:\n    \"\"\"\n    For each setpoint change (any change), compute time until the actual signal\n    is within \u00b1`tol` of the new setpoint for a continuous duration of `hold`.\n\n    Args:\n        actual_uuid: UUID of the actual/process value signal\n        tol: Absolute tolerance (used if settle_pct is None)\n        settle_pct: Percentage-based tolerance (e.g., 0.02 for 2% of step magnitude)\n        hold: Minimum duration the signal must stay within tolerance\n        lookahead: Maximum time window to search for settling\n\n    Returns:\n        DataFrame with columns: start, uuid, is_delta, t_settle_seconds, settled.\n    \"\"\"\n    if self.sp.empty:\n        return pd.DataFrame(columns=[\"start\", \"uuid\", \"is_delta\", \"t_settle_seconds\", \"settled\"])\n\n    # Use cached actual data\n    actual = self._get_actual(actual_uuid)\n    hold_td = pd.to_timedelta(hold)\n    look_td = pd.to_timedelta(lookahead)\n\n    # change instants\n    sp = self.sp[[self.time_column, self.value_column]].copy()\n    sp[\"prev\"] = sp[self.value_column].shift(1)\n    sp[\"delta\"] = sp[self.value_column] - sp[\"prev\"]\n    change_times = sp.loc[sp[\"delta\"].abs() &gt; 0, [self.time_column, self.value_column, \"delta\"]].reset_index(drop=True)\n\n    rows: List[Dict[str, Any]] = []\n    for _, c in change_times.iterrows():\n        t0 = c[self.time_column]\n        s_new = float(c[self.value_column])\n        delta = float(c[\"delta\"]) if pd.notna(c[\"delta\"]) else 0.0\n\n        # Calculate tolerance based on settle_pct or use absolute tol\n        if settle_pct is not None and delta != 0:\n            effective_tol = abs(delta) * settle_pct\n        else:\n            effective_tol = tol\n\n        window = actual[(actual[self.time_column] &gt;= t0) &amp; (actual[self.time_column] &lt;= t0 + look_td)]\n        if window.empty:\n            rows.append({\"start\": t0, \"uuid\": self.event_uuid, \"is_delta\": True, \"t_settle_seconds\": None, \"settled\": False})\n            continue\n        err = (window[self.value_column] - s_new).abs()\n        inside = err &lt;= effective_tol\n\n        # time to first entry within tolerance (ignores hold)\n        if inside.any():\n            first_idx = inside[inside].index[0]\n            t_enter = window.loc[first_idx, self.time_column]\n        else:\n            t_enter = None\n\n        # determine if any contiguous inside segment satisfies hold duration\n        settled = False\n        if inside.any():\n            gid = (inside.ne(inside.shift())).cumsum()\n            for _, seg in window.groupby(gid):\n                seg_inside = inside.loc[seg.index]\n                if not seg_inside.iloc[0]:\n                    continue\n                start_seg = seg[self.time_column].iloc[0]\n                end_seg = seg[self.time_column].iloc[-1]\n                if (end_seg - start_seg) &gt;= hold_td:\n                    settled = True\n                    break\n\n        rows.append(\n            {\n                \"start\": t0,\n                \"uuid\": self.event_uuid,\n                \"is_delta\": True,\n                \"t_settle_seconds\": (t_enter - t0).total_seconds() if t_enter is not None else None,\n                \"settled\": bool(settled),\n            }\n        )\n\n    return pd.DataFrame(rows)\n</code></pre>"},{"location":"reference/ts_shape/events/engineering/__init__/#ts_shape.events.engineering.SetpointChangeEvents.time_to_settle(actual_uuid)","title":"<code>actual_uuid</code>","text":"(<code>str</code>)           \u2013            <p>UUID of the actual/process value signal</p>"},{"location":"reference/ts_shape/events/engineering/__init__/#ts_shape.events.engineering.SetpointChangeEvents.time_to_settle(tol)","title":"<code>tol</code>","text":"(<code>float</code>, default:                   <code>0.0</code> )           \u2013            <p>Absolute tolerance (used if settle_pct is None)</p>"},{"location":"reference/ts_shape/events/engineering/__init__/#ts_shape.events.engineering.SetpointChangeEvents.time_to_settle(settle_pct)","title":"<code>settle_pct</code>","text":"(<code>Optional[float]</code>, default:                   <code>None</code> )           \u2013            <p>Percentage-based tolerance (e.g., 0.02 for 2% of step magnitude)</p>"},{"location":"reference/ts_shape/events/engineering/__init__/#ts_shape.events.engineering.SetpointChangeEvents.time_to_settle(hold)","title":"<code>hold</code>","text":"(<code>str</code>, default:                   <code>'0s'</code> )           \u2013            <p>Minimum duration the signal must stay within tolerance</p>"},{"location":"reference/ts_shape/events/engineering/__init__/#ts_shape.events.engineering.SetpointChangeEvents.time_to_settle(lookahead)","title":"<code>lookahead</code>","text":"(<code>str</code>, default:                   <code>'10m'</code> )           \u2013            <p>Maximum time window to search for settling</p>"},{"location":"reference/ts_shape/events/engineering/__init__/#ts_shape.events.engineering.SetpointChangeEvents.time_to_settle_derivative","title":"time_to_settle_derivative","text":"<pre><code>time_to_settle_derivative(actual_uuid: str, *, rate_threshold: float = 0.01, lookahead: str = '10m', hold: str = '0s') -&gt; DataFrame\n</code></pre> <p>Detect settling based on rate of change (derivative) falling below threshold. More sensitive to when the process has truly stopped moving.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>DataFrame with columns: start, uuid, is_delta, t_settle_seconds, settled, final_rate.</p> </li> </ul> Source code in <code>src/ts_shape/events/engineering/setpoint_events.py</code> <pre><code>def time_to_settle_derivative(\n    self,\n    actual_uuid: str,\n    *,\n    rate_threshold: float = 0.01,\n    lookahead: str = \"10m\",\n    hold: str = \"0s\",\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Detect settling based on rate of change (derivative) falling below threshold.\n    More sensitive to when the process has truly stopped moving.\n\n    Args:\n        actual_uuid: UUID of the actual/process value signal\n        rate_threshold: Maximum absolute rate of change to consider settled\n        lookahead: Maximum time window to search for settling\n        hold: Minimum duration the rate must stay below threshold\n\n    Returns:\n        DataFrame with columns: start, uuid, is_delta, t_settle_seconds, settled, final_rate.\n    \"\"\"\n    if self.sp.empty:\n        return pd.DataFrame(\n            columns=[\"start\", \"uuid\", \"is_delta\", \"t_settle_seconds\", \"settled\", \"final_rate\"]\n        )\n\n    # Use cached actual data\n    actual = self._get_actual(actual_uuid)\n    look_td = pd.to_timedelta(lookahead)\n    hold_td = pd.to_timedelta(hold)\n\n    # change instants\n    sp = self.sp[[self.time_column, self.value_column]].copy()\n    sp[\"prev\"] = sp[self.value_column].shift(1)\n    sp[\"delta\"] = sp[self.value_column] - sp[\"prev\"]\n    change_times = sp.loc[sp[\"delta\"].abs() &gt; 0, [self.time_column, self.value_column]].reset_index(drop=True)\n\n    rows: List[Dict[str, Any]] = []\n    for _, c in change_times.iterrows():\n        t0 = c[self.time_column]\n        window = actual[(actual[self.time_column] &gt;= t0) &amp; (actual[self.time_column] &lt;= t0 + look_td)]\n\n        if window.empty or len(window) &lt; 2:\n            rows.append(\n                {\n                    \"start\": t0,\n                    \"uuid\": self.event_uuid,\n                    \"is_delta\": True,\n                    \"t_settle_seconds\": None,\n                    \"settled\": False,\n                    \"final_rate\": None,\n                }\n            )\n            continue\n\n        # Calculate rate of change\n        win_copy = window.copy()\n        win_copy[\"dt_s\"] = win_copy[self.time_column].diff().dt.total_seconds()\n        win_copy[\"dv\"] = win_copy[self.value_column].diff()\n        win_copy[\"rate\"] = (win_copy[\"dv\"] / win_copy[\"dt_s\"]).abs()\n\n        # Find when rate drops below threshold\n        below_threshold = win_copy[\"rate\"] &lt;= rate_threshold\n\n        t_settle = None\n        settled = False\n        final_rate = None\n\n        if below_threshold.any():\n            # Find first entry below threshold\n            first_idx = below_threshold[below_threshold].index[0]\n            t_first = win_copy.loc[first_idx, self.time_column]\n\n            # Check if it stays below threshold for hold duration\n            if hold_td.total_seconds() &gt; 0:\n                gid = (below_threshold.ne(below_threshold.shift())).cumsum()\n                for _, seg in win_copy.groupby(gid):\n                    seg_below = below_threshold.loc[seg.index]\n                    if not seg_below.iloc[0]:\n                        continue\n                    start_seg = seg[self.time_column].iloc[0]\n                    end_seg = seg[self.time_column].iloc[-1]\n                    if (end_seg - start_seg) &gt;= hold_td:\n                        t_settle = start_seg\n                        settled = True\n                        final_rate = float(seg[\"rate\"].mean())\n                        break\n            else:\n                t_settle = t_first\n                settled = True\n                final_rate = float(win_copy.loc[first_idx, \"rate\"])\n\n        rows.append(\n            {\n                \"start\": t0,\n                \"uuid\": self.event_uuid,\n                \"is_delta\": True,\n                \"t_settle_seconds\": (t_settle - t0).total_seconds() if t_settle is not None else None,\n                \"settled\": settled,\n                \"final_rate\": final_rate,\n            }\n        )\n\n    return pd.DataFrame(rows)\n</code></pre>"},{"location":"reference/ts_shape/events/engineering/__init__/#ts_shape.events.engineering.SetpointChangeEvents.time_to_settle_derivative(actual_uuid)","title":"<code>actual_uuid</code>","text":"(<code>str</code>)           \u2013            <p>UUID of the actual/process value signal</p>"},{"location":"reference/ts_shape/events/engineering/__init__/#ts_shape.events.engineering.SetpointChangeEvents.time_to_settle_derivative(rate_threshold)","title":"<code>rate_threshold</code>","text":"(<code>float</code>, default:                   <code>0.01</code> )           \u2013            <p>Maximum absolute rate of change to consider settled</p>"},{"location":"reference/ts_shape/events/engineering/__init__/#ts_shape.events.engineering.SetpointChangeEvents.time_to_settle_derivative(lookahead)","title":"<code>lookahead</code>","text":"(<code>str</code>, default:                   <code>'10m'</code> )           \u2013            <p>Maximum time window to search for settling</p>"},{"location":"reference/ts_shape/events/engineering/__init__/#ts_shape.events.engineering.SetpointChangeEvents.time_to_settle_derivative(hold)","title":"<code>hold</code>","text":"(<code>str</code>, default:                   <code>'0s'</code> )           \u2013            <p>Minimum duration the rate must stay below threshold</p>"},{"location":"reference/ts_shape/events/engineering/__init__/#ts_shape.events.engineering.StartupDetectionEvents","title":"StartupDetectionEvents","text":"<pre><code>StartupDetectionEvents(dataframe: DataFrame, target_uuid: str, *, event_uuid: str = 'startup_event', value_column: str = 'value_double', time_column: str = 'systime')\n</code></pre> <p>               Bases: <code>Base</code></p> <p>Detect equipment startup intervals based on threshold crossings or sustained positive slope in a numeric metric (speed, temperature, etc.).</p> <p>Schema assumptions (columns): - uuid, sequence_number, systime, plctime, is_delta - value_integer, value_string, value_double, value_bool, value_bytes</p> <p>Methods:</p> <ul> <li> <code>assess_startup_quality</code>             \u2013              <p>Assess the quality of detected startup events.</p> </li> <li> <code>detect_failed_startups</code>             \u2013              <p>Detect failed or aborted startup attempts.</p> </li> <li> <code>detect_startup_adaptive</code>             \u2013              <p>Detect startups using adaptive thresholds calculated from historical baseline data.</p> </li> <li> <code>detect_startup_by_slope</code>             \u2013              <p>Startup intervals where per-second slope &gt;= <code>min_slope</code> for at least</p> </li> <li> <code>detect_startup_by_threshold</code>             \u2013              <p>Startup begins at first crossing above <code>threshold</code> (or hysteresis enter)</p> </li> <li> <code>detect_startup_multi_signal</code>             \u2013              <p>Detect startups based on multiple signals with configurable AND/OR logic.</p> </li> <li> <code>get_dataframe</code>             \u2013              <p>Returns the processed DataFrame.</p> </li> <li> <code>track_startup_phases</code>             \u2013              <p>Track progression through defined startup phases.</p> </li> </ul> Source code in <code>src/ts_shape/events/engineering/startup_events.py</code> <pre><code>def __init__(\n    self,\n    dataframe: pd.DataFrame,\n    target_uuid: str,\n    *,\n    event_uuid: str = \"startup_event\",\n    value_column: str = \"value_double\",\n    time_column: str = \"systime\",\n) -&gt; None:\n    super().__init__(dataframe, column_name=time_column)\n    self.target_uuid = target_uuid\n    self.event_uuid = event_uuid\n    self.value_column = value_column\n    self.time_column = time_column\n\n    self.series = (\n        self.dataframe[self.dataframe[\"uuid\"] == self.target_uuid]\n        .copy()\n        .sort_values(self.time_column)\n    )\n    self.series[self.time_column] = pd.to_datetime(self.series[self.time_column])\n</code></pre>"},{"location":"reference/ts_shape/events/engineering/__init__/#ts_shape.events.engineering.StartupDetectionEvents.assess_startup_quality","title":"assess_startup_quality","text":"<pre><code>assess_startup_quality(startup_events: DataFrame, *, smoothness_window: int = 5, anomaly_threshold: float = 3.0) -&gt; DataFrame\n</code></pre> <p>Assess the quality of detected startup events.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>DataFrame with quality metrics for each startup: - duration: Total duration of startup - smoothness_score: Inverse of derivative variance (higher = smoother) - anomaly_flags: Number of anomalous points detected - value_change: Total change in value during startup - avg_rate: Average rate of change - max_value: Maximum value reached - stability_score: Measure of how stable the final state is</p> </li> </ul> Source code in <code>src/ts_shape/events/engineering/startup_events.py</code> <pre><code>def assess_startup_quality(\n    self,\n    startup_events: pd.DataFrame,\n    *,\n    smoothness_window: int = 5,\n    anomaly_threshold: float = 3.0,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Assess the quality of detected startup events.\n\n    Args:\n        startup_events: DataFrame of detected startup events (must have 'start' and 'end' columns)\n        smoothness_window: Window size for calculating smoothness metrics\n        anomaly_threshold: Z-score threshold for detecting anomalies\n\n    Returns:\n        DataFrame with quality metrics for each startup:\n            - duration: Total duration of startup\n            - smoothness_score: Inverse of derivative variance (higher = smoother)\n            - anomaly_flags: Number of anomalous points detected\n            - value_change: Total change in value during startup\n            - avg_rate: Average rate of change\n            - max_value: Maximum value reached\n            - stability_score: Measure of how stable the final state is\n    \"\"\"\n    if startup_events.empty or self.series.empty:\n        return pd.DataFrame(columns=[\n            \"start\", \"end\", \"duration\", \"smoothness_score\", \"anomaly_flags\",\n            \"value_change\", \"avg_rate\", \"max_value\", \"stability_score\"\n        ])\n\n    quality_results = []\n    s = self.series[[self.time_column, self.value_column]].copy()\n\n    for _, event in startup_events.iterrows():\n        start = pd.to_datetime(event[\"start\"])\n        end = pd.to_datetime(event[\"end\"])\n\n        # Extract data for this startup period\n        period_data = s[\n            (s[self.time_column] &gt;= start) &amp;\n            (s[self.time_column] &lt;= end)\n        ].copy()\n\n        if period_data.empty or len(period_data) &lt; 2:\n            quality_results.append({\n                \"start\": start,\n                \"end\": end,\n                \"duration\": pd.Timedelta(0),\n                \"smoothness_score\": None,\n                \"anomaly_flags\": 0,\n                \"value_change\": None,\n                \"avg_rate\": None,\n                \"max_value\": None,\n                \"stability_score\": None,\n            })\n            continue\n\n        # Calculate duration\n        duration = end - start\n\n        # Calculate derivatives for smoothness\n        period_data[\"dt_s\"] = period_data[self.time_column].diff().dt.total_seconds()\n        period_data[\"derivative\"] = period_data[self.value_column].diff() / period_data[\"dt_s\"]\n\n        # Smoothness: inverse of derivative variance (normalized)\n        derivative_var = period_data[\"derivative\"].var()\n        smoothness_score = 1.0 / (1.0 + derivative_var) if pd.notna(derivative_var) else None\n\n        # Anomaly detection using z-scores\n        values = period_data[self.value_column]\n        z_scores = np.abs((values - values.mean()) / values.std()) if values.std() &gt; 0 else np.zeros(len(values))\n        anomaly_flags = int((z_scores &gt; anomaly_threshold).sum())\n\n        # Value change metrics\n        value_change = values.iloc[-1] - values.iloc[0] if len(values) &gt; 1 else 0\n        avg_rate = value_change / duration.total_seconds() if duration.total_seconds() &gt; 0 else 0\n        max_value = values.max()\n\n        # Stability score: inverse of coefficient of variation in final 20% of period\n        final_portion_size = max(1, len(period_data) // 5)\n        final_values = values.iloc[-final_portion_size:]\n        cv = final_values.std() / final_values.mean() if final_values.mean() != 0 else float('inf')\n        stability_score = 1.0 / (1.0 + cv) if np.isfinite(cv) else 0.0\n\n        quality_results.append({\n            \"start\": start,\n            \"end\": end,\n            \"duration\": duration,\n            \"smoothness_score\": float(smoothness_score) if smoothness_score is not None else None,\n            \"anomaly_flags\": anomaly_flags,\n            \"value_change\": float(value_change),\n            \"avg_rate\": float(avg_rate),\n            \"max_value\": float(max_value),\n            \"stability_score\": float(stability_score),\n        })\n\n    return pd.DataFrame(quality_results)\n</code></pre>"},{"location":"reference/ts_shape/events/engineering/__init__/#ts_shape.events.engineering.StartupDetectionEvents.assess_startup_quality(startup_events)","title":"<code>startup_events</code>","text":"(<code>DataFrame</code>)           \u2013            <p>DataFrame of detected startup events (must have 'start' and 'end' columns)</p>"},{"location":"reference/ts_shape/events/engineering/__init__/#ts_shape.events.engineering.StartupDetectionEvents.assess_startup_quality(smoothness_window)","title":"<code>smoothness_window</code>","text":"(<code>int</code>, default:                   <code>5</code> )           \u2013            <p>Window size for calculating smoothness metrics</p>"},{"location":"reference/ts_shape/events/engineering/__init__/#ts_shape.events.engineering.StartupDetectionEvents.assess_startup_quality(anomaly_threshold)","title":"<code>anomaly_threshold</code>","text":"(<code>float</code>, default:                   <code>3.0</code> )           \u2013            <p>Z-score threshold for detecting anomalies</p>"},{"location":"reference/ts_shape/events/engineering/__init__/#ts_shape.events.engineering.StartupDetectionEvents.detect_failed_startups","title":"detect_failed_startups","text":"<pre><code>detect_failed_startups(*, threshold: float, min_rise_duration: str = '5s', max_completion_time: str = '5m', completion_threshold: Optional[float] = None, required_stability: str = '10s') -&gt; DataFrame\n</code></pre> <p>Detect failed or aborted startup attempts.</p> <p>A failed startup is identified when: 1. Value rises above threshold for at least min_rise_duration 2. But fails to reach completion_threshold within max_completion_time 3. Or drops back below threshold before achieving required_stability</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>DataFrame with columns: start, end, uuid, is_delta, method, failure_reason,                     max_value_reached, time_to_failure</p> </li> </ul> Source code in <code>src/ts_shape/events/engineering/startup_events.py</code> <pre><code>def detect_failed_startups(\n    self,\n    *,\n    threshold: float,\n    min_rise_duration: str = \"5s\",\n    max_completion_time: str = \"5m\",\n    completion_threshold: Optional[float] = None,\n    required_stability: str = \"10s\",\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Detect failed or aborted startup attempts.\n\n    A failed startup is identified when:\n    1. Value rises above threshold for at least min_rise_duration\n    2. But fails to reach completion_threshold within max_completion_time\n    3. Or drops back below threshold before achieving required_stability\n\n    Args:\n        threshold: Initial threshold that must be crossed to begin startup\n        min_rise_duration: Minimum time above threshold to consider it a startup attempt\n        max_completion_time: Maximum time allowed to complete startup\n        completion_threshold: Target threshold for successful completion (default: 2x threshold)\n        required_stability: Time that must be maintained at completion level\n\n    Returns:\n        DataFrame with columns: start, end, uuid, is_delta, method, failure_reason,\n                                max_value_reached, time_to_failure\n    \"\"\"\n    if self.series.empty:\n        return pd.DataFrame(columns=[\n            \"start\", \"end\", \"uuid\", \"is_delta\", \"method\",\n            \"failure_reason\", \"max_value_reached\", \"time_to_failure\"\n        ])\n\n    if completion_threshold is None:\n        completion_threshold = threshold * 2.0\n\n    min_rise_td = pd.to_timedelta(min_rise_duration)\n    max_completion_td = pd.to_timedelta(max_completion_time)\n    stability_td = pd.to_timedelta(required_stability)\n\n    s = self.series[[self.time_column, self.value_column]].copy()\n\n    # Detect threshold crossings\n    above_threshold = s[self.value_column] &gt;= threshold\n    rising = (~above_threshold.shift(fill_value=False)) &amp; above_threshold\n    rise_times = s.loc[rising, self.time_column]\n\n    failed_events: List[Dict[str, Any]] = []\n\n    for t0 in rise_times:\n        # Get data for potential startup period\n        startup_window = s[\n            (s[self.time_column] &gt;= t0) &amp;\n            (s[self.time_column] &lt;= t0 + max_completion_td)\n        ].copy()\n\n        if len(startup_window) &lt; 2:\n            continue\n\n        # Check if initially above threshold for min_rise_duration\n        initial_window = startup_window[\n            startup_window[self.time_column] &lt;= t0 + min_rise_td\n        ]\n\n        if initial_window.empty or not (initial_window[self.value_column] &gt;= threshold).all():\n            continue  # Not a valid startup attempt\n\n        # Now check for failure modes\n        max_value = startup_window[self.value_column].max()\n        failure_detected = False\n        failure_reason = None\n        failure_time = None\n\n        # Check if value drops back below threshold before completion\n        above_in_window = startup_window[self.value_column] &gt;= threshold\n        if not above_in_window.all():\n            # Find when it dropped below\n            drop_idx = startup_window[~above_in_window].index[0]\n            failure_time = startup_window.loc[drop_idx, self.time_column]\n\n            # Check if this happened before reaching completion threshold\n            if max_value &lt; completion_threshold:\n                failure_detected = True\n                failure_reason = \"dropped_below_threshold_before_completion\"\n\n        # Check if completion threshold was never reached\n        if not failure_detected:\n            reached_completion = (startup_window[self.value_column] &gt;= completion_threshold).any()\n\n            if not reached_completion:\n                failure_detected = True\n                failure_reason = \"failed_to_reach_completion_threshold\"\n                failure_time = startup_window[self.time_column].iloc[-1]\n            else:\n                # Reached completion but check stability\n                completion_idx = startup_window[\n                    startup_window[self.value_column] &gt;= completion_threshold\n                ].index[0]\n                completion_time = startup_window.loc[completion_idx, self.time_column]\n\n                stability_window = startup_window[\n                    (startup_window[self.time_column] &gt;= completion_time) &amp;\n                    (startup_window[self.time_column] &lt;= completion_time + stability_td)\n                ]\n\n                if not stability_window.empty:\n                    if not (stability_window[self.value_column] &gt;= completion_threshold).all():\n                        failure_detected = True\n                        failure_reason = \"insufficient_stability_at_completion\"\n                        failure_time = completion_time + stability_td\n\n        if failure_detected:\n            time_to_failure = (failure_time - t0).total_seconds() if failure_time else None\n\n            failed_events.append({\n                \"start\": t0,\n                \"end\": failure_time if failure_time else t0 + max_completion_td,\n                \"uuid\": self.event_uuid,\n                \"is_delta\": True,\n                \"method\": \"failed_startup\",\n                \"failure_reason\": failure_reason,\n                \"max_value_reached\": float(max_value),\n                \"time_to_failure\": time_to_failure,\n            })\n\n    return pd.DataFrame(failed_events)\n</code></pre>"},{"location":"reference/ts_shape/events/engineering/__init__/#ts_shape.events.engineering.StartupDetectionEvents.detect_failed_startups(threshold)","title":"<code>threshold</code>","text":"(<code>float</code>)           \u2013            <p>Initial threshold that must be crossed to begin startup</p>"},{"location":"reference/ts_shape/events/engineering/__init__/#ts_shape.events.engineering.StartupDetectionEvents.detect_failed_startups(min_rise_duration)","title":"<code>min_rise_duration</code>","text":"(<code>str</code>, default:                   <code>'5s'</code> )           \u2013            <p>Minimum time above threshold to consider it a startup attempt</p>"},{"location":"reference/ts_shape/events/engineering/__init__/#ts_shape.events.engineering.StartupDetectionEvents.detect_failed_startups(max_completion_time)","title":"<code>max_completion_time</code>","text":"(<code>str</code>, default:                   <code>'5m'</code> )           \u2013            <p>Maximum time allowed to complete startup</p>"},{"location":"reference/ts_shape/events/engineering/__init__/#ts_shape.events.engineering.StartupDetectionEvents.detect_failed_startups(completion_threshold)","title":"<code>completion_threshold</code>","text":"(<code>Optional[float]</code>, default:                   <code>None</code> )           \u2013            <p>Target threshold for successful completion (default: 2x threshold)</p>"},{"location":"reference/ts_shape/events/engineering/__init__/#ts_shape.events.engineering.StartupDetectionEvents.detect_failed_startups(required_stability)","title":"<code>required_stability</code>","text":"(<code>str</code>, default:                   <code>'10s'</code> )           \u2013            <p>Time that must be maintained at completion level</p>"},{"location":"reference/ts_shape/events/engineering/__init__/#ts_shape.events.engineering.StartupDetectionEvents.detect_startup_adaptive","title":"detect_startup_adaptive","text":"<pre><code>detect_startup_adaptive(*, baseline_window: str = '1h', sensitivity: float = 2.0, min_above: str = '10s', lookback_periods: int = 5) -&gt; DataFrame\n</code></pre> <p>Detect startups using adaptive thresholds calculated from historical baseline data.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>DataFrame with columns: start, end, uuid, is_delta, method, adaptive_threshold,                     baseline_mean, baseline_std</p> </li> </ul> Source code in <code>src/ts_shape/events/engineering/startup_events.py</code> <pre><code>def detect_startup_adaptive(\n    self,\n    *,\n    baseline_window: str = \"1h\",\n    sensitivity: float = 2.0,\n    min_above: str = \"10s\",\n    lookback_periods: int = 5,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Detect startups using adaptive thresholds calculated from historical baseline data.\n\n    Args:\n        baseline_window: Window size for calculating baseline statistics\n        sensitivity: Multiplier for standard deviation (threshold = mean + sensitivity * std)\n        min_above: Minimum time the value must stay above threshold\n        lookback_periods: Number of baseline periods to use for statistics\n\n    Returns:\n        DataFrame with columns: start, end, uuid, is_delta, method, adaptive_threshold,\n                                baseline_mean, baseline_std\n    \"\"\"\n    if self.series.empty:\n        return pd.DataFrame(columns=[\n            \"start\", \"end\", \"uuid\", \"is_delta\", \"method\",\n            \"adaptive_threshold\", \"baseline_mean\", \"baseline_std\"\n        ])\n\n    window_td = pd.to_timedelta(baseline_window)\n    min_above_td = pd.to_timedelta(min_above)\n\n    s = self.series[[self.time_column, self.value_column]].copy()\n    s = s.sort_values(self.time_column).reset_index(drop=True)\n\n    events: List[Dict[str, Any]] = []\n\n    # Calculate rolling statistics\n    s[\"rolling_mean\"] = s[self.value_column].rolling(\n        window=lookback_periods, min_periods=1\n    ).mean()\n    s[\"rolling_std\"] = s[self.value_column].rolling(\n        window=lookback_periods, min_periods=1\n    ).std()\n\n    # Calculate adaptive threshold\n    s[\"adaptive_threshold\"] = s[\"rolling_mean\"] + sensitivity * s[\"rolling_std\"].fillna(0)\n\n    # Detect crossings above adaptive threshold\n    s[\"above_threshold\"] = s[self.value_column] &gt;= s[\"adaptive_threshold\"]\n    s[\"crossing_up\"] = (~s[\"above_threshold\"].shift(fill_value=False)) &amp; s[\"above_threshold\"]\n\n    # Find sustained periods above threshold\n    for idx in s[s[\"crossing_up\"]].index:\n        t0 = s.loc[idx, self.time_column]\n        threshold_at_crossing = s.loc[idx, \"adaptive_threshold\"]\n        baseline_mean = s.loc[idx, \"rolling_mean\"]\n        baseline_std = s.loc[idx, \"rolling_std\"]\n\n        # Check if value stays above threshold for min_above duration\n        win = s[\n            (s[self.time_column] &gt;= t0) &amp;\n            (s[self.time_column] &lt;= t0 + min_above_td)\n        ]\n\n        if win.empty:\n            continue\n\n        # Value must stay above the threshold calculated at crossing time\n        if (win[self.value_column] &gt;= threshold_at_crossing).all():\n            events.append({\n                \"start\": t0,\n                \"end\": t0 + min_above_td,\n                \"uuid\": self.event_uuid,\n                \"is_delta\": True,\n                \"method\": \"adaptive_threshold\",\n                \"adaptive_threshold\": float(threshold_at_crossing),\n                \"baseline_mean\": float(baseline_mean) if pd.notna(baseline_mean) else None,\n                \"baseline_std\": float(baseline_std) if pd.notna(baseline_std) else None,\n            })\n\n    return pd.DataFrame(events)\n</code></pre>"},{"location":"reference/ts_shape/events/engineering/__init__/#ts_shape.events.engineering.StartupDetectionEvents.detect_startup_adaptive(baseline_window)","title":"<code>baseline_window</code>","text":"(<code>str</code>, default:                   <code>'1h'</code> )           \u2013            <p>Window size for calculating baseline statistics</p>"},{"location":"reference/ts_shape/events/engineering/__init__/#ts_shape.events.engineering.StartupDetectionEvents.detect_startup_adaptive(sensitivity)","title":"<code>sensitivity</code>","text":"(<code>float</code>, default:                   <code>2.0</code> )           \u2013            <p>Multiplier for standard deviation (threshold = mean + sensitivity * std)</p>"},{"location":"reference/ts_shape/events/engineering/__init__/#ts_shape.events.engineering.StartupDetectionEvents.detect_startup_adaptive(min_above)","title":"<code>min_above</code>","text":"(<code>str</code>, default:                   <code>'10s'</code> )           \u2013            <p>Minimum time the value must stay above threshold</p>"},{"location":"reference/ts_shape/events/engineering/__init__/#ts_shape.events.engineering.StartupDetectionEvents.detect_startup_adaptive(lookback_periods)","title":"<code>lookback_periods</code>","text":"(<code>int</code>, default:                   <code>5</code> )           \u2013            <p>Number of baseline periods to use for statistics</p>"},{"location":"reference/ts_shape/events/engineering/__init__/#ts_shape.events.engineering.StartupDetectionEvents.detect_startup_by_slope","title":"detect_startup_by_slope","text":"<pre><code>detect_startup_by_slope(*, min_slope: float, slope_window: str = '0s', min_duration: str = '0s') -&gt; DataFrame\n</code></pre> <p>Startup intervals where per-second slope &gt;= <code>min_slope</code> for at least <code>min_duration</code>. <code>slope_window</code> is accepted for API completeness but the current implementation uses instantaneous slope between samples.</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>DataFrame with columns: start, end, uuid, is_delta, method, min_slope, avg_slope.</p> </li> </ul> Source code in <code>src/ts_shape/events/engineering/startup_events.py</code> <pre><code>def detect_startup_by_slope(\n    self,\n    *,\n    min_slope: float,\n    slope_window: str = \"0s\",\n    min_duration: str = \"0s\",\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Startup intervals where per-second slope &gt;= `min_slope` for at least\n    `min_duration`. `slope_window` is accepted for API completeness but the\n    current implementation uses instantaneous slope between samples.\n\n    Returns:\n        DataFrame with columns: start, end, uuid, is_delta, method, min_slope, avg_slope.\n    \"\"\"\n    if self.series.empty:\n        return pd.DataFrame(columns=[\"start\", \"end\", \"uuid\", \"is_delta\", \"method\", \"min_slope\", \"avg_slope\"])\n\n    s = self.series[[self.time_column, self.value_column]].copy()\n    s[\"dt_s\"] = s[self.time_column].diff().dt.total_seconds()\n    s[\"dv\"] = s[self.value_column].diff()\n    s[\"slope\"] = s[\"dv\"] / s[\"dt_s\"]\n    mask = s[\"slope\"] &gt;= float(min_slope)\n\n    gid = (mask != mask.shift()).cumsum()\n    min_d = pd.to_timedelta(min_duration)\n    events: List[Dict[str, Any]] = []\n    for _, seg in s.groupby(gid):\n        seg_mask = mask.loc[seg.index]\n        if not seg_mask.any():\n            continue\n        start_t = seg.loc[seg_mask, self.time_column].iloc[0]\n        end_t = seg.loc[seg_mask, self.time_column].iloc[-1]\n        if (end_t - start_t) &lt; min_d:\n            continue\n        avg_slope = seg.loc[seg_mask, \"slope\"].mean()\n        events.append(\n            {\n                \"start\": start_t,\n                \"end\": end_t,\n                \"uuid\": self.event_uuid,\n                \"is_delta\": True,\n                \"method\": \"slope\",\n                \"min_slope\": float(min_slope),\n                \"avg_slope\": float(avg_slope) if pd.notna(avg_slope) else None,\n            }\n        )\n\n    return pd.DataFrame(events)\n</code></pre>"},{"location":"reference/ts_shape/events/engineering/__init__/#ts_shape.events.engineering.StartupDetectionEvents.detect_startup_by_threshold","title":"detect_startup_by_threshold","text":"<pre><code>detect_startup_by_threshold(*, threshold: float, hysteresis: tuple[float, float] | None = None, min_above: str = '0s') -&gt; DataFrame\n</code></pre> <p>Startup begins at first crossing above <code>threshold</code> (or hysteresis enter) and is valid only if the metric stays above the (exit) threshold for at least <code>min_above</code>.</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>DataFrame with columns: start, end, uuid, is_delta, method, threshold.</p> </li> </ul> Source code in <code>src/ts_shape/events/engineering/startup_events.py</code> <pre><code>def detect_startup_by_threshold(\n    self,\n    *,\n    threshold: float,\n    hysteresis: tuple[float, float] | None = None,\n    min_above: str = \"0s\",\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Startup begins at first crossing above `threshold` (or hysteresis enter)\n    and is valid only if the metric stays above the (exit) threshold for at\n    least `min_above`.\n\n    Returns:\n        DataFrame with columns: start, end, uuid, is_delta, method, threshold.\n    \"\"\"\n    if self.series.empty:\n        return pd.DataFrame(columns=[\"start\", \"end\", \"uuid\", \"is_delta\", \"method\", \"threshold\"])\n\n    enter_thr = threshold if hysteresis is None else hysteresis[0]\n    exit_thr = threshold if hysteresis is None else hysteresis[1]\n    min_above_td = pd.to_timedelta(min_above)\n\n    s = self.series[[self.time_column, self.value_column]].copy()\n    above_enter = s[self.value_column] &gt;= enter_thr\n    rising = (~above_enter.shift(fill_value=False)) &amp; above_enter\n    rise_times = s.loc[rising, self.time_column]\n\n    events: List[Dict[str, Any]] = []\n    for t0 in rise_times:\n        # ensure dwell above exit threshold for min_above\n        win = s[(s[self.time_column] &gt;= t0) &amp; (s[self.time_column] &lt;= t0 + min_above_td)]\n        if win.empty:\n            continue\n        if (win[self.value_column] &gt;= exit_thr).all():\n            events.append(\n                {\n                    \"start\": t0,\n                    \"end\": t0 + min_above_td,\n                    \"uuid\": self.event_uuid,\n                    \"is_delta\": True,\n                    \"method\": \"threshold\",\n                    \"threshold\": float(threshold),\n                }\n            )\n\n    return pd.DataFrame(events)\n</code></pre>"},{"location":"reference/ts_shape/events/engineering/__init__/#ts_shape.events.engineering.StartupDetectionEvents.detect_startup_multi_signal","title":"detect_startup_multi_signal","text":"<pre><code>detect_startup_multi_signal(signals: Dict[str, Dict[str, Any]], logic: str = 'all', *, time_tolerance: str = '30s') -&gt; DataFrame\n</code></pre> <p>Detect startups based on multiple signals with configurable AND/OR logic.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>DataFrame with columns: start, end, uuid, is_delta, method, signals_triggered, signal_details</p> </li> </ul> Source code in <code>src/ts_shape/events/engineering/startup_events.py</code> <pre><code>def detect_startup_multi_signal(\n    self,\n    signals: Dict[str, Dict[str, Any]],\n    logic: str = \"all\",\n    *,\n    time_tolerance: str = \"30s\",\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Detect startups based on multiple signals with configurable AND/OR logic.\n\n    Args:\n        signals: Dict mapping uuid to detection config. Each config should contain:\n            - 'method': 'threshold' or 'slope'\n            - For threshold: 'threshold', optional 'hysteresis', 'min_above'\n            - For slope: 'min_slope', optional 'slope_window', 'min_duration'\n        logic: 'all' (AND - all signals must detect) or 'any' (OR - at least one)\n        time_tolerance: Maximum time difference between signals for 'all' logic\n\n    Returns:\n        DataFrame with columns: start, end, uuid, is_delta, method, signals_triggered, signal_details\n    \"\"\"\n    if logic not in [\"all\", \"any\"]:\n        raise ValueError(f\"logic must be 'all' or 'any', got '{logic}'\")\n\n    # Detect startups for each signal\n    signal_events: Dict[str, pd.DataFrame] = {}\n    for sig_uuid, config in signals.items():\n        # Temporarily store original settings\n        orig_uuid = self.target_uuid\n        orig_series = self.series.copy()\n\n        # Switch to the signal's uuid\n        self.target_uuid = sig_uuid\n        self.series = (\n            self.dataframe[self.dataframe[\"uuid\"] == sig_uuid]\n            .copy()\n            .sort_values(self.time_column)\n        )\n        self.series[self.time_column] = pd.to_datetime(self.series[self.time_column])\n\n        # Detect based on method\n        method = config.get(\"method\", \"threshold\")\n        if method == \"threshold\":\n            events = self.detect_startup_by_threshold(\n                threshold=config.get(\"threshold\", 0),\n                hysteresis=config.get(\"hysteresis\"),\n                min_above=config.get(\"min_above\", \"0s\"),\n            )\n        elif method == \"slope\":\n            events = self.detect_startup_by_slope(\n                min_slope=config.get(\"min_slope\", 0),\n                slope_window=config.get(\"slope_window\", \"0s\"),\n                min_duration=config.get(\"min_duration\", \"0s\"),\n            )\n        else:\n            raise ValueError(f\"Unknown method '{method}' for signal {sig_uuid}\")\n\n        signal_events[sig_uuid] = events\n\n        # Restore original settings\n        self.target_uuid = orig_uuid\n        self.series = orig_series\n\n    # Combine events based on logic\n    if logic == \"any\":\n        # Union: any signal detecting is sufficient\n        all_events = []\n        for sig_uuid, events in signal_events.items():\n            for _, event in events.iterrows():\n                all_events.append({\n                    \"start\": event[\"start\"],\n                    \"end\": event[\"end\"],\n                    \"uuid\": self.event_uuid,\n                    \"is_delta\": True,\n                    \"method\": \"multi_signal_any\",\n                    \"signals_triggered\": [sig_uuid],\n                    \"signal_details\": {sig_uuid: event.to_dict()},\n                })\n        return pd.DataFrame(all_events)\n\n    else:  # logic == \"all\"\n        # Intersection: all signals must detect within time_tolerance\n        if not signal_events or any(df.empty for df in signal_events.values()):\n            return pd.DataFrame(columns=[\n                \"start\", \"end\", \"uuid\", \"is_delta\", \"method\",\n                \"signals_triggered\", \"signal_details\"\n            ])\n\n        tolerance = pd.to_timedelta(time_tolerance)\n        combined_events = []\n\n        # Get events from first signal as reference\n        first_sig = list(signal_events.keys())[0]\n        for _, ref_event in signal_events[first_sig].iterrows():\n            ref_start = ref_event[\"start\"]\n\n            # Check if all other signals have an event within tolerance\n            matching_signals = {first_sig: ref_event.to_dict()}\n            all_match = True\n\n            for sig_uuid in list(signal_events.keys())[1:]:\n                sig_df = signal_events[sig_uuid]\n                # Find events within tolerance\n                matches = sig_df[\n                    (sig_df[\"start\"] &gt;= ref_start - tolerance) &amp;\n                    (sig_df[\"start\"] &lt;= ref_start + tolerance)\n                ]\n\n                if matches.empty:\n                    all_match = False\n                    break\n\n                # Use the closest match\n                closest_idx = (matches[\"start\"] - ref_start).abs().idxmin()\n                matching_signals[sig_uuid] = matches.loc[closest_idx].to_dict()\n\n            if all_match:\n                # Calculate overall start/end from all matching events\n                all_starts = [matching_signals[sig][\"start\"] for sig in matching_signals]\n                all_ends = [matching_signals[sig][\"end\"] for sig in matching_signals]\n\n                combined_events.append({\n                    \"start\": min(all_starts),\n                    \"end\": max(all_ends),\n                    \"uuid\": self.event_uuid,\n                    \"is_delta\": True,\n                    \"method\": \"multi_signal_all\",\n                    \"signals_triggered\": list(matching_signals.keys()),\n                    \"signal_details\": matching_signals,\n                })\n\n        return pd.DataFrame(combined_events)\n</code></pre>"},{"location":"reference/ts_shape/events/engineering/__init__/#ts_shape.events.engineering.StartupDetectionEvents.detect_startup_multi_signal(signals)","title":"<code>signals</code>","text":"(<code>Dict[str, Dict[str, Any]]</code>)           \u2013            <p>Dict mapping uuid to detection config. Each config should contain: - 'method': 'threshold' or 'slope' - For threshold: 'threshold', optional 'hysteresis', 'min_above' - For slope: 'min_slope', optional 'slope_window', 'min_duration'</p>"},{"location":"reference/ts_shape/events/engineering/__init__/#ts_shape.events.engineering.StartupDetectionEvents.detect_startup_multi_signal(logic)","title":"<code>logic</code>","text":"(<code>str</code>, default:                   <code>'all'</code> )           \u2013            <p>'all' (AND - all signals must detect) or 'any' (OR - at least one)</p>"},{"location":"reference/ts_shape/events/engineering/__init__/#ts_shape.events.engineering.StartupDetectionEvents.detect_startup_multi_signal(time_tolerance)","title":"<code>time_tolerance</code>","text":"(<code>str</code>, default:                   <code>'30s'</code> )           \u2013            <p>Maximum time difference between signals for 'all' logic</p>"},{"location":"reference/ts_shape/events/engineering/__init__/#ts_shape.events.engineering.StartupDetectionEvents.get_dataframe","title":"get_dataframe","text":"<pre><code>get_dataframe() -&gt; DataFrame\n</code></pre> <p>Returns the processed DataFrame.</p> Source code in <code>src/ts_shape/utils/base.py</code> <pre><code>def get_dataframe(self) -&gt; pd.DataFrame:\n    \"\"\"Returns the processed DataFrame.\"\"\"\n    return self.dataframe\n</code></pre>"},{"location":"reference/ts_shape/events/engineering/__init__/#ts_shape.events.engineering.StartupDetectionEvents.track_startup_phases","title":"track_startup_phases","text":"<pre><code>track_startup_phases(phases: List[Dict[str, Any]], *, min_phase_duration: str = '5s') -&gt; DataFrame\n</code></pre> <p>Track progression through defined startup phases.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>DataFrame with phase transitions: - phase_name: Name of the phase - phase_number: Sequential phase number (0-indexed) - start: Phase start time - end: Phase end time - duration: Time spent in phase - next_phase: Name of the next phase (None for last phase) - completed: Whether full startup sequence completed</p> </li> </ul> Source code in <code>src/ts_shape/events/engineering/startup_events.py</code> <pre><code>def track_startup_phases(\n    self,\n    phases: List[Dict[str, Any]],\n    *,\n    min_phase_duration: str = \"5s\",\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Track progression through defined startup phases.\n\n    Args:\n        phases: List of phase definitions, each containing:\n            - 'name': Phase name\n            - 'condition': 'threshold', 'range', or 'slope'\n            - For 'threshold': 'min_value' (value must be &gt;= min_value)\n            - For 'range': 'min_value' and 'max_value' (value in range)\n            - For 'slope': 'min_slope' (slope must be &gt;= min_slope)\n        min_phase_duration: Minimum time to stay in phase to be considered valid\n\n    Returns:\n        DataFrame with phase transitions:\n            - phase_name: Name of the phase\n            - phase_number: Sequential phase number (0-indexed)\n            - start: Phase start time\n            - end: Phase end time\n            - duration: Time spent in phase\n            - next_phase: Name of the next phase (None for last phase)\n            - completed: Whether full startup sequence completed\n    \"\"\"\n    if self.series.empty or not phases:\n        return pd.DataFrame(columns=[\n            \"phase_name\", \"phase_number\", \"start\", \"end\",\n            \"duration\", \"next_phase\", \"completed\"\n        ])\n\n    min_duration = pd.to_timedelta(min_phase_duration)\n    s = self.series[[self.time_column, self.value_column]].copy()\n\n    # Calculate slopes if needed\n    s[\"dt_s\"] = s[self.time_column].diff().dt.total_seconds()\n    s[\"dv\"] = s[self.value_column].diff()\n    s[\"slope\"] = s[\"dv\"] / s[\"dt_s\"]\n\n    phase_results = []\n    current_phase_idx = 0\n    phase_start = None\n    i = 0\n\n    while i &lt; len(s) and current_phase_idx &lt; len(phases):\n        row = s.iloc[i]\n        phase = phases[current_phase_idx]\n\n        # Check if current row satisfies phase condition\n        in_phase = self._check_phase_condition(row, phase)\n\n        if in_phase:\n            if phase_start is None:\n                phase_start = row[self.time_column]\n\n            # Check if we've been in this phase long enough\n            if row[self.time_column] - phase_start &gt;= min_duration:\n                # Check if we're transitioning to next phase\n                next_phase_idx = current_phase_idx + 1\n                if next_phase_idx &lt; len(phases):\n                    # Check if next phase condition is met\n                    next_phase = phases[next_phase_idx]\n                    if self._check_phase_condition(row, next_phase):\n                        # Record completed phase\n                        phase_results.append({\n                            \"phase_name\": phase[\"name\"],\n                            \"phase_number\": current_phase_idx,\n                            \"start\": phase_start,\n                            \"end\": row[self.time_column],\n                            \"duration\": row[self.time_column] - phase_start,\n                            \"next_phase\": next_phase[\"name\"],\n                            \"completed\": False,  # Will update at end\n                        })\n                        current_phase_idx = next_phase_idx\n                        phase_start = row[self.time_column]\n                else:\n                    # Last phase - check if it remains stable\n                    remaining = s.iloc[i:]\n                    if len(remaining) &gt; 0:\n                        # Check stability of last phase\n                        stable = all(\n                            self._check_phase_condition(s.iloc[j], phase)\n                            for j in range(i, min(i + 10, len(s)))\n                        )\n                        if stable:\n                            phase_results.append({\n                                \"phase_name\": phase[\"name\"],\n                                \"phase_number\": current_phase_idx,\n                                \"start\": phase_start,\n                                \"end\": row[self.time_column],\n                                \"duration\": row[self.time_column] - phase_start,\n                                \"next_phase\": None,\n                                \"completed\": True,\n                            })\n                            break\n        else:\n            # Lost phase condition\n            if phase_start is not None and row[self.time_column] - phase_start &gt;= min_duration:\n                # Phase was valid but didn't progress - potential failed startup\n                phase_results.append({\n                    \"phase_name\": phase[\"name\"],\n                    \"phase_number\": current_phase_idx,\n                    \"start\": phase_start,\n                    \"end\": row[self.time_column],\n                    \"duration\": row[self.time_column] - phase_start,\n                    \"next_phase\": None,\n                    \"completed\": False,\n                })\n            phase_start = None\n\n        i += 1\n\n    # Mark if full sequence completed\n    if phase_results and phase_results[-1][\"phase_number\"] == len(phases) - 1:\n        for result in phase_results:\n            result[\"completed\"] = True\n\n    return pd.DataFrame(phase_results)\n</code></pre>"},{"location":"reference/ts_shape/events/engineering/__init__/#ts_shape.events.engineering.StartupDetectionEvents.track_startup_phases(phases)","title":"<code>phases</code>","text":"(<code>List[Dict[str, Any]]</code>)           \u2013            <p>List of phase definitions, each containing: - 'name': Phase name - 'condition': 'threshold', 'range', or 'slope' - For 'threshold': 'min_value' (value must be &gt;= min_value) - For 'range': 'min_value' and 'max_value' (value in range) - For 'slope': 'min_slope' (slope must be &gt;= min_slope)</p>"},{"location":"reference/ts_shape/events/engineering/__init__/#ts_shape.events.engineering.StartupDetectionEvents.track_startup_phases(min_phase_duration)","title":"<code>min_phase_duration</code>","text":"(<code>str</code>, default:                   <code>'5s'</code> )           \u2013            <p>Minimum time to stay in phase to be considered valid</p>"},{"location":"reference/ts_shape/events/engineering/setpoint_events/","title":"setpoint_events","text":""},{"location":"reference/ts_shape/events/engineering/setpoint_events/#ts_shape.events.engineering.setpoint_events","title":"ts_shape.events.engineering.setpoint_events","text":"<p>Classes:</p> <ul> <li> <code>SetpointChangeEvents</code>           \u2013            <p>Detect step/ramp changes on a setpoint signal and compute follow-up KPIs</p> </li> </ul>"},{"location":"reference/ts_shape/events/engineering/setpoint_events/#ts_shape.events.engineering.setpoint_events.SetpointChangeEvents","title":"SetpointChangeEvents","text":"<pre><code>SetpointChangeEvents(dataframe: DataFrame, setpoint_uuid: str, *, event_uuid: str = 'setpoint_change_event', value_column: str = 'value_double', time_column: str = 'systime')\n</code></pre> <p>               Bases: <code>Base</code></p> <p>Detect step/ramp changes on a setpoint signal and compute follow-up KPIs like time-to-settle and overshoot based on an actual (process) value.</p> <p>Schema assumptions (columns): - uuid, sequence_number, systime, plctime, is_delta - value_integer, value_string, value_double, value_bool, value_bytes</p> <p>Methods:</p> <ul> <li> <code>control_quality_metrics</code>             \u2013              <p>Comprehensive control quality metrics combining multiple performance indicators.</p> </li> <li> <code>decay_rate</code>             \u2013              <p>Estimate exponential decay rate of the settling behavior.</p> </li> <li> <code>detect_setpoint_changes</code>             \u2013              <p>Unified setpoint change table (steps + ramps) with standardized columns.</p> </li> <li> <code>detect_setpoint_ramps</code>             \u2013              <p>Interval events where |dS/dt| &gt;= min_rate for at least <code>min_duration</code>.</p> </li> <li> <code>detect_setpoint_steps</code>             \u2013              <p>Point events at times where the setpoint changes by &gt;= min_delta and the</p> </li> <li> <code>get_dataframe</code>             \u2013              <p>Returns the processed DataFrame.</p> </li> <li> <code>oscillation_frequency</code>             \u2013              <p>Estimate the frequency of oscillations during settling.</p> </li> <li> <code>overshoot_metrics</code>             \u2013              <p>For each change, compute peak overshoot, undershoot, and oscillation metrics</p> </li> <li> <code>rise_time</code>             \u2013              <p>Compute rise time: time for actual to go from start_pct to end_pct of the setpoint change.</p> </li> <li> <code>time_to_settle</code>             \u2013              <p>For each setpoint change (any change), compute time until the actual signal</p> </li> <li> <code>time_to_settle_derivative</code>             \u2013              <p>Detect settling based on rate of change (derivative) falling below threshold.</p> </li> </ul> Source code in <code>src/ts_shape/events/engineering/setpoint_events.py</code> <pre><code>def __init__(\n    self,\n    dataframe: pd.DataFrame,\n    setpoint_uuid: str,\n    *,\n    event_uuid: str = \"setpoint_change_event\",\n    value_column: str = \"value_double\",\n    time_column: str = \"systime\",\n) -&gt; None:\n    super().__init__(dataframe, column_name=time_column)\n    self.setpoint_uuid = setpoint_uuid\n    self.event_uuid = event_uuid\n    self.value_column = value_column\n    self.time_column = time_column\n\n    # isolate setpoint series and ensure proper dtypes/sort\n    self.sp = (\n        self.dataframe[self.dataframe[\"uuid\"] == self.setpoint_uuid]\n        .copy()\n        .sort_values(self.time_column)\n    )\n    self.sp[self.time_column] = pd.to_datetime(self.sp[self.time_column])\n\n    # Cache for performance optimization\n    self._actual_cache: Dict[str, pd.DataFrame] = {}\n</code></pre>"},{"location":"reference/ts_shape/events/engineering/setpoint_events/#ts_shape.events.engineering.setpoint_events.SetpointChangeEvents.control_quality_metrics","title":"control_quality_metrics","text":"<pre><code>control_quality_metrics(actual_uuid: str, *, tol: float = 0.0, settle_pct: Optional[float] = None, hold: str = '0s', lookahead: str = '10m', rate_threshold: float = 0.01) -&gt; DataFrame\n</code></pre> <p>Comprehensive control quality metrics combining multiple performance indicators.</p> <p>Computes all available metrics for each setpoint change and returns them in a single DataFrame. This includes: settling time, rise time, overshoot, undershoot, oscillations, and decay characteristics.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>DataFrame with comprehensive metrics including:</p> </li> <li> <code>DataFrame</code>           \u2013            <ul> <li>start, uuid, is_delta</li> </ul> </li> <li> <code>DataFrame</code>           \u2013            <ul> <li>t_settle_seconds, settled (from time_to_settle)</li> </ul> </li> <li> <code>DataFrame</code>           \u2013            <ul> <li>t_settle_derivative_seconds (from time_to_settle_derivative)</li> </ul> </li> <li> <code>DataFrame</code>           \u2013            <ul> <li>rise_time_seconds (from rise_time)</li> </ul> </li> <li> <code>DataFrame</code>           \u2013            <ul> <li>overshoot_abs, overshoot_pct (from overshoot_metrics)</li> </ul> </li> <li> <code>DataFrame</code>           \u2013            <ul> <li>undershoot_abs, undershoot_pct</li> </ul> </li> <li> <code>DataFrame</code>           \u2013            <ul> <li>oscillation_count, oscillation_amplitude, oscillation_freq_hz</li> </ul> </li> <li> <code>DataFrame</code>           \u2013            <ul> <li>decay_rate_lambda, fit_quality_r2</li> </ul> </li> <li> <code>DataFrame</code>           \u2013            <ul> <li>steady_state_error (final error in window)</li> </ul> </li> </ul> Source code in <code>src/ts_shape/events/engineering/setpoint_events.py</code> <pre><code>def control_quality_metrics(\n    self,\n    actual_uuid: str,\n    *,\n    tol: float = 0.0,\n    settle_pct: Optional[float] = None,\n    hold: str = \"0s\",\n    lookahead: str = \"10m\",\n    rate_threshold: float = 0.01,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Comprehensive control quality metrics combining multiple performance indicators.\n\n    Computes all available metrics for each setpoint change and returns them in a single DataFrame.\n    This includes: settling time, rise time, overshoot, undershoot, oscillations, and decay characteristics.\n\n    Args:\n        actual_uuid: UUID of the actual/process value signal\n        tol: Absolute tolerance for settling (used if settle_pct is None)\n        settle_pct: Percentage-based tolerance for settling\n        hold: Minimum duration to confirm settling\n        lookahead: Time window for all analyses\n        rate_threshold: Rate threshold for derivative-based settling\n\n    Returns:\n        DataFrame with comprehensive metrics including:\n        - start, uuid, is_delta\n        - t_settle_seconds, settled (from time_to_settle)\n        - t_settle_derivative_seconds (from time_to_settle_derivative)\n        - rise_time_seconds (from rise_time)\n        - overshoot_abs, overshoot_pct (from overshoot_metrics)\n        - undershoot_abs, undershoot_pct\n        - oscillation_count, oscillation_amplitude, oscillation_freq_hz\n        - decay_rate_lambda, fit_quality_r2\n        - steady_state_error (final error in window)\n    \"\"\"\n    if self.sp.empty:\n        return pd.DataFrame(\n            columns=[\n                \"start\",\n                \"uuid\",\n                \"is_delta\",\n                \"t_settle_seconds\",\n                \"settled\",\n                \"t_settle_derivative_seconds\",\n                \"rise_time_seconds\",\n                \"overshoot_abs\",\n                \"overshoot_pct\",\n                \"undershoot_abs\",\n                \"undershoot_pct\",\n                \"oscillation_count\",\n                \"oscillation_amplitude\",\n                \"oscillation_freq_hz\",\n                \"decay_rate_lambda\",\n                \"fit_quality_r2\",\n                \"steady_state_error\",\n            ]\n        )\n\n    # Compute all individual metrics using cached actual data\n    settle_df = self.time_to_settle(\n        actual_uuid, tol=tol, settle_pct=settle_pct, hold=hold, lookahead=lookahead\n    )\n    settle_deriv_df = self.time_to_settle_derivative(\n        actual_uuid, rate_threshold=rate_threshold, lookahead=lookahead, hold=hold\n    )\n    rise_df = self.rise_time(actual_uuid, lookahead=lookahead)\n    overshoot_df = self.overshoot_metrics(actual_uuid, window=lookahead)\n    decay_df = self.decay_rate(actual_uuid, lookahead=lookahead)\n    freq_df = self.oscillation_frequency(actual_uuid, window=lookahead)\n\n    # Compute steady-state error\n    actual = self._get_actual(actual_uuid)\n    look_td = pd.to_timedelta(lookahead)\n\n    sp = self.sp[[self.time_column, self.value_column]].copy()\n    sp[\"prev\"] = sp[self.value_column].shift(1)\n    sp[\"delta\"] = sp[self.value_column] - sp[\"prev\"]\n    changes = sp.loc[sp[\"delta\"].abs() &gt; 0, [self.time_column, self.value_column]].reset_index(drop=True)\n\n    ss_error_rows: List[Dict[str, Any]] = []\n    for _, c in changes.iterrows():\n        t0 = c[self.time_column]\n        s_new = float(c[self.value_column])\n        win = actual[(actual[self.time_column] &gt;= t0) &amp; (actual[self.time_column] &lt;= t0 + look_td)]\n\n        if win.empty:\n            ss_error = None\n        else:\n            # Use last 10% of window for steady-state\n            n_points = len(win)\n            last_10pct = win.iloc[int(n_points * 0.9):]\n            if len(last_10pct) &gt; 0:\n                ss_error = float((last_10pct[self.value_column] - s_new).abs().mean())\n            else:\n                ss_error = None\n\n        ss_error_rows.append({\"start\": t0, \"steady_state_error\": ss_error})\n\n    ss_error_df = pd.DataFrame(ss_error_rows)\n\n    # Merge all metrics on 'start'\n    result = settle_df.copy()\n\n    # Merge settle_deriv\n    result = result.merge(\n        settle_deriv_df[[\"start\", \"t_settle_seconds\"]].rename(\n            columns={\"t_settle_seconds\": \"t_settle_derivative_seconds\"}\n        ),\n        on=\"start\",\n        how=\"left\",\n    )\n\n    # Merge rise time\n    result = result.merge(\n        rise_df[[\"start\", \"rise_time_seconds\"]],\n        on=\"start\",\n        how=\"left\",\n    )\n\n    # Merge overshoot metrics\n    result = result.merge(\n        overshoot_df[\n            [\n                \"start\",\n                \"overshoot_abs\",\n                \"overshoot_pct\",\n                \"undershoot_abs\",\n                \"undershoot_pct\",\n                \"oscillation_count\",\n                \"oscillation_amplitude\",\n            ]\n        ],\n        on=\"start\",\n        how=\"left\",\n    )\n\n    # Merge frequency\n    result = result.merge(\n        freq_df[[\"start\", \"oscillation_freq_hz\"]],\n        on=\"start\",\n        how=\"left\",\n    )\n\n    # Merge decay\n    result = result.merge(\n        decay_df[[\"start\", \"decay_rate_lambda\", \"fit_quality_r2\"]],\n        on=\"start\",\n        how=\"left\",\n    )\n\n    # Merge steady-state error\n    result = result.merge(\n        ss_error_df[[\"start\", \"steady_state_error\"]],\n        on=\"start\",\n        how=\"left\",\n    )\n\n    return result\n</code></pre>"},{"location":"reference/ts_shape/events/engineering/setpoint_events/#ts_shape.events.engineering.setpoint_events.SetpointChangeEvents.control_quality_metrics(actual_uuid)","title":"<code>actual_uuid</code>","text":"(<code>str</code>)           \u2013            <p>UUID of the actual/process value signal</p>"},{"location":"reference/ts_shape/events/engineering/setpoint_events/#ts_shape.events.engineering.setpoint_events.SetpointChangeEvents.control_quality_metrics(tol)","title":"<code>tol</code>","text":"(<code>float</code>, default:                   <code>0.0</code> )           \u2013            <p>Absolute tolerance for settling (used if settle_pct is None)</p>"},{"location":"reference/ts_shape/events/engineering/setpoint_events/#ts_shape.events.engineering.setpoint_events.SetpointChangeEvents.control_quality_metrics(settle_pct)","title":"<code>settle_pct</code>","text":"(<code>Optional[float]</code>, default:                   <code>None</code> )           \u2013            <p>Percentage-based tolerance for settling</p>"},{"location":"reference/ts_shape/events/engineering/setpoint_events/#ts_shape.events.engineering.setpoint_events.SetpointChangeEvents.control_quality_metrics(hold)","title":"<code>hold</code>","text":"(<code>str</code>, default:                   <code>'0s'</code> )           \u2013            <p>Minimum duration to confirm settling</p>"},{"location":"reference/ts_shape/events/engineering/setpoint_events/#ts_shape.events.engineering.setpoint_events.SetpointChangeEvents.control_quality_metrics(lookahead)","title":"<code>lookahead</code>","text":"(<code>str</code>, default:                   <code>'10m'</code> )           \u2013            <p>Time window for all analyses</p>"},{"location":"reference/ts_shape/events/engineering/setpoint_events/#ts_shape.events.engineering.setpoint_events.SetpointChangeEvents.control_quality_metrics(rate_threshold)","title":"<code>rate_threshold</code>","text":"(<code>float</code>, default:                   <code>0.01</code> )           \u2013            <p>Rate threshold for derivative-based settling</p>"},{"location":"reference/ts_shape/events/engineering/setpoint_events/#ts_shape.events.engineering.setpoint_events.SetpointChangeEvents.decay_rate","title":"decay_rate","text":"<pre><code>decay_rate(actual_uuid: str, *, lookahead: str = '10m', min_points: int = 5) -&gt; DataFrame\n</code></pre> <p>Estimate exponential decay rate of the settling behavior. Fits error(t) = A * exp(-lambda * t) and returns lambda.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>DataFrame with columns: start, uuid, is_delta, decay_rate_lambda, fit_quality_r2.</p> </li> </ul> Source code in <code>src/ts_shape/events/engineering/setpoint_events.py</code> <pre><code>def decay_rate(\n    self,\n    actual_uuid: str,\n    *,\n    lookahead: str = \"10m\",\n    min_points: int = 5,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Estimate exponential decay rate of the settling behavior.\n    Fits error(t) = A * exp(-lambda * t) and returns lambda.\n\n    Args:\n        actual_uuid: UUID of the actual/process value signal\n        lookahead: Time window for analysis\n        min_points: Minimum number of points required for fitting\n\n    Returns:\n        DataFrame with columns: start, uuid, is_delta, decay_rate_lambda, fit_quality_r2.\n    \"\"\"\n    if self.sp.empty:\n        return pd.DataFrame(\n            columns=[\"start\", \"uuid\", \"is_delta\", \"decay_rate_lambda\", \"fit_quality_r2\"]\n        )\n\n    # Use cached actual data\n    actual = self._get_actual(actual_uuid)\n    look_td = pd.to_timedelta(lookahead)\n\n    sp = self.sp[[self.time_column, self.value_column]].copy()\n    sp[\"prev\"] = sp[self.value_column].shift(1)\n    sp[\"delta\"] = sp[self.value_column] - sp[\"prev\"]\n    changes = sp.loc[sp[\"delta\"].abs() &gt; 0, [self.time_column, self.value_column]].reset_index(drop=True)\n\n    rows: List[Dict[str, Any]] = []\n    for _, c in changes.iterrows():\n        t0 = c[self.time_column]\n        s_new = float(c[self.value_column])\n        window = actual[(actual[self.time_column] &gt;= t0) &amp; (actual[self.time_column] &lt;= t0 + look_td)]\n\n        if window.empty or len(window) &lt; min_points:\n            rows.append(\n                {\n                    \"start\": t0,\n                    \"uuid\": self.event_uuid,\n                    \"is_delta\": True,\n                    \"decay_rate_lambda\": None,\n                    \"fit_quality_r2\": None,\n                }\n            )\n            continue\n\n        # Calculate error and time since change\n        err = (window[self.value_column] - s_new).abs()\n        t_sec = (window[self.time_column] - t0).dt.total_seconds()\n\n        # Filter out zero or near-zero errors for log fit\n        valid_mask = err &gt; 1e-6\n        if valid_mask.sum() &lt; min_points:\n            rows.append(\n                {\n                    \"start\": t0,\n                    \"uuid\": self.event_uuid,\n                    \"is_delta\": True,\n                    \"decay_rate_lambda\": None,\n                    \"fit_quality_r2\": None,\n                }\n            )\n            continue\n\n        err_valid = err[valid_mask]\n        t_valid = t_sec[valid_mask]\n\n        try:\n            # Linear fit to log(error) vs time: log(err) = log(A) - lambda*t\n            log_err = np.log(err_valid)\n            coeffs = np.polyfit(t_valid, log_err, 1)\n            decay_lambda = -coeffs[0]  # negative slope\n\n            # Calculate R^2\n            log_err_pred = np.polyval(coeffs, t_valid)\n            ss_res = np.sum((log_err - log_err_pred) ** 2)\n            ss_tot = np.sum((log_err - log_err.mean()) ** 2)\n            r2 = 1 - (ss_res / ss_tot) if ss_tot &gt; 0 else 0.0\n\n            rows.append(\n                {\n                    \"start\": t0,\n                    \"uuid\": self.event_uuid,\n                    \"is_delta\": True,\n                    \"decay_rate_lambda\": float(decay_lambda) if decay_lambda &gt; 0 else None,\n                    \"fit_quality_r2\": float(r2),\n                }\n            )\n        except Exception:\n            rows.append(\n                {\n                    \"start\": t0,\n                    \"uuid\": self.event_uuid,\n                    \"is_delta\": True,\n                    \"decay_rate_lambda\": None,\n                    \"fit_quality_r2\": None,\n                }\n            )\n\n    return pd.DataFrame(rows)\n</code></pre>"},{"location":"reference/ts_shape/events/engineering/setpoint_events/#ts_shape.events.engineering.setpoint_events.SetpointChangeEvents.decay_rate(actual_uuid)","title":"<code>actual_uuid</code>","text":"(<code>str</code>)           \u2013            <p>UUID of the actual/process value signal</p>"},{"location":"reference/ts_shape/events/engineering/setpoint_events/#ts_shape.events.engineering.setpoint_events.SetpointChangeEvents.decay_rate(lookahead)","title":"<code>lookahead</code>","text":"(<code>str</code>, default:                   <code>'10m'</code> )           \u2013            <p>Time window for analysis</p>"},{"location":"reference/ts_shape/events/engineering/setpoint_events/#ts_shape.events.engineering.setpoint_events.SetpointChangeEvents.decay_rate(min_points)","title":"<code>min_points</code>","text":"(<code>int</code>, default:                   <code>5</code> )           \u2013            <p>Minimum number of points required for fitting</p>"},{"location":"reference/ts_shape/events/engineering/setpoint_events/#ts_shape.events.engineering.setpoint_events.SetpointChangeEvents.detect_setpoint_changes","title":"detect_setpoint_changes","text":"<pre><code>detect_setpoint_changes(*, min_delta: float = 0.0, min_rate: Optional[float] = None, min_hold: str = '0s', min_duration: str = '0s') -&gt; DataFrame\n</code></pre> <p>Unified setpoint change table (steps + ramps) with standardized columns.</p> Source code in <code>src/ts_shape/events/engineering/setpoint_events.py</code> <pre><code>def detect_setpoint_changes(\n    self,\n    *,\n    min_delta: float = 0.0,\n    min_rate: Optional[float] = None,\n    min_hold: str = \"0s\",\n    min_duration: str = \"0s\",\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Unified setpoint change table (steps + ramps) with standardized columns.\n    \"\"\"\n    steps = self.detect_setpoint_steps(min_delta=min_delta, min_hold=min_hold)\n    ramps = (\n        self.detect_setpoint_ramps(min_rate=min_rate, min_duration=min_duration)\n        if min_rate is not None\n        else pd.DataFrame(columns=[\"start\", \"end\", \"uuid\", \"is_delta\", \"change_type\", \"avg_rate\", \"delta\"])\n    )\n    # ensure uniform columns\n    if not steps.empty:\n        steps = steps.assign(avg_rate=None, delta=None)[\n            [\n                \"start\",\n                \"end\",\n                \"uuid\",\n                \"is_delta\",\n                \"change_type\",\n                \"magnitude\",\n                \"prev_level\",\n                \"new_level\",\n                \"avg_rate\",\n                \"delta\",\n            ]\n        ]\n    if not ramps.empty:\n        ramps = ramps.assign(magnitude=None, prev_level=None, new_level=None)[\n            [\n                \"start\",\n                \"end\",\n                \"uuid\",\n                \"is_delta\",\n                \"change_type\",\n                \"magnitude\",\n                \"prev_level\",\n                \"new_level\",\n                \"avg_rate\",\n                \"delta\",\n            ]\n        ]\n    frames = [df for df in (steps, ramps) if not df.empty]\n    combined = pd.concat(frames, ignore_index=True) if frames else pd.DataFrame(\n        columns=[\n            \"start\",\n            \"end\",\n            \"uuid\",\n            \"is_delta\",\n            \"change_type\",\n            \"magnitude\",\n            \"prev_level\",\n            \"new_level\",\n            \"avg_rate\",\n            \"delta\",\n        ]\n    )\n    return combined.sort_values([\"start\", \"end\"]) if not combined.empty else combined\n</code></pre>"},{"location":"reference/ts_shape/events/engineering/setpoint_events/#ts_shape.events.engineering.setpoint_events.SetpointChangeEvents.detect_setpoint_ramps","title":"detect_setpoint_ramps","text":"<pre><code>detect_setpoint_ramps(min_rate: float, min_duration: str = '0s') -&gt; DataFrame\n</code></pre> <p>Interval events where |dS/dt| &gt;= min_rate for at least <code>min_duration</code>.</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>DataFrame with columns: start, end, uuid, is_delta, change_type='ramp',</p> </li> <li> <code>DataFrame</code>           \u2013            <p>avg_rate, delta.</p> </li> </ul> Source code in <code>src/ts_shape/events/engineering/setpoint_events.py</code> <pre><code>def detect_setpoint_ramps(self, min_rate: float, min_duration: str = \"0s\") -&gt; pd.DataFrame:\n    \"\"\"\n    Interval events where |dS/dt| &gt;= min_rate for at least `min_duration`.\n\n    Returns:\n        DataFrame with columns: start, end, uuid, is_delta, change_type='ramp',\n        avg_rate, delta.\n    \"\"\"\n    if self.sp.empty:\n        return pd.DataFrame(\n            columns=[\"start\", \"end\", \"uuid\", \"is_delta\", \"change_type\", \"avg_rate\", \"delta\"]\n        )\n\n    sp = self.sp[[self.time_column, self.value_column]].copy()\n    sp[\"dt_s\"] = sp[self.time_column].diff().dt.total_seconds()\n    sp[\"dv\"] = sp[self.value_column].diff()\n    sp[\"rate\"] = sp[\"dv\"] / sp[\"dt_s\"]\n    rate_mask = sp[\"rate\"].abs() &gt;= float(min_rate)\n\n    # group contiguous True segments\n    group_id = (rate_mask != rate_mask.shift()).cumsum()\n    events: List[Dict[str, Any]] = []\n    min_d = pd.to_timedelta(min_duration)\n    for gid, seg in sp.groupby(group_id):\n        seg_mask_true = rate_mask.loc[seg.index]\n        if not seg_mask_true.any():\n            continue\n        # boundaries\n        start_time = seg.loc[seg_mask_true, self.time_column].iloc[0]\n        end_time = seg.loc[seg_mask_true, self.time_column].iloc[-1]\n        if (end_time - start_time) &lt; min_d:\n            continue\n        avg_rate = seg.loc[seg_mask_true, \"rate\"].mean()\n        delta = seg.loc[seg_mask_true, \"dv\"].sum()\n        events.append(\n            {\n                \"start\": start_time,\n                \"end\": end_time,\n                \"uuid\": self.event_uuid,\n                \"is_delta\": True,\n                \"change_type\": \"ramp\",\n                \"avg_rate\": float(avg_rate) if pd.notna(avg_rate) else None,\n                \"delta\": float(delta) if pd.notna(delta) else None,\n            }\n        )\n\n    return pd.DataFrame(events)\n</code></pre>"},{"location":"reference/ts_shape/events/engineering/setpoint_events/#ts_shape.events.engineering.setpoint_events.SetpointChangeEvents.detect_setpoint_steps","title":"detect_setpoint_steps","text":"<pre><code>detect_setpoint_steps(min_delta: float, min_hold: str = '0s', filter_noise: bool = False, noise_threshold: float = 0.01) -&gt; DataFrame\n</code></pre> <p>Point events at times where the setpoint changes by &gt;= min_delta and the new level holds for at least <code>min_hold</code> (no subsequent change within that time).</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>DataFrame with columns: start, end (== start), uuid, is_delta,</p> </li> <li> <code>DataFrame</code>           \u2013            <p>change_type='step', magnitude, prev_level, new_level.</p> </li> </ul> Source code in <code>src/ts_shape/events/engineering/setpoint_events.py</code> <pre><code>def detect_setpoint_steps(\n    self,\n    min_delta: float,\n    min_hold: str = \"0s\",\n    filter_noise: bool = False,\n    noise_threshold: float = 0.01,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Point events at times where the setpoint changes by &gt;= min_delta and the\n    new level holds for at least `min_hold` (no subsequent change within that time).\n\n    Args:\n        min_delta: Minimum magnitude of change to detect\n        min_hold: Minimum duration the new level must hold\n        filter_noise: If True, filter out changes smaller than noise_threshold\n        noise_threshold: Threshold for noise filtering (absolute value)\n\n    Returns:\n        DataFrame with columns: start, end (== start), uuid, is_delta,\n        change_type='step', magnitude, prev_level, new_level.\n    \"\"\"\n    if self.sp.empty:\n        return pd.DataFrame(\n            columns=[\n                \"start\",\n                \"end\",\n                \"uuid\",\n                \"is_delta\",\n                \"change_type\",\n                \"magnitude\",\n                \"prev_level\",\n                \"new_level\",\n            ]\n        )\n\n    sp = self.sp[[self.time_column, self.value_column]].copy()\n\n    # Apply noise filtering if requested\n    if filter_noise:\n        # Group consecutive values within noise_threshold and use mean\n        sp[\"filtered_value\"] = sp[self.value_column]\n        current_group = sp[\"filtered_value\"].iloc[0]\n        for i in range(1, len(sp)):\n            if abs(sp[\"filtered_value\"].iloc[i] - current_group) &lt;= noise_threshold:\n                sp.loc[sp.index[i], \"filtered_value\"] = current_group\n            else:\n                current_group = sp[\"filtered_value\"].iloc[i]\n        sp[self.value_column] = sp[\"filtered_value\"]\n        sp = sp.drop(columns=[\"filtered_value\"])\n\n    sp[\"prev\"] = sp[self.value_column].shift(1)\n    sp[\"delta\"] = sp[self.value_column] - sp[\"prev\"]\n    change_mask = sp[\"delta\"].abs() &gt;= float(min_delta)\n\n    # hold condition: next change must be after min_hold\n    change_times = sp.loc[change_mask, self.time_column]\n    min_hold_td = pd.to_timedelta(min_hold)\n    next_change_times = change_times.shift(-1)\n    hold_ok = (next_change_times - change_times &gt;= min_hold_td) | next_change_times.isna()\n    valid_change_times = change_times[hold_ok]\n\n    rows: List[Dict[str, Any]] = []\n    for t in valid_change_times:\n        row = sp.loc[sp[self.time_column] == t].iloc[0]\n        rows.append(\n            {\n                \"start\": t,\n                \"end\": t,\n                \"uuid\": self.event_uuid,\n                \"is_delta\": True,\n                \"change_type\": \"step\",\n                \"magnitude\": float(row[\"delta\"]),\n                \"prev_level\": float(row[\"prev\"]) if pd.notna(row[\"prev\"]) else None,\n                \"new_level\": float(row[self.value_column]),\n            }\n        )\n\n    return pd.DataFrame(rows)\n</code></pre>"},{"location":"reference/ts_shape/events/engineering/setpoint_events/#ts_shape.events.engineering.setpoint_events.SetpointChangeEvents.detect_setpoint_steps(min_delta)","title":"<code>min_delta</code>","text":"(<code>float</code>)           \u2013            <p>Minimum magnitude of change to detect</p>"},{"location":"reference/ts_shape/events/engineering/setpoint_events/#ts_shape.events.engineering.setpoint_events.SetpointChangeEvents.detect_setpoint_steps(min_hold)","title":"<code>min_hold</code>","text":"(<code>str</code>, default:                   <code>'0s'</code> )           \u2013            <p>Minimum duration the new level must hold</p>"},{"location":"reference/ts_shape/events/engineering/setpoint_events/#ts_shape.events.engineering.setpoint_events.SetpointChangeEvents.detect_setpoint_steps(filter_noise)","title":"<code>filter_noise</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, filter out changes smaller than noise_threshold</p>"},{"location":"reference/ts_shape/events/engineering/setpoint_events/#ts_shape.events.engineering.setpoint_events.SetpointChangeEvents.detect_setpoint_steps(noise_threshold)","title":"<code>noise_threshold</code>","text":"(<code>float</code>, default:                   <code>0.01</code> )           \u2013            <p>Threshold for noise filtering (absolute value)</p>"},{"location":"reference/ts_shape/events/engineering/setpoint_events/#ts_shape.events.engineering.setpoint_events.SetpointChangeEvents.get_dataframe","title":"get_dataframe","text":"<pre><code>get_dataframe() -&gt; DataFrame\n</code></pre> <p>Returns the processed DataFrame.</p> Source code in <code>src/ts_shape/utils/base.py</code> <pre><code>def get_dataframe(self) -&gt; pd.DataFrame:\n    \"\"\"Returns the processed DataFrame.\"\"\"\n    return self.dataframe\n</code></pre>"},{"location":"reference/ts_shape/events/engineering/setpoint_events/#ts_shape.events.engineering.setpoint_events.SetpointChangeEvents.oscillation_frequency","title":"oscillation_frequency","text":"<pre><code>oscillation_frequency(actual_uuid: str, *, window: str = '10m', min_oscillations: int = 2) -&gt; DataFrame\n</code></pre> <p>Estimate the frequency of oscillations during settling. Counts zero crossings and estimates period.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>DataFrame with columns: start, uuid, is_delta, oscillation_freq_hz, period_seconds.</p> </li> </ul> Source code in <code>src/ts_shape/events/engineering/setpoint_events.py</code> <pre><code>def oscillation_frequency(\n    self,\n    actual_uuid: str,\n    *,\n    window: str = \"10m\",\n    min_oscillations: int = 2,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Estimate the frequency of oscillations during settling.\n    Counts zero crossings and estimates period.\n\n    Args:\n        actual_uuid: UUID of the actual/process value signal\n        window: Time window for analysis\n        min_oscillations: Minimum number of oscillations to compute frequency\n\n    Returns:\n        DataFrame with columns: start, uuid, is_delta, oscillation_freq_hz, period_seconds.\n    \"\"\"\n    if self.sp.empty:\n        return pd.DataFrame(\n            columns=[\"start\", \"uuid\", \"is_delta\", \"oscillation_freq_hz\", \"period_seconds\"]\n        )\n\n    # Use cached actual data\n    actual = self._get_actual(actual_uuid)\n    look_td = pd.to_timedelta(window)\n\n    sp = self.sp[[self.time_column, self.value_column]].copy()\n    sp[\"prev\"] = sp[self.value_column].shift(1)\n    sp[\"delta\"] = sp[self.value_column] - sp[\"prev\"]\n    changes = sp.loc[sp[\"delta\"].abs() &gt; 0, [self.time_column, self.value_column]].reset_index(drop=True)\n\n    rows: List[Dict[str, Any]] = []\n    for _, c in changes.iterrows():\n        t0 = c[self.time_column]\n        s_new = float(c[self.value_column])\n        win = actual[(actual[self.time_column] &gt;= t0) &amp; (actual[self.time_column] &lt;= t0 + look_td)]\n\n        if win.empty:\n            rows.append(\n                {\n                    \"start\": t0,\n                    \"uuid\": self.event_uuid,\n                    \"is_delta\": True,\n                    \"oscillation_freq_hz\": None,\n                    \"period_seconds\": None,\n                }\n            )\n            continue\n\n        # Calculate error\n        err = win[self.value_column] - s_new\n\n        # Detect zero crossings\n        err_sign = np.sign(err)\n        sign_changes = np.where(np.diff(err_sign) != 0)[0]\n        num_crossings = len(sign_changes)\n\n        if num_crossings &gt;= min_oscillations * 2:  # each oscillation has 2 crossings\n            # Calculate time between crossings\n            crossing_times = win[self.time_column].iloc[sign_changes]\n            time_diffs = crossing_times.diff().dt.total_seconds().dropna()\n\n            if len(time_diffs) &gt; 0:\n                # Average period (2 crossings = 1 period)\n                avg_half_period = time_diffs.mean()\n                period = avg_half_period * 2\n                freq = 1.0 / period if period &gt; 0 else None\n\n                rows.append(\n                    {\n                        \"start\": t0,\n                        \"uuid\": self.event_uuid,\n                        \"is_delta\": True,\n                        \"oscillation_freq_hz\": float(freq) if freq is not None else None,\n                        \"period_seconds\": float(period),\n                    }\n                )\n            else:\n                rows.append(\n                    {\n                        \"start\": t0,\n                        \"uuid\": self.event_uuid,\n                        \"is_delta\": True,\n                        \"oscillation_freq_hz\": None,\n                        \"period_seconds\": None,\n                    }\n                )\n        else:\n            rows.append(\n                {\n                    \"start\": t0,\n                    \"uuid\": self.event_uuid,\n                    \"is_delta\": True,\n                    \"oscillation_freq_hz\": None,\n                    \"period_seconds\": None,\n                }\n            )\n\n    return pd.DataFrame(rows)\n</code></pre>"},{"location":"reference/ts_shape/events/engineering/setpoint_events/#ts_shape.events.engineering.setpoint_events.SetpointChangeEvents.oscillation_frequency(actual_uuid)","title":"<code>actual_uuid</code>","text":"(<code>str</code>)           \u2013            <p>UUID of the actual/process value signal</p>"},{"location":"reference/ts_shape/events/engineering/setpoint_events/#ts_shape.events.engineering.setpoint_events.SetpointChangeEvents.oscillation_frequency(window)","title":"<code>window</code>","text":"(<code>str</code>, default:                   <code>'10m'</code> )           \u2013            <p>Time window for analysis</p>"},{"location":"reference/ts_shape/events/engineering/setpoint_events/#ts_shape.events.engineering.setpoint_events.SetpointChangeEvents.oscillation_frequency(min_oscillations)","title":"<code>min_oscillations</code>","text":"(<code>int</code>, default:                   <code>2</code> )           \u2013            <p>Minimum number of oscillations to compute frequency</p>"},{"location":"reference/ts_shape/events/engineering/setpoint_events/#ts_shape.events.engineering.setpoint_events.SetpointChangeEvents.overshoot_metrics","title":"overshoot_metrics","text":"<pre><code>overshoot_metrics(actual_uuid: str, *, window: str = '10m') -&gt; DataFrame\n</code></pre> <p>For each change, compute peak overshoot, undershoot, and oscillation metrics relative to the new setpoint within a lookahead window.</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>DataFrame with columns: start, uuid, is_delta, overshoot_abs, overshoot_pct,</p> </li> <li> <code>DataFrame</code>           \u2013            <p>t_peak_seconds, undershoot_abs, undershoot_pct, t_undershoot_seconds,</p> </li> <li> <code>DataFrame</code>           \u2013            <p>oscillation_count, oscillation_amplitude.</p> </li> </ul> Source code in <code>src/ts_shape/events/engineering/setpoint_events.py</code> <pre><code>def overshoot_metrics(\n    self,\n    actual_uuid: str,\n    *,\n    window: str = \"10m\",\n) -&gt; pd.DataFrame:\n    \"\"\"\n    For each change, compute peak overshoot, undershoot, and oscillation metrics\n    relative to the new setpoint within a lookahead window.\n\n    Returns:\n        DataFrame with columns: start, uuid, is_delta, overshoot_abs, overshoot_pct,\n        t_peak_seconds, undershoot_abs, undershoot_pct, t_undershoot_seconds,\n        oscillation_count, oscillation_amplitude.\n    \"\"\"\n    if self.sp.empty:\n        return pd.DataFrame(\n            columns=[\n                \"start\",\n                \"uuid\",\n                \"is_delta\",\n                \"overshoot_abs\",\n                \"overshoot_pct\",\n                \"t_peak_seconds\",\n                \"undershoot_abs\",\n                \"undershoot_pct\",\n                \"t_undershoot_seconds\",\n                \"oscillation_count\",\n                \"oscillation_amplitude\",\n            ]\n        )\n\n    # Use cached actual data\n    actual = self._get_actual(actual_uuid)\n    look_td = pd.to_timedelta(window)\n\n    sp = self.sp[[self.time_column, self.value_column]].copy()\n    sp[\"prev\"] = sp[self.value_column].shift(1)\n    sp[\"delta\"] = sp[self.value_column] - sp[\"prev\"]\n    changes = sp.loc[sp[\"delta\"].abs() &gt; 0, [self.time_column, self.value_column, \"delta\", \"prev\"]]\n\n    out_rows: List[Dict[str, Any]] = []\n    for _, r in changes.iterrows():\n        t0 = r[self.time_column]\n        s_new = float(r[self.value_column])\n        s_prev = float(r[\"prev\"]) if pd.notna(r[\"prev\"]) else s_new\n        delta = float(r[\"delta\"]) if pd.notna(r[\"delta\"]) else 0.0\n        win = actual[(actual[self.time_column] &gt;= t0) &amp; (actual[self.time_column] &lt;= t0 + look_td)]\n\n        if win.empty:\n            out_rows.append(\n                {\n                    \"start\": t0,\n                    \"uuid\": self.event_uuid,\n                    \"is_delta\": True,\n                    \"overshoot_abs\": None,\n                    \"overshoot_pct\": None,\n                    \"t_peak_seconds\": None,\n                    \"undershoot_abs\": None,\n                    \"undershoot_pct\": None,\n                    \"t_undershoot_seconds\": None,\n                    \"oscillation_count\": None,\n                    \"oscillation_amplitude\": None,\n                }\n            )\n            continue\n\n        err = win[self.value_column] - s_new\n\n        # Overshoot (in direction of change)\n        if delta &gt;= 0:\n            peak = err.max()\n            t_peak = win.loc[err.idxmax(), self.time_column] if peak &gt; 0 else None\n            # Undershoot (opposite direction)\n            undershoot_val = err.min()\n            t_undershoot = win.loc[err.idxmin(), self.time_column] if undershoot_val &lt; 0 else None\n        else:\n            peak = -err.min()  # magnitude for downward step\n            t_peak = win.loc[err.idxmin(), self.time_column] if err.min() &lt; 0 else None\n            # Undershoot (opposite direction)\n            undershoot_val = -err.max()\n            t_undershoot = win.loc[err.idxmax(), self.time_column] if err.max() &gt; 0 else None\n\n        overshoot_abs = float(peak) if pd.notna(peak) and peak &gt; 0 else 0.0\n        overshoot_pct = (overshoot_abs / abs(delta)) if (delta != 0 and overshoot_abs is not None) else None\n\n        undershoot_abs = float(abs(undershoot_val)) if pd.notna(undershoot_val) and abs(undershoot_val) &gt; 0 else 0.0\n        undershoot_pct = (undershoot_abs / abs(delta)) if (delta != 0 and undershoot_abs is not None) else None\n\n        # Oscillation detection: count zero crossings\n        err_sign = np.sign(err)\n        sign_changes = (err_sign.diff() != 0).sum() - 1  # -1 to exclude initial transition\n        oscillation_count = max(0, int(sign_changes))\n\n        # Oscillation amplitude: average of peak deviations\n        oscillation_amplitude = float(err.abs().mean()) if len(err) &gt; 0 else None\n\n        out_rows.append(\n            {\n                \"start\": t0,\n                \"uuid\": self.event_uuid,\n                \"is_delta\": True,\n                \"overshoot_abs\": overshoot_abs if overshoot_abs &gt; 0 else None,\n                \"overshoot_pct\": float(overshoot_pct) if overshoot_pct is not None and overshoot_abs &gt; 0 else None,\n                \"t_peak_seconds\": (t_peak - t0).total_seconds() if t_peak is not None else None,\n                \"undershoot_abs\": undershoot_abs if undershoot_abs &gt; 0 else None,\n                \"undershoot_pct\": float(undershoot_pct) if undershoot_pct is not None and undershoot_abs &gt; 0 else None,\n                \"t_undershoot_seconds\": (t_undershoot - t0).total_seconds() if t_undershoot is not None else None,\n                \"oscillation_count\": oscillation_count if oscillation_count &gt; 0 else 0,\n                \"oscillation_amplitude\": oscillation_amplitude,\n            }\n        )\n\n    return pd.DataFrame(out_rows)\n</code></pre>"},{"location":"reference/ts_shape/events/engineering/setpoint_events/#ts_shape.events.engineering.setpoint_events.SetpointChangeEvents.rise_time","title":"rise_time","text":"<pre><code>rise_time(actual_uuid: str, *, start_pct: float = 0.1, end_pct: float = 0.9, lookahead: str = '10m') -&gt; DataFrame\n</code></pre> <p>Compute rise time: time for actual to go from start_pct to end_pct of the setpoint change. Typically measured from 10% to 90% of the final value.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>DataFrame with columns: start, uuid, is_delta, rise_time_seconds, reached_end.</p> </li> </ul> Source code in <code>src/ts_shape/events/engineering/setpoint_events.py</code> <pre><code>def rise_time(\n    self,\n    actual_uuid: str,\n    *,\n    start_pct: float = 0.1,\n    end_pct: float = 0.9,\n    lookahead: str = \"10m\",\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Compute rise time: time for actual to go from start_pct to end_pct of the setpoint change.\n    Typically measured from 10% to 90% of the final value.\n\n    Args:\n        actual_uuid: UUID of the actual/process value signal\n        start_pct: Starting percentage of change (e.g., 0.1 for 10%)\n        end_pct: Ending percentage of change (e.g., 0.9 for 90%)\n        lookahead: Maximum time window to search\n\n    Returns:\n        DataFrame with columns: start, uuid, is_delta, rise_time_seconds, reached_end.\n    \"\"\"\n    if self.sp.empty:\n        return pd.DataFrame(\n            columns=[\"start\", \"uuid\", \"is_delta\", \"rise_time_seconds\", \"reached_end\"]\n        )\n\n    # Use cached actual data\n    actual = self._get_actual(actual_uuid)\n    look_td = pd.to_timedelta(lookahead)\n\n    sp = self.sp[[self.time_column, self.value_column]].copy()\n    sp[\"prev\"] = sp[self.value_column].shift(1)\n    sp[\"delta\"] = sp[self.value_column] - sp[\"prev\"]\n    changes = sp.loc[sp[\"delta\"].abs() &gt; 0, [self.time_column, self.value_column, \"delta\", \"prev\"]].reset_index(drop=True)\n\n    rows: List[Dict[str, Any]] = []\n    for _, c in changes.iterrows():\n        t0 = c[self.time_column]\n        s_new = float(c[self.value_column])\n        s_prev = float(c[\"prev\"]) if pd.notna(c[\"prev\"]) else s_new\n        delta = float(c[\"delta\"]) if pd.notna(c[\"delta\"]) else 0.0\n\n        window = actual[(actual[self.time_column] &gt;= t0) &amp; (actual[self.time_column] &lt;= t0 + look_td)]\n\n        if window.empty or delta == 0:\n            rows.append(\n                {\n                    \"start\": t0,\n                    \"uuid\": self.event_uuid,\n                    \"is_delta\": True,\n                    \"rise_time_seconds\": None,\n                    \"reached_end\": False,\n                }\n            )\n            continue\n\n        # Calculate target levels\n        start_level = s_prev + delta * start_pct\n        end_level = s_prev + delta * end_pct\n\n        # Find crossing times\n        t_start = None\n        t_end = None\n\n        if delta &gt; 0:\n            # Upward step\n            start_crossed = window[self.value_column] &gt;= start_level\n            end_crossed = window[self.value_column] &gt;= end_level\n        else:\n            # Downward step\n            start_crossed = window[self.value_column] &lt;= start_level\n            end_crossed = window[self.value_column] &lt;= end_level\n\n        if start_crossed.any():\n            t_start = window.loc[start_crossed.idxmax(), self.time_column]\n        if end_crossed.any():\n            t_end = window.loc[end_crossed.idxmax(), self.time_column]\n\n        if t_start is not None and t_end is not None:\n            rise_time_sec = (t_end - t_start).total_seconds()\n            reached_end = True\n        else:\n            rise_time_sec = None\n            reached_end = False\n\n        rows.append(\n            {\n                \"start\": t0,\n                \"uuid\": self.event_uuid,\n                \"is_delta\": True,\n                \"rise_time_seconds\": rise_time_sec,\n                \"reached_end\": reached_end,\n            }\n        )\n\n    return pd.DataFrame(rows)\n</code></pre>"},{"location":"reference/ts_shape/events/engineering/setpoint_events/#ts_shape.events.engineering.setpoint_events.SetpointChangeEvents.rise_time(actual_uuid)","title":"<code>actual_uuid</code>","text":"(<code>str</code>)           \u2013            <p>UUID of the actual/process value signal</p>"},{"location":"reference/ts_shape/events/engineering/setpoint_events/#ts_shape.events.engineering.setpoint_events.SetpointChangeEvents.rise_time(start_pct)","title":"<code>start_pct</code>","text":"(<code>float</code>, default:                   <code>0.1</code> )           \u2013            <p>Starting percentage of change (e.g., 0.1 for 10%)</p>"},{"location":"reference/ts_shape/events/engineering/setpoint_events/#ts_shape.events.engineering.setpoint_events.SetpointChangeEvents.rise_time(end_pct)","title":"<code>end_pct</code>","text":"(<code>float</code>, default:                   <code>0.9</code> )           \u2013            <p>Ending percentage of change (e.g., 0.9 for 90%)</p>"},{"location":"reference/ts_shape/events/engineering/setpoint_events/#ts_shape.events.engineering.setpoint_events.SetpointChangeEvents.rise_time(lookahead)","title":"<code>lookahead</code>","text":"(<code>str</code>, default:                   <code>'10m'</code> )           \u2013            <p>Maximum time window to search</p>"},{"location":"reference/ts_shape/events/engineering/setpoint_events/#ts_shape.events.engineering.setpoint_events.SetpointChangeEvents.time_to_settle","title":"time_to_settle","text":"<pre><code>time_to_settle(actual_uuid: str, *, tol: float = 0.0, settle_pct: Optional[float] = None, hold: str = '0s', lookahead: str = '10m') -&gt; DataFrame\n</code></pre> <p>For each setpoint change (any change), compute time until the actual signal is within \u00b1<code>tol</code> of the new setpoint for a continuous duration of <code>hold</code>.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>DataFrame with columns: start, uuid, is_delta, t_settle_seconds, settled.</p> </li> </ul> Source code in <code>src/ts_shape/events/engineering/setpoint_events.py</code> <pre><code>def time_to_settle(\n    self,\n    actual_uuid: str,\n    *,\n    tol: float = 0.0,\n    settle_pct: Optional[float] = None,\n    hold: str = \"0s\",\n    lookahead: str = \"10m\",\n) -&gt; pd.DataFrame:\n    \"\"\"\n    For each setpoint change (any change), compute time until the actual signal\n    is within \u00b1`tol` of the new setpoint for a continuous duration of `hold`.\n\n    Args:\n        actual_uuid: UUID of the actual/process value signal\n        tol: Absolute tolerance (used if settle_pct is None)\n        settle_pct: Percentage-based tolerance (e.g., 0.02 for 2% of step magnitude)\n        hold: Minimum duration the signal must stay within tolerance\n        lookahead: Maximum time window to search for settling\n\n    Returns:\n        DataFrame with columns: start, uuid, is_delta, t_settle_seconds, settled.\n    \"\"\"\n    if self.sp.empty:\n        return pd.DataFrame(columns=[\"start\", \"uuid\", \"is_delta\", \"t_settle_seconds\", \"settled\"])\n\n    # Use cached actual data\n    actual = self._get_actual(actual_uuid)\n    hold_td = pd.to_timedelta(hold)\n    look_td = pd.to_timedelta(lookahead)\n\n    # change instants\n    sp = self.sp[[self.time_column, self.value_column]].copy()\n    sp[\"prev\"] = sp[self.value_column].shift(1)\n    sp[\"delta\"] = sp[self.value_column] - sp[\"prev\"]\n    change_times = sp.loc[sp[\"delta\"].abs() &gt; 0, [self.time_column, self.value_column, \"delta\"]].reset_index(drop=True)\n\n    rows: List[Dict[str, Any]] = []\n    for _, c in change_times.iterrows():\n        t0 = c[self.time_column]\n        s_new = float(c[self.value_column])\n        delta = float(c[\"delta\"]) if pd.notna(c[\"delta\"]) else 0.0\n\n        # Calculate tolerance based on settle_pct or use absolute tol\n        if settle_pct is not None and delta != 0:\n            effective_tol = abs(delta) * settle_pct\n        else:\n            effective_tol = tol\n\n        window = actual[(actual[self.time_column] &gt;= t0) &amp; (actual[self.time_column] &lt;= t0 + look_td)]\n        if window.empty:\n            rows.append({\"start\": t0, \"uuid\": self.event_uuid, \"is_delta\": True, \"t_settle_seconds\": None, \"settled\": False})\n            continue\n        err = (window[self.value_column] - s_new).abs()\n        inside = err &lt;= effective_tol\n\n        # time to first entry within tolerance (ignores hold)\n        if inside.any():\n            first_idx = inside[inside].index[0]\n            t_enter = window.loc[first_idx, self.time_column]\n        else:\n            t_enter = None\n\n        # determine if any contiguous inside segment satisfies hold duration\n        settled = False\n        if inside.any():\n            gid = (inside.ne(inside.shift())).cumsum()\n            for _, seg in window.groupby(gid):\n                seg_inside = inside.loc[seg.index]\n                if not seg_inside.iloc[0]:\n                    continue\n                start_seg = seg[self.time_column].iloc[0]\n                end_seg = seg[self.time_column].iloc[-1]\n                if (end_seg - start_seg) &gt;= hold_td:\n                    settled = True\n                    break\n\n        rows.append(\n            {\n                \"start\": t0,\n                \"uuid\": self.event_uuid,\n                \"is_delta\": True,\n                \"t_settle_seconds\": (t_enter - t0).total_seconds() if t_enter is not None else None,\n                \"settled\": bool(settled),\n            }\n        )\n\n    return pd.DataFrame(rows)\n</code></pre>"},{"location":"reference/ts_shape/events/engineering/setpoint_events/#ts_shape.events.engineering.setpoint_events.SetpointChangeEvents.time_to_settle(actual_uuid)","title":"<code>actual_uuid</code>","text":"(<code>str</code>)           \u2013            <p>UUID of the actual/process value signal</p>"},{"location":"reference/ts_shape/events/engineering/setpoint_events/#ts_shape.events.engineering.setpoint_events.SetpointChangeEvents.time_to_settle(tol)","title":"<code>tol</code>","text":"(<code>float</code>, default:                   <code>0.0</code> )           \u2013            <p>Absolute tolerance (used if settle_pct is None)</p>"},{"location":"reference/ts_shape/events/engineering/setpoint_events/#ts_shape.events.engineering.setpoint_events.SetpointChangeEvents.time_to_settle(settle_pct)","title":"<code>settle_pct</code>","text":"(<code>Optional[float]</code>, default:                   <code>None</code> )           \u2013            <p>Percentage-based tolerance (e.g., 0.02 for 2% of step magnitude)</p>"},{"location":"reference/ts_shape/events/engineering/setpoint_events/#ts_shape.events.engineering.setpoint_events.SetpointChangeEvents.time_to_settle(hold)","title":"<code>hold</code>","text":"(<code>str</code>, default:                   <code>'0s'</code> )           \u2013            <p>Minimum duration the signal must stay within tolerance</p>"},{"location":"reference/ts_shape/events/engineering/setpoint_events/#ts_shape.events.engineering.setpoint_events.SetpointChangeEvents.time_to_settle(lookahead)","title":"<code>lookahead</code>","text":"(<code>str</code>, default:                   <code>'10m'</code> )           \u2013            <p>Maximum time window to search for settling</p>"},{"location":"reference/ts_shape/events/engineering/setpoint_events/#ts_shape.events.engineering.setpoint_events.SetpointChangeEvents.time_to_settle_derivative","title":"time_to_settle_derivative","text":"<pre><code>time_to_settle_derivative(actual_uuid: str, *, rate_threshold: float = 0.01, lookahead: str = '10m', hold: str = '0s') -&gt; DataFrame\n</code></pre> <p>Detect settling based on rate of change (derivative) falling below threshold. More sensitive to when the process has truly stopped moving.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>DataFrame with columns: start, uuid, is_delta, t_settle_seconds, settled, final_rate.</p> </li> </ul> Source code in <code>src/ts_shape/events/engineering/setpoint_events.py</code> <pre><code>def time_to_settle_derivative(\n    self,\n    actual_uuid: str,\n    *,\n    rate_threshold: float = 0.01,\n    lookahead: str = \"10m\",\n    hold: str = \"0s\",\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Detect settling based on rate of change (derivative) falling below threshold.\n    More sensitive to when the process has truly stopped moving.\n\n    Args:\n        actual_uuid: UUID of the actual/process value signal\n        rate_threshold: Maximum absolute rate of change to consider settled\n        lookahead: Maximum time window to search for settling\n        hold: Minimum duration the rate must stay below threshold\n\n    Returns:\n        DataFrame with columns: start, uuid, is_delta, t_settle_seconds, settled, final_rate.\n    \"\"\"\n    if self.sp.empty:\n        return pd.DataFrame(\n            columns=[\"start\", \"uuid\", \"is_delta\", \"t_settle_seconds\", \"settled\", \"final_rate\"]\n        )\n\n    # Use cached actual data\n    actual = self._get_actual(actual_uuid)\n    look_td = pd.to_timedelta(lookahead)\n    hold_td = pd.to_timedelta(hold)\n\n    # change instants\n    sp = self.sp[[self.time_column, self.value_column]].copy()\n    sp[\"prev\"] = sp[self.value_column].shift(1)\n    sp[\"delta\"] = sp[self.value_column] - sp[\"prev\"]\n    change_times = sp.loc[sp[\"delta\"].abs() &gt; 0, [self.time_column, self.value_column]].reset_index(drop=True)\n\n    rows: List[Dict[str, Any]] = []\n    for _, c in change_times.iterrows():\n        t0 = c[self.time_column]\n        window = actual[(actual[self.time_column] &gt;= t0) &amp; (actual[self.time_column] &lt;= t0 + look_td)]\n\n        if window.empty or len(window) &lt; 2:\n            rows.append(\n                {\n                    \"start\": t0,\n                    \"uuid\": self.event_uuid,\n                    \"is_delta\": True,\n                    \"t_settle_seconds\": None,\n                    \"settled\": False,\n                    \"final_rate\": None,\n                }\n            )\n            continue\n\n        # Calculate rate of change\n        win_copy = window.copy()\n        win_copy[\"dt_s\"] = win_copy[self.time_column].diff().dt.total_seconds()\n        win_copy[\"dv\"] = win_copy[self.value_column].diff()\n        win_copy[\"rate\"] = (win_copy[\"dv\"] / win_copy[\"dt_s\"]).abs()\n\n        # Find when rate drops below threshold\n        below_threshold = win_copy[\"rate\"] &lt;= rate_threshold\n\n        t_settle = None\n        settled = False\n        final_rate = None\n\n        if below_threshold.any():\n            # Find first entry below threshold\n            first_idx = below_threshold[below_threshold].index[0]\n            t_first = win_copy.loc[first_idx, self.time_column]\n\n            # Check if it stays below threshold for hold duration\n            if hold_td.total_seconds() &gt; 0:\n                gid = (below_threshold.ne(below_threshold.shift())).cumsum()\n                for _, seg in win_copy.groupby(gid):\n                    seg_below = below_threshold.loc[seg.index]\n                    if not seg_below.iloc[0]:\n                        continue\n                    start_seg = seg[self.time_column].iloc[0]\n                    end_seg = seg[self.time_column].iloc[-1]\n                    if (end_seg - start_seg) &gt;= hold_td:\n                        t_settle = start_seg\n                        settled = True\n                        final_rate = float(seg[\"rate\"].mean())\n                        break\n            else:\n                t_settle = t_first\n                settled = True\n                final_rate = float(win_copy.loc[first_idx, \"rate\"])\n\n        rows.append(\n            {\n                \"start\": t0,\n                \"uuid\": self.event_uuid,\n                \"is_delta\": True,\n                \"t_settle_seconds\": (t_settle - t0).total_seconds() if t_settle is not None else None,\n                \"settled\": settled,\n                \"final_rate\": final_rate,\n            }\n        )\n\n    return pd.DataFrame(rows)\n</code></pre>"},{"location":"reference/ts_shape/events/engineering/setpoint_events/#ts_shape.events.engineering.setpoint_events.SetpointChangeEvents.time_to_settle_derivative(actual_uuid)","title":"<code>actual_uuid</code>","text":"(<code>str</code>)           \u2013            <p>UUID of the actual/process value signal</p>"},{"location":"reference/ts_shape/events/engineering/setpoint_events/#ts_shape.events.engineering.setpoint_events.SetpointChangeEvents.time_to_settle_derivative(rate_threshold)","title":"<code>rate_threshold</code>","text":"(<code>float</code>, default:                   <code>0.01</code> )           \u2013            <p>Maximum absolute rate of change to consider settled</p>"},{"location":"reference/ts_shape/events/engineering/setpoint_events/#ts_shape.events.engineering.setpoint_events.SetpointChangeEvents.time_to_settle_derivative(lookahead)","title":"<code>lookahead</code>","text":"(<code>str</code>, default:                   <code>'10m'</code> )           \u2013            <p>Maximum time window to search for settling</p>"},{"location":"reference/ts_shape/events/engineering/setpoint_events/#ts_shape.events.engineering.setpoint_events.SetpointChangeEvents.time_to_settle_derivative(hold)","title":"<code>hold</code>","text":"(<code>str</code>, default:                   <code>'0s'</code> )           \u2013            <p>Minimum duration the rate must stay below threshold</p>"},{"location":"reference/ts_shape/events/engineering/startup_events/","title":"startup_events","text":""},{"location":"reference/ts_shape/events/engineering/startup_events/#ts_shape.events.engineering.startup_events","title":"ts_shape.events.engineering.startup_events","text":"<p>Classes:</p> <ul> <li> <code>StartupDetectionEvents</code>           \u2013            <p>Detect equipment startup intervals based on threshold crossings or</p> </li> </ul>"},{"location":"reference/ts_shape/events/engineering/startup_events/#ts_shape.events.engineering.startup_events.StartupDetectionEvents","title":"StartupDetectionEvents","text":"<pre><code>StartupDetectionEvents(dataframe: DataFrame, target_uuid: str, *, event_uuid: str = 'startup_event', value_column: str = 'value_double', time_column: str = 'systime')\n</code></pre> <p>               Bases: <code>Base</code></p> <p>Detect equipment startup intervals based on threshold crossings or sustained positive slope in a numeric metric (speed, temperature, etc.).</p> <p>Schema assumptions (columns): - uuid, sequence_number, systime, plctime, is_delta - value_integer, value_string, value_double, value_bool, value_bytes</p> <p>Methods:</p> <ul> <li> <code>assess_startup_quality</code>             \u2013              <p>Assess the quality of detected startup events.</p> </li> <li> <code>detect_failed_startups</code>             \u2013              <p>Detect failed or aborted startup attempts.</p> </li> <li> <code>detect_startup_adaptive</code>             \u2013              <p>Detect startups using adaptive thresholds calculated from historical baseline data.</p> </li> <li> <code>detect_startup_by_slope</code>             \u2013              <p>Startup intervals where per-second slope &gt;= <code>min_slope</code> for at least</p> </li> <li> <code>detect_startup_by_threshold</code>             \u2013              <p>Startup begins at first crossing above <code>threshold</code> (or hysteresis enter)</p> </li> <li> <code>detect_startup_multi_signal</code>             \u2013              <p>Detect startups based on multiple signals with configurable AND/OR logic.</p> </li> <li> <code>get_dataframe</code>             \u2013              <p>Returns the processed DataFrame.</p> </li> <li> <code>track_startup_phases</code>             \u2013              <p>Track progression through defined startup phases.</p> </li> </ul> Source code in <code>src/ts_shape/events/engineering/startup_events.py</code> <pre><code>def __init__(\n    self,\n    dataframe: pd.DataFrame,\n    target_uuid: str,\n    *,\n    event_uuid: str = \"startup_event\",\n    value_column: str = \"value_double\",\n    time_column: str = \"systime\",\n) -&gt; None:\n    super().__init__(dataframe, column_name=time_column)\n    self.target_uuid = target_uuid\n    self.event_uuid = event_uuid\n    self.value_column = value_column\n    self.time_column = time_column\n\n    self.series = (\n        self.dataframe[self.dataframe[\"uuid\"] == self.target_uuid]\n        .copy()\n        .sort_values(self.time_column)\n    )\n    self.series[self.time_column] = pd.to_datetime(self.series[self.time_column])\n</code></pre>"},{"location":"reference/ts_shape/events/engineering/startup_events/#ts_shape.events.engineering.startup_events.StartupDetectionEvents.assess_startup_quality","title":"assess_startup_quality","text":"<pre><code>assess_startup_quality(startup_events: DataFrame, *, smoothness_window: int = 5, anomaly_threshold: float = 3.0) -&gt; DataFrame\n</code></pre> <p>Assess the quality of detected startup events.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>DataFrame with quality metrics for each startup: - duration: Total duration of startup - smoothness_score: Inverse of derivative variance (higher = smoother) - anomaly_flags: Number of anomalous points detected - value_change: Total change in value during startup - avg_rate: Average rate of change - max_value: Maximum value reached - stability_score: Measure of how stable the final state is</p> </li> </ul> Source code in <code>src/ts_shape/events/engineering/startup_events.py</code> <pre><code>def assess_startup_quality(\n    self,\n    startup_events: pd.DataFrame,\n    *,\n    smoothness_window: int = 5,\n    anomaly_threshold: float = 3.0,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Assess the quality of detected startup events.\n\n    Args:\n        startup_events: DataFrame of detected startup events (must have 'start' and 'end' columns)\n        smoothness_window: Window size for calculating smoothness metrics\n        anomaly_threshold: Z-score threshold for detecting anomalies\n\n    Returns:\n        DataFrame with quality metrics for each startup:\n            - duration: Total duration of startup\n            - smoothness_score: Inverse of derivative variance (higher = smoother)\n            - anomaly_flags: Number of anomalous points detected\n            - value_change: Total change in value during startup\n            - avg_rate: Average rate of change\n            - max_value: Maximum value reached\n            - stability_score: Measure of how stable the final state is\n    \"\"\"\n    if startup_events.empty or self.series.empty:\n        return pd.DataFrame(columns=[\n            \"start\", \"end\", \"duration\", \"smoothness_score\", \"anomaly_flags\",\n            \"value_change\", \"avg_rate\", \"max_value\", \"stability_score\"\n        ])\n\n    quality_results = []\n    s = self.series[[self.time_column, self.value_column]].copy()\n\n    for _, event in startup_events.iterrows():\n        start = pd.to_datetime(event[\"start\"])\n        end = pd.to_datetime(event[\"end\"])\n\n        # Extract data for this startup period\n        period_data = s[\n            (s[self.time_column] &gt;= start) &amp;\n            (s[self.time_column] &lt;= end)\n        ].copy()\n\n        if period_data.empty or len(period_data) &lt; 2:\n            quality_results.append({\n                \"start\": start,\n                \"end\": end,\n                \"duration\": pd.Timedelta(0),\n                \"smoothness_score\": None,\n                \"anomaly_flags\": 0,\n                \"value_change\": None,\n                \"avg_rate\": None,\n                \"max_value\": None,\n                \"stability_score\": None,\n            })\n            continue\n\n        # Calculate duration\n        duration = end - start\n\n        # Calculate derivatives for smoothness\n        period_data[\"dt_s\"] = period_data[self.time_column].diff().dt.total_seconds()\n        period_data[\"derivative\"] = period_data[self.value_column].diff() / period_data[\"dt_s\"]\n\n        # Smoothness: inverse of derivative variance (normalized)\n        derivative_var = period_data[\"derivative\"].var()\n        smoothness_score = 1.0 / (1.0 + derivative_var) if pd.notna(derivative_var) else None\n\n        # Anomaly detection using z-scores\n        values = period_data[self.value_column]\n        z_scores = np.abs((values - values.mean()) / values.std()) if values.std() &gt; 0 else np.zeros(len(values))\n        anomaly_flags = int((z_scores &gt; anomaly_threshold).sum())\n\n        # Value change metrics\n        value_change = values.iloc[-1] - values.iloc[0] if len(values) &gt; 1 else 0\n        avg_rate = value_change / duration.total_seconds() if duration.total_seconds() &gt; 0 else 0\n        max_value = values.max()\n\n        # Stability score: inverse of coefficient of variation in final 20% of period\n        final_portion_size = max(1, len(period_data) // 5)\n        final_values = values.iloc[-final_portion_size:]\n        cv = final_values.std() / final_values.mean() if final_values.mean() != 0 else float('inf')\n        stability_score = 1.0 / (1.0 + cv) if np.isfinite(cv) else 0.0\n\n        quality_results.append({\n            \"start\": start,\n            \"end\": end,\n            \"duration\": duration,\n            \"smoothness_score\": float(smoothness_score) if smoothness_score is not None else None,\n            \"anomaly_flags\": anomaly_flags,\n            \"value_change\": float(value_change),\n            \"avg_rate\": float(avg_rate),\n            \"max_value\": float(max_value),\n            \"stability_score\": float(stability_score),\n        })\n\n    return pd.DataFrame(quality_results)\n</code></pre>"},{"location":"reference/ts_shape/events/engineering/startup_events/#ts_shape.events.engineering.startup_events.StartupDetectionEvents.assess_startup_quality(startup_events)","title":"<code>startup_events</code>","text":"(<code>DataFrame</code>)           \u2013            <p>DataFrame of detected startup events (must have 'start' and 'end' columns)</p>"},{"location":"reference/ts_shape/events/engineering/startup_events/#ts_shape.events.engineering.startup_events.StartupDetectionEvents.assess_startup_quality(smoothness_window)","title":"<code>smoothness_window</code>","text":"(<code>int</code>, default:                   <code>5</code> )           \u2013            <p>Window size for calculating smoothness metrics</p>"},{"location":"reference/ts_shape/events/engineering/startup_events/#ts_shape.events.engineering.startup_events.StartupDetectionEvents.assess_startup_quality(anomaly_threshold)","title":"<code>anomaly_threshold</code>","text":"(<code>float</code>, default:                   <code>3.0</code> )           \u2013            <p>Z-score threshold for detecting anomalies</p>"},{"location":"reference/ts_shape/events/engineering/startup_events/#ts_shape.events.engineering.startup_events.StartupDetectionEvents.detect_failed_startups","title":"detect_failed_startups","text":"<pre><code>detect_failed_startups(*, threshold: float, min_rise_duration: str = '5s', max_completion_time: str = '5m', completion_threshold: Optional[float] = None, required_stability: str = '10s') -&gt; DataFrame\n</code></pre> <p>Detect failed or aborted startup attempts.</p> <p>A failed startup is identified when: 1. Value rises above threshold for at least min_rise_duration 2. But fails to reach completion_threshold within max_completion_time 3. Or drops back below threshold before achieving required_stability</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>DataFrame with columns: start, end, uuid, is_delta, method, failure_reason,                     max_value_reached, time_to_failure</p> </li> </ul> Source code in <code>src/ts_shape/events/engineering/startup_events.py</code> <pre><code>def detect_failed_startups(\n    self,\n    *,\n    threshold: float,\n    min_rise_duration: str = \"5s\",\n    max_completion_time: str = \"5m\",\n    completion_threshold: Optional[float] = None,\n    required_stability: str = \"10s\",\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Detect failed or aborted startup attempts.\n\n    A failed startup is identified when:\n    1. Value rises above threshold for at least min_rise_duration\n    2. But fails to reach completion_threshold within max_completion_time\n    3. Or drops back below threshold before achieving required_stability\n\n    Args:\n        threshold: Initial threshold that must be crossed to begin startup\n        min_rise_duration: Minimum time above threshold to consider it a startup attempt\n        max_completion_time: Maximum time allowed to complete startup\n        completion_threshold: Target threshold for successful completion (default: 2x threshold)\n        required_stability: Time that must be maintained at completion level\n\n    Returns:\n        DataFrame with columns: start, end, uuid, is_delta, method, failure_reason,\n                                max_value_reached, time_to_failure\n    \"\"\"\n    if self.series.empty:\n        return pd.DataFrame(columns=[\n            \"start\", \"end\", \"uuid\", \"is_delta\", \"method\",\n            \"failure_reason\", \"max_value_reached\", \"time_to_failure\"\n        ])\n\n    if completion_threshold is None:\n        completion_threshold = threshold * 2.0\n\n    min_rise_td = pd.to_timedelta(min_rise_duration)\n    max_completion_td = pd.to_timedelta(max_completion_time)\n    stability_td = pd.to_timedelta(required_stability)\n\n    s = self.series[[self.time_column, self.value_column]].copy()\n\n    # Detect threshold crossings\n    above_threshold = s[self.value_column] &gt;= threshold\n    rising = (~above_threshold.shift(fill_value=False)) &amp; above_threshold\n    rise_times = s.loc[rising, self.time_column]\n\n    failed_events: List[Dict[str, Any]] = []\n\n    for t0 in rise_times:\n        # Get data for potential startup period\n        startup_window = s[\n            (s[self.time_column] &gt;= t0) &amp;\n            (s[self.time_column] &lt;= t0 + max_completion_td)\n        ].copy()\n\n        if len(startup_window) &lt; 2:\n            continue\n\n        # Check if initially above threshold for min_rise_duration\n        initial_window = startup_window[\n            startup_window[self.time_column] &lt;= t0 + min_rise_td\n        ]\n\n        if initial_window.empty or not (initial_window[self.value_column] &gt;= threshold).all():\n            continue  # Not a valid startup attempt\n\n        # Now check for failure modes\n        max_value = startup_window[self.value_column].max()\n        failure_detected = False\n        failure_reason = None\n        failure_time = None\n\n        # Check if value drops back below threshold before completion\n        above_in_window = startup_window[self.value_column] &gt;= threshold\n        if not above_in_window.all():\n            # Find when it dropped below\n            drop_idx = startup_window[~above_in_window].index[0]\n            failure_time = startup_window.loc[drop_idx, self.time_column]\n\n            # Check if this happened before reaching completion threshold\n            if max_value &lt; completion_threshold:\n                failure_detected = True\n                failure_reason = \"dropped_below_threshold_before_completion\"\n\n        # Check if completion threshold was never reached\n        if not failure_detected:\n            reached_completion = (startup_window[self.value_column] &gt;= completion_threshold).any()\n\n            if not reached_completion:\n                failure_detected = True\n                failure_reason = \"failed_to_reach_completion_threshold\"\n                failure_time = startup_window[self.time_column].iloc[-1]\n            else:\n                # Reached completion but check stability\n                completion_idx = startup_window[\n                    startup_window[self.value_column] &gt;= completion_threshold\n                ].index[0]\n                completion_time = startup_window.loc[completion_idx, self.time_column]\n\n                stability_window = startup_window[\n                    (startup_window[self.time_column] &gt;= completion_time) &amp;\n                    (startup_window[self.time_column] &lt;= completion_time + stability_td)\n                ]\n\n                if not stability_window.empty:\n                    if not (stability_window[self.value_column] &gt;= completion_threshold).all():\n                        failure_detected = True\n                        failure_reason = \"insufficient_stability_at_completion\"\n                        failure_time = completion_time + stability_td\n\n        if failure_detected:\n            time_to_failure = (failure_time - t0).total_seconds() if failure_time else None\n\n            failed_events.append({\n                \"start\": t0,\n                \"end\": failure_time if failure_time else t0 + max_completion_td,\n                \"uuid\": self.event_uuid,\n                \"is_delta\": True,\n                \"method\": \"failed_startup\",\n                \"failure_reason\": failure_reason,\n                \"max_value_reached\": float(max_value),\n                \"time_to_failure\": time_to_failure,\n            })\n\n    return pd.DataFrame(failed_events)\n</code></pre>"},{"location":"reference/ts_shape/events/engineering/startup_events/#ts_shape.events.engineering.startup_events.StartupDetectionEvents.detect_failed_startups(threshold)","title":"<code>threshold</code>","text":"(<code>float</code>)           \u2013            <p>Initial threshold that must be crossed to begin startup</p>"},{"location":"reference/ts_shape/events/engineering/startup_events/#ts_shape.events.engineering.startup_events.StartupDetectionEvents.detect_failed_startups(min_rise_duration)","title":"<code>min_rise_duration</code>","text":"(<code>str</code>, default:                   <code>'5s'</code> )           \u2013            <p>Minimum time above threshold to consider it a startup attempt</p>"},{"location":"reference/ts_shape/events/engineering/startup_events/#ts_shape.events.engineering.startup_events.StartupDetectionEvents.detect_failed_startups(max_completion_time)","title":"<code>max_completion_time</code>","text":"(<code>str</code>, default:                   <code>'5m'</code> )           \u2013            <p>Maximum time allowed to complete startup</p>"},{"location":"reference/ts_shape/events/engineering/startup_events/#ts_shape.events.engineering.startup_events.StartupDetectionEvents.detect_failed_startups(completion_threshold)","title":"<code>completion_threshold</code>","text":"(<code>Optional[float]</code>, default:                   <code>None</code> )           \u2013            <p>Target threshold for successful completion (default: 2x threshold)</p>"},{"location":"reference/ts_shape/events/engineering/startup_events/#ts_shape.events.engineering.startup_events.StartupDetectionEvents.detect_failed_startups(required_stability)","title":"<code>required_stability</code>","text":"(<code>str</code>, default:                   <code>'10s'</code> )           \u2013            <p>Time that must be maintained at completion level</p>"},{"location":"reference/ts_shape/events/engineering/startup_events/#ts_shape.events.engineering.startup_events.StartupDetectionEvents.detect_startup_adaptive","title":"detect_startup_adaptive","text":"<pre><code>detect_startup_adaptive(*, baseline_window: str = '1h', sensitivity: float = 2.0, min_above: str = '10s', lookback_periods: int = 5) -&gt; DataFrame\n</code></pre> <p>Detect startups using adaptive thresholds calculated from historical baseline data.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>DataFrame with columns: start, end, uuid, is_delta, method, adaptive_threshold,                     baseline_mean, baseline_std</p> </li> </ul> Source code in <code>src/ts_shape/events/engineering/startup_events.py</code> <pre><code>def detect_startup_adaptive(\n    self,\n    *,\n    baseline_window: str = \"1h\",\n    sensitivity: float = 2.0,\n    min_above: str = \"10s\",\n    lookback_periods: int = 5,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Detect startups using adaptive thresholds calculated from historical baseline data.\n\n    Args:\n        baseline_window: Window size for calculating baseline statistics\n        sensitivity: Multiplier for standard deviation (threshold = mean + sensitivity * std)\n        min_above: Minimum time the value must stay above threshold\n        lookback_periods: Number of baseline periods to use for statistics\n\n    Returns:\n        DataFrame with columns: start, end, uuid, is_delta, method, adaptive_threshold,\n                                baseline_mean, baseline_std\n    \"\"\"\n    if self.series.empty:\n        return pd.DataFrame(columns=[\n            \"start\", \"end\", \"uuid\", \"is_delta\", \"method\",\n            \"adaptive_threshold\", \"baseline_mean\", \"baseline_std\"\n        ])\n\n    window_td = pd.to_timedelta(baseline_window)\n    min_above_td = pd.to_timedelta(min_above)\n\n    s = self.series[[self.time_column, self.value_column]].copy()\n    s = s.sort_values(self.time_column).reset_index(drop=True)\n\n    events: List[Dict[str, Any]] = []\n\n    # Calculate rolling statistics\n    s[\"rolling_mean\"] = s[self.value_column].rolling(\n        window=lookback_periods, min_periods=1\n    ).mean()\n    s[\"rolling_std\"] = s[self.value_column].rolling(\n        window=lookback_periods, min_periods=1\n    ).std()\n\n    # Calculate adaptive threshold\n    s[\"adaptive_threshold\"] = s[\"rolling_mean\"] + sensitivity * s[\"rolling_std\"].fillna(0)\n\n    # Detect crossings above adaptive threshold\n    s[\"above_threshold\"] = s[self.value_column] &gt;= s[\"adaptive_threshold\"]\n    s[\"crossing_up\"] = (~s[\"above_threshold\"].shift(fill_value=False)) &amp; s[\"above_threshold\"]\n\n    # Find sustained periods above threshold\n    for idx in s[s[\"crossing_up\"]].index:\n        t0 = s.loc[idx, self.time_column]\n        threshold_at_crossing = s.loc[idx, \"adaptive_threshold\"]\n        baseline_mean = s.loc[idx, \"rolling_mean\"]\n        baseline_std = s.loc[idx, \"rolling_std\"]\n\n        # Check if value stays above threshold for min_above duration\n        win = s[\n            (s[self.time_column] &gt;= t0) &amp;\n            (s[self.time_column] &lt;= t0 + min_above_td)\n        ]\n\n        if win.empty:\n            continue\n\n        # Value must stay above the threshold calculated at crossing time\n        if (win[self.value_column] &gt;= threshold_at_crossing).all():\n            events.append({\n                \"start\": t0,\n                \"end\": t0 + min_above_td,\n                \"uuid\": self.event_uuid,\n                \"is_delta\": True,\n                \"method\": \"adaptive_threshold\",\n                \"adaptive_threshold\": float(threshold_at_crossing),\n                \"baseline_mean\": float(baseline_mean) if pd.notna(baseline_mean) else None,\n                \"baseline_std\": float(baseline_std) if pd.notna(baseline_std) else None,\n            })\n\n    return pd.DataFrame(events)\n</code></pre>"},{"location":"reference/ts_shape/events/engineering/startup_events/#ts_shape.events.engineering.startup_events.StartupDetectionEvents.detect_startup_adaptive(baseline_window)","title":"<code>baseline_window</code>","text":"(<code>str</code>, default:                   <code>'1h'</code> )           \u2013            <p>Window size for calculating baseline statistics</p>"},{"location":"reference/ts_shape/events/engineering/startup_events/#ts_shape.events.engineering.startup_events.StartupDetectionEvents.detect_startup_adaptive(sensitivity)","title":"<code>sensitivity</code>","text":"(<code>float</code>, default:                   <code>2.0</code> )           \u2013            <p>Multiplier for standard deviation (threshold = mean + sensitivity * std)</p>"},{"location":"reference/ts_shape/events/engineering/startup_events/#ts_shape.events.engineering.startup_events.StartupDetectionEvents.detect_startup_adaptive(min_above)","title":"<code>min_above</code>","text":"(<code>str</code>, default:                   <code>'10s'</code> )           \u2013            <p>Minimum time the value must stay above threshold</p>"},{"location":"reference/ts_shape/events/engineering/startup_events/#ts_shape.events.engineering.startup_events.StartupDetectionEvents.detect_startup_adaptive(lookback_periods)","title":"<code>lookback_periods</code>","text":"(<code>int</code>, default:                   <code>5</code> )           \u2013            <p>Number of baseline periods to use for statistics</p>"},{"location":"reference/ts_shape/events/engineering/startup_events/#ts_shape.events.engineering.startup_events.StartupDetectionEvents.detect_startup_by_slope","title":"detect_startup_by_slope","text":"<pre><code>detect_startup_by_slope(*, min_slope: float, slope_window: str = '0s', min_duration: str = '0s') -&gt; DataFrame\n</code></pre> <p>Startup intervals where per-second slope &gt;= <code>min_slope</code> for at least <code>min_duration</code>. <code>slope_window</code> is accepted for API completeness but the current implementation uses instantaneous slope between samples.</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>DataFrame with columns: start, end, uuid, is_delta, method, min_slope, avg_slope.</p> </li> </ul> Source code in <code>src/ts_shape/events/engineering/startup_events.py</code> <pre><code>def detect_startup_by_slope(\n    self,\n    *,\n    min_slope: float,\n    slope_window: str = \"0s\",\n    min_duration: str = \"0s\",\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Startup intervals where per-second slope &gt;= `min_slope` for at least\n    `min_duration`. `slope_window` is accepted for API completeness but the\n    current implementation uses instantaneous slope between samples.\n\n    Returns:\n        DataFrame with columns: start, end, uuid, is_delta, method, min_slope, avg_slope.\n    \"\"\"\n    if self.series.empty:\n        return pd.DataFrame(columns=[\"start\", \"end\", \"uuid\", \"is_delta\", \"method\", \"min_slope\", \"avg_slope\"])\n\n    s = self.series[[self.time_column, self.value_column]].copy()\n    s[\"dt_s\"] = s[self.time_column].diff().dt.total_seconds()\n    s[\"dv\"] = s[self.value_column].diff()\n    s[\"slope\"] = s[\"dv\"] / s[\"dt_s\"]\n    mask = s[\"slope\"] &gt;= float(min_slope)\n\n    gid = (mask != mask.shift()).cumsum()\n    min_d = pd.to_timedelta(min_duration)\n    events: List[Dict[str, Any]] = []\n    for _, seg in s.groupby(gid):\n        seg_mask = mask.loc[seg.index]\n        if not seg_mask.any():\n            continue\n        start_t = seg.loc[seg_mask, self.time_column].iloc[0]\n        end_t = seg.loc[seg_mask, self.time_column].iloc[-1]\n        if (end_t - start_t) &lt; min_d:\n            continue\n        avg_slope = seg.loc[seg_mask, \"slope\"].mean()\n        events.append(\n            {\n                \"start\": start_t,\n                \"end\": end_t,\n                \"uuid\": self.event_uuid,\n                \"is_delta\": True,\n                \"method\": \"slope\",\n                \"min_slope\": float(min_slope),\n                \"avg_slope\": float(avg_slope) if pd.notna(avg_slope) else None,\n            }\n        )\n\n    return pd.DataFrame(events)\n</code></pre>"},{"location":"reference/ts_shape/events/engineering/startup_events/#ts_shape.events.engineering.startup_events.StartupDetectionEvents.detect_startup_by_threshold","title":"detect_startup_by_threshold","text":"<pre><code>detect_startup_by_threshold(*, threshold: float, hysteresis: tuple[float, float] | None = None, min_above: str = '0s') -&gt; DataFrame\n</code></pre> <p>Startup begins at first crossing above <code>threshold</code> (or hysteresis enter) and is valid only if the metric stays above the (exit) threshold for at least <code>min_above</code>.</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>DataFrame with columns: start, end, uuid, is_delta, method, threshold.</p> </li> </ul> Source code in <code>src/ts_shape/events/engineering/startup_events.py</code> <pre><code>def detect_startup_by_threshold(\n    self,\n    *,\n    threshold: float,\n    hysteresis: tuple[float, float] | None = None,\n    min_above: str = \"0s\",\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Startup begins at first crossing above `threshold` (or hysteresis enter)\n    and is valid only if the metric stays above the (exit) threshold for at\n    least `min_above`.\n\n    Returns:\n        DataFrame with columns: start, end, uuid, is_delta, method, threshold.\n    \"\"\"\n    if self.series.empty:\n        return pd.DataFrame(columns=[\"start\", \"end\", \"uuid\", \"is_delta\", \"method\", \"threshold\"])\n\n    enter_thr = threshold if hysteresis is None else hysteresis[0]\n    exit_thr = threshold if hysteresis is None else hysteresis[1]\n    min_above_td = pd.to_timedelta(min_above)\n\n    s = self.series[[self.time_column, self.value_column]].copy()\n    above_enter = s[self.value_column] &gt;= enter_thr\n    rising = (~above_enter.shift(fill_value=False)) &amp; above_enter\n    rise_times = s.loc[rising, self.time_column]\n\n    events: List[Dict[str, Any]] = []\n    for t0 in rise_times:\n        # ensure dwell above exit threshold for min_above\n        win = s[(s[self.time_column] &gt;= t0) &amp; (s[self.time_column] &lt;= t0 + min_above_td)]\n        if win.empty:\n            continue\n        if (win[self.value_column] &gt;= exit_thr).all():\n            events.append(\n                {\n                    \"start\": t0,\n                    \"end\": t0 + min_above_td,\n                    \"uuid\": self.event_uuid,\n                    \"is_delta\": True,\n                    \"method\": \"threshold\",\n                    \"threshold\": float(threshold),\n                }\n            )\n\n    return pd.DataFrame(events)\n</code></pre>"},{"location":"reference/ts_shape/events/engineering/startup_events/#ts_shape.events.engineering.startup_events.StartupDetectionEvents.detect_startup_multi_signal","title":"detect_startup_multi_signal","text":"<pre><code>detect_startup_multi_signal(signals: Dict[str, Dict[str, Any]], logic: str = 'all', *, time_tolerance: str = '30s') -&gt; DataFrame\n</code></pre> <p>Detect startups based on multiple signals with configurable AND/OR logic.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>DataFrame with columns: start, end, uuid, is_delta, method, signals_triggered, signal_details</p> </li> </ul> Source code in <code>src/ts_shape/events/engineering/startup_events.py</code> <pre><code>def detect_startup_multi_signal(\n    self,\n    signals: Dict[str, Dict[str, Any]],\n    logic: str = \"all\",\n    *,\n    time_tolerance: str = \"30s\",\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Detect startups based on multiple signals with configurable AND/OR logic.\n\n    Args:\n        signals: Dict mapping uuid to detection config. Each config should contain:\n            - 'method': 'threshold' or 'slope'\n            - For threshold: 'threshold', optional 'hysteresis', 'min_above'\n            - For slope: 'min_slope', optional 'slope_window', 'min_duration'\n        logic: 'all' (AND - all signals must detect) or 'any' (OR - at least one)\n        time_tolerance: Maximum time difference between signals for 'all' logic\n\n    Returns:\n        DataFrame with columns: start, end, uuid, is_delta, method, signals_triggered, signal_details\n    \"\"\"\n    if logic not in [\"all\", \"any\"]:\n        raise ValueError(f\"logic must be 'all' or 'any', got '{logic}'\")\n\n    # Detect startups for each signal\n    signal_events: Dict[str, pd.DataFrame] = {}\n    for sig_uuid, config in signals.items():\n        # Temporarily store original settings\n        orig_uuid = self.target_uuid\n        orig_series = self.series.copy()\n\n        # Switch to the signal's uuid\n        self.target_uuid = sig_uuid\n        self.series = (\n            self.dataframe[self.dataframe[\"uuid\"] == sig_uuid]\n            .copy()\n            .sort_values(self.time_column)\n        )\n        self.series[self.time_column] = pd.to_datetime(self.series[self.time_column])\n\n        # Detect based on method\n        method = config.get(\"method\", \"threshold\")\n        if method == \"threshold\":\n            events = self.detect_startup_by_threshold(\n                threshold=config.get(\"threshold\", 0),\n                hysteresis=config.get(\"hysteresis\"),\n                min_above=config.get(\"min_above\", \"0s\"),\n            )\n        elif method == \"slope\":\n            events = self.detect_startup_by_slope(\n                min_slope=config.get(\"min_slope\", 0),\n                slope_window=config.get(\"slope_window\", \"0s\"),\n                min_duration=config.get(\"min_duration\", \"0s\"),\n            )\n        else:\n            raise ValueError(f\"Unknown method '{method}' for signal {sig_uuid}\")\n\n        signal_events[sig_uuid] = events\n\n        # Restore original settings\n        self.target_uuid = orig_uuid\n        self.series = orig_series\n\n    # Combine events based on logic\n    if logic == \"any\":\n        # Union: any signal detecting is sufficient\n        all_events = []\n        for sig_uuid, events in signal_events.items():\n            for _, event in events.iterrows():\n                all_events.append({\n                    \"start\": event[\"start\"],\n                    \"end\": event[\"end\"],\n                    \"uuid\": self.event_uuid,\n                    \"is_delta\": True,\n                    \"method\": \"multi_signal_any\",\n                    \"signals_triggered\": [sig_uuid],\n                    \"signal_details\": {sig_uuid: event.to_dict()},\n                })\n        return pd.DataFrame(all_events)\n\n    else:  # logic == \"all\"\n        # Intersection: all signals must detect within time_tolerance\n        if not signal_events or any(df.empty for df in signal_events.values()):\n            return pd.DataFrame(columns=[\n                \"start\", \"end\", \"uuid\", \"is_delta\", \"method\",\n                \"signals_triggered\", \"signal_details\"\n            ])\n\n        tolerance = pd.to_timedelta(time_tolerance)\n        combined_events = []\n\n        # Get events from first signal as reference\n        first_sig = list(signal_events.keys())[0]\n        for _, ref_event in signal_events[first_sig].iterrows():\n            ref_start = ref_event[\"start\"]\n\n            # Check if all other signals have an event within tolerance\n            matching_signals = {first_sig: ref_event.to_dict()}\n            all_match = True\n\n            for sig_uuid in list(signal_events.keys())[1:]:\n                sig_df = signal_events[sig_uuid]\n                # Find events within tolerance\n                matches = sig_df[\n                    (sig_df[\"start\"] &gt;= ref_start - tolerance) &amp;\n                    (sig_df[\"start\"] &lt;= ref_start + tolerance)\n                ]\n\n                if matches.empty:\n                    all_match = False\n                    break\n\n                # Use the closest match\n                closest_idx = (matches[\"start\"] - ref_start).abs().idxmin()\n                matching_signals[sig_uuid] = matches.loc[closest_idx].to_dict()\n\n            if all_match:\n                # Calculate overall start/end from all matching events\n                all_starts = [matching_signals[sig][\"start\"] for sig in matching_signals]\n                all_ends = [matching_signals[sig][\"end\"] for sig in matching_signals]\n\n                combined_events.append({\n                    \"start\": min(all_starts),\n                    \"end\": max(all_ends),\n                    \"uuid\": self.event_uuid,\n                    \"is_delta\": True,\n                    \"method\": \"multi_signal_all\",\n                    \"signals_triggered\": list(matching_signals.keys()),\n                    \"signal_details\": matching_signals,\n                })\n\n        return pd.DataFrame(combined_events)\n</code></pre>"},{"location":"reference/ts_shape/events/engineering/startup_events/#ts_shape.events.engineering.startup_events.StartupDetectionEvents.detect_startup_multi_signal(signals)","title":"<code>signals</code>","text":"(<code>Dict[str, Dict[str, Any]]</code>)           \u2013            <p>Dict mapping uuid to detection config. Each config should contain: - 'method': 'threshold' or 'slope' - For threshold: 'threshold', optional 'hysteresis', 'min_above' - For slope: 'min_slope', optional 'slope_window', 'min_duration'</p>"},{"location":"reference/ts_shape/events/engineering/startup_events/#ts_shape.events.engineering.startup_events.StartupDetectionEvents.detect_startup_multi_signal(logic)","title":"<code>logic</code>","text":"(<code>str</code>, default:                   <code>'all'</code> )           \u2013            <p>'all' (AND - all signals must detect) or 'any' (OR - at least one)</p>"},{"location":"reference/ts_shape/events/engineering/startup_events/#ts_shape.events.engineering.startup_events.StartupDetectionEvents.detect_startup_multi_signal(time_tolerance)","title":"<code>time_tolerance</code>","text":"(<code>str</code>, default:                   <code>'30s'</code> )           \u2013            <p>Maximum time difference between signals for 'all' logic</p>"},{"location":"reference/ts_shape/events/engineering/startup_events/#ts_shape.events.engineering.startup_events.StartupDetectionEvents.get_dataframe","title":"get_dataframe","text":"<pre><code>get_dataframe() -&gt; DataFrame\n</code></pre> <p>Returns the processed DataFrame.</p> Source code in <code>src/ts_shape/utils/base.py</code> <pre><code>def get_dataframe(self) -&gt; pd.DataFrame:\n    \"\"\"Returns the processed DataFrame.\"\"\"\n    return self.dataframe\n</code></pre>"},{"location":"reference/ts_shape/events/engineering/startup_events/#ts_shape.events.engineering.startup_events.StartupDetectionEvents.track_startup_phases","title":"track_startup_phases","text":"<pre><code>track_startup_phases(phases: List[Dict[str, Any]], *, min_phase_duration: str = '5s') -&gt; DataFrame\n</code></pre> <p>Track progression through defined startup phases.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>DataFrame with phase transitions: - phase_name: Name of the phase - phase_number: Sequential phase number (0-indexed) - start: Phase start time - end: Phase end time - duration: Time spent in phase - next_phase: Name of the next phase (None for last phase) - completed: Whether full startup sequence completed</p> </li> </ul> Source code in <code>src/ts_shape/events/engineering/startup_events.py</code> <pre><code>def track_startup_phases(\n    self,\n    phases: List[Dict[str, Any]],\n    *,\n    min_phase_duration: str = \"5s\",\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Track progression through defined startup phases.\n\n    Args:\n        phases: List of phase definitions, each containing:\n            - 'name': Phase name\n            - 'condition': 'threshold', 'range', or 'slope'\n            - For 'threshold': 'min_value' (value must be &gt;= min_value)\n            - For 'range': 'min_value' and 'max_value' (value in range)\n            - For 'slope': 'min_slope' (slope must be &gt;= min_slope)\n        min_phase_duration: Minimum time to stay in phase to be considered valid\n\n    Returns:\n        DataFrame with phase transitions:\n            - phase_name: Name of the phase\n            - phase_number: Sequential phase number (0-indexed)\n            - start: Phase start time\n            - end: Phase end time\n            - duration: Time spent in phase\n            - next_phase: Name of the next phase (None for last phase)\n            - completed: Whether full startup sequence completed\n    \"\"\"\n    if self.series.empty or not phases:\n        return pd.DataFrame(columns=[\n            \"phase_name\", \"phase_number\", \"start\", \"end\",\n            \"duration\", \"next_phase\", \"completed\"\n        ])\n\n    min_duration = pd.to_timedelta(min_phase_duration)\n    s = self.series[[self.time_column, self.value_column]].copy()\n\n    # Calculate slopes if needed\n    s[\"dt_s\"] = s[self.time_column].diff().dt.total_seconds()\n    s[\"dv\"] = s[self.value_column].diff()\n    s[\"slope\"] = s[\"dv\"] / s[\"dt_s\"]\n\n    phase_results = []\n    current_phase_idx = 0\n    phase_start = None\n    i = 0\n\n    while i &lt; len(s) and current_phase_idx &lt; len(phases):\n        row = s.iloc[i]\n        phase = phases[current_phase_idx]\n\n        # Check if current row satisfies phase condition\n        in_phase = self._check_phase_condition(row, phase)\n\n        if in_phase:\n            if phase_start is None:\n                phase_start = row[self.time_column]\n\n            # Check if we've been in this phase long enough\n            if row[self.time_column] - phase_start &gt;= min_duration:\n                # Check if we're transitioning to next phase\n                next_phase_idx = current_phase_idx + 1\n                if next_phase_idx &lt; len(phases):\n                    # Check if next phase condition is met\n                    next_phase = phases[next_phase_idx]\n                    if self._check_phase_condition(row, next_phase):\n                        # Record completed phase\n                        phase_results.append({\n                            \"phase_name\": phase[\"name\"],\n                            \"phase_number\": current_phase_idx,\n                            \"start\": phase_start,\n                            \"end\": row[self.time_column],\n                            \"duration\": row[self.time_column] - phase_start,\n                            \"next_phase\": next_phase[\"name\"],\n                            \"completed\": False,  # Will update at end\n                        })\n                        current_phase_idx = next_phase_idx\n                        phase_start = row[self.time_column]\n                else:\n                    # Last phase - check if it remains stable\n                    remaining = s.iloc[i:]\n                    if len(remaining) &gt; 0:\n                        # Check stability of last phase\n                        stable = all(\n                            self._check_phase_condition(s.iloc[j], phase)\n                            for j in range(i, min(i + 10, len(s)))\n                        )\n                        if stable:\n                            phase_results.append({\n                                \"phase_name\": phase[\"name\"],\n                                \"phase_number\": current_phase_idx,\n                                \"start\": phase_start,\n                                \"end\": row[self.time_column],\n                                \"duration\": row[self.time_column] - phase_start,\n                                \"next_phase\": None,\n                                \"completed\": True,\n                            })\n                            break\n        else:\n            # Lost phase condition\n            if phase_start is not None and row[self.time_column] - phase_start &gt;= min_duration:\n                # Phase was valid but didn't progress - potential failed startup\n                phase_results.append({\n                    \"phase_name\": phase[\"name\"],\n                    \"phase_number\": current_phase_idx,\n                    \"start\": phase_start,\n                    \"end\": row[self.time_column],\n                    \"duration\": row[self.time_column] - phase_start,\n                    \"next_phase\": None,\n                    \"completed\": False,\n                })\n            phase_start = None\n\n        i += 1\n\n    # Mark if full sequence completed\n    if phase_results and phase_results[-1][\"phase_number\"] == len(phases) - 1:\n        for result in phase_results:\n            result[\"completed\"] = True\n\n    return pd.DataFrame(phase_results)\n</code></pre>"},{"location":"reference/ts_shape/events/engineering/startup_events/#ts_shape.events.engineering.startup_events.StartupDetectionEvents.track_startup_phases(phases)","title":"<code>phases</code>","text":"(<code>List[Dict[str, Any]]</code>)           \u2013            <p>List of phase definitions, each containing: - 'name': Phase name - 'condition': 'threshold', 'range', or 'slope' - For 'threshold': 'min_value' (value must be &gt;= min_value) - For 'range': 'min_value' and 'max_value' (value in range) - For 'slope': 'min_slope' (slope must be &gt;= min_slope)</p>"},{"location":"reference/ts_shape/events/engineering/startup_events/#ts_shape.events.engineering.startup_events.StartupDetectionEvents.track_startup_phases(min_phase_duration)","title":"<code>min_phase_duration</code>","text":"(<code>str</code>, default:                   <code>'5s'</code> )           \u2013            <p>Minimum time to stay in phase to be considered valid</p>"},{"location":"reference/ts_shape/events/maintenance/__init__/","title":"maintenance","text":""},{"location":"reference/ts_shape/events/maintenance/__init__/#ts_shape.events.maintenance","title":"ts_shape.events.maintenance","text":"<p>Maintenance Events</p> <p>Detectors for maintenance-related patterns (e.g., downtime windows).</p> <p>Classes: - None yet: Placeholder module for future maintenance event detectors.</p>"},{"location":"reference/ts_shape/events/production/__init__/","title":"init","text":""},{"location":"reference/ts_shape/events/production/__init__/#ts_shape.events.production","title":"ts_shape.events.production","text":"<p>Production Events</p> <p>Detectors for production events in long-form timeseries (uuid-per-signal).</p> <ul> <li>MachineStateEvents: Run/idle intervals and transition points from a boolean state signal.</li> <li>detect_run_idle: Intervalize run/idle with optional min duration.</li> <li> <p>transition_events: Point events on idle\u2192run and run\u2192idle changes.</p> </li> <li> <p>LineThroughputEvents: Throughput metrics and takt adherence.</p> </li> <li>count_parts: Parts per fixed window from a counter uuid.</li> <li> <p>takt_adherence: Cycle time violations vs. a takt time.</p> </li> <li> <p>ChangeoverEvents: Product/recipe changes and end-of-changeover derivation.</p> </li> <li>detect_changeover: Point events at product value changes.</li> <li> <p>changeover_window: End via fixed window or stable band metrics.</p> </li> <li> <p>FlowConstraintEvents: Blocked/starved intervals between upstream/downstream run signals.</p> </li> <li>blocked_events: Upstream running while downstream not consuming.</li> <li>starved_events: Downstream running while upstream not supplying.</li> </ul> <p>Modules:</p> <ul> <li> <code>changeover</code>           \u2013            </li> <li> <code>flow_constraints</code>           \u2013            </li> <li> <code>line_throughput</code>           \u2013            </li> <li> <code>machine_state</code>           \u2013            </li> </ul> <p>Classes:</p> <ul> <li> <code>ChangeoverEvents</code>           \u2013            <p>Production: Changeover</p> </li> <li> <code>FlowConstraintEvents</code>           \u2013            <p>Production: Flow Constraints</p> </li> <li> <code>LineThroughputEvents</code>           \u2013            <p>Production: Line Throughput</p> </li> <li> <code>MachineStateEvents</code>           \u2013            <p>Production: Machine State</p> </li> </ul>"},{"location":"reference/ts_shape/events/production/__init__/#ts_shape.events.production.ChangeoverEvents","title":"ChangeoverEvents","text":"<pre><code>ChangeoverEvents(dataframe: DataFrame, *, event_uuid: str = 'prod:changeover', time_column: str = 'systime')\n</code></pre> <p>               Bases: <code>Base</code></p> <p>Production: Changeover</p> <p>Detect product/recipe changes and compute changeover windows without requiring a dedicated 'first good' signal.</p> <p>Methods: - detect_changeover: point events when product/recipe changes. - changeover_window: derive an end time via fixed window or 'stable_band' metrics.</p> <p>Methods:</p> <ul> <li> <code>changeover_quality_metrics</code>             \u2013              <p>Compute quality metrics for changeovers.</p> </li> <li> <code>changeover_window</code>             \u2013              <p>Compute changeover windows per product change with enhanced configurability.</p> </li> <li> <code>detect_changeover</code>             \u2013              <p>Emit point events when the product/recipe changes value.</p> </li> <li> <code>get_dataframe</code>             \u2013              <p>Returns the processed DataFrame.</p> </li> </ul> Source code in <code>src/ts_shape/events/production/changeover.py</code> <pre><code>def __init__(\n    self,\n    dataframe: pd.DataFrame,\n    *,\n    event_uuid: str = \"prod:changeover\",\n    time_column: str = \"systime\",\n) -&gt; None:\n    super().__init__(dataframe, column_name=time_column)\n    self.event_uuid = event_uuid\n    self.time_column = time_column\n</code></pre>"},{"location":"reference/ts_shape/events/production/__init__/#ts_shape.events.production.ChangeoverEvents.changeover_quality_metrics","title":"changeover_quality_metrics","text":"<pre><code>changeover_quality_metrics(product_uuid: str, *, value_column: str = 'value_string') -&gt; DataFrame\n</code></pre> <p>Compute quality metrics for changeovers.</p> <p>Returns metrics including: - changeover duration patterns - frequency statistics - time between changeovers - product-specific metrics</p> Source code in <code>src/ts_shape/events/production/changeover.py</code> <pre><code>def changeover_quality_metrics(\n    self,\n    product_uuid: str,\n    *,\n    value_column: str = \"value_string\",\n) -&gt; pd.DataFrame:\n    \"\"\"Compute quality metrics for changeovers.\n\n    Returns metrics including:\n    - changeover duration patterns\n    - frequency statistics\n    - time between changeovers\n    - product-specific metrics\n    \"\"\"\n    changes = self.detect_changeover(product_uuid, value_column=value_column)\n\n    if changes.empty or len(changes) &lt; 2:\n        return pd.DataFrame(\n            columns=[\n                \"product\", \"changeover_count\", \"avg_time_between_seconds\",\n                \"min_time_between_seconds\", \"max_time_between_seconds\",\n                \"std_time_between_seconds\"\n            ]\n        )\n\n    # Group by product (new_value)\n    product_metrics = []\n    for product in changes[\"new_value\"].unique():\n        product_changes = changes[changes[\"new_value\"] == product]\n        product_times = product_changes[\"systime\"].sort_values()\n\n        if len(product_times) &lt; 2:\n            metrics = {\n                \"product\": product,\n                \"changeover_count\": len(product_changes),\n                \"avg_time_between_seconds\": None,\n                \"min_time_between_seconds\": None,\n                \"max_time_between_seconds\": None,\n                \"std_time_between_seconds\": None,\n            }\n        else:\n            product_diffs = product_times.diff().dt.total_seconds().dropna()\n\n            metrics = {\n                \"product\": product,\n                \"changeover_count\": len(product_changes),\n                \"avg_time_between_seconds\": product_diffs.mean(),\n                \"min_time_between_seconds\": product_diffs.min(),\n                \"max_time_between_seconds\": product_diffs.max(),\n                \"std_time_between_seconds\": product_diffs.std(),\n            }\n        product_metrics.append(metrics)\n\n    return pd.DataFrame(product_metrics)\n</code></pre>"},{"location":"reference/ts_shape/events/production/__init__/#ts_shape.events.production.ChangeoverEvents.changeover_window","title":"changeover_window","text":"<pre><code>changeover_window(product_uuid: str, *, value_column: str = 'value_string', start_time: Optional[Timestamp] = None, until: str = 'fixed_window', config: Optional[Dict[str, Any]] = None, fallback: Optional[Dict[str, Any]] = None) -&gt; DataFrame\n</code></pre> <p>Compute changeover windows per product change with enhanced configurability.</p> until <ul> <li>fixed_window: end = start + config['duration'] (e.g., '10m')</li> <li>stable_band: end when all metrics stabilize within band for hold:       config = {         'metrics': [           {'uuid': 'm1', 'value_column': 'value_double', 'band': 0.2, 'hold': '2m'},           ...         ],         'reference_method': 'expanding_median' | 'rolling_mean' | 'ewma' | 'target_value',         'rolling_window': 5,  # for rolling_mean (number of points)         'ewma_span': 10,  # for ewma         'target_values': {'m1': 100.0, ...}  # for target_value       }</li> </ul> <p>fallback: {'default_duration': '10m', 'completed': False}</p> Source code in <code>src/ts_shape/events/production/changeover.py</code> <pre><code>def changeover_window(\n    self,\n    product_uuid: str,\n    *,\n    value_column: str = \"value_string\",\n    start_time: Optional[pd.Timestamp] = None,\n    until: str = \"fixed_window\",\n    config: Optional[Dict[str, Any]] = None,\n    fallback: Optional[Dict[str, Any]] = None,\n) -&gt; pd.DataFrame:\n    \"\"\"Compute changeover windows per product change with enhanced configurability.\n\n    until:\n      - fixed_window: end = start + config['duration'] (e.g., '10m')\n      - stable_band: end when all metrics stabilize within band for hold:\n            config = {\n              'metrics': [\n                {'uuid': 'm1', 'value_column': 'value_double', 'band': 0.2, 'hold': '2m'},\n                ...\n              ],\n              'reference_method': 'expanding_median' | 'rolling_mean' | 'ewma' | 'target_value',\n              'rolling_window': 5,  # for rolling_mean (number of points)\n              'ewma_span': 10,  # for ewma\n              'target_values': {'m1': 100.0, ...}  # for target_value\n            }\n    fallback: {'default_duration': '10m', 'completed': False}\n    \"\"\"\n    config = config or {}\n    fallback = fallback or {\"default_duration\": \"10m\", \"completed\": False}\n\n    changes = self.detect_changeover(product_uuid, value_column=value_column, min_hold=config.get(\"min_hold\", \"0s\"))\n    if start_time is not None:\n        changes = changes[changes[\"systime\"] &gt;= pd.to_datetime(start_time)]\n    if changes.empty:\n        return pd.DataFrame(\n            columns=[\"start\", \"end\", \"uuid\", \"source_uuid\", \"is_delta\", \"method\", \"completed\"]\n        )\n\n    rows: List[Dict[str, Any]] = []\n    for _, r in changes.iterrows():\n        t0 = pd.to_datetime(r[\"systime\"])\n        if until == \"fixed_window\":\n            duration = pd.to_timedelta(config.get(\"duration\", \"10m\"))\n            end = t0 + duration\n            rows.append(\n                {\n                    \"start\": t0,\n                    \"end\": end,\n                    \"uuid\": self.event_uuid,\n                    \"source_uuid\": product_uuid,\n                    \"is_delta\": True,\n                    \"method\": \"fixed_window\",\n                    \"completed\": True,\n                }\n            )\n            continue\n\n        if until == \"stable_band\":\n            result = self._compute_stable_band_end(t0, config)\n            if result is not None:\n                rows.append(\n                    {\n                        \"start\": t0,\n                        \"end\": result,\n                        \"uuid\": self.event_uuid,\n                        \"source_uuid\": product_uuid,\n                        \"is_delta\": True,\n                        \"method\": \"stable_band\",\n                        \"completed\": True,\n                    }\n                )\n                continue\n\n        # fallback\n        end = t0 + pd.to_timedelta(fallback.get(\"default_duration\", \"10m\"))\n        rows.append(\n            {\n                \"start\": t0,\n                \"end\": end,\n                \"uuid\": self.event_uuid,\n                \"source_uuid\": product_uuid,\n                \"is_delta\": True,\n                \"method\": until,\n                \"completed\": bool(fallback.get(\"completed\", False)),\n            }\n        )\n\n    return pd.DataFrame(rows)\n</code></pre>"},{"location":"reference/ts_shape/events/production/__init__/#ts_shape.events.production.ChangeoverEvents.detect_changeover","title":"detect_changeover","text":"<pre><code>detect_changeover(product_uuid: str, *, value_column: str = 'value_string', min_hold: str = '0s') -&gt; DataFrame\n</code></pre> <p>Emit point events when the product/recipe changes value.</p> <p>Uses a hold check: the new product must persist for at least min_hold until the next change.</p> Source code in <code>src/ts_shape/events/production/changeover.py</code> <pre><code>def detect_changeover(\n    self,\n    product_uuid: str,\n    *,\n    value_column: str = \"value_string\",\n    min_hold: str = \"0s\",\n) -&gt; pd.DataFrame:\n    \"\"\"Emit point events when the product/recipe changes value.\n\n    Uses a hold check: the new product must persist for at least min_hold\n    until the next change.\n    \"\"\"\n    p = (\n        self.dataframe[self.dataframe[\"uuid\"] == product_uuid]\n        .copy()\n        .sort_values(self.time_column)\n    )\n    if p.empty:\n        return pd.DataFrame(\n            columns=[\"systime\", \"uuid\", \"source_uuid\", \"is_delta\", \"new_value\"]\n        )\n    p[self.time_column] = pd.to_datetime(p[self.time_column])\n    series = p[value_column]\n    changed = series.ne(series.shift())\n    change_times = p.loc[changed, self.time_column]\n    min_td = pd.to_timedelta(min_hold)\n    next_change = change_times.shift(-1)\n    ok = (next_change - change_times &gt;= min_td) | next_change.isna()\n    change_times = change_times[ok]\n    out = p[p[self.time_column].isin(change_times)][\n        [self.time_column, value_column]\n    ].rename(columns={self.time_column: \"systime\", value_column: \"new_value\"})\n    out[\"uuid\"] = self.event_uuid\n    out[\"source_uuid\"] = product_uuid\n    out[\"is_delta\"] = True\n    return out[[\"systime\", \"uuid\", \"source_uuid\", \"is_delta\", \"new_value\"]]\n</code></pre>"},{"location":"reference/ts_shape/events/production/__init__/#ts_shape.events.production.ChangeoverEvents.get_dataframe","title":"get_dataframe","text":"<pre><code>get_dataframe() -&gt; DataFrame\n</code></pre> <p>Returns the processed DataFrame.</p> Source code in <code>src/ts_shape/utils/base.py</code> <pre><code>def get_dataframe(self) -&gt; pd.DataFrame:\n    \"\"\"Returns the processed DataFrame.\"\"\"\n    return self.dataframe\n</code></pre>"},{"location":"reference/ts_shape/events/production/__init__/#ts_shape.events.production.FlowConstraintEvents","title":"FlowConstraintEvents","text":"<pre><code>FlowConstraintEvents(dataframe: DataFrame, *, time_column: str = 'systime', event_uuid: str = 'prod:flow')\n</code></pre> <p>               Bases: <code>Base</code></p> <p>Production: Flow Constraints</p> <ul> <li>blocked_events: upstream running while downstream not consuming.</li> <li>starved_events: downstream idle due to lack of upstream supply.</li> </ul> <p>Methods:</p> <ul> <li> <code>blocked_events</code>             \u2013              <p>Blocked: upstream_run=True while downstream_run=False.</p> </li> <li> <code>flow_constraint_analytics</code>             \u2013              <p>Generate comprehensive analytics for flow constraints (blockages and starvations).</p> </li> <li> <code>get_dataframe</code>             \u2013              <p>Returns the processed DataFrame.</p> </li> <li> <code>starved_events</code>             \u2013              <p>Starved: downstream_run=True while upstream_run=False.</p> </li> </ul> Source code in <code>src/ts_shape/events/production/flow_constraints.py</code> <pre><code>def __init__(\n    self,\n    dataframe: pd.DataFrame,\n    *,\n    time_column: str = \"systime\",\n    event_uuid: str = \"prod:flow\",\n) -&gt; None:\n    super().__init__(dataframe, column_name=time_column)\n    self.time_column = time_column\n    self.event_uuid = event_uuid\n</code></pre>"},{"location":"reference/ts_shape/events/production/__init__/#ts_shape.events.production.FlowConstraintEvents.blocked_events","title":"blocked_events","text":"<pre><code>blocked_events(*, roles: Dict[str, str], tolerance: str = '200ms', tolerance_before: Optional[str] = None, tolerance_after: Optional[str] = None, min_duration: str = '0s') -&gt; DataFrame\n</code></pre> <p>Blocked: upstream_run=True while downstream_run=False.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>DataFrame with columns: start, end, uuid, source_uuid, is_delta, type,</p> </li> <li> <code>DataFrame</code>           \u2013            <p>time_alignment_quality, duration, severity</p> </li> </ul> Example <p>roles = {'upstream_run': 'uuid1', 'downstream_run': 'uuid2'}</p> Source code in <code>src/ts_shape/events/production/flow_constraints.py</code> <pre><code>def blocked_events(\n    self,\n    *,\n    roles: Dict[str, str],\n    tolerance: str = \"200ms\",\n    tolerance_before: Optional[str] = None,\n    tolerance_after: Optional[str] = None,\n    min_duration: str = \"0s\",\n) -&gt; pd.DataFrame:\n    \"\"\"Blocked: upstream_run=True while downstream_run=False.\n\n    Args:\n        roles: Dictionary mapping role names to UUIDs.\n               Expected keys: 'upstream_run', 'downstream_run'\n        tolerance: Default tolerance for time alignment (used if directional tolerances not provided)\n        tolerance_before: Tolerance for looking backward in time during alignment\n        tolerance_after: Tolerance for looking forward in time during alignment\n        min_duration: Minimum duration for an event to be included\n\n    Returns:\n        DataFrame with columns: start, end, uuid, source_uuid, is_delta, type,\n        time_alignment_quality, duration, severity\n\n    Example:\n        roles = {'upstream_run': 'uuid1', 'downstream_run': 'uuid2'}\n    \"\"\"\n    up = self._align_bool(roles[\"upstream_run\"])  # time, state\n    dn = self._align_bool(roles[\"downstream_run\"])  # time, state\n    if up.empty or dn.empty:\n        return pd.DataFrame(\n            columns=[\"start\", \"end\", \"uuid\", \"source_uuid\", \"is_delta\", \"type\",\n                    \"time_alignment_quality\", \"duration\", \"severity\"]\n        )\n\n    # Use directional tolerances if provided, otherwise use single tolerance\n    tol_before = pd.to_timedelta(tolerance_before) if tolerance_before else pd.to_timedelta(tolerance)\n    tol_after = pd.to_timedelta(tolerance_after) if tolerance_after else pd.to_timedelta(tolerance)\n    max_tol = max(tol_before, tol_after)\n\n    # Merge with maximum tolerance and track time differences\n    merged = pd.merge_asof(\n        up, dn,\n        on=self.time_column,\n        suffixes=(\"_up\", \"_dn\"),\n        tolerance=max_tol,\n        direction=\"nearest\"\n    )\n\n    # Store original upstream time for quality calculation\n    merged['time_up'] = up[self.time_column].values\n\n    # Apply directional tolerance filtering if asymmetric tolerances are specified\n    if tolerance_before or tolerance_after:\n        time_diff = merged[self.time_column + '_dn'] - merged[self.time_column]\n        # Keep only records within directional tolerance bounds\n        valid_mask = (\n            ((time_diff &lt;= pd.Timedelta(0)) &amp; (time_diff.abs() &lt;= tol_before)) |\n            ((time_diff &gt;= pd.Timedelta(0)) &amp; (time_diff &lt;= tol_after))\n        )\n        merged.loc[~valid_mask, 'state_dn'] = pd.NA\n\n    # Calculate alignment quality (percentage of records with matches)\n    alignment_quality = merged['state_dn'].notna().sum() / len(merged) if len(merged) &gt; 0 else 0.0\n\n    cond = merged[\"state_up\"] &amp; (~merged[\"state_dn\"].fillna(False))\n    gid = (cond.ne(cond.shift())).cumsum()\n    min_td = pd.to_timedelta(min_duration)\n    rows: List[Dict[str, Any]] = []\n    for _, seg in merged.groupby(gid):\n        m = cond.loc[seg.index]\n        if not m.any():\n            continue\n        start = seg[self.time_column].iloc[0]\n        end = seg[self.time_column].iloc[-1]\n        duration = end - start\n        if duration &lt; min_td:\n            continue\n\n        # Calculate severity based on duration\n        severity = self._calculate_severity(duration)\n\n        rows.append(\n            {\n                \"start\": start,\n                \"end\": end,\n                \"uuid\": self.event_uuid,\n                \"source_uuid\": roles[\"upstream_run\"],\n                \"is_delta\": True,\n                \"type\": \"blocked\",\n                \"time_alignment_quality\": alignment_quality,\n                \"duration\": duration,\n                \"severity\": severity,\n            }\n        )\n    return pd.DataFrame(rows)\n</code></pre>"},{"location":"reference/ts_shape/events/production/__init__/#ts_shape.events.production.FlowConstraintEvents.blocked_events(roles)","title":"<code>roles</code>","text":"(<code>Dict[str, str]</code>)           \u2013            <p>Dictionary mapping role names to UUIDs.    Expected keys: 'upstream_run', 'downstream_run'</p>"},{"location":"reference/ts_shape/events/production/__init__/#ts_shape.events.production.FlowConstraintEvents.blocked_events(tolerance)","title":"<code>tolerance</code>","text":"(<code>str</code>, default:                   <code>'200ms'</code> )           \u2013            <p>Default tolerance for time alignment (used if directional tolerances not provided)</p>"},{"location":"reference/ts_shape/events/production/__init__/#ts_shape.events.production.FlowConstraintEvents.blocked_events(tolerance_before)","title":"<code>tolerance_before</code>","text":"(<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Tolerance for looking backward in time during alignment</p>"},{"location":"reference/ts_shape/events/production/__init__/#ts_shape.events.production.FlowConstraintEvents.blocked_events(tolerance_after)","title":"<code>tolerance_after</code>","text":"(<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Tolerance for looking forward in time during alignment</p>"},{"location":"reference/ts_shape/events/production/__init__/#ts_shape.events.production.FlowConstraintEvents.blocked_events(min_duration)","title":"<code>min_duration</code>","text":"(<code>str</code>, default:                   <code>'0s'</code> )           \u2013            <p>Minimum duration for an event to be included</p>"},{"location":"reference/ts_shape/events/production/__init__/#ts_shape.events.production.FlowConstraintEvents.flow_constraint_analytics","title":"flow_constraint_analytics","text":"<pre><code>flow_constraint_analytics(*, roles: Dict[str, str], tolerance: str = '200ms', tolerance_before: Optional[str] = None, tolerance_after: Optional[str] = None, min_duration: str = '0s', minor_threshold: str = '5s', moderate_threshold: str = '30s') -&gt; Dict[str, Any]\n</code></pre> <p>Generate comprehensive analytics for flow constraints (blockages and starvations).</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>Dict[str, Any]</code>           \u2013            <p>Dictionary containing analytics for both blocked and starved events:</p> </li> <li> <code>Dict[str, Any]</code>           \u2013            <ul> <li>blocked_events: DataFrame of blocked events</li> </ul> </li> <li> <code>Dict[str, Any]</code>           \u2013            <ul> <li>starved_events: DataFrame of starved events</li> </ul> </li> <li> <code>Dict[str, Any]</code>           \u2013            <ul> <li>summary: Dictionary with statistics including:</li> <li>blocked_count: Total number of blocked events</li> <li>starved_count: Total number of starved events</li> <li>blocked_total_duration: Total duration of blocked events</li> <li>starved_total_duration: Total duration of starved events</li> <li>blocked_avg_duration: Average duration of blocked events</li> <li>starved_avg_duration: Average duration of starved events</li> <li>blocked_severity_breakdown: Count by severity level</li> <li>starved_severity_breakdown: Count by severity level</li> <li>overall_alignment_quality: Average alignment quality across both types</li> </ul> </li> </ul> Example <p>roles = {'upstream_run': 'uuid1', 'downstream_run': 'uuid2'} analytics = flow.flow_constraint_analytics(roles=roles) print(analytics['summary']['blocked_count'])</p> Source code in <code>src/ts_shape/events/production/flow_constraints.py</code> <pre><code>def flow_constraint_analytics(\n    self,\n    *,\n    roles: Dict[str, str],\n    tolerance: str = \"200ms\",\n    tolerance_before: Optional[str] = None,\n    tolerance_after: Optional[str] = None,\n    min_duration: str = \"0s\",\n    minor_threshold: str = \"5s\",\n    moderate_threshold: str = \"30s\",\n) -&gt; Dict[str, Any]:\n    \"\"\"Generate comprehensive analytics for flow constraints (blockages and starvations).\n\n    Args:\n        roles: Dictionary mapping role names to UUIDs.\n               Expected keys: 'upstream_run', 'downstream_run'\n        tolerance: Default tolerance for time alignment (used if directional tolerances not provided)\n        tolerance_before: Tolerance for looking backward in time during alignment\n        tolerance_after: Tolerance for looking forward in time during alignment\n        min_duration: Minimum duration for an event to be included\n        minor_threshold: Duration threshold for minor severity classification\n        moderate_threshold: Duration threshold for moderate severity classification\n\n    Returns:\n        Dictionary containing analytics for both blocked and starved events:\n        - blocked_events: DataFrame of blocked events\n        - starved_events: DataFrame of starved events\n        - summary: Dictionary with statistics including:\n            - blocked_count: Total number of blocked events\n            - starved_count: Total number of starved events\n            - blocked_total_duration: Total duration of blocked events\n            - starved_total_duration: Total duration of starved events\n            - blocked_avg_duration: Average duration of blocked events\n            - starved_avg_duration: Average duration of starved events\n            - blocked_severity_breakdown: Count by severity level\n            - starved_severity_breakdown: Count by severity level\n            - overall_alignment_quality: Average alignment quality across both types\n\n    Example:\n        roles = {'upstream_run': 'uuid1', 'downstream_run': 'uuid2'}\n        analytics = flow.flow_constraint_analytics(roles=roles)\n        print(analytics['summary']['blocked_count'])\n    \"\"\"\n    # Get blocked and starved events\n    blocked_df = self.blocked_events(\n        roles=roles,\n        tolerance=tolerance,\n        tolerance_before=tolerance_before,\n        tolerance_after=tolerance_after,\n        min_duration=min_duration,\n    )\n\n    starved_df = self.starved_events(\n        roles=roles,\n        tolerance=tolerance,\n        tolerance_before=tolerance_before,\n        tolerance_after=tolerance_after,\n        min_duration=min_duration,\n    )\n\n    # Calculate summary statistics\n    summary: Dict[str, Any] = {}\n\n    # Blocked events statistics\n    if not blocked_df.empty:\n        summary[\"blocked_count\"] = len(blocked_df)\n        summary[\"blocked_total_duration\"] = blocked_df[\"duration\"].sum()\n        summary[\"blocked_avg_duration\"] = blocked_df[\"duration\"].mean()\n        summary[\"blocked_severity_breakdown\"] = blocked_df[\"severity\"].value_counts().to_dict()\n        blocked_alignment_quality = blocked_df[\"time_alignment_quality\"].iloc[0] if len(blocked_df) &gt; 0 else 0.0\n    else:\n        summary[\"blocked_count\"] = 0\n        summary[\"blocked_total_duration\"] = pd.Timedelta(0)\n        summary[\"blocked_avg_duration\"] = pd.Timedelta(0)\n        summary[\"blocked_severity_breakdown\"] = {\"minor\": 0, \"moderate\": 0, \"severe\": 0}\n        blocked_alignment_quality = 0.0\n\n    # Starved events statistics\n    if not starved_df.empty:\n        summary[\"starved_count\"] = len(starved_df)\n        summary[\"starved_total_duration\"] = starved_df[\"duration\"].sum()\n        summary[\"starved_avg_duration\"] = starved_df[\"duration\"].mean()\n        summary[\"starved_severity_breakdown\"] = starved_df[\"severity\"].value_counts().to_dict()\n        starved_alignment_quality = starved_df[\"time_alignment_quality\"].iloc[0] if len(starved_df) &gt; 0 else 0.0\n    else:\n        summary[\"starved_count\"] = 0\n        summary[\"starved_total_duration\"] = pd.Timedelta(0)\n        summary[\"starved_avg_duration\"] = pd.Timedelta(0)\n        summary[\"starved_severity_breakdown\"] = {\"minor\": 0, \"moderate\": 0, \"severe\": 0}\n        starved_alignment_quality = 0.0\n\n    # Overall alignment quality (average of both)\n    quality_values = [blocked_alignment_quality, starved_alignment_quality]\n    summary[\"overall_alignment_quality\"] = sum(quality_values) / len(quality_values) if quality_values else 0.0\n\n    # Total events\n    summary[\"total_constraint_events\"] = summary[\"blocked_count\"] + summary[\"starved_count\"]\n\n    return {\n        \"blocked_events\": blocked_df,\n        \"starved_events\": starved_df,\n        \"summary\": summary,\n    }\n</code></pre>"},{"location":"reference/ts_shape/events/production/__init__/#ts_shape.events.production.FlowConstraintEvents.flow_constraint_analytics(roles)","title":"<code>roles</code>","text":"(<code>Dict[str, str]</code>)           \u2013            <p>Dictionary mapping role names to UUIDs.    Expected keys: 'upstream_run', 'downstream_run'</p>"},{"location":"reference/ts_shape/events/production/__init__/#ts_shape.events.production.FlowConstraintEvents.flow_constraint_analytics(tolerance)","title":"<code>tolerance</code>","text":"(<code>str</code>, default:                   <code>'200ms'</code> )           \u2013            <p>Default tolerance for time alignment (used if directional tolerances not provided)</p>"},{"location":"reference/ts_shape/events/production/__init__/#ts_shape.events.production.FlowConstraintEvents.flow_constraint_analytics(tolerance_before)","title":"<code>tolerance_before</code>","text":"(<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Tolerance for looking backward in time during alignment</p>"},{"location":"reference/ts_shape/events/production/__init__/#ts_shape.events.production.FlowConstraintEvents.flow_constraint_analytics(tolerance_after)","title":"<code>tolerance_after</code>","text":"(<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Tolerance for looking forward in time during alignment</p>"},{"location":"reference/ts_shape/events/production/__init__/#ts_shape.events.production.FlowConstraintEvents.flow_constraint_analytics(min_duration)","title":"<code>min_duration</code>","text":"(<code>str</code>, default:                   <code>'0s'</code> )           \u2013            <p>Minimum duration for an event to be included</p>"},{"location":"reference/ts_shape/events/production/__init__/#ts_shape.events.production.FlowConstraintEvents.flow_constraint_analytics(minor_threshold)","title":"<code>minor_threshold</code>","text":"(<code>str</code>, default:                   <code>'5s'</code> )           \u2013            <p>Duration threshold for minor severity classification</p>"},{"location":"reference/ts_shape/events/production/__init__/#ts_shape.events.production.FlowConstraintEvents.flow_constraint_analytics(moderate_threshold)","title":"<code>moderate_threshold</code>","text":"(<code>str</code>, default:                   <code>'30s'</code> )           \u2013            <p>Duration threshold for moderate severity classification</p>"},{"location":"reference/ts_shape/events/production/__init__/#ts_shape.events.production.FlowConstraintEvents.get_dataframe","title":"get_dataframe","text":"<pre><code>get_dataframe() -&gt; DataFrame\n</code></pre> <p>Returns the processed DataFrame.</p> Source code in <code>src/ts_shape/utils/base.py</code> <pre><code>def get_dataframe(self) -&gt; pd.DataFrame:\n    \"\"\"Returns the processed DataFrame.\"\"\"\n    return self.dataframe\n</code></pre>"},{"location":"reference/ts_shape/events/production/__init__/#ts_shape.events.production.FlowConstraintEvents.starved_events","title":"starved_events","text":"<pre><code>starved_events(*, roles: Dict[str, str], tolerance: str = '200ms', tolerance_before: Optional[str] = None, tolerance_after: Optional[str] = None, min_duration: str = '0s') -&gt; DataFrame\n</code></pre> <p>Starved: downstream_run=True while upstream_run=False.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>DataFrame with columns: start, end, uuid, source_uuid, is_delta, type,</p> </li> <li> <code>DataFrame</code>           \u2013            <p>time_alignment_quality, duration, severity</p> </li> </ul> Example <p>roles = {'upstream_run': 'uuid1', 'downstream_run': 'uuid2'}</p> Source code in <code>src/ts_shape/events/production/flow_constraints.py</code> <pre><code>def starved_events(\n    self,\n    *,\n    roles: Dict[str, str],\n    tolerance: str = \"200ms\",\n    tolerance_before: Optional[str] = None,\n    tolerance_after: Optional[str] = None,\n    min_duration: str = \"0s\",\n) -&gt; pd.DataFrame:\n    \"\"\"Starved: downstream_run=True while upstream_run=False.\n\n    Args:\n        roles: Dictionary mapping role names to UUIDs.\n               Expected keys: 'upstream_run', 'downstream_run'\n        tolerance: Default tolerance for time alignment (used if directional tolerances not provided)\n        tolerance_before: Tolerance for looking backward in time during alignment\n        tolerance_after: Tolerance for looking forward in time during alignment\n        min_duration: Minimum duration for an event to be included\n\n    Returns:\n        DataFrame with columns: start, end, uuid, source_uuid, is_delta, type,\n        time_alignment_quality, duration, severity\n\n    Example:\n        roles = {'upstream_run': 'uuid1', 'downstream_run': 'uuid2'}\n    \"\"\"\n    up = self._align_bool(roles[\"upstream_run\"])  # time, state\n    dn = self._align_bool(roles[\"downstream_run\"])  # time, state\n    if up.empty or dn.empty:\n        return pd.DataFrame(\n            columns=[\"start\", \"end\", \"uuid\", \"source_uuid\", \"is_delta\", \"type\",\n                    \"time_alignment_quality\", \"duration\", \"severity\"]\n        )\n\n    # Use directional tolerances if provided, otherwise use single tolerance\n    tol_before = pd.to_timedelta(tolerance_before) if tolerance_before else pd.to_timedelta(tolerance)\n    tol_after = pd.to_timedelta(tolerance_after) if tolerance_after else pd.to_timedelta(tolerance)\n    max_tol = max(tol_before, tol_after)\n\n    # Merge with maximum tolerance and track time differences\n    merged = pd.merge_asof(\n        dn, up,\n        on=self.time_column,\n        suffixes=(\"_dn\", \"_up\"),\n        tolerance=max_tol,\n        direction=\"nearest\"\n    )\n\n    # Store original downstream time for quality calculation\n    merged['time_dn'] = dn[self.time_column].values\n\n    # Apply directional tolerance filtering if asymmetric tolerances are specified\n    if tolerance_before or tolerance_after:\n        time_diff = merged[self.time_column + '_up'] - merged[self.time_column]\n        # Keep only records within directional tolerance bounds\n        valid_mask = (\n            ((time_diff &lt;= pd.Timedelta(0)) &amp; (time_diff.abs() &lt;= tol_before)) |\n            ((time_diff &gt;= pd.Timedelta(0)) &amp; (time_diff &lt;= tol_after))\n        )\n        merged.loc[~valid_mask, 'state_up'] = pd.NA\n\n    # Calculate alignment quality (percentage of records with matches)\n    alignment_quality = merged['state_up'].notna().sum() / len(merged) if len(merged) &gt; 0 else 0.0\n\n    cond = merged[\"state_dn\"] &amp; (~merged[\"state_up\"].fillna(False))\n    gid = (cond.ne(cond.shift())).cumsum()\n    min_td = pd.to_timedelta(min_duration)\n    rows: List[Dict[str, Any]] = []\n    for _, seg in merged.groupby(gid):\n        m = cond.loc[seg.index]\n        if not m.any():\n            continue\n        start = seg[self.time_column].iloc[0]\n        end = seg[self.time_column].iloc[-1]\n        duration = end - start\n        if duration &lt; min_td:\n            continue\n\n        # Calculate severity based on duration\n        severity = self._calculate_severity(duration)\n\n        rows.append(\n            {\n                \"start\": start,\n                \"end\": end,\n                \"uuid\": self.event_uuid,\n                \"source_uuid\": roles[\"downstream_run\"],\n                \"is_delta\": True,\n                \"type\": \"starved\",\n                \"time_alignment_quality\": alignment_quality,\n                \"duration\": duration,\n                \"severity\": severity,\n            }\n        )\n    return pd.DataFrame(rows)\n</code></pre>"},{"location":"reference/ts_shape/events/production/__init__/#ts_shape.events.production.FlowConstraintEvents.starved_events(roles)","title":"<code>roles</code>","text":"(<code>Dict[str, str]</code>)           \u2013            <p>Dictionary mapping role names to UUIDs.    Expected keys: 'upstream_run', 'downstream_run'</p>"},{"location":"reference/ts_shape/events/production/__init__/#ts_shape.events.production.FlowConstraintEvents.starved_events(tolerance)","title":"<code>tolerance</code>","text":"(<code>str</code>, default:                   <code>'200ms'</code> )           \u2013            <p>Default tolerance for time alignment (used if directional tolerances not provided)</p>"},{"location":"reference/ts_shape/events/production/__init__/#ts_shape.events.production.FlowConstraintEvents.starved_events(tolerance_before)","title":"<code>tolerance_before</code>","text":"(<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Tolerance for looking backward in time during alignment</p>"},{"location":"reference/ts_shape/events/production/__init__/#ts_shape.events.production.FlowConstraintEvents.starved_events(tolerance_after)","title":"<code>tolerance_after</code>","text":"(<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Tolerance for looking forward in time during alignment</p>"},{"location":"reference/ts_shape/events/production/__init__/#ts_shape.events.production.FlowConstraintEvents.starved_events(min_duration)","title":"<code>min_duration</code>","text":"(<code>str</code>, default:                   <code>'0s'</code> )           \u2013            <p>Minimum duration for an event to be included</p>"},{"location":"reference/ts_shape/events/production/__init__/#ts_shape.events.production.LineThroughputEvents","title":"LineThroughputEvents","text":"<pre><code>LineThroughputEvents(dataframe: DataFrame, *, event_uuid: str = 'prod:throughput', time_column: str = 'systime')\n</code></pre> <p>               Bases: <code>Base</code></p> <p>Production: Line Throughput</p> <p>Methods: - count_parts: Part counts per fixed window from a monotonically increasing counter. - takt_adherence: Cycle time violations against a takt time from step/boolean triggers.</p> <p>Methods:</p> <ul> <li> <code>count_parts</code>             \u2013              <p>Compute parts per window for a counter uuid.</p> </li> <li> <code>cycle_quality_check</code>             \u2013              <p>Enhanced cycle detection with quality validation.</p> </li> <li> <code>get_dataframe</code>             \u2013              <p>Returns the processed DataFrame.</p> </li> <li> <code>takt_adherence</code>             \u2013              <p>Flag cycles whose durations exceed the takt_time.</p> </li> <li> <code>throughput_oee</code>             \u2013              <p>Calculate Overall Equipment Effectiveness (OEE) metrics.</p> </li> <li> <code>throughput_trends</code>             \u2013              <p>Analyze throughput trends with moving averages and degradation detection.</p> </li> </ul> Source code in <code>src/ts_shape/events/production/line_throughput.py</code> <pre><code>def __init__(\n    self,\n    dataframe: pd.DataFrame,\n    *,\n    event_uuid: str = \"prod:throughput\",\n    time_column: str = \"systime\",\n) -&gt; None:\n    super().__init__(dataframe, column_name=time_column)\n    self.event_uuid = event_uuid\n    self.time_column = time_column\n</code></pre>"},{"location":"reference/ts_shape/events/production/__init__/#ts_shape.events.production.LineThroughputEvents.count_parts","title":"count_parts","text":"<pre><code>count_parts(counter_uuid: str, *, value_column: str = 'value_integer', window: str = '1m') -&gt; DataFrame\n</code></pre> <p>Compute parts per window for a counter uuid.</p> <p>Returns columns: window_start, uuid, source_uuid, is_delta, count</p> Source code in <code>src/ts_shape/events/production/line_throughput.py</code> <pre><code>def count_parts(\n    self,\n    counter_uuid: str,\n    *,\n    value_column: str = \"value_integer\",\n    window: str = \"1m\",\n) -&gt; pd.DataFrame:\n    \"\"\"Compute parts per window for a counter uuid.\n\n    Returns columns: window_start, uuid, source_uuid, is_delta, count\n    \"\"\"\n    c = (\n        self.dataframe[self.dataframe[\"uuid\"] == counter_uuid]\n        .copy()\n        .sort_values(self.time_column)\n    )\n    if c.empty:\n        return pd.DataFrame(\n            columns=[\"window_start\", \"uuid\", \"source_uuid\", \"is_delta\", \"count\"]\n        )\n    c[self.time_column] = pd.to_datetime(c[self.time_column])\n    c = c.set_index(self.time_column)\n    # take diff of last values within each window\n    grp = c[value_column].resample(window)\n    counts = grp.max().ffill().diff().fillna(0).clip(lower=0)\n    out = counts.to_frame(\"count\").reset_index().rename(columns={self.time_column: \"window_start\"})\n    out[\"uuid\"] = self.event_uuid\n    out[\"source_uuid\"] = counter_uuid\n    out[\"is_delta\"] = True\n    return out\n</code></pre>"},{"location":"reference/ts_shape/events/production/__init__/#ts_shape.events.production.LineThroughputEvents.cycle_quality_check","title":"cycle_quality_check","text":"<pre><code>cycle_quality_check(cycle_uuid: str, *, value_column: str = 'value_bool', expected_cycle_time: Optional[float] = None, tolerance_pct: float = 0.1) -&gt; DataFrame\n</code></pre> <p>Enhanced cycle detection with quality validation.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>DataFrame with cycle times, validation status, and quality flags</p> </li> </ul> Source code in <code>src/ts_shape/events/production/line_throughput.py</code> <pre><code>def cycle_quality_check(\n    self,\n    cycle_uuid: str,\n    *,\n    value_column: str = \"value_bool\",\n    expected_cycle_time: Optional[float] = None,\n    tolerance_pct: float = 0.1,\n) -&gt; pd.DataFrame:\n    \"\"\"Enhanced cycle detection with quality validation.\n\n    Args:\n        cycle_uuid: UUID for the cycle trigger signal\n        value_column: Column containing cycle trigger (bool/integer)\n        expected_cycle_time: Expected cycle time in seconds. If None, uses median\n        tolerance_pct: Tolerance percentage for cycle time validation\n\n    Returns:\n        DataFrame with cycle times, validation status, and quality flags\n    \"\"\"\n    s = (\n        self.dataframe[self.dataframe[\"uuid\"] == cycle_uuid]\n        .copy()\n        .sort_values(self.time_column)\n    )\n    if s.empty:\n        return pd.DataFrame(\n            columns=[\n                \"systime\", \"uuid\", \"source_uuid\", \"is_delta\",\n                \"cycle_time_seconds\", \"expected_time\", \"deviation_pct\",\n                \"is_valid\", \"quality_flag\"\n            ]\n        )\n\n    s[self.time_column] = pd.to_datetime(s[self.time_column])\n\n    # Detect cycle boundaries\n    if value_column == \"value_bool\":\n        s[\"prev\"] = s[value_column].shift(fill_value=False)\n        edges = s[(~s[\"prev\"]) &amp; (s[value_column].fillna(False))]\n        times = edges[self.time_column].reset_index(drop=True)\n    else:\n        s[\"prev\"] = s[value_column].shift(1)\n        edges = s[s[value_column].fillna(0) != s[\"prev\"].fillna(0)]\n        times = edges[self.time_column].reset_index(drop=True)\n\n    if len(times) &lt; 2:\n        return pd.DataFrame(\n            columns=[\n                \"systime\", \"uuid\", \"source_uuid\", \"is_delta\",\n                \"cycle_time_seconds\", \"expected_time\", \"deviation_pct\",\n                \"is_valid\", \"quality_flag\"\n            ]\n        )\n\n    cycle_times = (times.diff().dt.total_seconds()).iloc[1:].reset_index(drop=True)\n\n    # Calculate expected cycle time if not provided\n    if expected_cycle_time is None:\n        expected_cycle_time = cycle_times.median()\n\n    # Calculate deviation\n    deviation_pct = ((cycle_times - expected_cycle_time) / expected_cycle_time).abs()\n\n    # Validate cycles\n    is_valid = deviation_pct &lt;= tolerance_pct\n\n    # Quality flags: good, warning, critical\n    quality_flag = pd.cut(\n        deviation_pct,\n        bins=[-np.inf, 0.1, 0.25, np.inf],\n        labels=[\"good\", \"warning\", \"critical\"]\n    )\n\n    out = pd.DataFrame(\n        {\n            \"systime\": times.iloc[1:].reset_index(drop=True),\n            \"uuid\": self.event_uuid,\n            \"source_uuid\": cycle_uuid,\n            \"is_delta\": True,\n            \"cycle_time_seconds\": cycle_times,\n            \"expected_time\": expected_cycle_time,\n            \"deviation_pct\": deviation_pct,\n            \"is_valid\": is_valid,\n            \"quality_flag\": quality_flag,\n        }\n    )\n    return out\n</code></pre>"},{"location":"reference/ts_shape/events/production/__init__/#ts_shape.events.production.LineThroughputEvents.cycle_quality_check(cycle_uuid)","title":"<code>cycle_uuid</code>","text":"(<code>str</code>)           \u2013            <p>UUID for the cycle trigger signal</p>"},{"location":"reference/ts_shape/events/production/__init__/#ts_shape.events.production.LineThroughputEvents.cycle_quality_check(value_column)","title":"<code>value_column</code>","text":"(<code>str</code>, default:                   <code>'value_bool'</code> )           \u2013            <p>Column containing cycle trigger (bool/integer)</p>"},{"location":"reference/ts_shape/events/production/__init__/#ts_shape.events.production.LineThroughputEvents.cycle_quality_check(expected_cycle_time)","title":"<code>expected_cycle_time</code>","text":"(<code>Optional[float]</code>, default:                   <code>None</code> )           \u2013            <p>Expected cycle time in seconds. If None, uses median</p>"},{"location":"reference/ts_shape/events/production/__init__/#ts_shape.events.production.LineThroughputEvents.cycle_quality_check(tolerance_pct)","title":"<code>tolerance_pct</code>","text":"(<code>float</code>, default:                   <code>0.1</code> )           \u2013            <p>Tolerance percentage for cycle time validation</p>"},{"location":"reference/ts_shape/events/production/__init__/#ts_shape.events.production.LineThroughputEvents.get_dataframe","title":"get_dataframe","text":"<pre><code>get_dataframe() -&gt; DataFrame\n</code></pre> <p>Returns the processed DataFrame.</p> Source code in <code>src/ts_shape/utils/base.py</code> <pre><code>def get_dataframe(self) -&gt; pd.DataFrame:\n    \"\"\"Returns the processed DataFrame.\"\"\"\n    return self.dataframe\n</code></pre>"},{"location":"reference/ts_shape/events/production/__init__/#ts_shape.events.production.LineThroughputEvents.takt_adherence","title":"takt_adherence","text":"<pre><code>takt_adherence(cycle_uuid: str, *, value_column: str = 'value_bool', takt_time: str = '60s', min_violation: str = '0s') -&gt; DataFrame\n</code></pre> <p>Flag cycles whose durations exceed the takt_time.</p> <p>For boolean triggers: detect True rising edges as cycle boundaries. For integer steps: detect increments as cycle boundaries.</p> <p>Returns: systime (at boundary), uuid, source_uuid, is_delta, cycle_time_seconds, violation</p> Source code in <code>src/ts_shape/events/production/line_throughput.py</code> <pre><code>def takt_adherence(\n    self,\n    cycle_uuid: str,\n    *,\n    value_column: str = \"value_bool\",\n    takt_time: str = \"60s\",\n    min_violation: str = \"0s\",\n) -&gt; pd.DataFrame:\n    \"\"\"Flag cycles whose durations exceed the takt_time.\n\n    For boolean triggers: detect True rising edges as cycle boundaries.\n    For integer steps: detect increments as cycle boundaries.\n\n    Returns: systime (at boundary), uuid, source_uuid, is_delta, cycle_time_seconds, violation\n    \"\"\"\n    s = (\n        self.dataframe[self.dataframe[\"uuid\"] == cycle_uuid]\n        .copy()\n        .sort_values(self.time_column)\n    )\n    if s.empty:\n        return pd.DataFrame(\n            columns=[\n                \"systime\",\n                \"uuid\",\n                \"source_uuid\",\n                \"is_delta\",\n                \"cycle_time_seconds\",\n                \"violation\",\n            ]\n        )\n    s[self.time_column] = pd.to_datetime(s[self.time_column])\n    if value_column == \"value_bool\":\n        s[\"prev\"] = s[value_column].shift(fill_value=False)\n        edges = s[(~s[\"prev\"]) &amp; (s[value_column].fillna(False))]\n        times = edges[self.time_column].reset_index(drop=True)\n    else:\n        s[\"prev\"] = s[value_column].shift(1)\n        edges = s[s[value_column].fillna(0) != s[\"prev\"].fillna(0)]\n        times = edges[self.time_column].reset_index(drop=True)\n    if len(times) &lt; 2:\n        return pd.DataFrame(\n            columns=[\n                \"systime\",\n                \"uuid\",\n                \"source_uuid\",\n                \"is_delta\",\n                \"cycle_time_seconds\",\n                \"violation\",\n            ]\n        )\n    cycle_times = (times.diff().dt.total_seconds()).iloc[1:].reset_index(drop=True)\n    min_td = pd.to_timedelta(min_violation).total_seconds()\n    target = pd.to_timedelta(takt_time).total_seconds()\n    viol = (cycle_times - target) &gt;= min_td\n    out = pd.DataFrame(\n        {\n            \"systime\": times.iloc[1:].reset_index(drop=True),\n            \"uuid\": self.event_uuid,\n            \"source_uuid\": cycle_uuid,\n            \"is_delta\": True,\n            \"cycle_time_seconds\": cycle_times,\n            \"violation\": viol,\n        }\n    )\n    return out\n</code></pre>"},{"location":"reference/ts_shape/events/production/__init__/#ts_shape.events.production.LineThroughputEvents.throughput_oee","title":"throughput_oee","text":"<pre><code>throughput_oee(counter_uuid: str, *, value_column: str = 'value_integer', window: str = '1h', target_rate: Optional[float] = None, availability_threshold: float = 0.95) -&gt; DataFrame\n</code></pre> <p>Calculate Overall Equipment Effectiveness (OEE) metrics.</p> <p>OEE = Availability \u00d7 Performance \u00d7 Quality</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>DataFrame with columns: window_start, uuid, source_uuid, is_delta,</p> </li> <li> <code>DataFrame</code>           \u2013            <p>actual_count, target_count, availability, performance, oee_score</p> </li> </ul> Source code in <code>src/ts_shape/events/production/line_throughput.py</code> <pre><code>def throughput_oee(\n    self,\n    counter_uuid: str,\n    *,\n    value_column: str = \"value_integer\",\n    window: str = \"1h\",\n    target_rate: Optional[float] = None,\n    availability_threshold: float = 0.95,\n) -&gt; pd.DataFrame:\n    \"\"\"Calculate Overall Equipment Effectiveness (OEE) metrics.\n\n    OEE = Availability \u00d7 Performance \u00d7 Quality\n\n    Args:\n        counter_uuid: UUID for the part counter signal\n        value_column: Column containing counter values\n        window: Time window for aggregation\n        target_rate: Target production rate (parts per window). If None, uses max observed\n        availability_threshold: Threshold for considering equipment available\n\n    Returns:\n        DataFrame with columns: window_start, uuid, source_uuid, is_delta,\n        actual_count, target_count, availability, performance, oee_score\n    \"\"\"\n    parts_df = self.count_parts(counter_uuid, value_column=value_column, window=window)\n\n    if parts_df.empty:\n        return pd.DataFrame(\n            columns=[\n                \"window_start\", \"uuid\", \"source_uuid\", \"is_delta\",\n                \"actual_count\", \"target_count\", \"availability\", \"performance\", \"oee_score\"\n            ]\n        )\n\n    # Calculate target rate if not provided\n    if target_rate is None:\n        target_rate = parts_df[\"count\"].quantile(0.95)\n\n    parts_df[\"target_count\"] = target_rate\n    parts_df[\"actual_count\"] = parts_df[\"count\"]\n\n    # Availability: percentage of time equipment was running\n    parts_df[\"availability\"] = np.where(\n        parts_df[\"count\"] &gt; 0,\n        1.0,\n        0.0\n    )\n\n    # Performance: actual vs target rate\n    parts_df[\"performance\"] = np.minimum(\n        parts_df[\"count\"] / target_rate,\n        1.0\n    )\n\n    # OEE score (simplified - assumes quality = 1.0)\n    parts_df[\"oee_score\"] = parts_df[\"availability\"] * parts_df[\"performance\"]\n\n    return parts_df[[\n        \"window_start\", \"uuid\", \"source_uuid\", \"is_delta\",\n        \"actual_count\", \"target_count\", \"availability\", \"performance\", \"oee_score\"\n    ]]\n</code></pre>"},{"location":"reference/ts_shape/events/production/__init__/#ts_shape.events.production.LineThroughputEvents.throughput_oee(counter_uuid)","title":"<code>counter_uuid</code>","text":"(<code>str</code>)           \u2013            <p>UUID for the part counter signal</p>"},{"location":"reference/ts_shape/events/production/__init__/#ts_shape.events.production.LineThroughputEvents.throughput_oee(value_column)","title":"<code>value_column</code>","text":"(<code>str</code>, default:                   <code>'value_integer'</code> )           \u2013            <p>Column containing counter values</p>"},{"location":"reference/ts_shape/events/production/__init__/#ts_shape.events.production.LineThroughputEvents.throughput_oee(window)","title":"<code>window</code>","text":"(<code>str</code>, default:                   <code>'1h'</code> )           \u2013            <p>Time window for aggregation</p>"},{"location":"reference/ts_shape/events/production/__init__/#ts_shape.events.production.LineThroughputEvents.throughput_oee(target_rate)","title":"<code>target_rate</code>","text":"(<code>Optional[float]</code>, default:                   <code>None</code> )           \u2013            <p>Target production rate (parts per window). If None, uses max observed</p>"},{"location":"reference/ts_shape/events/production/__init__/#ts_shape.events.production.LineThroughputEvents.throughput_oee(availability_threshold)","title":"<code>availability_threshold</code>","text":"(<code>float</code>, default:                   <code>0.95</code> )           \u2013            <p>Threshold for considering equipment available</p>"},{"location":"reference/ts_shape/events/production/__init__/#ts_shape.events.production.LineThroughputEvents.throughput_trends","title":"throughput_trends","text":"<pre><code>throughput_trends(counter_uuid: str, *, value_column: str = 'value_integer', window: str = '1h', trend_window: int = 24) -&gt; DataFrame\n</code></pre> <p>Analyze throughput trends with moving averages and degradation detection.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>DataFrame with throughput, moving average, trend direction, and degradation flag</p> </li> </ul> Source code in <code>src/ts_shape/events/production/line_throughput.py</code> <pre><code>def throughput_trends(\n    self,\n    counter_uuid: str,\n    *,\n    value_column: str = \"value_integer\",\n    window: str = \"1h\",\n    trend_window: int = 24,\n) -&gt; pd.DataFrame:\n    \"\"\"Analyze throughput trends with moving averages and degradation detection.\n\n    Args:\n        counter_uuid: UUID for the part counter signal\n        value_column: Column containing counter values\n        window: Time window for counting parts\n        trend_window: Number of windows for trend calculation\n\n    Returns:\n        DataFrame with throughput, moving average, trend direction, and degradation flag\n    \"\"\"\n    parts_df = self.count_parts(counter_uuid, value_column=value_column, window=window)\n\n    if parts_df.empty or len(parts_df) &lt; trend_window:\n        return pd.DataFrame(\n            columns=[\n                \"window_start\", \"uuid\", \"source_uuid\", \"is_delta\", \"count\",\n                \"moving_avg\", \"trend_direction\", \"degradation_detected\"\n            ]\n        )\n\n    # Calculate moving average\n    parts_df[\"moving_avg\"] = parts_df[\"count\"].rolling(window=trend_window, min_periods=1).mean()\n\n    # Calculate trend (positive, negative, stable)\n    parts_df[\"trend_slope\"] = parts_df[\"moving_avg\"].diff()\n    parts_df[\"trend_direction\"] = pd.cut(\n        parts_df[\"trend_slope\"],\n        bins=[-np.inf, -0.5, 0.5, np.inf],\n        labels=[\"decreasing\", \"stable\", \"increasing\"]\n    )\n\n    # Detect degradation (current significantly below moving average)\n    parts_df[\"degradation_detected\"] = parts_df[\"count\"] &lt; (parts_df[\"moving_avg\"] * 0.85)\n\n    return parts_df[[\n        \"window_start\", \"uuid\", \"source_uuid\", \"is_delta\", \"count\",\n        \"moving_avg\", \"trend_direction\", \"degradation_detected\"\n    ]]\n</code></pre>"},{"location":"reference/ts_shape/events/production/__init__/#ts_shape.events.production.LineThroughputEvents.throughput_trends(counter_uuid)","title":"<code>counter_uuid</code>","text":"(<code>str</code>)           \u2013            <p>UUID for the part counter signal</p>"},{"location":"reference/ts_shape/events/production/__init__/#ts_shape.events.production.LineThroughputEvents.throughput_trends(value_column)","title":"<code>value_column</code>","text":"(<code>str</code>, default:                   <code>'value_integer'</code> )           \u2013            <p>Column containing counter values</p>"},{"location":"reference/ts_shape/events/production/__init__/#ts_shape.events.production.LineThroughputEvents.throughput_trends(window)","title":"<code>window</code>","text":"(<code>str</code>, default:                   <code>'1h'</code> )           \u2013            <p>Time window for counting parts</p>"},{"location":"reference/ts_shape/events/production/__init__/#ts_shape.events.production.LineThroughputEvents.throughput_trends(trend_window)","title":"<code>trend_window</code>","text":"(<code>int</code>, default:                   <code>24</code> )           \u2013            <p>Number of windows for trend calculation</p>"},{"location":"reference/ts_shape/events/production/__init__/#ts_shape.events.production.MachineStateEvents","title":"MachineStateEvents","text":"<pre><code>MachineStateEvents(dataframe: DataFrame, run_state_uuid: str, *, event_uuid: str = 'prod:run_idle', value_column: str = 'value_bool', time_column: str = 'systime')\n</code></pre> <p>               Bases: <code>Base</code></p> <p>Production: Machine State</p> <p>Detect run/idle transitions and intervals from a boolean state signal.</p> <ul> <li>MachineStateEvents: Run/idle state intervals and transitions.</li> <li>detect_run_idle: Intervalize run/idle states with optional min duration filter.</li> <li>transition_events: Point events on state changes (idle-&gt;run, run-&gt;idle).</li> </ul> <p>Methods:</p> <ul> <li> <code>detect_rapid_transitions</code>             \u2013              <p>Identify suspicious rapid state changes.</p> </li> <li> <code>detect_run_idle</code>             \u2013              <p>Return intervals labeled as 'run' or 'idle'.</p> </li> <li> <code>get_dataframe</code>             \u2013              <p>Returns the processed DataFrame.</p> </li> <li> <code>state_quality_metrics</code>             \u2013              <p>Return quality metrics for the state data.</p> </li> <li> <code>transition_events</code>             \u2013              <p>Return point events at state transitions.</p> </li> </ul> Source code in <code>src/ts_shape/events/production/machine_state.py</code> <pre><code>def __init__(\n    self,\n    dataframe: pd.DataFrame,\n    run_state_uuid: str,\n    *,\n    event_uuid: str = \"prod:run_idle\",\n    value_column: str = \"value_bool\",\n    time_column: str = \"systime\",\n) -&gt; None:\n    super().__init__(dataframe, column_name=time_column)\n    self.run_state_uuid = run_state_uuid\n    self.event_uuid = event_uuid\n    self.value_column = value_column\n    self.time_column = time_column\n    self.series = (\n        self.dataframe[self.dataframe[\"uuid\"] == self.run_state_uuid]\n        .copy()\n        .sort_values(self.time_column)\n    )\n    self.series[self.time_column] = pd.to_datetime(self.series[self.time_column])\n    self._state_groups: Optional[pd.Series] = None\n    self._compute_state_groups()\n</code></pre>"},{"location":"reference/ts_shape/events/production/__init__/#ts_shape.events.production.MachineStateEvents.detect_rapid_transitions","title":"detect_rapid_transitions","text":"<pre><code>detect_rapid_transitions(threshold: str = '5s', min_count: int = 3) -&gt; DataFrame\n</code></pre> <p>Identify suspicious rapid state changes.</p> <ul> <li>threshold: time window to look for rapid transitions</li> <li>min_count: minimum number of transitions within threshold to be considered rapid Returns: DataFrame with start_time, end_time, transition_count, duration_seconds</li> </ul> Source code in <code>src/ts_shape/events/production/machine_state.py</code> <pre><code>def detect_rapid_transitions(self, threshold: str = \"5s\", min_count: int = 3) -&gt; pd.DataFrame:\n    \"\"\"Identify suspicious rapid state changes.\n\n    - threshold: time window to look for rapid transitions\n    - min_count: minimum number of transitions within threshold to be considered rapid\n    Returns: DataFrame with start_time, end_time, transition_count, duration_seconds\n    \"\"\"\n    transitions = self.transition_events()\n    if transitions.empty or len(transitions) &lt; min_count:\n        return pd.DataFrame(\n            columns=[\"start_time\", \"end_time\", \"transition_count\", \"duration_seconds\"]\n        )\n\n    threshold_td = pd.to_timedelta(threshold)\n    rapid_events: List[Dict[str, Any]] = []\n\n    for i in range(len(transitions) - min_count + 1):\n        window_start = transitions.iloc[i][\"systime\"]\n        for j in range(i + min_count - 1, len(transitions)):\n            window_end = transitions.iloc[j][\"systime\"]\n            duration = window_end - window_start\n            if duration &lt;= threshold_td:\n                transition_count = j - i + 1\n                rapid_events.append({\n                    \"start_time\": window_start,\n                    \"end_time\": window_end,\n                    \"transition_count\": transition_count,\n                    \"duration_seconds\": duration.total_seconds(),\n                })\n            else:\n                break\n\n    return pd.DataFrame(rapid_events)\n</code></pre>"},{"location":"reference/ts_shape/events/production/__init__/#ts_shape.events.production.MachineStateEvents.detect_run_idle","title":"detect_run_idle","text":"<pre><code>detect_run_idle(min_duration: str = '0s') -&gt; DataFrame\n</code></pre> <p>Return intervals labeled as 'run' or 'idle'.</p> <ul> <li>min_duration: discard intervals shorter than this duration. Columns: start, end, uuid, source_uuid, is_delta, state, duration_seconds</li> </ul> Source code in <code>src/ts_shape/events/production/machine_state.py</code> <pre><code>def detect_run_idle(self, min_duration: str = \"0s\") -&gt; pd.DataFrame:\n    \"\"\"Return intervals labeled as 'run' or 'idle'.\n\n    - min_duration: discard intervals shorter than this duration.\n    Columns: start, end, uuid, source_uuid, is_delta, state, duration_seconds\n    \"\"\"\n    if self.series.empty:\n        return pd.DataFrame(\n            columns=[\"start\", \"end\", \"uuid\", \"source_uuid\", \"is_delta\", \"state\", \"duration_seconds\"]\n        )\n    s = self.series[[self.time_column, self.value_column]].copy()\n    s[\"state\"] = s[self.value_column].fillna(False).astype(bool)\n    state_change = (s[\"state\"] != s[\"state\"].shift()).cumsum()\n    min_td = pd.to_timedelta(min_duration)\n    rows: List[Dict[str, Any]] = []\n    for _, seg in s.groupby(state_change):\n        state = bool(seg[\"state\"].iloc[0])\n        start = seg[self.time_column].iloc[0]\n        end = seg[self.time_column].iloc[-1]\n        if (end - start) &lt; min_td:\n            continue\n        rows.append(\n            {\n                \"start\": start,\n                \"end\": end,\n                \"uuid\": self.event_uuid,\n                \"source_uuid\": self.run_state_uuid,\n                \"is_delta\": True,\n                \"state\": \"run\" if state else \"idle\",\n                \"duration_seconds\": (end - start).total_seconds(),\n            }\n        )\n    return pd.DataFrame(rows)\n</code></pre>"},{"location":"reference/ts_shape/events/production/__init__/#ts_shape.events.production.MachineStateEvents.get_dataframe","title":"get_dataframe","text":"<pre><code>get_dataframe() -&gt; DataFrame\n</code></pre> <p>Returns the processed DataFrame.</p> Source code in <code>src/ts_shape/utils/base.py</code> <pre><code>def get_dataframe(self) -&gt; pd.DataFrame:\n    \"\"\"Returns the processed DataFrame.\"\"\"\n    return self.dataframe\n</code></pre>"},{"location":"reference/ts_shape/events/production/__init__/#ts_shape.events.production.MachineStateEvents.state_quality_metrics","title":"state_quality_metrics","text":"<pre><code>state_quality_metrics() -&gt; Dict[str, Any]\n</code></pre> <p>Return quality metrics for the state data.</p> <p>Returns dictionary with: - total_transitions: total number of state transitions - avg_run_duration: average duration of run states in seconds - avg_idle_duration: average duration of idle states in seconds - run_idle_ratio: ratio of run time to idle time - data_gaps_detected: number of data gaps found - rapid_transitions_detected: number of rapid transition events</p> Source code in <code>src/ts_shape/events/production/machine_state.py</code> <pre><code>def state_quality_metrics(self) -&gt; Dict[str, Any]:\n    \"\"\"Return quality metrics for the state data.\n\n    Returns dictionary with:\n    - total_transitions: total number of state transitions\n    - avg_run_duration: average duration of run states in seconds\n    - avg_idle_duration: average duration of idle states in seconds\n    - run_idle_ratio: ratio of run time to idle time\n    - data_gaps_detected: number of data gaps found\n    - rapid_transitions_detected: number of rapid transition events\n    \"\"\"\n    transitions = self.transition_events()\n    intervals = self.detect_run_idle()\n\n    total_transitions = len(transitions)\n\n    if intervals.empty:\n        avg_run_duration = 0.0\n        avg_idle_duration = 0.0\n        run_idle_ratio = 0.0\n    else:\n        run_intervals = intervals[intervals[\"state\"] == \"run\"]\n        idle_intervals = intervals[intervals[\"state\"] == \"idle\"]\n\n        avg_run_duration = run_intervals[\"duration_seconds\"].mean() if not run_intervals.empty else 0.0\n        avg_idle_duration = idle_intervals[\"duration_seconds\"].mean() if not idle_intervals.empty else 0.0\n\n        total_run_time = run_intervals[\"duration_seconds\"].sum() if not run_intervals.empty else 0.0\n        total_idle_time = idle_intervals[\"duration_seconds\"].sum() if not idle_intervals.empty else 0.0\n        run_idle_ratio = total_run_time / total_idle_time if total_idle_time &gt; 0 else 0.0\n\n    data_gaps_detected = self._detect_data_gaps()\n    rapid_transitions_detected = len(self.detect_rapid_transitions())\n\n    return {\n        \"total_transitions\": total_transitions,\n        \"avg_run_duration\": float(avg_run_duration) if not np.isnan(avg_run_duration) else 0.0,\n        \"avg_idle_duration\": float(avg_idle_duration) if not np.isnan(avg_idle_duration) else 0.0,\n        \"run_idle_ratio\": float(run_idle_ratio) if not np.isnan(run_idle_ratio) else 0.0,\n        \"data_gaps_detected\": data_gaps_detected,\n        \"rapid_transitions_detected\": rapid_transitions_detected,\n    }\n</code></pre>"},{"location":"reference/ts_shape/events/production/__init__/#ts_shape.events.production.MachineStateEvents.transition_events","title":"transition_events","text":"<pre><code>transition_events() -&gt; DataFrame\n</code></pre> <p>Return point events at state transitions.</p> <p>Columns: systime, uuid, source_uuid, is_delta, transition ('idle_to_run'|'run_to_idle'), time_since_last_transition_seconds</p> Source code in <code>src/ts_shape/events/production/machine_state.py</code> <pre><code>def transition_events(self) -&gt; pd.DataFrame:\n    \"\"\"Return point events at state transitions.\n\n    Columns: systime, uuid, source_uuid, is_delta, transition ('idle_to_run'|'run_to_idle'), time_since_last_transition_seconds\n    \"\"\"\n    if self.series.empty:\n        return pd.DataFrame(\n            columns=[\"systime\", \"uuid\", \"source_uuid\", \"is_delta\", \"transition\", \"time_since_last_transition_seconds\"]\n        )\n    s = self.series[[self.time_column, self.value_column]].copy()\n    s[\"state\"] = s[self.value_column].fillna(False).astype(bool)\n    s[\"prev\"] = s[\"state\"].shift()\n    changes = s[s[\"state\"] != s[\"prev\"]].dropna(subset=[\"prev\"])  # ignore first row\n    if changes.empty:\n        return pd.DataFrame(\n            columns=[\"systime\", \"uuid\", \"source_uuid\", \"is_delta\", \"transition\", \"time_since_last_transition_seconds\"]\n        )\n    changes = changes.rename(columns={self.time_column: \"systime\"})\n    changes[\"transition\"] = changes.apply(\n        lambda r: \"idle_to_run\" if (r[\"prev\"] is False and r[\"state\"] is True) else \"run_to_idle\",\n        axis=1,\n    )\n    changes[\"time_since_last_transition_seconds\"] = changes[\"systime\"].diff().dt.total_seconds()\n    return pd.DataFrame(\n        {\n            \"systime\": changes[\"systime\"],\n            \"uuid\": self.event_uuid,\n            \"source_uuid\": self.run_state_uuid,\n            \"is_delta\": True,\n            \"transition\": changes[\"transition\"],\n            \"time_since_last_transition_seconds\": changes[\"time_since_last_transition_seconds\"],\n        }\n    )\n</code></pre>"},{"location":"reference/ts_shape/events/production/changeover/","title":"changeover","text":""},{"location":"reference/ts_shape/events/production/changeover/#ts_shape.events.production.changeover","title":"ts_shape.events.production.changeover","text":"<p>Classes:</p> <ul> <li> <code>ChangeoverEvents</code>           \u2013            <p>Production: Changeover</p> </li> </ul>"},{"location":"reference/ts_shape/events/production/changeover/#ts_shape.events.production.changeover.ChangeoverEvents","title":"ChangeoverEvents","text":"<pre><code>ChangeoverEvents(dataframe: DataFrame, *, event_uuid: str = 'prod:changeover', time_column: str = 'systime')\n</code></pre> <p>               Bases: <code>Base</code></p> <p>Production: Changeover</p> <p>Detect product/recipe changes and compute changeover windows without requiring a dedicated 'first good' signal.</p> <p>Methods: - detect_changeover: point events when product/recipe changes. - changeover_window: derive an end time via fixed window or 'stable_band' metrics.</p> <p>Methods:</p> <ul> <li> <code>changeover_quality_metrics</code>             \u2013              <p>Compute quality metrics for changeovers.</p> </li> <li> <code>changeover_window</code>             \u2013              <p>Compute changeover windows per product change with enhanced configurability.</p> </li> <li> <code>detect_changeover</code>             \u2013              <p>Emit point events when the product/recipe changes value.</p> </li> <li> <code>get_dataframe</code>             \u2013              <p>Returns the processed DataFrame.</p> </li> </ul> Source code in <code>src/ts_shape/events/production/changeover.py</code> <pre><code>def __init__(\n    self,\n    dataframe: pd.DataFrame,\n    *,\n    event_uuid: str = \"prod:changeover\",\n    time_column: str = \"systime\",\n) -&gt; None:\n    super().__init__(dataframe, column_name=time_column)\n    self.event_uuid = event_uuid\n    self.time_column = time_column\n</code></pre>"},{"location":"reference/ts_shape/events/production/changeover/#ts_shape.events.production.changeover.ChangeoverEvents.changeover_quality_metrics","title":"changeover_quality_metrics","text":"<pre><code>changeover_quality_metrics(product_uuid: str, *, value_column: str = 'value_string') -&gt; DataFrame\n</code></pre> <p>Compute quality metrics for changeovers.</p> <p>Returns metrics including: - changeover duration patterns - frequency statistics - time between changeovers - product-specific metrics</p> Source code in <code>src/ts_shape/events/production/changeover.py</code> <pre><code>def changeover_quality_metrics(\n    self,\n    product_uuid: str,\n    *,\n    value_column: str = \"value_string\",\n) -&gt; pd.DataFrame:\n    \"\"\"Compute quality metrics for changeovers.\n\n    Returns metrics including:\n    - changeover duration patterns\n    - frequency statistics\n    - time between changeovers\n    - product-specific metrics\n    \"\"\"\n    changes = self.detect_changeover(product_uuid, value_column=value_column)\n\n    if changes.empty or len(changes) &lt; 2:\n        return pd.DataFrame(\n            columns=[\n                \"product\", \"changeover_count\", \"avg_time_between_seconds\",\n                \"min_time_between_seconds\", \"max_time_between_seconds\",\n                \"std_time_between_seconds\"\n            ]\n        )\n\n    # Group by product (new_value)\n    product_metrics = []\n    for product in changes[\"new_value\"].unique():\n        product_changes = changes[changes[\"new_value\"] == product]\n        product_times = product_changes[\"systime\"].sort_values()\n\n        if len(product_times) &lt; 2:\n            metrics = {\n                \"product\": product,\n                \"changeover_count\": len(product_changes),\n                \"avg_time_between_seconds\": None,\n                \"min_time_between_seconds\": None,\n                \"max_time_between_seconds\": None,\n                \"std_time_between_seconds\": None,\n            }\n        else:\n            product_diffs = product_times.diff().dt.total_seconds().dropna()\n\n            metrics = {\n                \"product\": product,\n                \"changeover_count\": len(product_changes),\n                \"avg_time_between_seconds\": product_diffs.mean(),\n                \"min_time_between_seconds\": product_diffs.min(),\n                \"max_time_between_seconds\": product_diffs.max(),\n                \"std_time_between_seconds\": product_diffs.std(),\n            }\n        product_metrics.append(metrics)\n\n    return pd.DataFrame(product_metrics)\n</code></pre>"},{"location":"reference/ts_shape/events/production/changeover/#ts_shape.events.production.changeover.ChangeoverEvents.changeover_window","title":"changeover_window","text":"<pre><code>changeover_window(product_uuid: str, *, value_column: str = 'value_string', start_time: Optional[Timestamp] = None, until: str = 'fixed_window', config: Optional[Dict[str, Any]] = None, fallback: Optional[Dict[str, Any]] = None) -&gt; DataFrame\n</code></pre> <p>Compute changeover windows per product change with enhanced configurability.</p> until <ul> <li>fixed_window: end = start + config['duration'] (e.g., '10m')</li> <li>stable_band: end when all metrics stabilize within band for hold:       config = {         'metrics': [           {'uuid': 'm1', 'value_column': 'value_double', 'band': 0.2, 'hold': '2m'},           ...         ],         'reference_method': 'expanding_median' | 'rolling_mean' | 'ewma' | 'target_value',         'rolling_window': 5,  # for rolling_mean (number of points)         'ewma_span': 10,  # for ewma         'target_values': {'m1': 100.0, ...}  # for target_value       }</li> </ul> <p>fallback: {'default_duration': '10m', 'completed': False}</p> Source code in <code>src/ts_shape/events/production/changeover.py</code> <pre><code>def changeover_window(\n    self,\n    product_uuid: str,\n    *,\n    value_column: str = \"value_string\",\n    start_time: Optional[pd.Timestamp] = None,\n    until: str = \"fixed_window\",\n    config: Optional[Dict[str, Any]] = None,\n    fallback: Optional[Dict[str, Any]] = None,\n) -&gt; pd.DataFrame:\n    \"\"\"Compute changeover windows per product change with enhanced configurability.\n\n    until:\n      - fixed_window: end = start + config['duration'] (e.g., '10m')\n      - stable_band: end when all metrics stabilize within band for hold:\n            config = {\n              'metrics': [\n                {'uuid': 'm1', 'value_column': 'value_double', 'band': 0.2, 'hold': '2m'},\n                ...\n              ],\n              'reference_method': 'expanding_median' | 'rolling_mean' | 'ewma' | 'target_value',\n              'rolling_window': 5,  # for rolling_mean (number of points)\n              'ewma_span': 10,  # for ewma\n              'target_values': {'m1': 100.0, ...}  # for target_value\n            }\n    fallback: {'default_duration': '10m', 'completed': False}\n    \"\"\"\n    config = config or {}\n    fallback = fallback or {\"default_duration\": \"10m\", \"completed\": False}\n\n    changes = self.detect_changeover(product_uuid, value_column=value_column, min_hold=config.get(\"min_hold\", \"0s\"))\n    if start_time is not None:\n        changes = changes[changes[\"systime\"] &gt;= pd.to_datetime(start_time)]\n    if changes.empty:\n        return pd.DataFrame(\n            columns=[\"start\", \"end\", \"uuid\", \"source_uuid\", \"is_delta\", \"method\", \"completed\"]\n        )\n\n    rows: List[Dict[str, Any]] = []\n    for _, r in changes.iterrows():\n        t0 = pd.to_datetime(r[\"systime\"])\n        if until == \"fixed_window\":\n            duration = pd.to_timedelta(config.get(\"duration\", \"10m\"))\n            end = t0 + duration\n            rows.append(\n                {\n                    \"start\": t0,\n                    \"end\": end,\n                    \"uuid\": self.event_uuid,\n                    \"source_uuid\": product_uuid,\n                    \"is_delta\": True,\n                    \"method\": \"fixed_window\",\n                    \"completed\": True,\n                }\n            )\n            continue\n\n        if until == \"stable_band\":\n            result = self._compute_stable_band_end(t0, config)\n            if result is not None:\n                rows.append(\n                    {\n                        \"start\": t0,\n                        \"end\": result,\n                        \"uuid\": self.event_uuid,\n                        \"source_uuid\": product_uuid,\n                        \"is_delta\": True,\n                        \"method\": \"stable_band\",\n                        \"completed\": True,\n                    }\n                )\n                continue\n\n        # fallback\n        end = t0 + pd.to_timedelta(fallback.get(\"default_duration\", \"10m\"))\n        rows.append(\n            {\n                \"start\": t0,\n                \"end\": end,\n                \"uuid\": self.event_uuid,\n                \"source_uuid\": product_uuid,\n                \"is_delta\": True,\n                \"method\": until,\n                \"completed\": bool(fallback.get(\"completed\", False)),\n            }\n        )\n\n    return pd.DataFrame(rows)\n</code></pre>"},{"location":"reference/ts_shape/events/production/changeover/#ts_shape.events.production.changeover.ChangeoverEvents.detect_changeover","title":"detect_changeover","text":"<pre><code>detect_changeover(product_uuid: str, *, value_column: str = 'value_string', min_hold: str = '0s') -&gt; DataFrame\n</code></pre> <p>Emit point events when the product/recipe changes value.</p> <p>Uses a hold check: the new product must persist for at least min_hold until the next change.</p> Source code in <code>src/ts_shape/events/production/changeover.py</code> <pre><code>def detect_changeover(\n    self,\n    product_uuid: str,\n    *,\n    value_column: str = \"value_string\",\n    min_hold: str = \"0s\",\n) -&gt; pd.DataFrame:\n    \"\"\"Emit point events when the product/recipe changes value.\n\n    Uses a hold check: the new product must persist for at least min_hold\n    until the next change.\n    \"\"\"\n    p = (\n        self.dataframe[self.dataframe[\"uuid\"] == product_uuid]\n        .copy()\n        .sort_values(self.time_column)\n    )\n    if p.empty:\n        return pd.DataFrame(\n            columns=[\"systime\", \"uuid\", \"source_uuid\", \"is_delta\", \"new_value\"]\n        )\n    p[self.time_column] = pd.to_datetime(p[self.time_column])\n    series = p[value_column]\n    changed = series.ne(series.shift())\n    change_times = p.loc[changed, self.time_column]\n    min_td = pd.to_timedelta(min_hold)\n    next_change = change_times.shift(-1)\n    ok = (next_change - change_times &gt;= min_td) | next_change.isna()\n    change_times = change_times[ok]\n    out = p[p[self.time_column].isin(change_times)][\n        [self.time_column, value_column]\n    ].rename(columns={self.time_column: \"systime\", value_column: \"new_value\"})\n    out[\"uuid\"] = self.event_uuid\n    out[\"source_uuid\"] = product_uuid\n    out[\"is_delta\"] = True\n    return out[[\"systime\", \"uuid\", \"source_uuid\", \"is_delta\", \"new_value\"]]\n</code></pre>"},{"location":"reference/ts_shape/events/production/changeover/#ts_shape.events.production.changeover.ChangeoverEvents.get_dataframe","title":"get_dataframe","text":"<pre><code>get_dataframe() -&gt; DataFrame\n</code></pre> <p>Returns the processed DataFrame.</p> Source code in <code>src/ts_shape/utils/base.py</code> <pre><code>def get_dataframe(self) -&gt; pd.DataFrame:\n    \"\"\"Returns the processed DataFrame.\"\"\"\n    return self.dataframe\n</code></pre>"},{"location":"reference/ts_shape/events/production/downtime/","title":"downtime","text":""},{"location":"reference/ts_shape/events/production/downtime/#ts_shape.events.production.downtime","title":"ts_shape.events.production.downtime","text":""},{"location":"reference/ts_shape/events/production/flow_constraints/","title":"flow_constraints","text":""},{"location":"reference/ts_shape/events/production/flow_constraints/#ts_shape.events.production.flow_constraints","title":"ts_shape.events.production.flow_constraints","text":"<p>Classes:</p> <ul> <li> <code>FlowConstraintEvents</code>           \u2013            <p>Production: Flow Constraints</p> </li> </ul>"},{"location":"reference/ts_shape/events/production/flow_constraints/#ts_shape.events.production.flow_constraints.FlowConstraintEvents","title":"FlowConstraintEvents","text":"<pre><code>FlowConstraintEvents(dataframe: DataFrame, *, time_column: str = 'systime', event_uuid: str = 'prod:flow')\n</code></pre> <p>               Bases: <code>Base</code></p> <p>Production: Flow Constraints</p> <ul> <li>blocked_events: upstream running while downstream not consuming.</li> <li>starved_events: downstream idle due to lack of upstream supply.</li> </ul> <p>Methods:</p> <ul> <li> <code>blocked_events</code>             \u2013              <p>Blocked: upstream_run=True while downstream_run=False.</p> </li> <li> <code>flow_constraint_analytics</code>             \u2013              <p>Generate comprehensive analytics for flow constraints (blockages and starvations).</p> </li> <li> <code>get_dataframe</code>             \u2013              <p>Returns the processed DataFrame.</p> </li> <li> <code>starved_events</code>             \u2013              <p>Starved: downstream_run=True while upstream_run=False.</p> </li> </ul> Source code in <code>src/ts_shape/events/production/flow_constraints.py</code> <pre><code>def __init__(\n    self,\n    dataframe: pd.DataFrame,\n    *,\n    time_column: str = \"systime\",\n    event_uuid: str = \"prod:flow\",\n) -&gt; None:\n    super().__init__(dataframe, column_name=time_column)\n    self.time_column = time_column\n    self.event_uuid = event_uuid\n</code></pre>"},{"location":"reference/ts_shape/events/production/flow_constraints/#ts_shape.events.production.flow_constraints.FlowConstraintEvents.blocked_events","title":"blocked_events","text":"<pre><code>blocked_events(*, roles: Dict[str, str], tolerance: str = '200ms', tolerance_before: Optional[str] = None, tolerance_after: Optional[str] = None, min_duration: str = '0s') -&gt; DataFrame\n</code></pre> <p>Blocked: upstream_run=True while downstream_run=False.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>DataFrame with columns: start, end, uuid, source_uuid, is_delta, type,</p> </li> <li> <code>DataFrame</code>           \u2013            <p>time_alignment_quality, duration, severity</p> </li> </ul> Example <p>roles = {'upstream_run': 'uuid1', 'downstream_run': 'uuid2'}</p> Source code in <code>src/ts_shape/events/production/flow_constraints.py</code> <pre><code>def blocked_events(\n    self,\n    *,\n    roles: Dict[str, str],\n    tolerance: str = \"200ms\",\n    tolerance_before: Optional[str] = None,\n    tolerance_after: Optional[str] = None,\n    min_duration: str = \"0s\",\n) -&gt; pd.DataFrame:\n    \"\"\"Blocked: upstream_run=True while downstream_run=False.\n\n    Args:\n        roles: Dictionary mapping role names to UUIDs.\n               Expected keys: 'upstream_run', 'downstream_run'\n        tolerance: Default tolerance for time alignment (used if directional tolerances not provided)\n        tolerance_before: Tolerance for looking backward in time during alignment\n        tolerance_after: Tolerance for looking forward in time during alignment\n        min_duration: Minimum duration for an event to be included\n\n    Returns:\n        DataFrame with columns: start, end, uuid, source_uuid, is_delta, type,\n        time_alignment_quality, duration, severity\n\n    Example:\n        roles = {'upstream_run': 'uuid1', 'downstream_run': 'uuid2'}\n    \"\"\"\n    up = self._align_bool(roles[\"upstream_run\"])  # time, state\n    dn = self._align_bool(roles[\"downstream_run\"])  # time, state\n    if up.empty or dn.empty:\n        return pd.DataFrame(\n            columns=[\"start\", \"end\", \"uuid\", \"source_uuid\", \"is_delta\", \"type\",\n                    \"time_alignment_quality\", \"duration\", \"severity\"]\n        )\n\n    # Use directional tolerances if provided, otherwise use single tolerance\n    tol_before = pd.to_timedelta(tolerance_before) if tolerance_before else pd.to_timedelta(tolerance)\n    tol_after = pd.to_timedelta(tolerance_after) if tolerance_after else pd.to_timedelta(tolerance)\n    max_tol = max(tol_before, tol_after)\n\n    # Merge with maximum tolerance and track time differences\n    merged = pd.merge_asof(\n        up, dn,\n        on=self.time_column,\n        suffixes=(\"_up\", \"_dn\"),\n        tolerance=max_tol,\n        direction=\"nearest\"\n    )\n\n    # Store original upstream time for quality calculation\n    merged['time_up'] = up[self.time_column].values\n\n    # Apply directional tolerance filtering if asymmetric tolerances are specified\n    if tolerance_before or tolerance_after:\n        time_diff = merged[self.time_column + '_dn'] - merged[self.time_column]\n        # Keep only records within directional tolerance bounds\n        valid_mask = (\n            ((time_diff &lt;= pd.Timedelta(0)) &amp; (time_diff.abs() &lt;= tol_before)) |\n            ((time_diff &gt;= pd.Timedelta(0)) &amp; (time_diff &lt;= tol_after))\n        )\n        merged.loc[~valid_mask, 'state_dn'] = pd.NA\n\n    # Calculate alignment quality (percentage of records with matches)\n    alignment_quality = merged['state_dn'].notna().sum() / len(merged) if len(merged) &gt; 0 else 0.0\n\n    cond = merged[\"state_up\"] &amp; (~merged[\"state_dn\"].fillna(False))\n    gid = (cond.ne(cond.shift())).cumsum()\n    min_td = pd.to_timedelta(min_duration)\n    rows: List[Dict[str, Any]] = []\n    for _, seg in merged.groupby(gid):\n        m = cond.loc[seg.index]\n        if not m.any():\n            continue\n        start = seg[self.time_column].iloc[0]\n        end = seg[self.time_column].iloc[-1]\n        duration = end - start\n        if duration &lt; min_td:\n            continue\n\n        # Calculate severity based on duration\n        severity = self._calculate_severity(duration)\n\n        rows.append(\n            {\n                \"start\": start,\n                \"end\": end,\n                \"uuid\": self.event_uuid,\n                \"source_uuid\": roles[\"upstream_run\"],\n                \"is_delta\": True,\n                \"type\": \"blocked\",\n                \"time_alignment_quality\": alignment_quality,\n                \"duration\": duration,\n                \"severity\": severity,\n            }\n        )\n    return pd.DataFrame(rows)\n</code></pre>"},{"location":"reference/ts_shape/events/production/flow_constraints/#ts_shape.events.production.flow_constraints.FlowConstraintEvents.blocked_events(roles)","title":"<code>roles</code>","text":"(<code>Dict[str, str]</code>)           \u2013            <p>Dictionary mapping role names to UUIDs.    Expected keys: 'upstream_run', 'downstream_run'</p>"},{"location":"reference/ts_shape/events/production/flow_constraints/#ts_shape.events.production.flow_constraints.FlowConstraintEvents.blocked_events(tolerance)","title":"<code>tolerance</code>","text":"(<code>str</code>, default:                   <code>'200ms'</code> )           \u2013            <p>Default tolerance for time alignment (used if directional tolerances not provided)</p>"},{"location":"reference/ts_shape/events/production/flow_constraints/#ts_shape.events.production.flow_constraints.FlowConstraintEvents.blocked_events(tolerance_before)","title":"<code>tolerance_before</code>","text":"(<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Tolerance for looking backward in time during alignment</p>"},{"location":"reference/ts_shape/events/production/flow_constraints/#ts_shape.events.production.flow_constraints.FlowConstraintEvents.blocked_events(tolerance_after)","title":"<code>tolerance_after</code>","text":"(<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Tolerance for looking forward in time during alignment</p>"},{"location":"reference/ts_shape/events/production/flow_constraints/#ts_shape.events.production.flow_constraints.FlowConstraintEvents.blocked_events(min_duration)","title":"<code>min_duration</code>","text":"(<code>str</code>, default:                   <code>'0s'</code> )           \u2013            <p>Minimum duration for an event to be included</p>"},{"location":"reference/ts_shape/events/production/flow_constraints/#ts_shape.events.production.flow_constraints.FlowConstraintEvents.flow_constraint_analytics","title":"flow_constraint_analytics","text":"<pre><code>flow_constraint_analytics(*, roles: Dict[str, str], tolerance: str = '200ms', tolerance_before: Optional[str] = None, tolerance_after: Optional[str] = None, min_duration: str = '0s', minor_threshold: str = '5s', moderate_threshold: str = '30s') -&gt; Dict[str, Any]\n</code></pre> <p>Generate comprehensive analytics for flow constraints (blockages and starvations).</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>Dict[str, Any]</code>           \u2013            <p>Dictionary containing analytics for both blocked and starved events:</p> </li> <li> <code>Dict[str, Any]</code>           \u2013            <ul> <li>blocked_events: DataFrame of blocked events</li> </ul> </li> <li> <code>Dict[str, Any]</code>           \u2013            <ul> <li>starved_events: DataFrame of starved events</li> </ul> </li> <li> <code>Dict[str, Any]</code>           \u2013            <ul> <li>summary: Dictionary with statistics including:</li> <li>blocked_count: Total number of blocked events</li> <li>starved_count: Total number of starved events</li> <li>blocked_total_duration: Total duration of blocked events</li> <li>starved_total_duration: Total duration of starved events</li> <li>blocked_avg_duration: Average duration of blocked events</li> <li>starved_avg_duration: Average duration of starved events</li> <li>blocked_severity_breakdown: Count by severity level</li> <li>starved_severity_breakdown: Count by severity level</li> <li>overall_alignment_quality: Average alignment quality across both types</li> </ul> </li> </ul> Example <p>roles = {'upstream_run': 'uuid1', 'downstream_run': 'uuid2'} analytics = flow.flow_constraint_analytics(roles=roles) print(analytics['summary']['blocked_count'])</p> Source code in <code>src/ts_shape/events/production/flow_constraints.py</code> <pre><code>def flow_constraint_analytics(\n    self,\n    *,\n    roles: Dict[str, str],\n    tolerance: str = \"200ms\",\n    tolerance_before: Optional[str] = None,\n    tolerance_after: Optional[str] = None,\n    min_duration: str = \"0s\",\n    minor_threshold: str = \"5s\",\n    moderate_threshold: str = \"30s\",\n) -&gt; Dict[str, Any]:\n    \"\"\"Generate comprehensive analytics for flow constraints (blockages and starvations).\n\n    Args:\n        roles: Dictionary mapping role names to UUIDs.\n               Expected keys: 'upstream_run', 'downstream_run'\n        tolerance: Default tolerance for time alignment (used if directional tolerances not provided)\n        tolerance_before: Tolerance for looking backward in time during alignment\n        tolerance_after: Tolerance for looking forward in time during alignment\n        min_duration: Minimum duration for an event to be included\n        minor_threshold: Duration threshold for minor severity classification\n        moderate_threshold: Duration threshold for moderate severity classification\n\n    Returns:\n        Dictionary containing analytics for both blocked and starved events:\n        - blocked_events: DataFrame of blocked events\n        - starved_events: DataFrame of starved events\n        - summary: Dictionary with statistics including:\n            - blocked_count: Total number of blocked events\n            - starved_count: Total number of starved events\n            - blocked_total_duration: Total duration of blocked events\n            - starved_total_duration: Total duration of starved events\n            - blocked_avg_duration: Average duration of blocked events\n            - starved_avg_duration: Average duration of starved events\n            - blocked_severity_breakdown: Count by severity level\n            - starved_severity_breakdown: Count by severity level\n            - overall_alignment_quality: Average alignment quality across both types\n\n    Example:\n        roles = {'upstream_run': 'uuid1', 'downstream_run': 'uuid2'}\n        analytics = flow.flow_constraint_analytics(roles=roles)\n        print(analytics['summary']['blocked_count'])\n    \"\"\"\n    # Get blocked and starved events\n    blocked_df = self.blocked_events(\n        roles=roles,\n        tolerance=tolerance,\n        tolerance_before=tolerance_before,\n        tolerance_after=tolerance_after,\n        min_duration=min_duration,\n    )\n\n    starved_df = self.starved_events(\n        roles=roles,\n        tolerance=tolerance,\n        tolerance_before=tolerance_before,\n        tolerance_after=tolerance_after,\n        min_duration=min_duration,\n    )\n\n    # Calculate summary statistics\n    summary: Dict[str, Any] = {}\n\n    # Blocked events statistics\n    if not blocked_df.empty:\n        summary[\"blocked_count\"] = len(blocked_df)\n        summary[\"blocked_total_duration\"] = blocked_df[\"duration\"].sum()\n        summary[\"blocked_avg_duration\"] = blocked_df[\"duration\"].mean()\n        summary[\"blocked_severity_breakdown\"] = blocked_df[\"severity\"].value_counts().to_dict()\n        blocked_alignment_quality = blocked_df[\"time_alignment_quality\"].iloc[0] if len(blocked_df) &gt; 0 else 0.0\n    else:\n        summary[\"blocked_count\"] = 0\n        summary[\"blocked_total_duration\"] = pd.Timedelta(0)\n        summary[\"blocked_avg_duration\"] = pd.Timedelta(0)\n        summary[\"blocked_severity_breakdown\"] = {\"minor\": 0, \"moderate\": 0, \"severe\": 0}\n        blocked_alignment_quality = 0.0\n\n    # Starved events statistics\n    if not starved_df.empty:\n        summary[\"starved_count\"] = len(starved_df)\n        summary[\"starved_total_duration\"] = starved_df[\"duration\"].sum()\n        summary[\"starved_avg_duration\"] = starved_df[\"duration\"].mean()\n        summary[\"starved_severity_breakdown\"] = starved_df[\"severity\"].value_counts().to_dict()\n        starved_alignment_quality = starved_df[\"time_alignment_quality\"].iloc[0] if len(starved_df) &gt; 0 else 0.0\n    else:\n        summary[\"starved_count\"] = 0\n        summary[\"starved_total_duration\"] = pd.Timedelta(0)\n        summary[\"starved_avg_duration\"] = pd.Timedelta(0)\n        summary[\"starved_severity_breakdown\"] = {\"minor\": 0, \"moderate\": 0, \"severe\": 0}\n        starved_alignment_quality = 0.0\n\n    # Overall alignment quality (average of both)\n    quality_values = [blocked_alignment_quality, starved_alignment_quality]\n    summary[\"overall_alignment_quality\"] = sum(quality_values) / len(quality_values) if quality_values else 0.0\n\n    # Total events\n    summary[\"total_constraint_events\"] = summary[\"blocked_count\"] + summary[\"starved_count\"]\n\n    return {\n        \"blocked_events\": blocked_df,\n        \"starved_events\": starved_df,\n        \"summary\": summary,\n    }\n</code></pre>"},{"location":"reference/ts_shape/events/production/flow_constraints/#ts_shape.events.production.flow_constraints.FlowConstraintEvents.flow_constraint_analytics(roles)","title":"<code>roles</code>","text":"(<code>Dict[str, str]</code>)           \u2013            <p>Dictionary mapping role names to UUIDs.    Expected keys: 'upstream_run', 'downstream_run'</p>"},{"location":"reference/ts_shape/events/production/flow_constraints/#ts_shape.events.production.flow_constraints.FlowConstraintEvents.flow_constraint_analytics(tolerance)","title":"<code>tolerance</code>","text":"(<code>str</code>, default:                   <code>'200ms'</code> )           \u2013            <p>Default tolerance for time alignment (used if directional tolerances not provided)</p>"},{"location":"reference/ts_shape/events/production/flow_constraints/#ts_shape.events.production.flow_constraints.FlowConstraintEvents.flow_constraint_analytics(tolerance_before)","title":"<code>tolerance_before</code>","text":"(<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Tolerance for looking backward in time during alignment</p>"},{"location":"reference/ts_shape/events/production/flow_constraints/#ts_shape.events.production.flow_constraints.FlowConstraintEvents.flow_constraint_analytics(tolerance_after)","title":"<code>tolerance_after</code>","text":"(<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Tolerance for looking forward in time during alignment</p>"},{"location":"reference/ts_shape/events/production/flow_constraints/#ts_shape.events.production.flow_constraints.FlowConstraintEvents.flow_constraint_analytics(min_duration)","title":"<code>min_duration</code>","text":"(<code>str</code>, default:                   <code>'0s'</code> )           \u2013            <p>Minimum duration for an event to be included</p>"},{"location":"reference/ts_shape/events/production/flow_constraints/#ts_shape.events.production.flow_constraints.FlowConstraintEvents.flow_constraint_analytics(minor_threshold)","title":"<code>minor_threshold</code>","text":"(<code>str</code>, default:                   <code>'5s'</code> )           \u2013            <p>Duration threshold for minor severity classification</p>"},{"location":"reference/ts_shape/events/production/flow_constraints/#ts_shape.events.production.flow_constraints.FlowConstraintEvents.flow_constraint_analytics(moderate_threshold)","title":"<code>moderate_threshold</code>","text":"(<code>str</code>, default:                   <code>'30s'</code> )           \u2013            <p>Duration threshold for moderate severity classification</p>"},{"location":"reference/ts_shape/events/production/flow_constraints/#ts_shape.events.production.flow_constraints.FlowConstraintEvents.get_dataframe","title":"get_dataframe","text":"<pre><code>get_dataframe() -&gt; DataFrame\n</code></pre> <p>Returns the processed DataFrame.</p> Source code in <code>src/ts_shape/utils/base.py</code> <pre><code>def get_dataframe(self) -&gt; pd.DataFrame:\n    \"\"\"Returns the processed DataFrame.\"\"\"\n    return self.dataframe\n</code></pre>"},{"location":"reference/ts_shape/events/production/flow_constraints/#ts_shape.events.production.flow_constraints.FlowConstraintEvents.starved_events","title":"starved_events","text":"<pre><code>starved_events(*, roles: Dict[str, str], tolerance: str = '200ms', tolerance_before: Optional[str] = None, tolerance_after: Optional[str] = None, min_duration: str = '0s') -&gt; DataFrame\n</code></pre> <p>Starved: downstream_run=True while upstream_run=False.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>DataFrame with columns: start, end, uuid, source_uuid, is_delta, type,</p> </li> <li> <code>DataFrame</code>           \u2013            <p>time_alignment_quality, duration, severity</p> </li> </ul> Example <p>roles = {'upstream_run': 'uuid1', 'downstream_run': 'uuid2'}</p> Source code in <code>src/ts_shape/events/production/flow_constraints.py</code> <pre><code>def starved_events(\n    self,\n    *,\n    roles: Dict[str, str],\n    tolerance: str = \"200ms\",\n    tolerance_before: Optional[str] = None,\n    tolerance_after: Optional[str] = None,\n    min_duration: str = \"0s\",\n) -&gt; pd.DataFrame:\n    \"\"\"Starved: downstream_run=True while upstream_run=False.\n\n    Args:\n        roles: Dictionary mapping role names to UUIDs.\n               Expected keys: 'upstream_run', 'downstream_run'\n        tolerance: Default tolerance for time alignment (used if directional tolerances not provided)\n        tolerance_before: Tolerance for looking backward in time during alignment\n        tolerance_after: Tolerance for looking forward in time during alignment\n        min_duration: Minimum duration for an event to be included\n\n    Returns:\n        DataFrame with columns: start, end, uuid, source_uuid, is_delta, type,\n        time_alignment_quality, duration, severity\n\n    Example:\n        roles = {'upstream_run': 'uuid1', 'downstream_run': 'uuid2'}\n    \"\"\"\n    up = self._align_bool(roles[\"upstream_run\"])  # time, state\n    dn = self._align_bool(roles[\"downstream_run\"])  # time, state\n    if up.empty or dn.empty:\n        return pd.DataFrame(\n            columns=[\"start\", \"end\", \"uuid\", \"source_uuid\", \"is_delta\", \"type\",\n                    \"time_alignment_quality\", \"duration\", \"severity\"]\n        )\n\n    # Use directional tolerances if provided, otherwise use single tolerance\n    tol_before = pd.to_timedelta(tolerance_before) if tolerance_before else pd.to_timedelta(tolerance)\n    tol_after = pd.to_timedelta(tolerance_after) if tolerance_after else pd.to_timedelta(tolerance)\n    max_tol = max(tol_before, tol_after)\n\n    # Merge with maximum tolerance and track time differences\n    merged = pd.merge_asof(\n        dn, up,\n        on=self.time_column,\n        suffixes=(\"_dn\", \"_up\"),\n        tolerance=max_tol,\n        direction=\"nearest\"\n    )\n\n    # Store original downstream time for quality calculation\n    merged['time_dn'] = dn[self.time_column].values\n\n    # Apply directional tolerance filtering if asymmetric tolerances are specified\n    if tolerance_before or tolerance_after:\n        time_diff = merged[self.time_column + '_up'] - merged[self.time_column]\n        # Keep only records within directional tolerance bounds\n        valid_mask = (\n            ((time_diff &lt;= pd.Timedelta(0)) &amp; (time_diff.abs() &lt;= tol_before)) |\n            ((time_diff &gt;= pd.Timedelta(0)) &amp; (time_diff &lt;= tol_after))\n        )\n        merged.loc[~valid_mask, 'state_up'] = pd.NA\n\n    # Calculate alignment quality (percentage of records with matches)\n    alignment_quality = merged['state_up'].notna().sum() / len(merged) if len(merged) &gt; 0 else 0.0\n\n    cond = merged[\"state_dn\"] &amp; (~merged[\"state_up\"].fillna(False))\n    gid = (cond.ne(cond.shift())).cumsum()\n    min_td = pd.to_timedelta(min_duration)\n    rows: List[Dict[str, Any]] = []\n    for _, seg in merged.groupby(gid):\n        m = cond.loc[seg.index]\n        if not m.any():\n            continue\n        start = seg[self.time_column].iloc[0]\n        end = seg[self.time_column].iloc[-1]\n        duration = end - start\n        if duration &lt; min_td:\n            continue\n\n        # Calculate severity based on duration\n        severity = self._calculate_severity(duration)\n\n        rows.append(\n            {\n                \"start\": start,\n                \"end\": end,\n                \"uuid\": self.event_uuid,\n                \"source_uuid\": roles[\"downstream_run\"],\n                \"is_delta\": True,\n                \"type\": \"starved\",\n                \"time_alignment_quality\": alignment_quality,\n                \"duration\": duration,\n                \"severity\": severity,\n            }\n        )\n    return pd.DataFrame(rows)\n</code></pre>"},{"location":"reference/ts_shape/events/production/flow_constraints/#ts_shape.events.production.flow_constraints.FlowConstraintEvents.starved_events(roles)","title":"<code>roles</code>","text":"(<code>Dict[str, str]</code>)           \u2013            <p>Dictionary mapping role names to UUIDs.    Expected keys: 'upstream_run', 'downstream_run'</p>"},{"location":"reference/ts_shape/events/production/flow_constraints/#ts_shape.events.production.flow_constraints.FlowConstraintEvents.starved_events(tolerance)","title":"<code>tolerance</code>","text":"(<code>str</code>, default:                   <code>'200ms'</code> )           \u2013            <p>Default tolerance for time alignment (used if directional tolerances not provided)</p>"},{"location":"reference/ts_shape/events/production/flow_constraints/#ts_shape.events.production.flow_constraints.FlowConstraintEvents.starved_events(tolerance_before)","title":"<code>tolerance_before</code>","text":"(<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Tolerance for looking backward in time during alignment</p>"},{"location":"reference/ts_shape/events/production/flow_constraints/#ts_shape.events.production.flow_constraints.FlowConstraintEvents.starved_events(tolerance_after)","title":"<code>tolerance_after</code>","text":"(<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Tolerance for looking forward in time during alignment</p>"},{"location":"reference/ts_shape/events/production/flow_constraints/#ts_shape.events.production.flow_constraints.FlowConstraintEvents.starved_events(min_duration)","title":"<code>min_duration</code>","text":"(<code>str</code>, default:                   <code>'0s'</code> )           \u2013            <p>Minimum duration for an event to be included</p>"},{"location":"reference/ts_shape/events/production/line_throughput/","title":"line_throughput","text":""},{"location":"reference/ts_shape/events/production/line_throughput/#ts_shape.events.production.line_throughput","title":"ts_shape.events.production.line_throughput","text":"<p>Classes:</p> <ul> <li> <code>LineThroughputEvents</code>           \u2013            <p>Production: Line Throughput</p> </li> </ul>"},{"location":"reference/ts_shape/events/production/line_throughput/#ts_shape.events.production.line_throughput.LineThroughputEvents","title":"LineThroughputEvents","text":"<pre><code>LineThroughputEvents(dataframe: DataFrame, *, event_uuid: str = 'prod:throughput', time_column: str = 'systime')\n</code></pre> <p>               Bases: <code>Base</code></p> <p>Production: Line Throughput</p> <p>Methods: - count_parts: Part counts per fixed window from a monotonically increasing counter. - takt_adherence: Cycle time violations against a takt time from step/boolean triggers.</p> <p>Methods:</p> <ul> <li> <code>count_parts</code>             \u2013              <p>Compute parts per window for a counter uuid.</p> </li> <li> <code>cycle_quality_check</code>             \u2013              <p>Enhanced cycle detection with quality validation.</p> </li> <li> <code>get_dataframe</code>             \u2013              <p>Returns the processed DataFrame.</p> </li> <li> <code>takt_adherence</code>             \u2013              <p>Flag cycles whose durations exceed the takt_time.</p> </li> <li> <code>throughput_oee</code>             \u2013              <p>Calculate Overall Equipment Effectiveness (OEE) metrics.</p> </li> <li> <code>throughput_trends</code>             \u2013              <p>Analyze throughput trends with moving averages and degradation detection.</p> </li> </ul> Source code in <code>src/ts_shape/events/production/line_throughput.py</code> <pre><code>def __init__(\n    self,\n    dataframe: pd.DataFrame,\n    *,\n    event_uuid: str = \"prod:throughput\",\n    time_column: str = \"systime\",\n) -&gt; None:\n    super().__init__(dataframe, column_name=time_column)\n    self.event_uuid = event_uuid\n    self.time_column = time_column\n</code></pre>"},{"location":"reference/ts_shape/events/production/line_throughput/#ts_shape.events.production.line_throughput.LineThroughputEvents.count_parts","title":"count_parts","text":"<pre><code>count_parts(counter_uuid: str, *, value_column: str = 'value_integer', window: str = '1m') -&gt; DataFrame\n</code></pre> <p>Compute parts per window for a counter uuid.</p> <p>Returns columns: window_start, uuid, source_uuid, is_delta, count</p> Source code in <code>src/ts_shape/events/production/line_throughput.py</code> <pre><code>def count_parts(\n    self,\n    counter_uuid: str,\n    *,\n    value_column: str = \"value_integer\",\n    window: str = \"1m\",\n) -&gt; pd.DataFrame:\n    \"\"\"Compute parts per window for a counter uuid.\n\n    Returns columns: window_start, uuid, source_uuid, is_delta, count\n    \"\"\"\n    c = (\n        self.dataframe[self.dataframe[\"uuid\"] == counter_uuid]\n        .copy()\n        .sort_values(self.time_column)\n    )\n    if c.empty:\n        return pd.DataFrame(\n            columns=[\"window_start\", \"uuid\", \"source_uuid\", \"is_delta\", \"count\"]\n        )\n    c[self.time_column] = pd.to_datetime(c[self.time_column])\n    c = c.set_index(self.time_column)\n    # take diff of last values within each window\n    grp = c[value_column].resample(window)\n    counts = grp.max().ffill().diff().fillna(0).clip(lower=0)\n    out = counts.to_frame(\"count\").reset_index().rename(columns={self.time_column: \"window_start\"})\n    out[\"uuid\"] = self.event_uuid\n    out[\"source_uuid\"] = counter_uuid\n    out[\"is_delta\"] = True\n    return out\n</code></pre>"},{"location":"reference/ts_shape/events/production/line_throughput/#ts_shape.events.production.line_throughput.LineThroughputEvents.cycle_quality_check","title":"cycle_quality_check","text":"<pre><code>cycle_quality_check(cycle_uuid: str, *, value_column: str = 'value_bool', expected_cycle_time: Optional[float] = None, tolerance_pct: float = 0.1) -&gt; DataFrame\n</code></pre> <p>Enhanced cycle detection with quality validation.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>DataFrame with cycle times, validation status, and quality flags</p> </li> </ul> Source code in <code>src/ts_shape/events/production/line_throughput.py</code> <pre><code>def cycle_quality_check(\n    self,\n    cycle_uuid: str,\n    *,\n    value_column: str = \"value_bool\",\n    expected_cycle_time: Optional[float] = None,\n    tolerance_pct: float = 0.1,\n) -&gt; pd.DataFrame:\n    \"\"\"Enhanced cycle detection with quality validation.\n\n    Args:\n        cycle_uuid: UUID for the cycle trigger signal\n        value_column: Column containing cycle trigger (bool/integer)\n        expected_cycle_time: Expected cycle time in seconds. If None, uses median\n        tolerance_pct: Tolerance percentage for cycle time validation\n\n    Returns:\n        DataFrame with cycle times, validation status, and quality flags\n    \"\"\"\n    s = (\n        self.dataframe[self.dataframe[\"uuid\"] == cycle_uuid]\n        .copy()\n        .sort_values(self.time_column)\n    )\n    if s.empty:\n        return pd.DataFrame(\n            columns=[\n                \"systime\", \"uuid\", \"source_uuid\", \"is_delta\",\n                \"cycle_time_seconds\", \"expected_time\", \"deviation_pct\",\n                \"is_valid\", \"quality_flag\"\n            ]\n        )\n\n    s[self.time_column] = pd.to_datetime(s[self.time_column])\n\n    # Detect cycle boundaries\n    if value_column == \"value_bool\":\n        s[\"prev\"] = s[value_column].shift(fill_value=False)\n        edges = s[(~s[\"prev\"]) &amp; (s[value_column].fillna(False))]\n        times = edges[self.time_column].reset_index(drop=True)\n    else:\n        s[\"prev\"] = s[value_column].shift(1)\n        edges = s[s[value_column].fillna(0) != s[\"prev\"].fillna(0)]\n        times = edges[self.time_column].reset_index(drop=True)\n\n    if len(times) &lt; 2:\n        return pd.DataFrame(\n            columns=[\n                \"systime\", \"uuid\", \"source_uuid\", \"is_delta\",\n                \"cycle_time_seconds\", \"expected_time\", \"deviation_pct\",\n                \"is_valid\", \"quality_flag\"\n            ]\n        )\n\n    cycle_times = (times.diff().dt.total_seconds()).iloc[1:].reset_index(drop=True)\n\n    # Calculate expected cycle time if not provided\n    if expected_cycle_time is None:\n        expected_cycle_time = cycle_times.median()\n\n    # Calculate deviation\n    deviation_pct = ((cycle_times - expected_cycle_time) / expected_cycle_time).abs()\n\n    # Validate cycles\n    is_valid = deviation_pct &lt;= tolerance_pct\n\n    # Quality flags: good, warning, critical\n    quality_flag = pd.cut(\n        deviation_pct,\n        bins=[-np.inf, 0.1, 0.25, np.inf],\n        labels=[\"good\", \"warning\", \"critical\"]\n    )\n\n    out = pd.DataFrame(\n        {\n            \"systime\": times.iloc[1:].reset_index(drop=True),\n            \"uuid\": self.event_uuid,\n            \"source_uuid\": cycle_uuid,\n            \"is_delta\": True,\n            \"cycle_time_seconds\": cycle_times,\n            \"expected_time\": expected_cycle_time,\n            \"deviation_pct\": deviation_pct,\n            \"is_valid\": is_valid,\n            \"quality_flag\": quality_flag,\n        }\n    )\n    return out\n</code></pre>"},{"location":"reference/ts_shape/events/production/line_throughput/#ts_shape.events.production.line_throughput.LineThroughputEvents.cycle_quality_check(cycle_uuid)","title":"<code>cycle_uuid</code>","text":"(<code>str</code>)           \u2013            <p>UUID for the cycle trigger signal</p>"},{"location":"reference/ts_shape/events/production/line_throughput/#ts_shape.events.production.line_throughput.LineThroughputEvents.cycle_quality_check(value_column)","title":"<code>value_column</code>","text":"(<code>str</code>, default:                   <code>'value_bool'</code> )           \u2013            <p>Column containing cycle trigger (bool/integer)</p>"},{"location":"reference/ts_shape/events/production/line_throughput/#ts_shape.events.production.line_throughput.LineThroughputEvents.cycle_quality_check(expected_cycle_time)","title":"<code>expected_cycle_time</code>","text":"(<code>Optional[float]</code>, default:                   <code>None</code> )           \u2013            <p>Expected cycle time in seconds. If None, uses median</p>"},{"location":"reference/ts_shape/events/production/line_throughput/#ts_shape.events.production.line_throughput.LineThroughputEvents.cycle_quality_check(tolerance_pct)","title":"<code>tolerance_pct</code>","text":"(<code>float</code>, default:                   <code>0.1</code> )           \u2013            <p>Tolerance percentage for cycle time validation</p>"},{"location":"reference/ts_shape/events/production/line_throughput/#ts_shape.events.production.line_throughput.LineThroughputEvents.get_dataframe","title":"get_dataframe","text":"<pre><code>get_dataframe() -&gt; DataFrame\n</code></pre> <p>Returns the processed DataFrame.</p> Source code in <code>src/ts_shape/utils/base.py</code> <pre><code>def get_dataframe(self) -&gt; pd.DataFrame:\n    \"\"\"Returns the processed DataFrame.\"\"\"\n    return self.dataframe\n</code></pre>"},{"location":"reference/ts_shape/events/production/line_throughput/#ts_shape.events.production.line_throughput.LineThroughputEvents.takt_adherence","title":"takt_adherence","text":"<pre><code>takt_adherence(cycle_uuid: str, *, value_column: str = 'value_bool', takt_time: str = '60s', min_violation: str = '0s') -&gt; DataFrame\n</code></pre> <p>Flag cycles whose durations exceed the takt_time.</p> <p>For boolean triggers: detect True rising edges as cycle boundaries. For integer steps: detect increments as cycle boundaries.</p> <p>Returns: systime (at boundary), uuid, source_uuid, is_delta, cycle_time_seconds, violation</p> Source code in <code>src/ts_shape/events/production/line_throughput.py</code> <pre><code>def takt_adherence(\n    self,\n    cycle_uuid: str,\n    *,\n    value_column: str = \"value_bool\",\n    takt_time: str = \"60s\",\n    min_violation: str = \"0s\",\n) -&gt; pd.DataFrame:\n    \"\"\"Flag cycles whose durations exceed the takt_time.\n\n    For boolean triggers: detect True rising edges as cycle boundaries.\n    For integer steps: detect increments as cycle boundaries.\n\n    Returns: systime (at boundary), uuid, source_uuid, is_delta, cycle_time_seconds, violation\n    \"\"\"\n    s = (\n        self.dataframe[self.dataframe[\"uuid\"] == cycle_uuid]\n        .copy()\n        .sort_values(self.time_column)\n    )\n    if s.empty:\n        return pd.DataFrame(\n            columns=[\n                \"systime\",\n                \"uuid\",\n                \"source_uuid\",\n                \"is_delta\",\n                \"cycle_time_seconds\",\n                \"violation\",\n            ]\n        )\n    s[self.time_column] = pd.to_datetime(s[self.time_column])\n    if value_column == \"value_bool\":\n        s[\"prev\"] = s[value_column].shift(fill_value=False)\n        edges = s[(~s[\"prev\"]) &amp; (s[value_column].fillna(False))]\n        times = edges[self.time_column].reset_index(drop=True)\n    else:\n        s[\"prev\"] = s[value_column].shift(1)\n        edges = s[s[value_column].fillna(0) != s[\"prev\"].fillna(0)]\n        times = edges[self.time_column].reset_index(drop=True)\n    if len(times) &lt; 2:\n        return pd.DataFrame(\n            columns=[\n                \"systime\",\n                \"uuid\",\n                \"source_uuid\",\n                \"is_delta\",\n                \"cycle_time_seconds\",\n                \"violation\",\n            ]\n        )\n    cycle_times = (times.diff().dt.total_seconds()).iloc[1:].reset_index(drop=True)\n    min_td = pd.to_timedelta(min_violation).total_seconds()\n    target = pd.to_timedelta(takt_time).total_seconds()\n    viol = (cycle_times - target) &gt;= min_td\n    out = pd.DataFrame(\n        {\n            \"systime\": times.iloc[1:].reset_index(drop=True),\n            \"uuid\": self.event_uuid,\n            \"source_uuid\": cycle_uuid,\n            \"is_delta\": True,\n            \"cycle_time_seconds\": cycle_times,\n            \"violation\": viol,\n        }\n    )\n    return out\n</code></pre>"},{"location":"reference/ts_shape/events/production/line_throughput/#ts_shape.events.production.line_throughput.LineThroughputEvents.throughput_oee","title":"throughput_oee","text":"<pre><code>throughput_oee(counter_uuid: str, *, value_column: str = 'value_integer', window: str = '1h', target_rate: Optional[float] = None, availability_threshold: float = 0.95) -&gt; DataFrame\n</code></pre> <p>Calculate Overall Equipment Effectiveness (OEE) metrics.</p> <p>OEE = Availability \u00d7 Performance \u00d7 Quality</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>DataFrame with columns: window_start, uuid, source_uuid, is_delta,</p> </li> <li> <code>DataFrame</code>           \u2013            <p>actual_count, target_count, availability, performance, oee_score</p> </li> </ul> Source code in <code>src/ts_shape/events/production/line_throughput.py</code> <pre><code>def throughput_oee(\n    self,\n    counter_uuid: str,\n    *,\n    value_column: str = \"value_integer\",\n    window: str = \"1h\",\n    target_rate: Optional[float] = None,\n    availability_threshold: float = 0.95,\n) -&gt; pd.DataFrame:\n    \"\"\"Calculate Overall Equipment Effectiveness (OEE) metrics.\n\n    OEE = Availability \u00d7 Performance \u00d7 Quality\n\n    Args:\n        counter_uuid: UUID for the part counter signal\n        value_column: Column containing counter values\n        window: Time window for aggregation\n        target_rate: Target production rate (parts per window). If None, uses max observed\n        availability_threshold: Threshold for considering equipment available\n\n    Returns:\n        DataFrame with columns: window_start, uuid, source_uuid, is_delta,\n        actual_count, target_count, availability, performance, oee_score\n    \"\"\"\n    parts_df = self.count_parts(counter_uuid, value_column=value_column, window=window)\n\n    if parts_df.empty:\n        return pd.DataFrame(\n            columns=[\n                \"window_start\", \"uuid\", \"source_uuid\", \"is_delta\",\n                \"actual_count\", \"target_count\", \"availability\", \"performance\", \"oee_score\"\n            ]\n        )\n\n    # Calculate target rate if not provided\n    if target_rate is None:\n        target_rate = parts_df[\"count\"].quantile(0.95)\n\n    parts_df[\"target_count\"] = target_rate\n    parts_df[\"actual_count\"] = parts_df[\"count\"]\n\n    # Availability: percentage of time equipment was running\n    parts_df[\"availability\"] = np.where(\n        parts_df[\"count\"] &gt; 0,\n        1.0,\n        0.0\n    )\n\n    # Performance: actual vs target rate\n    parts_df[\"performance\"] = np.minimum(\n        parts_df[\"count\"] / target_rate,\n        1.0\n    )\n\n    # OEE score (simplified - assumes quality = 1.0)\n    parts_df[\"oee_score\"] = parts_df[\"availability\"] * parts_df[\"performance\"]\n\n    return parts_df[[\n        \"window_start\", \"uuid\", \"source_uuid\", \"is_delta\",\n        \"actual_count\", \"target_count\", \"availability\", \"performance\", \"oee_score\"\n    ]]\n</code></pre>"},{"location":"reference/ts_shape/events/production/line_throughput/#ts_shape.events.production.line_throughput.LineThroughputEvents.throughput_oee(counter_uuid)","title":"<code>counter_uuid</code>","text":"(<code>str</code>)           \u2013            <p>UUID for the part counter signal</p>"},{"location":"reference/ts_shape/events/production/line_throughput/#ts_shape.events.production.line_throughput.LineThroughputEvents.throughput_oee(value_column)","title":"<code>value_column</code>","text":"(<code>str</code>, default:                   <code>'value_integer'</code> )           \u2013            <p>Column containing counter values</p>"},{"location":"reference/ts_shape/events/production/line_throughput/#ts_shape.events.production.line_throughput.LineThroughputEvents.throughput_oee(window)","title":"<code>window</code>","text":"(<code>str</code>, default:                   <code>'1h'</code> )           \u2013            <p>Time window for aggregation</p>"},{"location":"reference/ts_shape/events/production/line_throughput/#ts_shape.events.production.line_throughput.LineThroughputEvents.throughput_oee(target_rate)","title":"<code>target_rate</code>","text":"(<code>Optional[float]</code>, default:                   <code>None</code> )           \u2013            <p>Target production rate (parts per window). If None, uses max observed</p>"},{"location":"reference/ts_shape/events/production/line_throughput/#ts_shape.events.production.line_throughput.LineThroughputEvents.throughput_oee(availability_threshold)","title":"<code>availability_threshold</code>","text":"(<code>float</code>, default:                   <code>0.95</code> )           \u2013            <p>Threshold for considering equipment available</p>"},{"location":"reference/ts_shape/events/production/line_throughput/#ts_shape.events.production.line_throughput.LineThroughputEvents.throughput_trends","title":"throughput_trends","text":"<pre><code>throughput_trends(counter_uuid: str, *, value_column: str = 'value_integer', window: str = '1h', trend_window: int = 24) -&gt; DataFrame\n</code></pre> <p>Analyze throughput trends with moving averages and degradation detection.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>DataFrame with throughput, moving average, trend direction, and degradation flag</p> </li> </ul> Source code in <code>src/ts_shape/events/production/line_throughput.py</code> <pre><code>def throughput_trends(\n    self,\n    counter_uuid: str,\n    *,\n    value_column: str = \"value_integer\",\n    window: str = \"1h\",\n    trend_window: int = 24,\n) -&gt; pd.DataFrame:\n    \"\"\"Analyze throughput trends with moving averages and degradation detection.\n\n    Args:\n        counter_uuid: UUID for the part counter signal\n        value_column: Column containing counter values\n        window: Time window for counting parts\n        trend_window: Number of windows for trend calculation\n\n    Returns:\n        DataFrame with throughput, moving average, trend direction, and degradation flag\n    \"\"\"\n    parts_df = self.count_parts(counter_uuid, value_column=value_column, window=window)\n\n    if parts_df.empty or len(parts_df) &lt; trend_window:\n        return pd.DataFrame(\n            columns=[\n                \"window_start\", \"uuid\", \"source_uuid\", \"is_delta\", \"count\",\n                \"moving_avg\", \"trend_direction\", \"degradation_detected\"\n            ]\n        )\n\n    # Calculate moving average\n    parts_df[\"moving_avg\"] = parts_df[\"count\"].rolling(window=trend_window, min_periods=1).mean()\n\n    # Calculate trend (positive, negative, stable)\n    parts_df[\"trend_slope\"] = parts_df[\"moving_avg\"].diff()\n    parts_df[\"trend_direction\"] = pd.cut(\n        parts_df[\"trend_slope\"],\n        bins=[-np.inf, -0.5, 0.5, np.inf],\n        labels=[\"decreasing\", \"stable\", \"increasing\"]\n    )\n\n    # Detect degradation (current significantly below moving average)\n    parts_df[\"degradation_detected\"] = parts_df[\"count\"] &lt; (parts_df[\"moving_avg\"] * 0.85)\n\n    return parts_df[[\n        \"window_start\", \"uuid\", \"source_uuid\", \"is_delta\", \"count\",\n        \"moving_avg\", \"trend_direction\", \"degradation_detected\"\n    ]]\n</code></pre>"},{"location":"reference/ts_shape/events/production/line_throughput/#ts_shape.events.production.line_throughput.LineThroughputEvents.throughput_trends(counter_uuid)","title":"<code>counter_uuid</code>","text":"(<code>str</code>)           \u2013            <p>UUID for the part counter signal</p>"},{"location":"reference/ts_shape/events/production/line_throughput/#ts_shape.events.production.line_throughput.LineThroughputEvents.throughput_trends(value_column)","title":"<code>value_column</code>","text":"(<code>str</code>, default:                   <code>'value_integer'</code> )           \u2013            <p>Column containing counter values</p>"},{"location":"reference/ts_shape/events/production/line_throughput/#ts_shape.events.production.line_throughput.LineThroughputEvents.throughput_trends(window)","title":"<code>window</code>","text":"(<code>str</code>, default:                   <code>'1h'</code> )           \u2013            <p>Time window for counting parts</p>"},{"location":"reference/ts_shape/events/production/line_throughput/#ts_shape.events.production.line_throughput.LineThroughputEvents.throughput_trends(trend_window)","title":"<code>trend_window</code>","text":"(<code>int</code>, default:                   <code>24</code> )           \u2013            <p>Number of windows for trend calculation</p>"},{"location":"reference/ts_shape/events/production/machine_state/","title":"machine_state","text":""},{"location":"reference/ts_shape/events/production/machine_state/#ts_shape.events.production.machine_state","title":"ts_shape.events.production.machine_state","text":"<p>Classes:</p> <ul> <li> <code>MachineStateEvents</code>           \u2013            <p>Production: Machine State</p> </li> </ul>"},{"location":"reference/ts_shape/events/production/machine_state/#ts_shape.events.production.machine_state.MachineStateEvents","title":"MachineStateEvents","text":"<pre><code>MachineStateEvents(dataframe: DataFrame, run_state_uuid: str, *, event_uuid: str = 'prod:run_idle', value_column: str = 'value_bool', time_column: str = 'systime')\n</code></pre> <p>               Bases: <code>Base</code></p> <p>Production: Machine State</p> <p>Detect run/idle transitions and intervals from a boolean state signal.</p> <ul> <li>MachineStateEvents: Run/idle state intervals and transitions.</li> <li>detect_run_idle: Intervalize run/idle states with optional min duration filter.</li> <li>transition_events: Point events on state changes (idle-&gt;run, run-&gt;idle).</li> </ul> <p>Methods:</p> <ul> <li> <code>detect_rapid_transitions</code>             \u2013              <p>Identify suspicious rapid state changes.</p> </li> <li> <code>detect_run_idle</code>             \u2013              <p>Return intervals labeled as 'run' or 'idle'.</p> </li> <li> <code>get_dataframe</code>             \u2013              <p>Returns the processed DataFrame.</p> </li> <li> <code>state_quality_metrics</code>             \u2013              <p>Return quality metrics for the state data.</p> </li> <li> <code>transition_events</code>             \u2013              <p>Return point events at state transitions.</p> </li> </ul> Source code in <code>src/ts_shape/events/production/machine_state.py</code> <pre><code>def __init__(\n    self,\n    dataframe: pd.DataFrame,\n    run_state_uuid: str,\n    *,\n    event_uuid: str = \"prod:run_idle\",\n    value_column: str = \"value_bool\",\n    time_column: str = \"systime\",\n) -&gt; None:\n    super().__init__(dataframe, column_name=time_column)\n    self.run_state_uuid = run_state_uuid\n    self.event_uuid = event_uuid\n    self.value_column = value_column\n    self.time_column = time_column\n    self.series = (\n        self.dataframe[self.dataframe[\"uuid\"] == self.run_state_uuid]\n        .copy()\n        .sort_values(self.time_column)\n    )\n    self.series[self.time_column] = pd.to_datetime(self.series[self.time_column])\n    self._state_groups: Optional[pd.Series] = None\n    self._compute_state_groups()\n</code></pre>"},{"location":"reference/ts_shape/events/production/machine_state/#ts_shape.events.production.machine_state.MachineStateEvents.detect_rapid_transitions","title":"detect_rapid_transitions","text":"<pre><code>detect_rapid_transitions(threshold: str = '5s', min_count: int = 3) -&gt; DataFrame\n</code></pre> <p>Identify suspicious rapid state changes.</p> <ul> <li>threshold: time window to look for rapid transitions</li> <li>min_count: minimum number of transitions within threshold to be considered rapid Returns: DataFrame with start_time, end_time, transition_count, duration_seconds</li> </ul> Source code in <code>src/ts_shape/events/production/machine_state.py</code> <pre><code>def detect_rapid_transitions(self, threshold: str = \"5s\", min_count: int = 3) -&gt; pd.DataFrame:\n    \"\"\"Identify suspicious rapid state changes.\n\n    - threshold: time window to look for rapid transitions\n    - min_count: minimum number of transitions within threshold to be considered rapid\n    Returns: DataFrame with start_time, end_time, transition_count, duration_seconds\n    \"\"\"\n    transitions = self.transition_events()\n    if transitions.empty or len(transitions) &lt; min_count:\n        return pd.DataFrame(\n            columns=[\"start_time\", \"end_time\", \"transition_count\", \"duration_seconds\"]\n        )\n\n    threshold_td = pd.to_timedelta(threshold)\n    rapid_events: List[Dict[str, Any]] = []\n\n    for i in range(len(transitions) - min_count + 1):\n        window_start = transitions.iloc[i][\"systime\"]\n        for j in range(i + min_count - 1, len(transitions)):\n            window_end = transitions.iloc[j][\"systime\"]\n            duration = window_end - window_start\n            if duration &lt;= threshold_td:\n                transition_count = j - i + 1\n                rapid_events.append({\n                    \"start_time\": window_start,\n                    \"end_time\": window_end,\n                    \"transition_count\": transition_count,\n                    \"duration_seconds\": duration.total_seconds(),\n                })\n            else:\n                break\n\n    return pd.DataFrame(rapid_events)\n</code></pre>"},{"location":"reference/ts_shape/events/production/machine_state/#ts_shape.events.production.machine_state.MachineStateEvents.detect_run_idle","title":"detect_run_idle","text":"<pre><code>detect_run_idle(min_duration: str = '0s') -&gt; DataFrame\n</code></pre> <p>Return intervals labeled as 'run' or 'idle'.</p> <ul> <li>min_duration: discard intervals shorter than this duration. Columns: start, end, uuid, source_uuid, is_delta, state, duration_seconds</li> </ul> Source code in <code>src/ts_shape/events/production/machine_state.py</code> <pre><code>def detect_run_idle(self, min_duration: str = \"0s\") -&gt; pd.DataFrame:\n    \"\"\"Return intervals labeled as 'run' or 'idle'.\n\n    - min_duration: discard intervals shorter than this duration.\n    Columns: start, end, uuid, source_uuid, is_delta, state, duration_seconds\n    \"\"\"\n    if self.series.empty:\n        return pd.DataFrame(\n            columns=[\"start\", \"end\", \"uuid\", \"source_uuid\", \"is_delta\", \"state\", \"duration_seconds\"]\n        )\n    s = self.series[[self.time_column, self.value_column]].copy()\n    s[\"state\"] = s[self.value_column].fillna(False).astype(bool)\n    state_change = (s[\"state\"] != s[\"state\"].shift()).cumsum()\n    min_td = pd.to_timedelta(min_duration)\n    rows: List[Dict[str, Any]] = []\n    for _, seg in s.groupby(state_change):\n        state = bool(seg[\"state\"].iloc[0])\n        start = seg[self.time_column].iloc[0]\n        end = seg[self.time_column].iloc[-1]\n        if (end - start) &lt; min_td:\n            continue\n        rows.append(\n            {\n                \"start\": start,\n                \"end\": end,\n                \"uuid\": self.event_uuid,\n                \"source_uuid\": self.run_state_uuid,\n                \"is_delta\": True,\n                \"state\": \"run\" if state else \"idle\",\n                \"duration_seconds\": (end - start).total_seconds(),\n            }\n        )\n    return pd.DataFrame(rows)\n</code></pre>"},{"location":"reference/ts_shape/events/production/machine_state/#ts_shape.events.production.machine_state.MachineStateEvents.get_dataframe","title":"get_dataframe","text":"<pre><code>get_dataframe() -&gt; DataFrame\n</code></pre> <p>Returns the processed DataFrame.</p> Source code in <code>src/ts_shape/utils/base.py</code> <pre><code>def get_dataframe(self) -&gt; pd.DataFrame:\n    \"\"\"Returns the processed DataFrame.\"\"\"\n    return self.dataframe\n</code></pre>"},{"location":"reference/ts_shape/events/production/machine_state/#ts_shape.events.production.machine_state.MachineStateEvents.state_quality_metrics","title":"state_quality_metrics","text":"<pre><code>state_quality_metrics() -&gt; Dict[str, Any]\n</code></pre> <p>Return quality metrics for the state data.</p> <p>Returns dictionary with: - total_transitions: total number of state transitions - avg_run_duration: average duration of run states in seconds - avg_idle_duration: average duration of idle states in seconds - run_idle_ratio: ratio of run time to idle time - data_gaps_detected: number of data gaps found - rapid_transitions_detected: number of rapid transition events</p> Source code in <code>src/ts_shape/events/production/machine_state.py</code> <pre><code>def state_quality_metrics(self) -&gt; Dict[str, Any]:\n    \"\"\"Return quality metrics for the state data.\n\n    Returns dictionary with:\n    - total_transitions: total number of state transitions\n    - avg_run_duration: average duration of run states in seconds\n    - avg_idle_duration: average duration of idle states in seconds\n    - run_idle_ratio: ratio of run time to idle time\n    - data_gaps_detected: number of data gaps found\n    - rapid_transitions_detected: number of rapid transition events\n    \"\"\"\n    transitions = self.transition_events()\n    intervals = self.detect_run_idle()\n\n    total_transitions = len(transitions)\n\n    if intervals.empty:\n        avg_run_duration = 0.0\n        avg_idle_duration = 0.0\n        run_idle_ratio = 0.0\n    else:\n        run_intervals = intervals[intervals[\"state\"] == \"run\"]\n        idle_intervals = intervals[intervals[\"state\"] == \"idle\"]\n\n        avg_run_duration = run_intervals[\"duration_seconds\"].mean() if not run_intervals.empty else 0.0\n        avg_idle_duration = idle_intervals[\"duration_seconds\"].mean() if not idle_intervals.empty else 0.0\n\n        total_run_time = run_intervals[\"duration_seconds\"].sum() if not run_intervals.empty else 0.0\n        total_idle_time = idle_intervals[\"duration_seconds\"].sum() if not idle_intervals.empty else 0.0\n        run_idle_ratio = total_run_time / total_idle_time if total_idle_time &gt; 0 else 0.0\n\n    data_gaps_detected = self._detect_data_gaps()\n    rapid_transitions_detected = len(self.detect_rapid_transitions())\n\n    return {\n        \"total_transitions\": total_transitions,\n        \"avg_run_duration\": float(avg_run_duration) if not np.isnan(avg_run_duration) else 0.0,\n        \"avg_idle_duration\": float(avg_idle_duration) if not np.isnan(avg_idle_duration) else 0.0,\n        \"run_idle_ratio\": float(run_idle_ratio) if not np.isnan(run_idle_ratio) else 0.0,\n        \"data_gaps_detected\": data_gaps_detected,\n        \"rapid_transitions_detected\": rapid_transitions_detected,\n    }\n</code></pre>"},{"location":"reference/ts_shape/events/production/machine_state/#ts_shape.events.production.machine_state.MachineStateEvents.transition_events","title":"transition_events","text":"<pre><code>transition_events() -&gt; DataFrame\n</code></pre> <p>Return point events at state transitions.</p> <p>Columns: systime, uuid, source_uuid, is_delta, transition ('idle_to_run'|'run_to_idle'), time_since_last_transition_seconds</p> Source code in <code>src/ts_shape/events/production/machine_state.py</code> <pre><code>def transition_events(self) -&gt; pd.DataFrame:\n    \"\"\"Return point events at state transitions.\n\n    Columns: systime, uuid, source_uuid, is_delta, transition ('idle_to_run'|'run_to_idle'), time_since_last_transition_seconds\n    \"\"\"\n    if self.series.empty:\n        return pd.DataFrame(\n            columns=[\"systime\", \"uuid\", \"source_uuid\", \"is_delta\", \"transition\", \"time_since_last_transition_seconds\"]\n        )\n    s = self.series[[self.time_column, self.value_column]].copy()\n    s[\"state\"] = s[self.value_column].fillna(False).astype(bool)\n    s[\"prev\"] = s[\"state\"].shift()\n    changes = s[s[\"state\"] != s[\"prev\"]].dropna(subset=[\"prev\"])  # ignore first row\n    if changes.empty:\n        return pd.DataFrame(\n            columns=[\"systime\", \"uuid\", \"source_uuid\", \"is_delta\", \"transition\", \"time_since_last_transition_seconds\"]\n        )\n    changes = changes.rename(columns={self.time_column: \"systime\"})\n    changes[\"transition\"] = changes.apply(\n        lambda r: \"idle_to_run\" if (r[\"prev\"] is False and r[\"state\"] is True) else \"run_to_idle\",\n        axis=1,\n    )\n    changes[\"time_since_last_transition_seconds\"] = changes[\"systime\"].diff().dt.total_seconds()\n    return pd.DataFrame(\n        {\n            \"systime\": changes[\"systime\"],\n            \"uuid\": self.event_uuid,\n            \"source_uuid\": self.run_state_uuid,\n            \"is_delta\": True,\n            \"transition\": changes[\"transition\"],\n            \"time_since_last_transition_seconds\": changes[\"time_since_last_transition_seconds\"],\n        }\n    )\n</code></pre>"},{"location":"reference/ts_shape/events/quality/__init__/","title":"init","text":""},{"location":"reference/ts_shape/events/quality/__init__/#ts_shape.events.quality","title":"ts_shape.events.quality","text":"<p>Quality Events</p> <p>Detectors for quality-related events: outliers, statistical process control, and tolerance deviations over time series.</p> <ul> <li>OutlierDetectionEvents: Detect and group outlier events in a time series.</li> <li>detect_outliers_zscore: Detect outliers using Z-score thresholding and group nearby points.</li> <li> <p>detect_outliers_iqr: Detect outliers using IQR bounds and group nearby points.</p> </li> <li> <p>StatisticalProcessControlRuleBased: Apply Western Electric rules to actual values   using tolerance context to flag control-limit violations.</p> </li> <li>calculate_control_limits: Compute mean and \u00b11/\u00b12/\u00b13 standard-deviation bands from tolerance rows.</li> <li>process: Apply selected rules and emit event rows for violations.</li> <li>rule_1: One point beyond the 3-sigma control limits.</li> <li>rule_2: Nine consecutive points on one side of the mean.</li> <li>rule_3: Six consecutive points steadily increasing or decreasing.</li> <li>rule_4: Fourteen consecutive points alternating up and down.</li> <li>rule_5: Two of three consecutive points near the control limit (between 2 and 3 sigma).</li> <li>rule_6: Four of five consecutive points near the control limit (between 1 and 2 sigma).</li> <li>rule_7: Fifteen consecutive points within 1 sigma of the mean.</li> <li> <p>rule_8: Eight consecutive points on both sides of the mean within 1 sigma.</p> </li> <li> <p>ToleranceDeviationEvents: Flag intervals where actual values cross/compare against   tolerance settings and group them into start/end events.</p> </li> <li>process_and_group_data_with_events: Build grouped deviation events with event UUIDs.</li> </ul> <p>Modules:</p> <ul> <li> <code>outlier_detection</code>           \u2013            </li> <li> <code>statistical_process_control</code>           \u2013            </li> <li> <code>tolerance_deviation</code>           \u2013            </li> </ul>"},{"location":"reference/ts_shape/events/quality/outlier_detection/","title":"outlier_detection","text":""},{"location":"reference/ts_shape/events/quality/outlier_detection/#ts_shape.events.quality.outlier_detection","title":"ts_shape.events.quality.outlier_detection","text":"<p>Classes:</p> <ul> <li> <code>OutlierDetectionEvents</code>           \u2013            <p>Processes time series data to detect outliers based on specified statistical methods.</p> </li> </ul>"},{"location":"reference/ts_shape/events/quality/outlier_detection/#ts_shape.events.quality.outlier_detection.OutlierDetectionEvents","title":"OutlierDetectionEvents","text":"<pre><code>OutlierDetectionEvents(dataframe: DataFrame, value_column: str, event_uuid: str = 'outlier_event', time_threshold: str = '5min')\n</code></pre> <p>               Bases: <code>Base</code></p> <p>Processes time series data to detect outliers based on specified statistical methods.</p> <p>Parameters:</p> <p>Methods:</p> <ul> <li> <code>detect_outliers_iqr</code>             \u2013              <p>Detects outliers using the IQR method.</p> </li> <li> <code>detect_outliers_isolation_forest</code>             \u2013              <p>Detects outliers using sklearn's IsolationForest algorithm.</p> </li> <li> <code>detect_outliers_mad</code>             \u2013              <p>Detects outliers using the Median Absolute Deviation (MAD) method.</p> </li> <li> <code>detect_outliers_zscore</code>             \u2013              <p>Detects outliers using the Z-score method.</p> </li> <li> <code>get_dataframe</code>             \u2013              <p>Returns the processed DataFrame.</p> </li> </ul> Source code in <code>src/ts_shape/events/quality/outlier_detection.py</code> <pre><code>def __init__(self, dataframe: pd.DataFrame, value_column: str, event_uuid: str = 'outlier_event', \n             time_threshold: str = '5min') -&gt; None:\n    \"\"\"\n    Initializes the OutlierDetectionEvents with specific attributes for outlier detection.\n\n    Args:\n        dataframe (pd.DataFrame): The input time series DataFrame.\n        value_column (str): The name of the column containing the values for outlier detection.\n        event_uuid (str): A UUID or identifier for detected outlier events.\n        time_threshold (str): The time threshold to group close events together.\n    \"\"\"\n    super().__init__(dataframe)\n    self.value_column = value_column\n    self.event_uuid = event_uuid\n    self.time_threshold = time_threshold\n</code></pre>"},{"location":"reference/ts_shape/events/quality/outlier_detection/#ts_shape.events.quality.outlier_detection.OutlierDetectionEvents(dataframe)","title":"<code>dataframe</code>","text":"(<code>DataFrame</code>)           \u2013            <p>The input time series DataFrame.</p>"},{"location":"reference/ts_shape/events/quality/outlier_detection/#ts_shape.events.quality.outlier_detection.OutlierDetectionEvents(value_column)","title":"<code>value_column</code>","text":"(<code>str</code>)           \u2013            <p>The name of the column containing the values for outlier detection.</p>"},{"location":"reference/ts_shape/events/quality/outlier_detection/#ts_shape.events.quality.outlier_detection.OutlierDetectionEvents(event_uuid)","title":"<code>event_uuid</code>","text":"(<code>str</code>, default:                   <code>'outlier_event'</code> )           \u2013            <p>A UUID or identifier for detected outlier events.</p>"},{"location":"reference/ts_shape/events/quality/outlier_detection/#ts_shape.events.quality.outlier_detection.OutlierDetectionEvents(time_threshold)","title":"<code>time_threshold</code>","text":"(<code>str</code>, default:                   <code>'5min'</code> )           \u2013            <p>The time threshold to group close events together.</p>"},{"location":"reference/ts_shape/events/quality/outlier_detection/#ts_shape.events.quality.outlier_detection.OutlierDetectionEvents.detect_outliers_iqr","title":"detect_outliers_iqr","text":"<pre><code>detect_outliers_iqr(threshold: tuple = (1.5, 1.5), include_singles: bool = True) -&gt; DataFrame\n</code></pre> <p>Detects outliers using the IQR method.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>pd.DataFrame: A DataFrame of detected outliers and grouped events.</p> </li> </ul> Source code in <code>src/ts_shape/events/quality/outlier_detection.py</code> <pre><code>def detect_outliers_iqr(self, threshold: tuple = (1.5, 1.5), include_singles: bool = True) -&gt; pd.DataFrame:\n    \"\"\"\n    Detects outliers using the IQR method.\n\n    Args:\n        threshold (tuple): The multipliers for the IQR range for detecting outliers (lower, upper).\n        include_singles (bool): Whether to include single outliers in the output. Default is True.\n\n    Returns:\n        pd.DataFrame: A DataFrame of detected outliers and grouped events.\n    \"\"\"\n    df = self.dataframe.copy()\n\n    # Convert 'systime' to datetime and sort the DataFrame by 'systime'\n    df['systime'] = pd.to_datetime(df['systime'])\n    df = df.sort_values(by='systime', ascending=True)\n\n    # Detect outliers using the IQR method\n    Q1 = df[self.value_column].quantile(0.25)\n    Q3 = df[self.value_column].quantile(0.75)\n    IQR = Q3 - Q1\n    lower_bound = Q1 - threshold[0] * IQR\n    upper_bound = Q3 + threshold[1] * IQR\n    df['outlier'] = (df[self.value_column] &lt; lower_bound) | (df[self.value_column] &gt; upper_bound)\n\n    # Filter to keep only outliers\n    outliers_df = df.loc[df['outlier']].copy()\n\n    # Add severity score (normalized distance from bounds in terms of IQR)\n    if IQR &gt; 0:\n        lower_distance = np.maximum(0, (lower_bound - outliers_df[self.value_column]) / IQR)\n        upper_distance = np.maximum(0, (outliers_df[self.value_column] - upper_bound) / IQR)\n        outliers_df['severity_score'] = lower_distance + upper_distance\n    else:\n        outliers_df['severity_score'] = 0.0\n\n    # Group and return the outliers\n    return self._group_outliers(outliers_df, include_singles=include_singles)\n</code></pre>"},{"location":"reference/ts_shape/events/quality/outlier_detection/#ts_shape.events.quality.outlier_detection.OutlierDetectionEvents.detect_outliers_iqr(threshold)","title":"<code>threshold</code>","text":"(<code>tuple</code>, default:                   <code>(1.5, 1.5)</code> )           \u2013            <p>The multipliers for the IQR range for detecting outliers (lower, upper).</p>"},{"location":"reference/ts_shape/events/quality/outlier_detection/#ts_shape.events.quality.outlier_detection.OutlierDetectionEvents.detect_outliers_iqr(include_singles)","title":"<code>include_singles</code>","text":"(<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to include single outliers in the output. Default is True.</p>"},{"location":"reference/ts_shape/events/quality/outlier_detection/#ts_shape.events.quality.outlier_detection.OutlierDetectionEvents.detect_outliers_isolation_forest","title":"detect_outliers_isolation_forest","text":"<pre><code>detect_outliers_isolation_forest(contamination: float = 0.1, include_singles: bool = True, random_state: Optional[int] = 42) -&gt; DataFrame\n</code></pre> <p>Detects outliers using sklearn's IsolationForest algorithm. Falls back gracefully if sklearn is not available.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>pd.DataFrame: A DataFrame of detected outliers and grouped events.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ImportError</code>             \u2013            <p>If sklearn is not installed.</p> </li> </ul> Source code in <code>src/ts_shape/events/quality/outlier_detection.py</code> <pre><code>def detect_outliers_isolation_forest(self, contamination: float = 0.1, include_singles: bool = True,\n                                    random_state: Optional[int] = 42) -&gt; pd.DataFrame:\n    \"\"\"\n    Detects outliers using sklearn's IsolationForest algorithm.\n    Falls back gracefully if sklearn is not available.\n\n    Args:\n        contamination (float): The proportion of outliers in the dataset. Default is 0.1.\n        include_singles (bool): Whether to include single outliers in the output. Default is True.\n        random_state (Optional[int]): Random state for reproducibility. Default is 42.\n\n    Returns:\n        pd.DataFrame: A DataFrame of detected outliers and grouped events.\n\n    Raises:\n        ImportError: If sklearn is not installed.\n    \"\"\"\n    if not SKLEARN_AVAILABLE:\n        raise ImportError(\n            \"sklearn is not available. Please install it with: pip install scikit-learn\"\n        )\n\n    df = self.dataframe.copy()\n\n    # Convert 'systime' to datetime and sort the DataFrame by 'systime'\n    df['systime'] = pd.to_datetime(df['systime'])\n    df = df.sort_values(by='systime', ascending=True)\n\n    # Prepare data for IsolationForest (needs 2D array)\n    X = df[[self.value_column]].values\n\n    # Detect outliers using IsolationForest\n    iso_forest = IsolationForest(contamination=contamination, random_state=random_state)\n    predictions = iso_forest.fit_predict(X)\n    anomaly_scores = iso_forest.score_samples(X)\n\n    # Mark outliers (IsolationForest returns -1 for outliers, 1 for inliers)\n    df['outlier'] = predictions == -1\n\n    # Filter to keep only outliers\n    outliers_df = df.loc[df['outlier']].copy()\n\n    # Add severity score (inverse of anomaly score, normalized to positive values)\n    # More negative scores indicate more anomalous points\n    outliers_df['severity_score'] = -anomaly_scores[df['outlier']]\n\n    # Group and return the outliers\n    return self._group_outliers(outliers_df, include_singles=include_singles)\n</code></pre>"},{"location":"reference/ts_shape/events/quality/outlier_detection/#ts_shape.events.quality.outlier_detection.OutlierDetectionEvents.detect_outliers_isolation_forest(contamination)","title":"<code>contamination</code>","text":"(<code>float</code>, default:                   <code>0.1</code> )           \u2013            <p>The proportion of outliers in the dataset. Default is 0.1.</p>"},{"location":"reference/ts_shape/events/quality/outlier_detection/#ts_shape.events.quality.outlier_detection.OutlierDetectionEvents.detect_outliers_isolation_forest(include_singles)","title":"<code>include_singles</code>","text":"(<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to include single outliers in the output. Default is True.</p>"},{"location":"reference/ts_shape/events/quality/outlier_detection/#ts_shape.events.quality.outlier_detection.OutlierDetectionEvents.detect_outliers_isolation_forest(random_state)","title":"<code>random_state</code>","text":"(<code>Optional[int]</code>, default:                   <code>42</code> )           \u2013            <p>Random state for reproducibility. Default is 42.</p>"},{"location":"reference/ts_shape/events/quality/outlier_detection/#ts_shape.events.quality.outlier_detection.OutlierDetectionEvents.detect_outliers_mad","title":"detect_outliers_mad","text":"<pre><code>detect_outliers_mad(threshold: float = 3.5, include_singles: bool = True) -&gt; DataFrame\n</code></pre> <p>Detects outliers using the Median Absolute Deviation (MAD) method. This method is more robust to outliers than z-score.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>pd.DataFrame: A DataFrame of detected outliers and grouped events.</p> </li> </ul> Source code in <code>src/ts_shape/events/quality/outlier_detection.py</code> <pre><code>def detect_outliers_mad(self, threshold: float = 3.5, include_singles: bool = True) -&gt; pd.DataFrame:\n    \"\"\"\n    Detects outliers using the Median Absolute Deviation (MAD) method.\n    This method is more robust to outliers than z-score.\n\n    Args:\n        threshold (float): The MAD threshold for detecting outliers. Default is 3.5.\n        include_singles (bool): Whether to include single outliers in the output. Default is True.\n\n    Returns:\n        pd.DataFrame: A DataFrame of detected outliers and grouped events.\n    \"\"\"\n    df = self.dataframe.copy()\n\n    # Convert 'systime' to datetime and sort the DataFrame by 'systime'\n    df['systime'] = pd.to_datetime(df['systime'])\n    df = df.sort_values(by='systime', ascending=True)\n\n    # Calculate MAD\n    median = df[self.value_column].median()\n    mad = np.median(np.abs(df[self.value_column] - median))\n\n    # Handle case where MAD is 0 (all values are the same)\n    if mad == 0:\n        # Use a small constant to avoid division by zero\n        mad = np.finfo(float).eps\n\n    # Detect outliers using the MAD method\n    modified_z_scores = 0.6745 * (df[self.value_column] - median) / mad\n    df['outlier'] = np.abs(modified_z_scores) &gt; threshold\n\n    # Filter to keep only outliers\n    outliers_df = df.loc[df['outlier']].copy()\n\n    # Add severity score (absolute modified z-score)\n    outliers_df['severity_score'] = np.abs(modified_z_scores[df['outlier']])\n\n    # Group and return the outliers\n    return self._group_outliers(outliers_df, include_singles=include_singles)\n</code></pre>"},{"location":"reference/ts_shape/events/quality/outlier_detection/#ts_shape.events.quality.outlier_detection.OutlierDetectionEvents.detect_outliers_mad(threshold)","title":"<code>threshold</code>","text":"(<code>float</code>, default:                   <code>3.5</code> )           \u2013            <p>The MAD threshold for detecting outliers. Default is 3.5.</p>"},{"location":"reference/ts_shape/events/quality/outlier_detection/#ts_shape.events.quality.outlier_detection.OutlierDetectionEvents.detect_outliers_mad(include_singles)","title":"<code>include_singles</code>","text":"(<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to include single outliers in the output. Default is True.</p>"},{"location":"reference/ts_shape/events/quality/outlier_detection/#ts_shape.events.quality.outlier_detection.OutlierDetectionEvents.detect_outliers_zscore","title":"detect_outliers_zscore","text":"<pre><code>detect_outliers_zscore(threshold: float = 3.0, include_singles: bool = True) -&gt; DataFrame\n</code></pre> <p>Detects outliers using the Z-score method.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>pd.DataFrame: A DataFrame of detected outliers and grouped events.</p> </li> </ul> Source code in <code>src/ts_shape/events/quality/outlier_detection.py</code> <pre><code>def detect_outliers_zscore(self, threshold: float = 3.0, include_singles: bool = True) -&gt; pd.DataFrame:\n    \"\"\"\n    Detects outliers using the Z-score method.\n\n    Args:\n        threshold (float): The Z-score threshold for detecting outliers.\n        include_singles (bool): Whether to include single outliers in the output. Default is True.\n\n    Returns:\n        pd.DataFrame: A DataFrame of detected outliers and grouped events.\n    \"\"\"\n    df = self.dataframe.copy()\n\n    # Convert 'systime' to datetime and sort the DataFrame by 'systime'\n    df['systime'] = pd.to_datetime(df['systime'])\n    df = df.sort_values(by='systime', ascending=True)\n\n    # Calculate z-scores and detect outliers\n    z_scores = np.abs(zscore(df[self.value_column]))\n    df['outlier'] = z_scores &gt; threshold\n\n    # Filter to keep only outliers\n    outliers_df = df.loc[df['outlier']].copy()\n\n    # Add severity score (absolute z-score value)\n    outliers_df['severity_score'] = z_scores[df['outlier']]\n\n    # Group and return the outliers\n    return self._group_outliers(outliers_df, include_singles=include_singles)\n</code></pre>"},{"location":"reference/ts_shape/events/quality/outlier_detection/#ts_shape.events.quality.outlier_detection.OutlierDetectionEvents.detect_outliers_zscore(threshold)","title":"<code>threshold</code>","text":"(<code>float</code>, default:                   <code>3.0</code> )           \u2013            <p>The Z-score threshold for detecting outliers.</p>"},{"location":"reference/ts_shape/events/quality/outlier_detection/#ts_shape.events.quality.outlier_detection.OutlierDetectionEvents.detect_outliers_zscore(include_singles)","title":"<code>include_singles</code>","text":"(<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to include single outliers in the output. Default is True.</p>"},{"location":"reference/ts_shape/events/quality/outlier_detection/#ts_shape.events.quality.outlier_detection.OutlierDetectionEvents.get_dataframe","title":"get_dataframe","text":"<pre><code>get_dataframe() -&gt; DataFrame\n</code></pre> <p>Returns the processed DataFrame.</p> Source code in <code>src/ts_shape/utils/base.py</code> <pre><code>def get_dataframe(self) -&gt; pd.DataFrame:\n    \"\"\"Returns the processed DataFrame.\"\"\"\n    return self.dataframe\n</code></pre>"},{"location":"reference/ts_shape/events/quality/statistical_process_control/","title":"statistical_process_control","text":""},{"location":"reference/ts_shape/events/quality/statistical_process_control/#ts_shape.events.quality.statistical_process_control","title":"ts_shape.events.quality.statistical_process_control","text":"<p>Classes:</p> <ul> <li> <code>StatisticalProcessControlRuleBased</code>           \u2013            <p>Inherits from Base and applies SPC rules (Western Electric Rules) to a DataFrame for event detection.</p> </li> </ul>"},{"location":"reference/ts_shape/events/quality/statistical_process_control/#ts_shape.events.quality.statistical_process_control.StatisticalProcessControlRuleBased","title":"StatisticalProcessControlRuleBased","text":"<pre><code>StatisticalProcessControlRuleBased(dataframe: DataFrame, value_column: str, tolerance_uuid: str, actual_uuid: str, event_uuid: str)\n</code></pre> <p>               Bases: <code>Base</code></p> <p>Inherits from Base and applies SPC rules (Western Electric Rules) to a DataFrame for event detection. Processes data based on control limit UUIDs, actual value UUIDs, and generates events with an event UUID.</p> <p>Parameters:</p> <p>Methods:</p> <ul> <li> <code>apply_rules_vectorized</code>             \u2013              <p>Applies SPC rules using vectorized operations with optimized multi-rule processing.</p> </li> <li> <code>calculate_control_limits</code>             \u2013              <p>Calculate the control limits (mean \u00b1 1\u03c3, 2\u03c3, 3\u03c3) for the tolerance values.</p> </li> <li> <code>calculate_dynamic_control_limits</code>             \u2013              <p>Calculate dynamic control limits that adapt over time.</p> </li> <li> <code>detect_cusum_shifts</code>             \u2013              <p>Detect process shifts using CUSUM (Cumulative Sum) control chart.</p> </li> <li> <code>get_dataframe</code>             \u2013              <p>Returns the processed DataFrame.</p> </li> <li> <code>interpret_violations</code>             \u2013              <p>Add human-readable interpretations to rule violations.</p> </li> <li> <code>process</code>             \u2013              <p>Applies the selected SPC rules and generates a DataFrame of events where any rules are violated.</p> </li> <li> <code>rule_1</code>             \u2013              <p>Rule 1: One point beyond the 3\u03c3 control limits.</p> </li> <li> <code>rule_2</code>             \u2013              <p>Rule 2: Nine consecutive points on one side of the mean.</p> </li> <li> <code>rule_3</code>             \u2013              <p>Rule 3: Six consecutive points steadily increasing or decreasing.</p> </li> <li> <code>rule_4</code>             \u2013              <p>Rule 4: Fourteen consecutive points alternating up and down.</p> </li> <li> <code>rule_5</code>             \u2013              <p>Rule 5: Two out of three consecutive points near the control limit (beyond 2\u03c3 but within 3\u03c3).</p> </li> <li> <code>rule_6</code>             \u2013              <p>Rule 6: Four out of five consecutive points near the control limit (beyond 1\u03c3 but within 2\u03c3).</p> </li> <li> <code>rule_7</code>             \u2013              <p>Rule 7: Fifteen consecutive points within 1\u03c3 of the centerline.</p> </li> <li> <code>rule_8</code>             \u2013              <p>Rule 8: Eight consecutive points on both sides of the mean within 1\u03c3.</p> </li> </ul> Source code in <code>src/ts_shape/events/quality/statistical_process_control.py</code> <pre><code>def __init__(self, dataframe: pd.DataFrame, value_column: str, tolerance_uuid: str, actual_uuid: str, event_uuid: str) -&gt; None:\n    \"\"\"\n    Initializes the SPCMonitor with UUIDs for tolerance, actual, and event values.\n    Inherits the sorted dataframe from the Base class.\n\n    Args:\n        dataframe (pd.DataFrame): The input DataFrame containing the data to be processed.\n        value_column (str): The column containing the values to monitor.\n        tolerance_uuid (str): UUID identifier for rows that set tolerance values.\n        actual_uuid (str): UUID identifier for rows containing actual values.\n        event_uuid (str): UUID to assign to generated events.\n    \"\"\"\n    super().__init__(dataframe)  # Initialize the Base class\n    self.value_column: str = value_column\n    self.tolerance_uuid: str = tolerance_uuid\n    self.actual_uuid: str = actual_uuid\n    self.event_uuid: str = event_uuid\n</code></pre>"},{"location":"reference/ts_shape/events/quality/statistical_process_control/#ts_shape.events.quality.statistical_process_control.StatisticalProcessControlRuleBased(dataframe)","title":"<code>dataframe</code>","text":"(<code>DataFrame</code>)           \u2013            <p>The input DataFrame containing the data to be processed.</p>"},{"location":"reference/ts_shape/events/quality/statistical_process_control/#ts_shape.events.quality.statistical_process_control.StatisticalProcessControlRuleBased(value_column)","title":"<code>value_column</code>","text":"(<code>str</code>)           \u2013            <p>The column containing the values to monitor.</p>"},{"location":"reference/ts_shape/events/quality/statistical_process_control/#ts_shape.events.quality.statistical_process_control.StatisticalProcessControlRuleBased(tolerance_uuid)","title":"<code>tolerance_uuid</code>","text":"(<code>str</code>)           \u2013            <p>UUID identifier for rows that set tolerance values.</p>"},{"location":"reference/ts_shape/events/quality/statistical_process_control/#ts_shape.events.quality.statistical_process_control.StatisticalProcessControlRuleBased(actual_uuid)","title":"<code>actual_uuid</code>","text":"(<code>str</code>)           \u2013            <p>UUID identifier for rows containing actual values.</p>"},{"location":"reference/ts_shape/events/quality/statistical_process_control/#ts_shape.events.quality.statistical_process_control.StatisticalProcessControlRuleBased(event_uuid)","title":"<code>event_uuid</code>","text":"(<code>str</code>)           \u2013            <p>UUID to assign to generated events.</p>"},{"location":"reference/ts_shape/events/quality/statistical_process_control/#ts_shape.events.quality.statistical_process_control.StatisticalProcessControlRuleBased.apply_rules_vectorized","title":"apply_rules_vectorized","text":"<pre><code>apply_rules_vectorized(selected_rules: Optional[List[str]] = None) -&gt; DataFrame\n</code></pre> <p>Applies SPC rules using vectorized operations with optimized multi-rule processing. Processes multiple rules in fewer passes through the data for better performance.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>pd.DataFrame: DataFrame with rule violations, including rule name and severity.</p> </li> </ul> Source code in <code>src/ts_shape/events/quality/statistical_process_control.py</code> <pre><code>def apply_rules_vectorized(\n    self,\n    selected_rules: Optional[List[str]] = None\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Applies SPC rules using vectorized operations with optimized multi-rule processing.\n    Processes multiple rules in fewer passes through the data for better performance.\n\n    Args:\n        selected_rules (Optional[List[str]]): List of rule names to apply.\n            If None, applies all rules.\n\n    Returns:\n        pd.DataFrame: DataFrame with rule violations, including rule name and severity.\n    \"\"\"\n    df = self.dataframe[self.dataframe['uuid'] == self.actual_uuid].copy()\n    df['systime'] = pd.to_datetime(df['systime'])\n    df = df.sort_values(by='systime').reset_index(drop=True)\n\n    limits = self.calculate_control_limits()\n\n    if selected_rules is None:\n        selected_rules = ['rule_1', 'rule_2', 'rule_3', 'rule_4', 'rule_5', 'rule_6', 'rule_7', 'rule_8']\n\n    # Pre-compute values used across multiple rules\n    values = df[self.value_column]\n    mean = values.mean()\n    upper_3sigma = limits['3sigma_upper'].values[0]\n    lower_3sigma = limits['3sigma_lower'].values[0]\n    upper_2sigma = limits['2sigma_upper'].values[0]\n    lower_2sigma = limits['2sigma_lower'].values[0]\n    upper_1sigma = limits['1sigma_upper'].values[0]\n    lower_1sigma = limits['1sigma_lower'].values[0]\n\n    # Initialize results\n    violations = []\n\n    # Rule 1: One point beyond 3\u03c3\n    if 'rule_1' in selected_rules:\n        rule_1_mask = (values &gt; upper_3sigma) | (values &lt; lower_3sigma)\n        if rule_1_mask.any():\n            for idx in df[rule_1_mask].index:\n                violations.append({\n                    'systime': df.loc[idx, 'systime'],\n                    'value': df.loc[idx, self.value_column],\n                    'rule': 'rule_1',\n                    'severity': 'critical'\n                })\n\n    # Optimized rules 2, 7, 8 (processed together)\n    if any(r in selected_rules for r in ['rule_2', 'rule_7', 'rule_8']):\n        rule_2_mask, rule_7_mask, rule_8_mask = self._calculate_rule_2_7_8_optimized(df, limits)\n\n        if 'rule_2' in selected_rules and rule_2_mask.any():\n            for idx in df[rule_2_mask].index:\n                violations.append({\n                    'systime': df.loc[idx, 'systime'],\n                    'value': df.loc[idx, self.value_column],\n                    'rule': 'rule_2',\n                    'severity': 'medium'\n                })\n\n        if 'rule_7' in selected_rules and rule_7_mask.any():\n            for idx in df[rule_7_mask].index:\n                violations.append({\n                    'systime': df.loc[idx, 'systime'],\n                    'value': df.loc[idx, self.value_column],\n                    'rule': 'rule_7',\n                    'severity': 'low'\n                })\n\n        if 'rule_8' in selected_rules and rule_8_mask.any():\n            for idx in df[rule_8_mask].index:\n                violations.append({\n                    'systime': df.loc[idx, 'systime'],\n                    'value': df.loc[idx, self.value_column],\n                    'rule': 'rule_8',\n                    'severity': 'low'\n                })\n\n    # Rule 3: Six consecutive points steadily increasing or decreasing\n    if 'rule_3' in selected_rules:\n        diffs = values.diff()\n        increasing = diffs &gt; 0\n        decreasing = diffs &lt; 0\n        rule_3_mask = (increasing.rolling(window=6).sum() == 6) | (decreasing.rolling(window=6).sum() == 6)\n        if rule_3_mask.any():\n            for idx in df[rule_3_mask].index:\n                violations.append({\n                    'systime': df.loc[idx, 'systime'],\n                    'value': df.loc[idx, self.value_column],\n                    'rule': 'rule_3',\n                    'severity': 'medium'\n                })\n\n    # Rule 4: Fourteen consecutive points alternating\n    if 'rule_4' in selected_rules:\n        alternating = values.diff().apply(np.sign)\n        rule_4_mask = alternating.rolling(window=14).apply(\n            lambda x: (x != x.shift()).sum() == 13 if len(x) == 14 else False,\n            raw=True\n        ) == 1\n        if rule_4_mask.any():\n            for idx in df[rule_4_mask].index:\n                violations.append({\n                    'systime': df.loc[idx, 'systime'],\n                    'value': df.loc[idx, self.value_column],\n                    'rule': 'rule_4',\n                    'severity': 'medium'\n                })\n\n    # Rule 5: Two out of three beyond 2\u03c3\n    if 'rule_5' in selected_rules:\n        beyond_2sigma = ((values &gt; upper_2sigma) &amp; (values &lt; upper_3sigma)) | \\\n                       ((values &lt; lower_2sigma) &amp; (values &gt; lower_3sigma))\n        rule_5_mask = beyond_2sigma.rolling(window=3).sum() &gt;= 2\n        if rule_5_mask.any():\n            for idx in df[rule_5_mask].index:\n                violations.append({\n                    'systime': df.loc[idx, 'systime'],\n                    'value': df.loc[idx, self.value_column],\n                    'rule': 'rule_5',\n                    'severity': 'high'\n                })\n\n    # Rule 6: Four out of five beyond 1\u03c3\n    if 'rule_6' in selected_rules:\n        beyond_1sigma = ((values &gt; upper_1sigma) &amp; (values &lt; upper_2sigma)) | \\\n                       ((values &lt; lower_1sigma) &amp; (values &gt; lower_2sigma))\n        rule_6_mask = beyond_1sigma.rolling(window=5).sum() &gt;= 4\n        if rule_6_mask.any():\n            for idx in df[rule_6_mask].index:\n                violations.append({\n                    'systime': df.loc[idx, 'systime'],\n                    'value': df.loc[idx, self.value_column],\n                    'rule': 'rule_6',\n                    'severity': 'medium'\n                })\n\n    # Create violations DataFrame\n    if violations:\n        violations_df = pd.DataFrame(violations)\n        violations_df['uuid'] = self.event_uuid\n        return violations_df.drop_duplicates()\n    else:\n        return pd.DataFrame(columns=['systime', 'value', 'rule', 'severity', 'uuid'])\n</code></pre>"},{"location":"reference/ts_shape/events/quality/statistical_process_control/#ts_shape.events.quality.statistical_process_control.StatisticalProcessControlRuleBased.apply_rules_vectorized(selected_rules)","title":"<code>selected_rules</code>","text":"(<code>Optional[List[str]]</code>, default:                   <code>None</code> )           \u2013            <p>List of rule names to apply. If None, applies all rules.</p>"},{"location":"reference/ts_shape/events/quality/statistical_process_control/#ts_shape.events.quality.statistical_process_control.StatisticalProcessControlRuleBased.calculate_control_limits","title":"calculate_control_limits","text":"<pre><code>calculate_control_limits() -&gt; DataFrame\n</code></pre> <p>Calculate the control limits (mean \u00b1 1\u03c3, 2\u03c3, 3\u03c3) for the tolerance values.</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>pd.DataFrame: DataFrame with control limits for each tolerance group.</p> </li> </ul> Source code in <code>src/ts_shape/events/quality/statistical_process_control.py</code> <pre><code>def calculate_control_limits(self) -&gt; pd.DataFrame:\n    \"\"\"\n    Calculate the control limits (mean \u00b1 1\u03c3, 2\u03c3, 3\u03c3) for the tolerance values.\n\n    Returns:\n        pd.DataFrame: DataFrame with control limits for each tolerance group.\n    \"\"\"\n    df = self.dataframe[self.dataframe['uuid'] == self.tolerance_uuid]\n    mean = df[self.value_column].mean()\n    sigma = df[self.value_column].std()\n\n    control_limits = {\n        'mean': mean,\n        '1sigma_upper': mean + sigma,\n        '1sigma_lower': mean - sigma,\n        '2sigma_upper': mean + 2 * sigma,\n        '2sigma_lower': mean - 2 * sigma,\n        '3sigma_upper': mean + 3 * sigma,\n        '3sigma_lower': mean - 3 * sigma,\n    }\n\n    return pd.DataFrame([control_limits])\n</code></pre>"},{"location":"reference/ts_shape/events/quality/statistical_process_control/#ts_shape.events.quality.statistical_process_control.StatisticalProcessControlRuleBased.calculate_dynamic_control_limits","title":"calculate_dynamic_control_limits","text":"<pre><code>calculate_dynamic_control_limits(method: str = 'moving_range', window: int = 20) -&gt; DataFrame\n</code></pre> <p>Calculate dynamic control limits that adapt over time.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>pd.DataFrame: DataFrame with dynamic control limits indexed by time.</p> </li> </ul> Source code in <code>src/ts_shape/events/quality/statistical_process_control.py</code> <pre><code>def calculate_dynamic_control_limits(\n    self,\n    method: str = 'moving_range',\n    window: int = 20\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Calculate dynamic control limits that adapt over time.\n\n    Args:\n        method (str): Method for calculating dynamic limits. Options:\n            - 'moving_range': Uses moving window statistics\n            - 'ewma': Uses Exponentially Weighted Moving Average\n        window (int): Window size for moving calculations (default: 20)\n\n    Returns:\n        pd.DataFrame: DataFrame with dynamic control limits indexed by time.\n    \"\"\"\n    df = self.dataframe[self.dataframe['uuid'] == self.actual_uuid].copy()\n    df = df.sort_values(by='systime')\n\n    if method == 'moving_range':\n        # Calculate rolling mean and std\n        rolling_mean = df[self.value_column].rolling(window=window, min_periods=1).mean()\n        rolling_std = df[self.value_column].rolling(window=window, min_periods=1).std()\n\n        control_limits = pd.DataFrame({\n            'systime': df['systime'],\n            'mean': rolling_mean,\n            '1sigma_upper': rolling_mean + rolling_std,\n            '1sigma_lower': rolling_mean - rolling_std,\n            '2sigma_upper': rolling_mean + 2 * rolling_std,\n            '2sigma_lower': rolling_mean - 2 * rolling_std,\n            '3sigma_upper': rolling_mean + 3 * rolling_std,\n            '3sigma_lower': rolling_mean - 3 * rolling_std,\n        })\n\n    elif method == 'ewma':\n        # Calculate EWMA-based control limits\n        # Convert window to span for EWMA\n        span = window\n        ewma_mean = df[self.value_column].ewm(span=span, adjust=False).mean()\n\n        # Calculate EWMA variance\n        squared_diff = (df[self.value_column] - ewma_mean) ** 2\n        ewma_var = squared_diff.ewm(span=span, adjust=False).mean()\n        ewma_std = np.sqrt(ewma_var)\n\n        control_limits = pd.DataFrame({\n            'systime': df['systime'],\n            'mean': ewma_mean,\n            '1sigma_upper': ewma_mean + ewma_std,\n            '1sigma_lower': ewma_mean - ewma_std,\n            '2sigma_upper': ewma_mean + 2 * ewma_std,\n            '2sigma_lower': ewma_mean - 2 * ewma_std,\n            '3sigma_upper': ewma_mean + 3 * ewma_std,\n            '3sigma_lower': ewma_mean - 3 * ewma_std,\n        })\n    else:\n        raise ValueError(f\"Unknown method: {method}. Use 'moving_range' or 'ewma'.\")\n\n    return control_limits.reset_index(drop=True)\n</code></pre>"},{"location":"reference/ts_shape/events/quality/statistical_process_control/#ts_shape.events.quality.statistical_process_control.StatisticalProcessControlRuleBased.calculate_dynamic_control_limits(method)","title":"<code>method</code>","text":"(<code>str</code>, default:                   <code>'moving_range'</code> )           \u2013            <p>Method for calculating dynamic limits. Options: - 'moving_range': Uses moving window statistics - 'ewma': Uses Exponentially Weighted Moving Average</p>"},{"location":"reference/ts_shape/events/quality/statistical_process_control/#ts_shape.events.quality.statistical_process_control.StatisticalProcessControlRuleBased.calculate_dynamic_control_limits(window)","title":"<code>window</code>","text":"(<code>int</code>, default:                   <code>20</code> )           \u2013            <p>Window size for moving calculations (default: 20)</p>"},{"location":"reference/ts_shape/events/quality/statistical_process_control/#ts_shape.events.quality.statistical_process_control.StatisticalProcessControlRuleBased.detect_cusum_shifts","title":"detect_cusum_shifts","text":"<pre><code>detect_cusum_shifts(target: Optional[float] = None, k: float = 0.5, h: float = 5.0) -&gt; DataFrame\n</code></pre> <p>Detect process shifts using CUSUM (Cumulative Sum) control chart.</p> <p>CUSUM charts are effective at detecting small shifts in the process mean and are more sensitive than traditional Shewhart control charts.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>pd.DataFrame: DataFrame with CUSUM statistics and detected shifts. Columns: systime, value, cusum_high, cusum_low, shift_detected, shift_direction</p> </li> </ul> Source code in <code>src/ts_shape/events/quality/statistical_process_control.py</code> <pre><code>def detect_cusum_shifts(\n    self,\n    target: Optional[float] = None,\n    k: float = 0.5,\n    h: float = 5.0\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Detect process shifts using CUSUM (Cumulative Sum) control chart.\n\n    CUSUM charts are effective at detecting small shifts in the process mean\n    and are more sensitive than traditional Shewhart control charts.\n\n    Args:\n        target (Optional[float]): Target mean value. If None, uses the mean of tolerance data.\n        k (float): Reference value (slack parameter), typically 0.5 to 1.0 times sigma.\n            Smaller k detects smaller shifts. Default: 0.5\n        h (float): Decision interval (threshold). Typical values are 4-5.\n            Smaller h gives faster detection but more false alarms. Default: 5.0\n\n    Returns:\n        pd.DataFrame: DataFrame with CUSUM statistics and detected shifts.\n            Columns: systime, value, cusum_high, cusum_low, shift_detected, shift_direction\n    \"\"\"\n    df = self.dataframe[self.dataframe['uuid'] == self.actual_uuid].copy()\n    df = df.sort_values(by='systime').reset_index(drop=True)\n\n    # Determine target and sigma\n    if target is None:\n        tolerance_df = self.dataframe[self.dataframe['uuid'] == self.tolerance_uuid]\n        target = tolerance_df[self.value_column].mean()\n\n    tolerance_df = self.dataframe[self.dataframe['uuid'] == self.tolerance_uuid]\n    sigma = tolerance_df[self.value_column].std()\n\n    # Initialize CUSUM statistics\n    cusum_high = np.zeros(len(df))\n    cusum_low = np.zeros(len(df))\n\n    # Calculate CUSUM\n    for i in range(len(df)):\n        value = df.loc[i, self.value_column]\n\n        if i == 0:\n            cusum_high[i] = max(0, value - target - k * sigma)\n            cusum_low[i] = max(0, target - value - k * sigma)\n        else:\n            cusum_high[i] = max(0, cusum_high[i-1] + value - target - k * sigma)\n            cusum_low[i] = max(0, cusum_low[i-1] + target - value - k * sigma)\n\n    # Add CUSUM statistics to dataframe\n    df['cusum_high'] = cusum_high\n    df['cusum_low'] = cusum_low\n\n    # Detect shifts (when CUSUM exceeds threshold h*sigma)\n    threshold = h * sigma\n    df['shift_detected'] = (cusum_high &gt; threshold) | (cusum_low &gt; threshold)\n    df['shift_direction'] = np.where(\n        cusum_high &gt; threshold,\n        'upward',\n        np.where(cusum_low &gt; threshold, 'downward', 'none')\n    )\n    df['severity'] = np.where(\n        df['shift_detected'],\n        np.where((cusum_high &gt; 2 * threshold) | (cusum_low &gt; 2 * threshold), 'critical', 'high'),\n        'none'\n    )\n\n    # Add event UUID to detected shifts\n    df.loc[df['shift_detected'], 'uuid'] = self.event_uuid\n\n    # Return only rows with detected shifts\n    result = df[df['shift_detected']].copy()\n    result = result[['systime', self.value_column, 'cusum_high', 'cusum_low',\n                    'shift_direction', 'severity', 'uuid']]\n\n    return result\n</code></pre>"},{"location":"reference/ts_shape/events/quality/statistical_process_control/#ts_shape.events.quality.statistical_process_control.StatisticalProcessControlRuleBased.detect_cusum_shifts(target)","title":"<code>target</code>","text":"(<code>Optional[float]</code>, default:                   <code>None</code> )           \u2013            <p>Target mean value. If None, uses the mean of tolerance data.</p>"},{"location":"reference/ts_shape/events/quality/statistical_process_control/#ts_shape.events.quality.statistical_process_control.StatisticalProcessControlRuleBased.detect_cusum_shifts(k)","title":"<code>k</code>","text":"(<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>Reference value (slack parameter), typically 0.5 to 1.0 times sigma. Smaller k detects smaller shifts. Default: 0.5</p>"},{"location":"reference/ts_shape/events/quality/statistical_process_control/#ts_shape.events.quality.statistical_process_control.StatisticalProcessControlRuleBased.detect_cusum_shifts(h)","title":"<code>h</code>","text":"(<code>float</code>, default:                   <code>5.0</code> )           \u2013            <p>Decision interval (threshold). Typical values are 4-5. Smaller h gives faster detection but more false alarms. Default: 5.0</p>"},{"location":"reference/ts_shape/events/quality/statistical_process_control/#ts_shape.events.quality.statistical_process_control.StatisticalProcessControlRuleBased.get_dataframe","title":"get_dataframe","text":"<pre><code>get_dataframe() -&gt; DataFrame\n</code></pre> <p>Returns the processed DataFrame.</p> Source code in <code>src/ts_shape/utils/base.py</code> <pre><code>def get_dataframe(self) -&gt; pd.DataFrame:\n    \"\"\"Returns the processed DataFrame.\"\"\"\n    return self.dataframe\n</code></pre>"},{"location":"reference/ts_shape/events/quality/statistical_process_control/#ts_shape.events.quality.statistical_process_control.StatisticalProcessControlRuleBased.interpret_violations","title":"interpret_violations","text":"<pre><code>interpret_violations(violations_df: DataFrame) -&gt; DataFrame\n</code></pre> <p>Add human-readable interpretations to rule violations.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>pd.DataFrame: Enhanced DataFrame with interpretation and recommendation columns.</p> </li> </ul> Source code in <code>src/ts_shape/events/quality/statistical_process_control.py</code> <pre><code>def interpret_violations(\n    self,\n    violations_df: pd.DataFrame\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Add human-readable interpretations to rule violations.\n\n    Args:\n        violations_df (pd.DataFrame): DataFrame with rule violations\n            (output from apply_rules_vectorized or process methods)\n\n    Returns:\n        pd.DataFrame: Enhanced DataFrame with interpretation and recommendation columns.\n    \"\"\"\n    # Rule interpretations\n    rule_interpretations = {\n        'rule_1': {\n            'interpretation': 'One or more points beyond 3-sigma control limits',\n            'meaning': 'Indicates a special cause - an unusual event or significant process change',\n            'recommendation': 'Investigate immediately for assignable causes such as equipment failure, '\n                             'operator error, or material defects',\n            'default_severity': 'critical'\n        },\n        'rule_2': {\n            'interpretation': 'Nine consecutive points on one side of the center line',\n            'meaning': 'Process mean has shifted - indicates a sustained change in process level',\n            'recommendation': 'Check for systematic changes in materials, methods, equipment settings, '\n                             'or environmental conditions',\n            'default_severity': 'medium'\n        },\n        'rule_3': {\n            'interpretation': 'Six consecutive points steadily increasing or decreasing',\n            'meaning': 'Indicates a trend - gradual systematic change in the process',\n            'recommendation': 'Look for tool wear, temperature drift, operator fatigue, '\n                             'or gradual equipment degradation',\n            'default_severity': 'medium'\n        },\n        'rule_4': {\n            'interpretation': 'Fourteen consecutive points alternating up and down',\n            'meaning': 'Indicates systematic oscillation - two alternating causes affecting the process',\n            'recommendation': 'Check for alternating operators, materials from two sources, '\n                             'or temperature cycling effects',\n            'default_severity': 'medium'\n        },\n        'rule_5': {\n            'interpretation': 'Two out of three consecutive points beyond 2-sigma limits',\n            'meaning': 'Process variation has increased or mean is shifting',\n            'recommendation': 'Monitor closely and prepare to investigate. May indicate the start '\n                             'of a larger problem',\n            'default_severity': 'high'\n        },\n        'rule_6': {\n            'interpretation': 'Four out of five consecutive points beyond 1-sigma limits',\n            'meaning': 'Process variation or mean has likely changed',\n            'recommendation': 'Check for changes in process inputs or measurement system accuracy',\n            'default_severity': 'medium'\n        },\n        'rule_7': {\n            'interpretation': 'Fifteen consecutive points within 1-sigma of center line',\n            'meaning': 'Unusually low variation - may indicate stratification or measurement issues',\n            'recommendation': 'Verify measurement system accuracy and check if data is being '\n                             'manipulated or averaged incorrectly',\n            'default_severity': 'low'\n        },\n        'rule_8': {\n            'interpretation': 'Eight consecutive points beyond 1-sigma on both sides',\n            'meaning': 'Process variation may be higher than expected',\n            'recommendation': 'Review process capability and consider if control limits need recalculation',\n            'default_severity': 'low'\n        }\n    }\n\n    # Add interpretations to violations\n    result = violations_df.copy()\n\n    # Ensure severity column exists\n    if 'severity' not in result.columns:\n        result['severity'] = result['rule'].map(\n            lambda r: rule_interpretations.get(r, {}).get('default_severity', 'medium')\n        )\n\n    # Add interpretation columns\n    result['interpretation'] = result['rule'].map(\n        lambda r: rule_interpretations.get(r, {}).get('interpretation', 'Unknown rule')\n    )\n    result['meaning'] = result['rule'].map(\n        lambda r: rule_interpretations.get(r, {}).get('meaning', 'No description available')\n    )\n    result['recommendation'] = result['rule'].map(\n        lambda r: rule_interpretations.get(r, {}).get('recommendation', 'Review process documentation')\n    )\n\n    return result\n</code></pre>"},{"location":"reference/ts_shape/events/quality/statistical_process_control/#ts_shape.events.quality.statistical_process_control.StatisticalProcessControlRuleBased.interpret_violations(violations_df)","title":"<code>violations_df</code>","text":"(<code>DataFrame</code>)           \u2013            <p>DataFrame with rule violations (output from apply_rules_vectorized or process methods)</p>"},{"location":"reference/ts_shape/events/quality/statistical_process_control/#ts_shape.events.quality.statistical_process_control.StatisticalProcessControlRuleBased.process","title":"process","text":"<pre><code>process(selected_rules: Optional[List[str]] = None, include_severity: bool = False) -&gt; DataFrame\n</code></pre> <p>Applies the selected SPC rules and generates a DataFrame of events where any rules are violated.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>pd.DataFrame: DataFrame with rule violations and detected events. If include_severity=False: columns are [systime, value_column, uuid] If include_severity=True: columns include [systime, value_column, uuid, rule, severity]</p> </li> </ul> Source code in <code>src/ts_shape/events/quality/statistical_process_control.py</code> <pre><code>def process(self, selected_rules: Optional[List[str]] = None, include_severity: bool = False) -&gt; pd.DataFrame:\n    \"\"\"\n    Applies the selected SPC rules and generates a DataFrame of events where any rules are violated.\n\n    Args:\n        selected_rules (Optional[List[str]]): List of rule names (e.g., ['rule_1', 'rule_3']) to apply.\n        include_severity (bool): If True, includes severity and rule information in output.\n            Default: False (maintains backward compatibility)\n\n    Returns:\n        pd.DataFrame: DataFrame with rule violations and detected events.\n            If include_severity=False: columns are [systime, value_column, uuid]\n            If include_severity=True: columns include [systime, value_column, uuid, rule, severity]\n    \"\"\"\n    df = self.dataframe[self.dataframe['uuid'] == self.actual_uuid].copy()\n    df['systime'] = pd.to_datetime(df['systime'])\n    df = df.sort_values(by='systime')\n\n    limits = self.calculate_control_limits()\n\n    # Dictionary of rule functions\n    rules = {\n        'rule_1': lambda df: self.rule_1(df, limits),\n        'rule_2': lambda df: self.rule_2(df),\n        'rule_3': lambda df: self.rule_3(df),\n        'rule_4': lambda df: self.rule_4(df),\n        'rule_5': lambda df: self.rule_5(df, limits),\n        'rule_6': lambda df: self.rule_6(df, limits),\n        'rule_7': lambda df: self.rule_7(df, limits),\n        'rule_8': lambda df: self.rule_8(df, limits)\n    }\n\n    # Severity mapping for rules\n    rule_severity = {\n        'rule_1': 'critical',\n        'rule_2': 'medium',\n        'rule_3': 'medium',\n        'rule_4': 'medium',\n        'rule_5': 'high',\n        'rule_6': 'medium',\n        'rule_7': 'low',\n        'rule_8': 'low'\n    }\n\n    # If no specific rules are provided, use all rules\n    if selected_rules is None:\n        selected_rules = list(rules.keys())\n\n    # Apply selected rules and track which rule triggered each event\n    all_events = []\n    for rule_name in selected_rules:\n        if rule_name in rules:\n            rule_events = rules[rule_name](df.copy())\n            if not rule_events.empty:\n                rule_events['triggered_rule'] = rule_name\n                rule_events['severity'] = rule_severity.get(rule_name, 'medium')\n                all_events.append(rule_events)\n\n    if all_events:\n        events = pd.concat(all_events).drop_duplicates(subset=['systime', self.value_column])\n    else:\n        events = pd.DataFrame(columns=['systime', self.value_column])\n\n    # Add the event UUID to the detected events\n    events['uuid'] = self.event_uuid\n\n    # Return appropriate columns based on include_severity flag\n    if include_severity and 'triggered_rule' in events.columns:\n        return events[['systime', self.value_column, 'uuid', 'triggered_rule', 'severity']].drop_duplicates()\n    else:\n        # Backward compatible output\n        return events[['systime', self.value_column, 'uuid']].drop_duplicates()\n</code></pre>"},{"location":"reference/ts_shape/events/quality/statistical_process_control/#ts_shape.events.quality.statistical_process_control.StatisticalProcessControlRuleBased.process(selected_rules)","title":"<code>selected_rules</code>","text":"(<code>Optional[List[str]]</code>, default:                   <code>None</code> )           \u2013            <p>List of rule names (e.g., ['rule_1', 'rule_3']) to apply.</p>"},{"location":"reference/ts_shape/events/quality/statistical_process_control/#ts_shape.events.quality.statistical_process_control.StatisticalProcessControlRuleBased.process(include_severity)","title":"<code>include_severity</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, includes severity and rule information in output. Default: False (maintains backward compatibility)</p>"},{"location":"reference/ts_shape/events/quality/statistical_process_control/#ts_shape.events.quality.statistical_process_control.StatisticalProcessControlRuleBased.rule_1","title":"rule_1","text":"<pre><code>rule_1(df: DataFrame, limits: DataFrame) -&gt; DataFrame\n</code></pre> <p>Rule 1: One point beyond the 3\u03c3 control limits.</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>pd.DataFrame: Filtered DataFrame with rule violations.</p> </li> </ul> Source code in <code>src/ts_shape/events/quality/statistical_process_control.py</code> <pre><code>def rule_1(self, df: pd.DataFrame, limits: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Rule 1: One point beyond the 3\u03c3 control limits.\n\n    Returns:\n        pd.DataFrame: Filtered DataFrame with rule violations.\n    \"\"\"\n    df['rule_1'] = (df[self.value_column] &gt; limits['3sigma_upper'].values[0]) | (df[self.value_column] &lt; limits['3sigma_lower'].values[0])\n    return df[df['rule_1']]\n</code></pre>"},{"location":"reference/ts_shape/events/quality/statistical_process_control/#ts_shape.events.quality.statistical_process_control.StatisticalProcessControlRuleBased.rule_2","title":"rule_2","text":"<pre><code>rule_2(df: DataFrame) -&gt; DataFrame\n</code></pre> <p>Rule 2: Nine consecutive points on one side of the mean.</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>pd.DataFrame: Filtered DataFrame with rule violations.</p> </li> </ul> Source code in <code>src/ts_shape/events/quality/statistical_process_control.py</code> <pre><code>def rule_2(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Rule 2: Nine consecutive points on one side of the mean.\n\n    Returns:\n        pd.DataFrame: Filtered DataFrame with rule violations.\n    \"\"\"\n    mean = df[self.value_column].mean()\n    df['above_mean'] = df[self.value_column] &gt; mean\n    df['below_mean'] = df[self.value_column] &lt; mean\n    df['rule_2'] = (df['above_mean'].rolling(window=9).sum() == 9) | (df['below_mean'].rolling(window=9).sum() == 9)\n    return df[df['rule_2']]\n</code></pre>"},{"location":"reference/ts_shape/events/quality/statistical_process_control/#ts_shape.events.quality.statistical_process_control.StatisticalProcessControlRuleBased.rule_3","title":"rule_3","text":"<pre><code>rule_3(df: DataFrame) -&gt; DataFrame\n</code></pre> <p>Rule 3: Six consecutive points steadily increasing or decreasing.</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>pd.DataFrame: Filtered DataFrame with rule violations.</p> </li> </ul> Source code in <code>src/ts_shape/events/quality/statistical_process_control.py</code> <pre><code>def rule_3(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Rule 3: Six consecutive points steadily increasing or decreasing.\n\n    Returns:\n        pd.DataFrame: Filtered DataFrame with rule violations.\n    \"\"\"\n    df['increasing'] = df[self.value_column].diff().gt(0)\n    df['decreasing'] = df[self.value_column].diff().lt(0)\n    df['rule_3'] = (df['increasing'].rolling(window=6).sum() == 6) | (df['decreasing'].rolling(window=6).sum() == 6)\n    return df[df['rule_3']]\n</code></pre>"},{"location":"reference/ts_shape/events/quality/statistical_process_control/#ts_shape.events.quality.statistical_process_control.StatisticalProcessControlRuleBased.rule_4","title":"rule_4","text":"<pre><code>rule_4(df: DataFrame) -&gt; DataFrame\n</code></pre> <p>Rule 4: Fourteen consecutive points alternating up and down.</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>pd.DataFrame: Filtered DataFrame with rule violations.</p> </li> </ul> Source code in <code>src/ts_shape/events/quality/statistical_process_control.py</code> <pre><code>def rule_4(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Rule 4: Fourteen consecutive points alternating up and down.\n\n    Returns:\n        pd.DataFrame: Filtered DataFrame with rule violations.\n    \"\"\"\n    df['alternating'] = df[self.value_column].diff().apply(np.sign)\n    df['rule_4'] = df['alternating'].rolling(window=14).apply(lambda x: (x != x.shift()).sum() == 13, raw=True)\n    return df[df['rule_4']]\n</code></pre>"},{"location":"reference/ts_shape/events/quality/statistical_process_control/#ts_shape.events.quality.statistical_process_control.StatisticalProcessControlRuleBased.rule_5","title":"rule_5","text":"<pre><code>rule_5(df: DataFrame, limits: DataFrame) -&gt; DataFrame\n</code></pre> <p>Rule 5: Two out of three consecutive points near the control limit (beyond 2\u03c3 but within 3\u03c3).</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>pd.DataFrame: Filtered DataFrame with rule violations.</p> </li> </ul> Source code in <code>src/ts_shape/events/quality/statistical_process_control.py</code> <pre><code>def rule_5(self, df: pd.DataFrame, limits: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Rule 5: Two out of three consecutive points near the control limit (beyond 2\u03c3 but within 3\u03c3).\n\n    Returns:\n        pd.DataFrame: Filtered DataFrame with rule violations.\n    \"\"\"\n    df['rule_5'] = df[self.value_column].apply(\n        lambda x: 1 if ((x &gt; limits['2sigma_upper'].values[0] and x &lt; limits['3sigma_upper'].values[0]) or \n                        (x &lt; limits['2sigma_lower'].values[0] and x &gt; limits['3sigma_lower'].values[0])) else 0\n    )\n    df['rule_5'] = df['rule_5'].rolling(window=3).sum() &gt;= 2\n    return df[df['rule_5']]\n</code></pre>"},{"location":"reference/ts_shape/events/quality/statistical_process_control/#ts_shape.events.quality.statistical_process_control.StatisticalProcessControlRuleBased.rule_6","title":"rule_6","text":"<pre><code>rule_6(df: DataFrame, limits: DataFrame) -&gt; DataFrame\n</code></pre> <p>Rule 6: Four out of five consecutive points near the control limit (beyond 1\u03c3 but within 2\u03c3).</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>pd.DataFrame: Filtered DataFrame with rule violations.</p> </li> </ul> Source code in <code>src/ts_shape/events/quality/statistical_process_control.py</code> <pre><code>def rule_6(self, df: pd.DataFrame, limits: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Rule 6: Four out of five consecutive points near the control limit (beyond 1\u03c3 but within 2\u03c3).\n\n    Returns:\n        pd.DataFrame: Filtered DataFrame with rule violations.\n    \"\"\"\n    df['rule_6'] = df[self.value_column].apply(\n        lambda x: 1 if ((x &gt; limits['1sigma_upper'].values[0] and x &lt; limits['2sigma_upper'].values[0]) or \n                        (x &lt; limits['1sigma_lower'].values[0] and x &gt; limits['2sigma_lower'].values[0])) else 0\n    )\n    df['rule_6'] = df['rule_6'].rolling(window=5).sum() &gt;= 4\n    return df[df['rule_6']]\n</code></pre>"},{"location":"reference/ts_shape/events/quality/statistical_process_control/#ts_shape.events.quality.statistical_process_control.StatisticalProcessControlRuleBased.rule_7","title":"rule_7","text":"<pre><code>rule_7(df: DataFrame, limits: DataFrame) -&gt; DataFrame\n</code></pre> <p>Rule 7: Fifteen consecutive points within 1\u03c3 of the centerline.</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>pd.DataFrame: Filtered DataFrame with rule violations.</p> </li> </ul> Source code in <code>src/ts_shape/events/quality/statistical_process_control.py</code> <pre><code>def rule_7(self, df: pd.DataFrame, limits: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Rule 7: Fifteen consecutive points within 1\u03c3 of the centerline.\n\n    Returns:\n        pd.DataFrame: Filtered DataFrame with rule violations.\n    \"\"\"\n    df['rule_7'] = df[self.value_column].apply(\n        lambda x: 1 if (x &lt; limits['1sigma_upper'].values[0] and x &gt; limits['1sigma_lower'].values[0]) else 0\n    )\n    df['rule_7'] = df['rule_7'].rolling(window=15).sum() == 15\n    return df[df['rule_7']]\n</code></pre>"},{"location":"reference/ts_shape/events/quality/statistical_process_control/#ts_shape.events.quality.statistical_process_control.StatisticalProcessControlRuleBased.rule_8","title":"rule_8","text":"<pre><code>rule_8(df: DataFrame, limits: DataFrame) -&gt; DataFrame\n</code></pre> <p>Rule 8: Eight consecutive points on both sides of the mean within 1\u03c3.</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>pd.DataFrame: Filtered DataFrame with rule violations.</p> </li> </ul> Source code in <code>src/ts_shape/events/quality/statistical_process_control.py</code> <pre><code>def rule_8(self, df: pd.DataFrame, limits: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Rule 8: Eight consecutive points on both sides of the mean within 1\u03c3.\n\n    Returns:\n        pd.DataFrame: Filtered DataFrame with rule violations.\n    \"\"\"\n    df['rule_8'] = df[self.value_column].apply(\n        lambda x: 1 if (x &lt; limits['1sigma_upper'].values[0] and x &gt; limits['1sigma_lower'].values[0]) else 0\n    )\n    df['rule_8'] = df['rule_8'].rolling(window=8).sum() == 8\n    return df[df['rule_8']]\n</code></pre>"},{"location":"reference/ts_shape/events/quality/tolerance_deviation/","title":"tolerance_deviation","text":""},{"location":"reference/ts_shape/events/quality/tolerance_deviation/#ts_shape.events.quality.tolerance_deviation","title":"ts_shape.events.quality.tolerance_deviation","text":"<p>Classes:</p> <ul> <li> <code>ToleranceDeviationEvents</code>           \u2013            <p>Inherits from Base and processes DataFrame data for specific events, comparing tolerance and actual values.</p> </li> </ul>"},{"location":"reference/ts_shape/events/quality/tolerance_deviation/#ts_shape.events.quality.tolerance_deviation.ToleranceDeviationEvents","title":"ToleranceDeviationEvents","text":"<pre><code>ToleranceDeviationEvents(dataframe: DataFrame, tolerance_column: str, actual_column: str, tolerance_uuid: Optional[str] = None, actual_uuid: str = None, event_uuid: str = None, compare_func: Callable[[Series, Series], Series] = ge, time_threshold: str = '5min', upper_tolerance_uuid: Optional[str] = None, lower_tolerance_uuid: Optional[str] = None, warning_threshold: float = 0.8, tolerance_lag: str = '0s')\n</code></pre> <p>               Bases: <code>Base</code></p> <p>Inherits from Base and processes DataFrame data for specific events, comparing tolerance and actual values.</p> <p>Enhanced features: - Separate upper and lower tolerances - Warning zones with configurable thresholds - Deviation magnitude tracking (absolute and percentage) - Severity level classification (minor, major, critical) - Process capability indices (Cp, Cpk, Pp, Ppk) - Time-lagged tolerance application</p> <p>Parameters:</p> <p>Methods:</p> <ul> <li> <code>compute_capability_indices</code>             \u2013              <p>Calculate process capability indices (Cp, Cpk, Pp, Ppk).</p> </li> <li> <code>get_dataframe</code>             \u2013              <p>Returns the processed DataFrame.</p> </li> <li> <code>process_and_group_data_with_events</code>             \u2013              <p>Processes DataFrame to apply tolerance checks, group events by time, and generate an events DataFrame.</p> </li> </ul> Source code in <code>src/ts_shape/events/quality/tolerance_deviation.py</code> <pre><code>def __init__(self, dataframe: pd.DataFrame, tolerance_column: str, actual_column: str,\n             tolerance_uuid: Optional[str] = None, actual_uuid: str = None, event_uuid: str = None,\n             compare_func: Callable[[pd.Series, pd.Series], pd.Series] = operator.ge,\n             time_threshold: str = '5min',\n             upper_tolerance_uuid: Optional[str] = None,\n             lower_tolerance_uuid: Optional[str] = None,\n             warning_threshold: float = 0.8,\n             tolerance_lag: str = '0s') -&gt; None:\n    \"\"\"\n    Initializes the ToleranceDeviationEvents with specific event attributes.\n    Inherits the sorted dataframe from the Base class.\n\n    Args:\n        dataframe: Input DataFrame with measurement data\n        tolerance_column: Column name containing tolerance values\n        actual_column: Column name containing actual measurement values\n        tolerance_uuid: UUID for tolerance (backward compatibility, used if upper/lower not specified)\n        actual_uuid: UUID for actual measurements\n        event_uuid: UUID for generated events\n        compare_func: Comparison function (default: operator.ge for greater-than-or-equal)\n        time_threshold: Time window for grouping events (default: '5min')\n        upper_tolerance_uuid: UUID for upper tolerance limit (optional)\n        lower_tolerance_uuid: UUID for lower tolerance limit (optional)\n        warning_threshold: Threshold ratio for warning zone (default: 0.8 = 80% of tolerance)\n        tolerance_lag: Time lag for tolerance application (default: '0s')\n    \"\"\"\n    super().__init__(dataframe)  # Inherit and initialize Base class\n\n    self.tolerance_column: str = tolerance_column\n    self.actual_column: str = actual_column\n    self.actual_uuid: str = actual_uuid\n    self.event_uuid: str = event_uuid\n    self.compare_func: Callable[[pd.Series, pd.Series], pd.Series] = compare_func\n    self.time_threshold: str = time_threshold\n    self.warning_threshold: float = warning_threshold\n    self.tolerance_lag: str = tolerance_lag\n\n    # Handle backward compatibility for tolerance_uuid\n    if upper_tolerance_uuid is None and lower_tolerance_uuid is None:\n        if tolerance_uuid is None:\n            raise ValueError(\"Either tolerance_uuid or both upper_tolerance_uuid and lower_tolerance_uuid must be provided\")\n        # Use single tolerance for both upper and lower (backward compatibility)\n        self.tolerance_uuid: Optional[str] = tolerance_uuid\n        self.upper_tolerance_uuid: Optional[str] = None\n        self.lower_tolerance_uuid: Optional[str] = None\n        self.separate_tolerances: bool = False\n    else:\n        # Use separate upper and lower tolerances\n        self.tolerance_uuid: Optional[str] = None\n        self.upper_tolerance_uuid: Optional[str] = upper_tolerance_uuid\n        self.lower_tolerance_uuid: Optional[str] = lower_tolerance_uuid\n        self.separate_tolerances: bool = True\n</code></pre>"},{"location":"reference/ts_shape/events/quality/tolerance_deviation/#ts_shape.events.quality.tolerance_deviation.ToleranceDeviationEvents(dataframe)","title":"<code>dataframe</code>","text":"(<code>DataFrame</code>)           \u2013            <p>Input DataFrame with measurement data</p>"},{"location":"reference/ts_shape/events/quality/tolerance_deviation/#ts_shape.events.quality.tolerance_deviation.ToleranceDeviationEvents(tolerance_column)","title":"<code>tolerance_column</code>","text":"(<code>str</code>)           \u2013            <p>Column name containing tolerance values</p>"},{"location":"reference/ts_shape/events/quality/tolerance_deviation/#ts_shape.events.quality.tolerance_deviation.ToleranceDeviationEvents(actual_column)","title":"<code>actual_column</code>","text":"(<code>str</code>)           \u2013            <p>Column name containing actual measurement values</p>"},{"location":"reference/ts_shape/events/quality/tolerance_deviation/#ts_shape.events.quality.tolerance_deviation.ToleranceDeviationEvents(tolerance_uuid)","title":"<code>tolerance_uuid</code>","text":"(<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>UUID for tolerance (backward compatibility, used if upper/lower not specified)</p>"},{"location":"reference/ts_shape/events/quality/tolerance_deviation/#ts_shape.events.quality.tolerance_deviation.ToleranceDeviationEvents(actual_uuid)","title":"<code>actual_uuid</code>","text":"(<code>str</code>, default:                   <code>None</code> )           \u2013            <p>UUID for actual measurements</p>"},{"location":"reference/ts_shape/events/quality/tolerance_deviation/#ts_shape.events.quality.tolerance_deviation.ToleranceDeviationEvents(event_uuid)","title":"<code>event_uuid</code>","text":"(<code>str</code>, default:                   <code>None</code> )           \u2013            <p>UUID for generated events</p>"},{"location":"reference/ts_shape/events/quality/tolerance_deviation/#ts_shape.events.quality.tolerance_deviation.ToleranceDeviationEvents(compare_func)","title":"<code>compare_func</code>","text":"(<code>Callable[[Series, Series], Series]</code>, default:                   <code>ge</code> )           \u2013            <p>Comparison function (default: operator.ge for greater-than-or-equal)</p>"},{"location":"reference/ts_shape/events/quality/tolerance_deviation/#ts_shape.events.quality.tolerance_deviation.ToleranceDeviationEvents(time_threshold)","title":"<code>time_threshold</code>","text":"(<code>str</code>, default:                   <code>'5min'</code> )           \u2013            <p>Time window for grouping events (default: '5min')</p>"},{"location":"reference/ts_shape/events/quality/tolerance_deviation/#ts_shape.events.quality.tolerance_deviation.ToleranceDeviationEvents(upper_tolerance_uuid)","title":"<code>upper_tolerance_uuid</code>","text":"(<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>UUID for upper tolerance limit (optional)</p>"},{"location":"reference/ts_shape/events/quality/tolerance_deviation/#ts_shape.events.quality.tolerance_deviation.ToleranceDeviationEvents(lower_tolerance_uuid)","title":"<code>lower_tolerance_uuid</code>","text":"(<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>UUID for lower tolerance limit (optional)</p>"},{"location":"reference/ts_shape/events/quality/tolerance_deviation/#ts_shape.events.quality.tolerance_deviation.ToleranceDeviationEvents(warning_threshold)","title":"<code>warning_threshold</code>","text":"(<code>float</code>, default:                   <code>0.8</code> )           \u2013            <p>Threshold ratio for warning zone (default: 0.8 = 80% of tolerance)</p>"},{"location":"reference/ts_shape/events/quality/tolerance_deviation/#ts_shape.events.quality.tolerance_deviation.ToleranceDeviationEvents(tolerance_lag)","title":"<code>tolerance_lag</code>","text":"(<code>str</code>, default:                   <code>'0s'</code> )           \u2013            <p>Time lag for tolerance application (default: '0s')</p>"},{"location":"reference/ts_shape/events/quality/tolerance_deviation/#ts_shape.events.quality.tolerance_deviation.ToleranceDeviationEvents.compute_capability_indices","title":"compute_capability_indices","text":"<pre><code>compute_capability_indices(target_value: Optional[float] = None) -&gt; Dict[str, float]\n</code></pre> <p>Calculate process capability indices (Cp, Cpk, Pp, Ppk).</p> <p>Process capability indices measure how well a process meets specification limits: - Cp: Process capability (potential capability assuming perfect centering) - Cpk: Process capability index (accounts for process centering) - Pp: Process performance (overall variability) - Ppk: Process performance index (accounts for centering)</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>Dict[str, float]</code>           \u2013            <p>Dictionary containing: - 'Cp': Process capability - 'Cpk': Process capability index - 'Pp': Process performance - 'Ppk': Process performance index - 'mean': Process mean - 'std': Process standard deviation - 'usl': Upper specification limit - 'lsl': Lower specification limit - 'target': Target value used</p> </li> </ul> Note <ul> <li>Cp/Cpk use short-term variation (within-subgroup)</li> <li>Pp/Ppk use long-term variation (overall)</li> <li>Values &gt; 1.33 are generally considered acceptable</li> <li>Values &gt; 1.67 are considered good</li> </ul> Source code in <code>src/ts_shape/events/quality/tolerance_deviation.py</code> <pre><code>def compute_capability_indices(self, target_value: Optional[float] = None) -&gt; Dict[str, float]:\n    \"\"\"\n    Calculate process capability indices (Cp, Cpk, Pp, Ppk).\n\n    Process capability indices measure how well a process meets specification limits:\n    - Cp: Process capability (potential capability assuming perfect centering)\n    - Cpk: Process capability index (accounts for process centering)\n    - Pp: Process performance (overall variability)\n    - Ppk: Process performance index (accounts for centering)\n\n    Args:\n        target_value: Target/nominal value for the process. If None, uses midpoint of tolerances.\n\n    Returns:\n        Dictionary containing:\n            - 'Cp': Process capability\n            - 'Cpk': Process capability index\n            - 'Pp': Process performance\n            - 'Ppk': Process performance index\n            - 'mean': Process mean\n            - 'std': Process standard deviation\n            - 'usl': Upper specification limit\n            - 'lsl': Lower specification limit\n            - 'target': Target value used\n\n    Note:\n        - Cp/Cpk use short-term variation (within-subgroup)\n        - Pp/Ppk use long-term variation (overall)\n        - Values &gt; 1.33 are generally considered acceptable\n        - Values &gt; 1.67 are considered good\n    \"\"\"\n    df = self.dataframe.copy()\n    df['systime'] = pd.to_datetime(df['systime'])\n    df = df.sort_values(by='systime', ascending=False)\n\n    # Get tolerance limits\n    if not self.separate_tolerances:\n        # Single tolerance case\n        tolerance_rows = df[df['uuid'] == self.tolerance_uuid]\n        if tolerance_rows.empty:\n            raise ValueError(f\"No tolerance data found for uuid: {self.tolerance_uuid}\")\n\n        tolerance_value = tolerance_rows[self.tolerance_column].iloc[0]\n        usl = tolerance_value  # Upper Specification Limit\n        lsl = tolerance_value  # Lower Specification Limit (same as upper for single tolerance)\n    else:\n        # Separate tolerances case\n        upper_rows = df[df['uuid'] == self.upper_tolerance_uuid]\n        lower_rows = df[df['uuid'] == self.lower_tolerance_uuid]\n\n        if upper_rows.empty:\n            raise ValueError(f\"No upper tolerance data found for uuid: {self.upper_tolerance_uuid}\")\n        if lower_rows.empty:\n            raise ValueError(f\"No lower tolerance data found for uuid: {self.lower_tolerance_uuid}\")\n\n        usl = upper_rows[self.tolerance_column].iloc[0]\n        lsl = lower_rows[self.tolerance_column].iloc[0]\n\n    # Get actual measurements\n    actual_data = df[df['uuid'] == self.actual_uuid][self.actual_column].dropna()\n\n    if actual_data.empty:\n        raise ValueError(f\"No actual measurement data found for uuid: {self.actual_uuid}\")\n\n    # Calculate statistics\n    process_mean = actual_data.mean()\n    process_std = actual_data.std(ddof=1)  # Sample standard deviation\n\n    if process_std == 0:\n        raise ValueError(\"Process standard deviation is zero - cannot calculate capability indices\")\n\n    # Determine target value\n    if target_value is None:\n        target_value = (usl + lsl) / 2\n\n    # Calculate Cp (potential capability)\n    cp = (usl - lsl) / (6 * process_std)\n\n    # Calculate Cpk (actual capability accounting for centering)\n    cpu = (usl - process_mean) / (3 * process_std)\n    cpl = (process_mean - lsl) / (3 * process_std)\n    cpk = min(cpu, cpl)\n\n    # For Pp and Ppk, we use the same formulas as Cp and Cpk\n    # In practice, Pp/Ppk would use long-term std dev, but without subgroups, we use overall std\n    pp = cp  # Using overall variation\n    ppk = cpk\n\n    return {\n        'Cp': round(cp, 4),\n        'Cpk': round(cpk, 4),\n        'Pp': round(pp, 4),\n        'Ppk': round(ppk, 4),\n        'mean': round(process_mean, 4),\n        'std': round(process_std, 4),\n        'usl': round(usl, 4),\n        'lsl': round(lsl, 4),\n        'target': round(target_value, 4)\n    }\n</code></pre>"},{"location":"reference/ts_shape/events/quality/tolerance_deviation/#ts_shape.events.quality.tolerance_deviation.ToleranceDeviationEvents.compute_capability_indices(target_value)","title":"<code>target_value</code>","text":"(<code>Optional[float]</code>, default:                   <code>None</code> )           \u2013            <p>Target/nominal value for the process. If None, uses midpoint of tolerances.</p>"},{"location":"reference/ts_shape/events/quality/tolerance_deviation/#ts_shape.events.quality.tolerance_deviation.ToleranceDeviationEvents.get_dataframe","title":"get_dataframe","text":"<pre><code>get_dataframe() -&gt; DataFrame\n</code></pre> <p>Returns the processed DataFrame.</p> Source code in <code>src/ts_shape/utils/base.py</code> <pre><code>def get_dataframe(self) -&gt; pd.DataFrame:\n    \"\"\"Returns the processed DataFrame.\"\"\"\n    return self.dataframe\n</code></pre>"},{"location":"reference/ts_shape/events/quality/tolerance_deviation/#ts_shape.events.quality.tolerance_deviation.ToleranceDeviationEvents.process_and_group_data_with_events","title":"process_and_group_data_with_events","text":"<pre><code>process_and_group_data_with_events() -&gt; DataFrame\n</code></pre> <p>Processes DataFrame to apply tolerance checks, group events by time, and generate an events DataFrame.</p> <p>Enhanced with: - Separate upper/lower tolerances - Deviation magnitude tracking (absolute and percentage) - Warning zones - Severity level classification - Time-lagged tolerance application</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>pd.DataFrame: A DataFrame of processed and grouped event data with enhanced metrics.</p> </li> </ul> Source code in <code>src/ts_shape/events/quality/tolerance_deviation.py</code> <pre><code>def process_and_group_data_with_events(self) -&gt; pd.DataFrame:\n    \"\"\"\n    Processes DataFrame to apply tolerance checks, group events by time, and generate an events DataFrame.\n\n    Enhanced with:\n    - Separate upper/lower tolerances\n    - Deviation magnitude tracking (absolute and percentage)\n    - Warning zones\n    - Severity level classification\n    - Time-lagged tolerance application\n\n    Returns:\n        pd.DataFrame: A DataFrame of processed and grouped event data with enhanced metrics.\n    \"\"\"\n    df = self.dataframe.copy()  # Inherited from Base class\n\n    # Convert 'systime' to datetime and sort the DataFrame by 'systime' in descending order\n    df['systime'] = pd.to_datetime(df['systime'])\n    df = df.sort_values(by='systime', ascending=False)\n\n    if not self.separate_tolerances:\n        # Backward compatibility: use single tolerance\n        df = self._process_single_tolerance(df)\n    else:\n        # Use separate upper and lower tolerances\n        df = self._process_separate_tolerances(df)\n\n    # Apply time lag to tolerance if specified\n    if self.tolerance_lag != '0s':\n        df = self._apply_tolerance_lag(df, 'tolerance_value')\n\n    return df\n</code></pre>"},{"location":"reference/ts_shape/events/supplychain/__init__/","title":"supplychain","text":""},{"location":"reference/ts_shape/events/supplychain/__init__/#ts_shape.events.supplychain","title":"ts_shape.events.supplychain","text":"<p>Supply Chain Events</p> <p>Detectors for supply chain\u2013related events and anomalies over shaped timeseries.</p> <p>Classes: - None yet: Placeholder module for future supply chain event detectors.</p>"},{"location":"reference/ts_shape/features/__init__/","title":"init","text":""},{"location":"reference/ts_shape/features/__init__/#ts_shape.features","title":"ts_shape.features","text":"<p>Features</p> <p>Feature extraction and summarization utilities for shaped timeseries.</p> <ul> <li>NumericStatistics: Compute descriptive statistics for numeric columns.</li> <li>column_mean: Mean of a column.</li> <li>column_median: Median of a column.</li> <li>column_std: Standard deviation of a column.</li> <li>column_variance: Variance of a column.</li> <li>column_min: Minimum value.</li> <li>column_max: Maximum value.</li> <li>column_sum: Sum of values.</li> <li>column_kurtosis: Kurtosis of values.</li> <li>column_skewness: Skewness of values.</li> <li>column_quantile: Quantile of a column.</li> <li>column_iqr: Interquartile range.</li> <li>column_range: Range (max - min).</li> <li>column_mad: Mean absolute deviation.</li> <li>coefficient_of_variation: Standard deviation divided by mean (guarded).</li> <li>standard_error_mean: Standard error of the mean.</li> <li>describe: Pandas describe wrapper.</li> <li>summary_as_dict: Comprehensive numeric summary as dict.</li> <li> <p>summary_as_dataframe: Comprehensive numeric summary as DataFrame.</p> </li> <li> <p>StringStatistics: String-based statistics for categorical/text columns.</p> </li> <li>count_unique: Number of unique strings.</li> <li>most_frequent: Most frequent string.</li> <li>count_most_frequent: Count of the most frequent string.</li> <li>count_null: Number of nulls.</li> <li>average_string_length: Average length of non-null strings.</li> <li>longest_string: Longest string.</li> <li>shortest_string: Shortest string.</li> <li>string_length_summary: Summary of lengths.</li> <li>most_common_n_strings: Top-N most frequent strings.</li> <li>contains_substring_count: Count of strings containing a substring.</li> <li>starts_with_count: Count of strings starting with a prefix.</li> <li>ends_with_count: Count of strings ending with a suffix.</li> <li>uppercase_percentage: Percentage of uppercase strings.</li> <li>lowercase_percentage: Percentage of lowercase strings.</li> <li>contains_digit_count: Count of strings containing digits.</li> <li>summary_as_dict: Comprehensive string summary as dict.</li> <li> <p>summary_as_dataframe: Comprehensive string summary as DataFrame.</p> </li> <li> <p>BooleanStatistics: Boolean column statistics.</p> </li> <li>count_true: Count of True values.</li> <li>count_false: Count of False values.</li> <li>count_null: Count of nulls.</li> <li>count_not_null: Count of non-nulls.</li> <li>true_percentage: Percentage True.</li> <li>false_percentage: Percentage False.</li> <li>mode: Most common boolean value.</li> <li>is_balanced: Whether distribution is 50/50.</li> <li>summary_as_dict: Summary as dict.</li> <li> <p>summary_as_dataframe: Summary as DataFrame.</p> </li> <li> <p>TimestampStatistics: Timestamp distributions and ranges.</p> </li> <li>count_null: Count of null timestamps.</li> <li>count_not_null: Count of non-null timestamps.</li> <li>earliest_timestamp: Earliest timestamp.</li> <li>latest_timestamp: Latest timestamp.</li> <li>timestamp_range: Time range (latest - earliest).</li> <li>most_frequent_timestamp: Most frequent timestamp.</li> <li>count_most_frequent_timestamp: Count of the modal timestamp.</li> <li>year_distribution: Distribution by year.</li> <li>month_distribution: Distribution by month.</li> <li>weekday_distribution: Distribution by weekday.</li> <li>hour_distribution: Distribution by hour.</li> <li>most_frequent_day: Most frequent weekday.</li> <li>most_frequent_hour: Most frequent hour.</li> <li>average_time_gap: Average gap between consecutive timestamps.</li> <li>median_timestamp: Median timestamp.</li> <li>standard_deviation_timestamps: Standard deviation of consecutive differences.</li> <li>timestamp_quartiles: 25th/50th/75th percentiles.</li> <li> <p>days_with_most_activity: Top-N active days.</p> </li> <li> <p>TimeGroupedStatistics: Time-windowed aggregations for numeric series.</p> </li> <li>calculate_statistic: Single aggregation per window (mean/sum/min/max/diff/range).</li> <li>calculate_statistics: Multiple aggregations merged.</li> <li> <p>calculate_custom_func: Apply a custom aggregation per window.</p> </li> <li> <p>CycleExtractor: Build cycles from state/step/value changes.</p> </li> <li>process_persistent_cycle: True stretches define cycles.</li> <li>process_trigger_cycle: True-to-False transition defines a cycle end.</li> <li>process_separate_start_end_cycle: Separate starts and ends signals.</li> <li>process_step_sequence: Start/end steps in integer values.</li> <li>process_state_change_cycle: Sequential rows define boundaries.</li> <li> <p>process_value_change_cycle: Any value change defines a boundary.</p> </li> <li> <p>CycleDataProcessor: Split/merge/group by cycle windows.</p> </li> <li>split_by_cycle: Split values by cycle ranges.</li> <li>merge_dataframes_by_cycle: Annotate values with cycle UUIDs.</li> <li>group_by_cycle_uuid: Group values by cycle key.</li> <li>split_dataframes_by_group: Further split by column groupings.</li> </ul> <p>Modules:</p> <ul> <li> <code>cycles</code>           \u2013            <p>Cycles</p> </li> <li> <code>stats</code>           \u2013            <p>Statistics</p> </li> <li> <code>time_stats</code>           \u2013            <p>Time-Grouped Statistics</p> </li> </ul>"},{"location":"reference/ts_shape/features/cycles/__init__/","title":"init","text":""},{"location":"reference/ts_shape/features/cycles/__init__/#ts_shape.features.cycles","title":"ts_shape.features.cycles","text":"<p>Cycles</p> <p>Utilities to detect and process cycles in timeseries.</p> <ul> <li>CycleExtractor: Build cycles from state/step/value changes.</li> <li>process_persistent_cycle: True stretches define cycles.</li> <li>process_trigger_cycle: True-to-False transition defines a cycle end.</li> <li>process_separate_start_end_cycle: Separate starts and ends signals.</li> <li>process_step_sequence: Start/end steps in integer values.</li> <li>process_state_change_cycle: Sequential rows define boundaries.</li> <li> <p>process_value_change_cycle: Any value change defines a boundary.</p> </li> <li> <p>CycleDataProcessor: Split/merge/group by cycle windows.</p> </li> <li>split_by_cycle: Split values by cycle ranges.</li> <li>merge_dataframes_by_cycle: Annotate values with cycle UUIDs.</li> <li>group_by_cycle_uuid: Group values by cycle key.</li> <li>split_dataframes_by_group: Further split by column groupings.</li> </ul> <p>Modules:</p> <ul> <li> <code>cycle_processor</code>           \u2013            </li> <li> <code>cycles_extractor</code>           \u2013            </li> </ul>"},{"location":"reference/ts_shape/features/cycles/cycle_processor/","title":"cycle_processor","text":""},{"location":"reference/ts_shape/features/cycles/cycle_processor/#ts_shape.features.cycles.cycle_processor","title":"ts_shape.features.cycles.cycle_processor","text":"<p>Classes:</p> <ul> <li> <code>CycleDataProcessor</code>           \u2013            <p>A class to process cycle-based data and values with optimized performance.</p> </li> </ul>"},{"location":"reference/ts_shape/features/cycles/cycle_processor/#ts_shape.features.cycles.cycle_processor.CycleDataProcessor","title":"CycleDataProcessor","text":"<pre><code>CycleDataProcessor(cycles_df: DataFrame, values_df: DataFrame, cycle_uuid_col: str = 'cycle_uuid', systime_col: str = 'systime')\n</code></pre> <p>               Bases: <code>Base</code></p> <p>A class to process cycle-based data and values with optimized performance. Uses pandas IntervalIndex for efficient cycle assignment instead of nested loops.</p> <p>Parameters:</p> <p>Methods:</p> <ul> <li> <code>compare_cycles</code>             \u2013              <p>Compare all cycles against a reference cycle.</p> </li> <li> <code>compute_cycle_statistics</code>             \u2013              <p>Compute statistics for each cycle.</p> </li> <li> <code>get_dataframe</code>             \u2013              <p>Returns the processed DataFrame.</p> </li> <li> <code>group_by_cycle_uuid</code>             \u2013              <p>Group the DataFrame by the cycle_uuid column, resulting in a list of DataFrames, each containing data for one cycle.</p> </li> <li> <code>identify_golden_cycles</code>             \u2013              <p>Identify the best performing cycles (golden cycles).</p> </li> <li> <code>merge_dataframes_by_cycle</code>             \u2013              <p>Merges the values DataFrame with the cycles DataFrame based on the cycle time intervals.</p> </li> <li> <code>split_by_cycle</code>             \u2013              <p>Splits the values DataFrame by cycles defined in the cycles DataFrame.</p> </li> <li> <code>split_dataframes_by_group</code>             \u2013              <p>Splits a list of DataFrames by groups based on a specified column.</p> </li> </ul> Source code in <code>src/ts_shape/features/cycles/cycle_processor.py</code> <pre><code>def __init__(self, cycles_df: pd.DataFrame, values_df: pd.DataFrame, cycle_uuid_col: str = \"cycle_uuid\", systime_col: str = \"systime\"):\n    \"\"\"\n    Initializes the CycleDataProcessor with cycles and values DataFrames.\n\n    Args:\n        cycles_df: DataFrame containing columns 'cycle_start', 'cycle_end', and 'cycle_uuid'.\n        values_df: DataFrame containing the values and timestamps in the 'systime' column.\n        cycle_uuid_col: Name of the column representing cycle UUIDs.\n        systime_col: Name of the column representing the timestamps for the values.\n    \"\"\"\n    super().__init__(values_df)  # Call the parent constructor\n    self.values_df = values_df.copy()  # Initialize self.values_df explicitly\n    self.cycles_df = cycles_df.copy()\n    self.cycle_uuid_col = cycle_uuid_col\n    self.systime_col = systime_col\n\n    # Ensure proper datetime format\n    self.cycles_df['cycle_start'] = pd.to_datetime(self.cycles_df['cycle_start'])\n    self.cycles_df['cycle_end'] = pd.to_datetime(self.cycles_df['cycle_end'])\n    self.values_df[systime_col] = pd.to_datetime(self.values_df[systime_col])\n\n    # Pre-build interval index for efficient lookups\n    self._cycle_intervals = None\n    self._build_interval_index()\n\n    logging.info(\"CycleDataProcessor initialized with cycles and values DataFrames.\")\n</code></pre>"},{"location":"reference/ts_shape/features/cycles/cycle_processor/#ts_shape.features.cycles.cycle_processor.CycleDataProcessor(cycles_df)","title":"<code>cycles_df</code>","text":"(<code>DataFrame</code>)           \u2013            <p>DataFrame containing columns 'cycle_start', 'cycle_end', and 'cycle_uuid'.</p>"},{"location":"reference/ts_shape/features/cycles/cycle_processor/#ts_shape.features.cycles.cycle_processor.CycleDataProcessor(values_df)","title":"<code>values_df</code>","text":"(<code>DataFrame</code>)           \u2013            <p>DataFrame containing the values and timestamps in the 'systime' column.</p>"},{"location":"reference/ts_shape/features/cycles/cycle_processor/#ts_shape.features.cycles.cycle_processor.CycleDataProcessor(cycle_uuid_col)","title":"<code>cycle_uuid_col</code>","text":"(<code>str</code>, default:                   <code>'cycle_uuid'</code> )           \u2013            <p>Name of the column representing cycle UUIDs.</p>"},{"location":"reference/ts_shape/features/cycles/cycle_processor/#ts_shape.features.cycles.cycle_processor.CycleDataProcessor(systime_col)","title":"<code>systime_col</code>","text":"(<code>str</code>, default:                   <code>'systime'</code> )           \u2013            <p>Name of the column representing the timestamps for the values.</p>"},{"location":"reference/ts_shape/features/cycles/cycle_processor/#ts_shape.features.cycles.cycle_processor.CycleDataProcessor.compare_cycles","title":"compare_cycles","text":"<pre><code>compare_cycles(reference_cycle_uuid: str, metric: str = 'value_double') -&gt; DataFrame\n</code></pre> <p>Compare all cycles against a reference cycle.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>DataFrame with comparison metrics for each cycle</p> </li> </ul> Source code in <code>src/ts_shape/features/cycles/cycle_processor.py</code> <pre><code>def compare_cycles(self, reference_cycle_uuid: str, metric: str = 'value_double') -&gt; pd.DataFrame:\n    \"\"\"\n    Compare all cycles against a reference cycle.\n\n    Args:\n        reference_cycle_uuid: UUID of the reference cycle\n        metric: Column name to use for comparison\n\n    Returns:\n        DataFrame with comparison metrics for each cycle\n    \"\"\"\n    if reference_cycle_uuid not in self.cycles_df[self.cycle_uuid_col].values:\n        logging.error(f\"Reference cycle '{reference_cycle_uuid}' not found.\")\n        return pd.DataFrame()\n\n    # Get reference cycle data\n    ref_cycle = self.cycles_df[self.cycles_df[self.cycle_uuid_col] == reference_cycle_uuid].iloc[0]\n    ref_mask = (self.values_df[self.systime_col] &gt;= ref_cycle['cycle_start']) &amp; (self.values_df[self.systime_col] &lt;= ref_cycle['cycle_end'])\n    ref_values = self.values_df[ref_mask][metric].dropna()\n\n    if ref_values.empty:\n        logging.warning(\"Reference cycle has no data for the specified metric.\")\n        return pd.DataFrame()\n\n    ref_mean = ref_values.mean()\n    ref_std = ref_values.std()\n\n    # Compare each cycle\n    comparisons = []\n    for _, cycle in self.cycles_df.iterrows():\n        cycle_uuid = cycle[self.cycle_uuid_col]\n        mask = (self.values_df[self.systime_col] &gt;= cycle['cycle_start']) &amp; (self.values_df[self.systime_col] &lt;= cycle['cycle_end'])\n        cycle_values = self.values_df[mask][metric].dropna()\n\n        if cycle_values.empty:\n            continue\n\n        comparison = {\n            self.cycle_uuid_col: cycle_uuid,\n            'is_reference': cycle_uuid == reference_cycle_uuid,\n            'mean_value': cycle_values.mean(),\n            'std_value': cycle_values.std(),\n            'deviation_from_ref': cycle_values.mean() - ref_mean,\n            'deviation_pct': ((cycle_values.mean() - ref_mean) / ref_mean * 100) if ref_mean != 0 else np.nan,\n            'variability_ratio': (cycle_values.std() / ref_std) if ref_std != 0 else np.nan,\n        }\n        comparisons.append(comparison)\n\n    result = pd.DataFrame(comparisons)\n    logging.info(f\"Compared {len(result)} cycles against reference cycle '{reference_cycle_uuid}'.\")\n    return result\n</code></pre>"},{"location":"reference/ts_shape/features/cycles/cycle_processor/#ts_shape.features.cycles.cycle_processor.CycleDataProcessor.compare_cycles(reference_cycle_uuid)","title":"<code>reference_cycle_uuid</code>","text":"(<code>str</code>)           \u2013            <p>UUID of the reference cycle</p>"},{"location":"reference/ts_shape/features/cycles/cycle_processor/#ts_shape.features.cycles.cycle_processor.CycleDataProcessor.compare_cycles(metric)","title":"<code>metric</code>","text":"(<code>str</code>, default:                   <code>'value_double'</code> )           \u2013            <p>Column name to use for comparison</p>"},{"location":"reference/ts_shape/features/cycles/cycle_processor/#ts_shape.features.cycles.cycle_processor.CycleDataProcessor.compute_cycle_statistics","title":"compute_cycle_statistics","text":"<pre><code>compute_cycle_statistics() -&gt; DataFrame\n</code></pre> <p>Compute statistics for each cycle.</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>DataFrame with cycle-level statistics including duration, value counts, etc.</p> </li> </ul> Source code in <code>src/ts_shape/features/cycles/cycle_processor.py</code> <pre><code>def compute_cycle_statistics(self) -&gt; pd.DataFrame:\n    \"\"\"\n    Compute statistics for each cycle.\n\n    Returns:\n        DataFrame with cycle-level statistics including duration, value counts, etc.\n    \"\"\"\n    if self.cycles_df.empty:\n        return pd.DataFrame()\n\n    stats = []\n    for _, cycle in self.cycles_df.iterrows():\n        cycle_uuid = cycle[self.cycle_uuid_col]\n        cycle_start = cycle['cycle_start']\n        cycle_end = cycle['cycle_end']\n\n        # Get values for this cycle\n        mask = (self.values_df[self.systime_col] &gt;= cycle_start) &amp; (self.values_df[self.systime_col] &lt;= cycle_end)\n        cycle_values = self.values_df[mask]\n\n        # Compute stats\n        stat = {\n            self.cycle_uuid_col: cycle_uuid,\n            'cycle_start': cycle_start,\n            'cycle_end': cycle_end,\n            'duration_seconds': (cycle_end - cycle_start).total_seconds(),\n            'value_count': len(cycle_values),\n            'unique_uuids': cycle_values['uuid'].nunique() if 'uuid' in cycle_values.columns else 0,\n        }\n\n        # Add value-type specific stats if columns exist\n        if 'value_double' in cycle_values.columns:\n            stat['mean_value_double'] = cycle_values['value_double'].mean()\n            stat['std_value_double'] = cycle_values['value_double'].std()\n\n        stats.append(stat)\n\n    result = pd.DataFrame(stats)\n    logging.info(f\"Computed statistics for {len(result)} cycles.\")\n    return result\n</code></pre>"},{"location":"reference/ts_shape/features/cycles/cycle_processor/#ts_shape.features.cycles.cycle_processor.CycleDataProcessor.get_dataframe","title":"get_dataframe","text":"<pre><code>get_dataframe() -&gt; DataFrame\n</code></pre> <p>Returns the processed DataFrame.</p> Source code in <code>src/ts_shape/utils/base.py</code> <pre><code>def get_dataframe(self) -&gt; pd.DataFrame:\n    \"\"\"Returns the processed DataFrame.\"\"\"\n    return self.dataframe\n</code></pre>"},{"location":"reference/ts_shape/features/cycles/cycle_processor/#ts_shape.features.cycles.cycle_processor.CycleDataProcessor.group_by_cycle_uuid","title":"group_by_cycle_uuid","text":"<pre><code>group_by_cycle_uuid(data: Optional[DataFrame] = None) -&gt; List[DataFrame]\n</code></pre> <p>Group the DataFrame by the cycle_uuid column, resulting in a list of DataFrames, each containing data for one cycle.</p> <p>Parameters:</p> Return <p>List of DataFrames, each containing data for a unique cycle_uuid.</p> Source code in <code>src/ts_shape/features/cycles/cycle_processor.py</code> <pre><code>def group_by_cycle_uuid(self, data: Optional[pd.DataFrame] = None) -&gt; List[pd.DataFrame]:\n    \"\"\"\n    Group the DataFrame by the cycle_uuid column, resulting in a list of DataFrames, each containing data for one cycle.\n\n    Args:\n        data: DataFrame containing the data to be grouped by cycle_uuid. If None, uses the internal values_df.\n\n    Return:\n        List of DataFrames, each containing data for a unique cycle_uuid.\n    \"\"\"\n    if data is None:\n        data = self.values_df\n\n    if self.cycle_uuid_col not in data.columns:\n        logging.warning(f\"Column '{self.cycle_uuid_col}' not found in data. Cannot group.\")\n        return []\n\n    grouped_dataframes = [group for _, group in data.groupby(self.cycle_uuid_col)]\n    logging.info(f\"Grouped data into {len(grouped_dataframes)} cycle UUID groups.\")\n    return grouped_dataframes\n</code></pre>"},{"location":"reference/ts_shape/features/cycles/cycle_processor/#ts_shape.features.cycles.cycle_processor.CycleDataProcessor.group_by_cycle_uuid(data)","title":"<code>data</code>","text":"(<code>Optional[DataFrame]</code>, default:                   <code>None</code> )           \u2013            <p>DataFrame containing the data to be grouped by cycle_uuid. If None, uses the internal values_df.</p>"},{"location":"reference/ts_shape/features/cycles/cycle_processor/#ts_shape.features.cycles.cycle_processor.CycleDataProcessor.identify_golden_cycles","title":"identify_golden_cycles","text":"<pre><code>identify_golden_cycles(metric: str = 'value_double', method: str = 'low_variability', top_n: int = 5) -&gt; List[str]\n</code></pre> <p>Identify the best performing cycles (golden cycles).</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>List[str]</code>           \u2013            <p>List of cycle UUIDs identified as golden cycles</p> </li> </ul> Source code in <code>src/ts_shape/features/cycles/cycle_processor.py</code> <pre><code>def identify_golden_cycles(self, metric: str = 'value_double', method: str = 'low_variability', top_n: int = 5) -&gt; List[str]:\n    \"\"\"\n    Identify the best performing cycles (golden cycles).\n\n    Args:\n        metric: Column name to evaluate\n        method: Method for identification ('low_variability', 'high_mean', 'target_value')\n        top_n: Number of golden cycles to identify\n\n    Returns:\n        List of cycle UUIDs identified as golden cycles\n    \"\"\"\n    stats = self.compute_cycle_statistics()\n\n    if stats.empty:\n        logging.warning(\"No cycle statistics available.\")\n        return []\n\n    # Calculate metric-specific scores for each cycle\n    scores = []\n    for _, cycle in self.cycles_df.iterrows():\n        cycle_uuid = cycle[self.cycle_uuid_col]\n        mask = (self.values_df[self.systime_col] &gt;= cycle['cycle_start']) &amp; (self.values_df[self.systime_col] &lt;= cycle['cycle_end'])\n        cycle_values = self.values_df[mask][metric].dropna()\n\n        if cycle_values.empty:\n            continue\n\n        if method == 'low_variability':\n            # Lower coefficient of variation is better\n            mean_val = cycle_values.mean()\n            std_val = cycle_values.std()\n            score = -(std_val / mean_val) if mean_val != 0 else -np.inf\n        elif method == 'high_mean':\n            score = cycle_values.mean()\n        else:  # target_value - would need target parameter\n            score = -cycle_values.std()  # fallback to low variability\n\n        scores.append({'cycle_uuid': cycle_uuid, 'score': score})\n\n    if not scores:\n        logging.warning(\"Could not compute scores for any cycles.\")\n        return []\n\n    scores_df = pd.DataFrame(scores).sort_values('score', ascending=False)\n    golden_cycles = scores_df.head(top_n)['cycle_uuid'].tolist()\n\n    logging.info(f\"Identified {len(golden_cycles)} golden cycles using method '{method}'.\")\n    return golden_cycles\n</code></pre>"},{"location":"reference/ts_shape/features/cycles/cycle_processor/#ts_shape.features.cycles.cycle_processor.CycleDataProcessor.identify_golden_cycles(metric)","title":"<code>metric</code>","text":"(<code>str</code>, default:                   <code>'value_double'</code> )           \u2013            <p>Column name to evaluate</p>"},{"location":"reference/ts_shape/features/cycles/cycle_processor/#ts_shape.features.cycles.cycle_processor.CycleDataProcessor.identify_golden_cycles(method)","title":"<code>method</code>","text":"(<code>str</code>, default:                   <code>'low_variability'</code> )           \u2013            <p>Method for identification ('low_variability', 'high_mean', 'target_value')</p>"},{"location":"reference/ts_shape/features/cycles/cycle_processor/#ts_shape.features.cycles.cycle_processor.CycleDataProcessor.identify_golden_cycles(top_n)","title":"<code>top_n</code>","text":"(<code>int</code>, default:                   <code>5</code> )           \u2013            <p>Number of golden cycles to identify</p>"},{"location":"reference/ts_shape/features/cycles/cycle_processor/#ts_shape.features.cycles.cycle_processor.CycleDataProcessor.merge_dataframes_by_cycle","title":"merge_dataframes_by_cycle","text":"<pre><code>merge_dataframes_by_cycle() -&gt; DataFrame\n</code></pre> <p>Merges the values DataFrame with the cycles DataFrame based on the cycle time intervals. Uses optimized interval-based assignment instead of nested loops.</p> Return <p>DataFrame with an added 'cycle_uuid' column.</p> Source code in <code>src/ts_shape/features/cycles/cycle_processor.py</code> <pre><code>def merge_dataframes_by_cycle(self) -&gt; pd.DataFrame:\n    \"\"\"\n    Merges the values DataFrame with the cycles DataFrame based on the cycle time intervals.\n    Uses optimized interval-based assignment instead of nested loops.\n\n    Return:\n        DataFrame with an added 'cycle_uuid' column.\n    \"\"\"\n    if self._cycle_intervals is None or self.values_df.empty:\n        logging.warning(\"No cycles available for merging.\")\n        result = self.values_df.copy()\n        result[self.cycle_uuid_col] = None\n        return result\n\n    # Create a copy to avoid modifying the original\n    merged_df = self.values_df.copy()\n\n    # Use pd.cut with interval index for vectorized assignment\n    # This is much faster than iterating over cycles\n    try:\n        # Get time values as int64 (nanoseconds) for efficient indexing\n        time_values = merged_df[self.systime_col]\n\n        # Find which interval each timestamp belongs to\n        cycle_assignment = pd.Series(index=merged_df.index, dtype='object')\n\n        for interval, cycle_uuid in self._cycle_intervals.items():\n            mask = (time_values &gt;= interval.left) &amp; (time_values &lt;= interval.right)\n            cycle_assignment.loc[mask] = cycle_uuid\n\n        merged_df[self.cycle_uuid_col] = cycle_assignment\n\n    except Exception as e:\n        logging.error(f\"Error in vectorized cycle assignment: {e}. Falling back to iterative method.\")\n        # Fallback to original method if vectorization fails\n        merged_df[self.cycle_uuid_col] = None\n        for _, row in self.cycles_df.iterrows():\n            mask = (merged_df[self.systime_col] &gt;= row['cycle_start']) &amp; (merged_df[self.systime_col] &lt;= row['cycle_end'])\n            merged_df.loc[mask, self.cycle_uuid_col] = row[self.cycle_uuid_col]\n\n    # Drop rows not assigned to any cycle\n    result = merged_df.dropna(subset=[self.cycle_uuid_col])\n    logging.info(f\"Merged DataFrame contains {len(result)} records across {result[self.cycle_uuid_col].nunique()} cycles.\")\n    return result\n</code></pre>"},{"location":"reference/ts_shape/features/cycles/cycle_processor/#ts_shape.features.cycles.cycle_processor.CycleDataProcessor.split_by_cycle","title":"split_by_cycle","text":"<pre><code>split_by_cycle() -&gt; Dict[str, DataFrame]\n</code></pre> <p>Splits the values DataFrame by cycles defined in the cycles DataFrame. Uses optimized interval-based assignment.</p> Return <p>Dictionary where keys are cycle_uuids and values are DataFrames with the corresponding cycle data.</p> Source code in <code>src/ts_shape/features/cycles/cycle_processor.py</code> <pre><code>def split_by_cycle(self) -&gt; Dict[str, pd.DataFrame]:\n    \"\"\"\n    Splits the values DataFrame by cycles defined in the cycles DataFrame.\n    Uses optimized interval-based assignment.\n\n    Return:\n        Dictionary where keys are cycle_uuids and values are DataFrames with the corresponding cycle data.\n    \"\"\"\n    if self._cycle_intervals is None or self.values_df.empty:\n        logging.warning(\"No cycles or values available for splitting.\")\n        return {}\n\n    # Use merge_dataframes_by_cycle to assign cycle_uuids efficiently\n    merged = self.merge_dataframes_by_cycle()\n\n    # Split into dictionary\n    result = {\n        cycle_uuid: group.drop(columns=[self.cycle_uuid_col])\n        for cycle_uuid, group in merged.groupby(self.cycle_uuid_col)\n    }\n\n    logging.info(f\"Split {len(result)} cycles.\")\n    return result\n</code></pre>"},{"location":"reference/ts_shape/features/cycles/cycle_processor/#ts_shape.features.cycles.cycle_processor.CycleDataProcessor.split_dataframes_by_group","title":"split_dataframes_by_group","text":"<pre><code>split_dataframes_by_group(dfs: List[DataFrame], column: str) -&gt; List[DataFrame]\n</code></pre> <p>Splits a list of DataFrames by groups based on a specified column. This function performs a groupby operation on each DataFrame in the list and then flattens the result.</p> <p>Parameters:</p> Return <p>List of DataFrames, each corresponding to a group in the original DataFrames.</p> Source code in <code>src/ts_shape/features/cycles/cycle_processor.py</code> <pre><code>def split_dataframes_by_group(self, dfs: List[pd.DataFrame], column: str) -&gt; List[pd.DataFrame]:\n    \"\"\"\n    Splits a list of DataFrames by groups based on a specified column.\n    This function performs a groupby operation on each DataFrame in the list and then flattens the result.\n\n    Args:\n        dfs: List of DataFrames to be split.\n        column: Column name to group by.\n\n    Return:\n        List of DataFrames, each corresponding to a group in the original DataFrames.\n    \"\"\"\n    split_dfs = []\n    for df in dfs:\n        if column not in df.columns:\n            logging.warning(f\"Column '{column}' not found in DataFrame. Skipping.\")\n            continue\n        groups = df.groupby(column)\n        for _, group in groups:\n            split_dfs.append(group)\n\n    logging.info(f\"Split data into {len(split_dfs)} groups based on column '{column}'.\")\n    return split_dfs\n</code></pre>"},{"location":"reference/ts_shape/features/cycles/cycle_processor/#ts_shape.features.cycles.cycle_processor.CycleDataProcessor.split_dataframes_by_group(dfs)","title":"<code>dfs</code>","text":"(<code>List[DataFrame]</code>)           \u2013            <p>List of DataFrames to be split.</p>"},{"location":"reference/ts_shape/features/cycles/cycle_processor/#ts_shape.features.cycles.cycle_processor.CycleDataProcessor.split_dataframes_by_group(column)","title":"<code>column</code>","text":"(<code>str</code>)           \u2013            <p>Column name to group by.</p>"},{"location":"reference/ts_shape/features/cycles/cycles_extractor/","title":"cycles_extractor","text":""},{"location":"reference/ts_shape/features/cycles/cycles_extractor/#ts_shape.features.cycles.cycles_extractor","title":"ts_shape.features.cycles.cycles_extractor","text":"<p>Classes:</p> <ul> <li> <code>CycleExtractor</code>           \u2013            <p>Class for processing cycles based on different criteria.</p> </li> </ul>"},{"location":"reference/ts_shape/features/cycles/cycles_extractor/#ts_shape.features.cycles.cycles_extractor.CycleExtractor","title":"CycleExtractor","text":"<pre><code>CycleExtractor(dataframe: DataFrame, start_uuid: str, end_uuid: Optional[str] = None, value_change_threshold: float = 0.0)\n</code></pre> <p>               Bases: <code>Base</code></p> <p>Class for processing cycles based on different criteria.</p> <p>Parameters:</p> <p>Methods:</p> <ul> <li> <code>detect_overlapping_cycles</code>             \u2013              <p>Detect and optionally resolve overlapping cycles.</p> </li> <li> <code>get_dataframe</code>             \u2013              <p>Returns the processed DataFrame.</p> </li> <li> <code>get_extraction_stats</code>             \u2013              <p>Get statistics about the last cycle extraction.</p> </li> <li> <code>process_persistent_cycle</code>             \u2013              <p>Processes cycles where the value of the variable stays true during the cycle.</p> </li> <li> <code>process_separate_start_end_cycle</code>             \u2013              <p>Processes cycles where different variables indicate cycle start and end.</p> </li> <li> <code>process_state_change_cycle</code>             \u2013              <p>Processes cycles where the start of a new cycle is the end of the previous cycle.</p> </li> <li> <code>process_step_sequence</code>             \u2013              <p>Processes cycles based on a step sequence, where specific integer values denote cycle start and end.</p> </li> <li> <code>process_trigger_cycle</code>             \u2013              <p>Processes cycles where the value of the variable goes from true to false during the cycle.</p> </li> <li> <code>process_value_change_cycle</code>             \u2013              <p>Processes cycles where a change in the value indicates a new cycle.</p> </li> <li> <code>reset_stats</code>             \u2013              <p>Reset extraction statistics.</p> </li> <li> <code>suggest_method</code>             \u2013              <p>Suggest the best cycle extraction method based on data characteristics.</p> </li> <li> <code>validate_cycles</code>             \u2013              <p>Validate cycles based on duration constraints.</p> </li> </ul> Source code in <code>src/ts_shape/features/cycles/cycles_extractor.py</code> <pre><code>def __init__(self, dataframe: pd.DataFrame, start_uuid: str, end_uuid: Optional[str] = None,\n             value_change_threshold: float = 0.0):\n    \"\"\"Initializes the class with the data and the UUIDs for cycle start and end.\n\n    Args:\n        dataframe: Input DataFrame with cycle data\n        start_uuid: UUID for cycle start variable\n        end_uuid: UUID for cycle end variable (defaults to start_uuid)\n        value_change_threshold: Minimum threshold for considering a value change significant (default: 0.0)\n    \"\"\"\n    super().__init__(dataframe)\n\n    # Validate input types\n    if not isinstance(dataframe, pd.DataFrame):\n        raise ValueError(\"dataframe must be a pandas DataFrame\")\n    if not isinstance(start_uuid, str):\n        raise ValueError(\"start_uuid must be a string\")\n\n    self.df = dataframe  # Use the provided DataFrame directly\n    self.start_uuid = start_uuid\n    self.end_uuid = end_uuid if end_uuid else start_uuid\n    self.value_change_threshold = abs(value_change_threshold)\n\n    # Statistics tracking\n    self._stats: Dict[str, Any] = {\n        'total_cycles': 0,\n        'complete_cycles': 0,\n        'incomplete_cycles': 0,\n        'unmatched_starts': 0,\n        'unmatched_ends': 0,\n        'overlapping_cycles': 0,\n        'warnings': []\n    }\n\n    logging.info(f\"CycleExtractor initialized with start_uuid: {self.start_uuid} and end_uuid: {self.end_uuid}\")\n</code></pre>"},{"location":"reference/ts_shape/features/cycles/cycles_extractor/#ts_shape.features.cycles.cycles_extractor.CycleExtractor(dataframe)","title":"<code>dataframe</code>","text":"(<code>DataFrame</code>)           \u2013            <p>Input DataFrame with cycle data</p>"},{"location":"reference/ts_shape/features/cycles/cycles_extractor/#ts_shape.features.cycles.cycles_extractor.CycleExtractor(start_uuid)","title":"<code>start_uuid</code>","text":"(<code>str</code>)           \u2013            <p>UUID for cycle start variable</p>"},{"location":"reference/ts_shape/features/cycles/cycles_extractor/#ts_shape.features.cycles.cycles_extractor.CycleExtractor(end_uuid)","title":"<code>end_uuid</code>","text":"(<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>UUID for cycle end variable (defaults to start_uuid)</p>"},{"location":"reference/ts_shape/features/cycles/cycles_extractor/#ts_shape.features.cycles.cycles_extractor.CycleExtractor(value_change_threshold)","title":"<code>value_change_threshold</code>","text":"(<code>float</code>, default:                   <code>0.0</code> )           \u2013            <p>Minimum threshold for considering a value change significant (default: 0.0)</p>"},{"location":"reference/ts_shape/features/cycles/cycles_extractor/#ts_shape.features.cycles.cycles_extractor.CycleExtractor.detect_overlapping_cycles","title":"detect_overlapping_cycles","text":"<pre><code>detect_overlapping_cycles(cycle_df: DataFrame, resolve: str = 'flag') -&gt; DataFrame\n</code></pre> <p>Detect and optionally resolve overlapping cycles.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>DataFrame with 'has_overlap' column and potentially filtered rows</p> </li> </ul> Source code in <code>src/ts_shape/features/cycles/cycles_extractor.py</code> <pre><code>def detect_overlapping_cycles(self, cycle_df: pd.DataFrame, resolve: str = 'flag') -&gt; pd.DataFrame:\n    \"\"\"Detect and optionally resolve overlapping cycles.\n\n    Args:\n        cycle_df: DataFrame with cycle data\n        resolve: How to handle overlaps - 'flag' (mark only), 'keep_first', 'keep_last', 'keep_longest'\n\n    Returns:\n        DataFrame with 'has_overlap' column and potentially filtered rows\n    \"\"\"\n    if cycle_df.empty:\n        logging.warning(\"Empty cycle DataFrame provided for overlap detection.\")\n        return cycle_df\n\n    # Create a copy to avoid modifying the original\n    result_df = cycle_df.copy()\n\n    # Sort by start time\n    result_df = result_df.sort_values('cycle_start').reset_index(drop=True)\n\n    # Initialize overlap column\n    result_df['has_overlap'] = False\n\n    # Check for overlaps (only for complete cycles)\n    overlaps = []\n    for i in range(len(result_df) - 1):\n        if not result_df.loc[i, 'is_complete']:\n            continue\n\n        current_end = result_df.loc[i, 'cycle_end']\n\n        for j in range(i + 1, len(result_df)):\n            if not result_df.loc[j, 'is_complete']:\n                continue\n\n            next_start = result_df.loc[j, 'cycle_start']\n\n            if current_end &gt; next_start:\n                # Overlap detected\n                result_df.loc[i, 'has_overlap'] = True\n                result_df.loc[j, 'has_overlap'] = True\n                overlaps.append((i, j))\n            else:\n                break  # No more overlaps for current cycle\n\n    overlap_count = len(overlaps)\n    if overlap_count &gt; 0:\n        logging.warning(f\"Detected {overlap_count} overlapping cycle pairs.\")\n        self._stats['overlapping_cycles'] = overlap_count\n\n    # Resolve overlaps based on strategy\n    if resolve != 'flag' and overlap_count &gt; 0:\n        indices_to_drop = set()\n\n        for i, j in overlaps:\n            if resolve == 'keep_first':\n                indices_to_drop.add(j)\n            elif resolve == 'keep_last':\n                indices_to_drop.add(i)\n            elif resolve == 'keep_longest':\n                duration_i = result_df.loc[i, 'cycle_end'] - result_df.loc[i, 'cycle_start']\n                duration_j = result_df.loc[j, 'cycle_end'] - result_df.loc[j, 'cycle_start']\n                if duration_i &gt;= duration_j:\n                    indices_to_drop.add(j)\n                else:\n                    indices_to_drop.add(i)\n\n        if indices_to_drop:\n            result_df = result_df.drop(list(indices_to_drop)).reset_index(drop=True)\n            logging.info(f\"Resolved overlaps: removed {len(indices_to_drop)} cycles using '{resolve}' strategy.\")\n\n    return result_df\n</code></pre>"},{"location":"reference/ts_shape/features/cycles/cycles_extractor/#ts_shape.features.cycles.cycles_extractor.CycleExtractor.detect_overlapping_cycles(cycle_df)","title":"<code>cycle_df</code>","text":"(<code>DataFrame</code>)           \u2013            <p>DataFrame with cycle data</p>"},{"location":"reference/ts_shape/features/cycles/cycles_extractor/#ts_shape.features.cycles.cycles_extractor.CycleExtractor.detect_overlapping_cycles(resolve)","title":"<code>resolve</code>","text":"(<code>str</code>, default:                   <code>'flag'</code> )           \u2013            <p>How to handle overlaps - 'flag' (mark only), 'keep_first', 'keep_last', 'keep_longest'</p>"},{"location":"reference/ts_shape/features/cycles/cycles_extractor/#ts_shape.features.cycles.cycles_extractor.CycleExtractor.get_dataframe","title":"get_dataframe","text":"<pre><code>get_dataframe() -&gt; DataFrame\n</code></pre> <p>Returns the processed DataFrame.</p> Source code in <code>src/ts_shape/utils/base.py</code> <pre><code>def get_dataframe(self) -&gt; pd.DataFrame:\n    \"\"\"Returns the processed DataFrame.\"\"\"\n    return self.dataframe\n</code></pre>"},{"location":"reference/ts_shape/features/cycles/cycles_extractor/#ts_shape.features.cycles.cycles_extractor.CycleExtractor.get_extraction_stats","title":"get_extraction_stats","text":"<pre><code>get_extraction_stats() -&gt; Dict[str, Any]\n</code></pre> <p>Get statistics about the last cycle extraction.</p> <p>Returns:</p> <ul> <li> <code>Dict[str, Any]</code>           \u2013            <p>Dictionary with extraction statistics including counts, success rate, and warnings</p> </li> </ul> Source code in <code>src/ts_shape/features/cycles/cycles_extractor.py</code> <pre><code>def get_extraction_stats(self) -&gt; Dict[str, Any]:\n    \"\"\"Get statistics about the last cycle extraction.\n\n    Returns:\n        Dictionary with extraction statistics including counts, success rate, and warnings\n    \"\"\"\n    stats = self._stats.copy()\n\n    # Calculate success rate\n    if stats['total_cycles'] &gt; 0:\n        stats['success_rate'] = stats['complete_cycles'] / stats['total_cycles']\n    else:\n        stats['success_rate'] = 0.0\n\n    # Add configuration info\n    stats['configuration'] = {\n        'start_uuid': self.start_uuid,\n        'end_uuid': self.end_uuid,\n        'value_change_threshold': self.value_change_threshold\n    }\n\n    return stats\n</code></pre>"},{"location":"reference/ts_shape/features/cycles/cycles_extractor/#ts_shape.features.cycles.cycles_extractor.CycleExtractor.process_persistent_cycle","title":"process_persistent_cycle","text":"<pre><code>process_persistent_cycle() -&gt; DataFrame\n</code></pre> <p>Processes cycles where the value of the variable stays true during the cycle.</p> Source code in <code>src/ts_shape/features/cycles/cycles_extractor.py</code> <pre><code>def process_persistent_cycle(self) -&gt; pd.DataFrame:\n    \"\"\"Processes cycles where the value of the variable stays true during the cycle.\"\"\"\n    # Assuming dataframe is pre-filtered\n    cycle_starts = self.df[self.df['value_bool'] == True]\n    cycle_ends = self.df[self.df['value_bool'] == False]\n\n    return self._generate_cycle_dataframe(cycle_starts, cycle_ends)\n</code></pre>"},{"location":"reference/ts_shape/features/cycles/cycles_extractor/#ts_shape.features.cycles.cycles_extractor.CycleExtractor.process_separate_start_end_cycle","title":"process_separate_start_end_cycle","text":"<pre><code>process_separate_start_end_cycle() -&gt; DataFrame\n</code></pre> <p>Processes cycles where different variables indicate cycle start and end.</p> Source code in <code>src/ts_shape/features/cycles/cycles_extractor.py</code> <pre><code>def process_separate_start_end_cycle(self) -&gt; pd.DataFrame:\n    \"\"\"Processes cycles where different variables indicate cycle start and end.\"\"\"\n    # Assuming dataframe is pre-filtered for both start_uuid and end_uuid\n    cycle_starts = self.df[self.df['value_bool'] == True]\n    cycle_ends = self.df[self.df['value_bool'] == True]\n\n    return self._generate_cycle_dataframe(cycle_starts, cycle_ends)\n</code></pre>"},{"location":"reference/ts_shape/features/cycles/cycles_extractor/#ts_shape.features.cycles.cycles_extractor.CycleExtractor.process_state_change_cycle","title":"process_state_change_cycle","text":"<pre><code>process_state_change_cycle() -&gt; DataFrame\n</code></pre> <p>Processes cycles where the start of a new cycle is the end of the previous cycle.</p> Source code in <code>src/ts_shape/features/cycles/cycles_extractor.py</code> <pre><code>def process_state_change_cycle(self) -&gt; pd.DataFrame:\n    \"\"\"Processes cycles where the start of a new cycle is the end of the previous cycle.\"\"\"\n    # Assuming dataframe is pre-filtered\n    cycle_starts = self.df.copy()\n    cycle_ends = self.df.shift(-1)\n\n    return self._generate_cycle_dataframe(cycle_starts, cycle_ends)\n</code></pre>"},{"location":"reference/ts_shape/features/cycles/cycles_extractor/#ts_shape.features.cycles.cycles_extractor.CycleExtractor.process_step_sequence","title":"process_step_sequence","text":"<pre><code>process_step_sequence(start_step: int, end_step: int) -&gt; DataFrame\n</code></pre> <p>Processes cycles based on a step sequence, where specific integer values denote cycle start and end.</p> Source code in <code>src/ts_shape/features/cycles/cycles_extractor.py</code> <pre><code>def process_step_sequence(self, start_step: int, end_step: int) -&gt; pd.DataFrame:\n    \"\"\"Processes cycles based on a step sequence, where specific integer values denote cycle start and end.\"\"\"\n    # Assuming dataframe is pre-filtered\n    cycle_starts = self.df[self.df['value_integer'] == start_step]\n    cycle_ends = self.df[self.df['value_integer'] == end_step]\n\n    return self._generate_cycle_dataframe(cycle_starts, cycle_ends)\n</code></pre>"},{"location":"reference/ts_shape/features/cycles/cycles_extractor/#ts_shape.features.cycles.cycles_extractor.CycleExtractor.process_trigger_cycle","title":"process_trigger_cycle","text":"<pre><code>process_trigger_cycle() -&gt; DataFrame\n</code></pre> <p>Processes cycles where the value of the variable goes from true to false during the cycle.</p> Source code in <code>src/ts_shape/features/cycles/cycles_extractor.py</code> <pre><code>def process_trigger_cycle(self) -&gt; pd.DataFrame:\n    \"\"\"Processes cycles where the value of the variable goes from true to false during the cycle.\"\"\"\n    # Assuming dataframe is pre-filtered\n    cycle_starts = self.df[self.df['value_bool'] == True]\n    cycle_ends = self.df[self.df['value_bool'] == False].shift(-1)\n\n    return self._generate_cycle_dataframe(cycle_starts, cycle_ends)\n</code></pre>"},{"location":"reference/ts_shape/features/cycles/cycles_extractor/#ts_shape.features.cycles.cycles_extractor.CycleExtractor.process_value_change_cycle","title":"process_value_change_cycle","text":"<pre><code>process_value_change_cycle() -&gt; DataFrame\n</code></pre> <p>Processes cycles where a change in the value indicates a new cycle.</p> <p>Uses the value_change_threshold to determine if a numeric change is significant.</p> Source code in <code>src/ts_shape/features/cycles/cycles_extractor.py</code> <pre><code>def process_value_change_cycle(self) -&gt; pd.DataFrame:\n    \"\"\"Processes cycles where a change in the value indicates a new cycle.\n\n    Uses the value_change_threshold to determine if a numeric change is significant.\n    \"\"\"\n    # Assuming dataframe is pre-filtered\n\n    # Fill NaN or None values with appropriate defaults for diff() to work\n    self.df['value_double'] = self.df['value_double'].fillna(0)  # Assuming numeric column\n    self.df['value_bool'] = self.df['value_bool'].fillna(False)  # Assuming boolean column\n    self.df['value_string'] = self.df['value_string'].fillna('')  # Assuming string column\n    self.df['value_integer'] = self.df['value_integer'].fillna(0)  # Assuming integer column\n\n    # Detect changes across the relevant columns using diff() with threshold\n    self.df['value_change'] = (\n        (self.df['value_double'].diff().abs() &gt; self.value_change_threshold) |\n        (self.df['value_bool'].diff().ne(0)) |\n        (self.df['value_string'].shift().ne(self.df['value_string'])) |\n        (self.df['value_integer'].diff().abs() &gt; self.value_change_threshold)\n    )\n\n    # Define cycle starts and ends based on changes\n    cycle_starts = self.df[self.df['value_change'] == True]\n    cycle_ends = self.df[self.df['value_change'] == True].shift(-1)\n\n    return self._generate_cycle_dataframe(cycle_starts, cycle_ends)\n</code></pre>"},{"location":"reference/ts_shape/features/cycles/cycles_extractor/#ts_shape.features.cycles.cycles_extractor.CycleExtractor.reset_stats","title":"reset_stats","text":"<pre><code>reset_stats()\n</code></pre> <p>Reset extraction statistics.</p> Source code in <code>src/ts_shape/features/cycles/cycles_extractor.py</code> <pre><code>def reset_stats(self):\n    \"\"\"Reset extraction statistics.\"\"\"\n    self._stats = {\n        'total_cycles': 0,\n        'complete_cycles': 0,\n        'incomplete_cycles': 0,\n        'unmatched_starts': 0,\n        'unmatched_ends': 0,\n        'overlapping_cycles': 0,\n        'warnings': []\n    }\n    logging.info(\"Statistics reset.\")\n</code></pre>"},{"location":"reference/ts_shape/features/cycles/cycles_extractor/#ts_shape.features.cycles.cycles_extractor.CycleExtractor.suggest_method","title":"suggest_method","text":"<pre><code>suggest_method() -&gt; Dict[str, Any]\n</code></pre> <p>Suggest the best cycle extraction method based on data characteristics.</p> <p>Analyzes the input DataFrame to recommend appropriate extraction method(s).</p> <p>Returns:</p> <ul> <li> <code>Dict[str, Any]</code>           \u2013            <p>Dictionary with method suggestions and reasoning</p> </li> </ul> Source code in <code>src/ts_shape/features/cycles/cycles_extractor.py</code> <pre><code>def suggest_method(self) -&gt; Dict[str, Any]:\n    \"\"\"Suggest the best cycle extraction method based on data characteristics.\n\n    Analyzes the input DataFrame to recommend appropriate extraction method(s).\n\n    Returns:\n        Dictionary with method suggestions and reasoning\n    \"\"\"\n    suggestions = {\n        'recommended_methods': [],\n        'reasoning': [],\n        'data_characteristics': {}\n    }\n\n    # Analyze data characteristics\n    has_bool = 'value_bool' in self.df.columns and self.df['value_bool'].notna().any()\n    has_integer = 'value_integer' in self.df.columns and self.df['value_integer'].notna().any()\n    has_double = 'value_double' in self.df.columns and self.df['value_double'].notna().any()\n    has_string = 'value_string' in self.df.columns and self.df['value_string'].notna().any()\n\n    suggestions['data_characteristics'] = {\n        'has_boolean_values': has_bool,\n        'has_integer_values': has_integer,\n        'has_double_values': has_double,\n        'has_string_values': has_string,\n        'row_count': len(self.df),\n        'separate_start_end': self.start_uuid != self.end_uuid\n    }\n\n    # Suggest methods based on characteristics\n    if suggestions['data_characteristics']['separate_start_end']:\n        suggestions['recommended_methods'].append('process_separate_start_end_cycle')\n        suggestions['reasoning'].append('Separate start and end UUIDs detected')\n\n    if has_bool:\n        # Analyze boolean patterns\n        bool_data = self.df['value_bool'].dropna()\n        if len(bool_data) &gt; 1:\n            # Check for transitions\n            transitions = bool_data.diff().abs().sum()\n            if transitions &gt; 0:\n                suggestions['recommended_methods'].append('process_persistent_cycle')\n                suggestions['reasoning'].append('Boolean transitions detected, suitable for persistent cycles')\n\n                suggestions['recommended_methods'].append('process_trigger_cycle')\n                suggestions['reasoning'].append('Boolean data present, can use trigger-based extraction')\n\n    if has_integer:\n        # Check if integers represent steps\n        int_data = self.df['value_integer'].dropna()\n        unique_values = int_data.nunique()\n        if 2 &lt;= unique_values &lt;= 20:  # Reasonable step count\n            suggestions['recommended_methods'].append('process_step_sequence')\n            suggestions['reasoning'].append(f'Integer data with {unique_values} unique values suggests step sequence')\n\n    if has_double or has_integer or has_string:\n        # Value change method is versatile\n        suggestions['recommended_methods'].append('process_value_change_cycle')\n        suggestions['reasoning'].append('Data shows changing values, suitable for value change detection')\n\n    # Check for state-like patterns\n    if has_integer or has_string:\n        suggestions['recommended_methods'].append('process_state_change_cycle')\n        suggestions['reasoning'].append('State-based data detected, consider state change cycles')\n\n    # If no specific method suggested, recommend value_change as fallback\n    if not suggestions['recommended_methods']:\n        suggestions['recommended_methods'].append('process_value_change_cycle')\n        suggestions['reasoning'].append('Default method: works with any value changes')\n\n    logging.info(f\"Method suggestion: {suggestions['recommended_methods']}\")\n    return suggestions\n</code></pre>"},{"location":"reference/ts_shape/features/cycles/cycles_extractor/#ts_shape.features.cycles.cycles_extractor.CycleExtractor.validate_cycles","title":"validate_cycles","text":"<pre><code>validate_cycles(cycle_df: DataFrame, min_duration: str = '1s', max_duration: str = '1h', warn: bool = True) -&gt; DataFrame\n</code></pre> <p>Validate cycles based on duration constraints.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>DataFrame with additional 'is_valid' column and 'validation_issue' column</p> </li> </ul> Source code in <code>src/ts_shape/features/cycles/cycles_extractor.py</code> <pre><code>def validate_cycles(self, cycle_df: pd.DataFrame, min_duration: str = '1s',\n                   max_duration: str = '1h', warn: bool = True) -&gt; pd.DataFrame:\n    \"\"\"Validate cycles based on duration constraints.\n\n    Args:\n        cycle_df: DataFrame with cycle data (output from process_* methods)\n        min_duration: Minimum acceptable cycle duration (default: '1s')\n        max_duration: Maximum acceptable cycle duration (default: '1h')\n        warn: Whether to log warnings for invalid cycles (default: True)\n\n    Returns:\n        DataFrame with additional 'is_valid' column and 'validation_issue' column\n    \"\"\"\n    if cycle_df.empty:\n        logging.warning(\"Empty cycle DataFrame provided for validation.\")\n        return cycle_df\n\n    # Parse duration constraints\n    min_td = self._parse_duration(min_duration)\n    max_td = self._parse_duration(max_duration)\n\n    # Create a copy to avoid modifying the original\n    validated_df = cycle_df.copy()\n\n    # Calculate cycle durations\n    validated_df['cycle_duration'] = validated_df['cycle_end'] - validated_df['cycle_start']\n\n    # Initialize validation columns\n    validated_df['is_valid'] = True\n    validated_df['validation_issue'] = ''\n\n    # Check for incomplete cycles\n    incomplete_mask = ~validated_df['is_complete']\n    if incomplete_mask.any():\n        validated_df.loc[incomplete_mask, 'is_valid'] = False\n        validated_df.loc[incomplete_mask, 'validation_issue'] = 'incomplete_cycle'\n        if warn:\n            logging.warning(f\"Found {incomplete_mask.sum()} incomplete cycles.\")\n\n    # Check duration constraints (only for complete cycles)\n    complete_mask = validated_df['is_complete']\n    too_short_mask = complete_mask &amp; (validated_df['cycle_duration'] &lt; min_td)\n    too_long_mask = complete_mask &amp; (validated_df['cycle_duration'] &gt; max_td)\n\n    if too_short_mask.any():\n        validated_df.loc[too_short_mask, 'is_valid'] = False\n        validated_df.loc[too_short_mask, 'validation_issue'] = validated_df.loc[too_short_mask, 'validation_issue'] + 'too_short;'\n        if warn:\n            logging.warning(f\"Found {too_short_mask.sum()} cycles shorter than {min_duration}.\")\n\n    if too_long_mask.any():\n        validated_df.loc[too_long_mask, 'is_valid'] = False\n        validated_df.loc[too_long_mask, 'validation_issue'] = validated_df.loc[too_long_mask, 'validation_issue'] + 'too_long;'\n        if warn:\n            logging.warning(f\"Found {too_long_mask.sum()} cycles longer than {max_duration}.\")\n\n    valid_count = validated_df['is_valid'].sum()\n    invalid_count = (~validated_df['is_valid']).sum()\n\n    logging.info(f\"Validation complete: {valid_count} valid cycles, {invalid_count} invalid cycles.\")\n\n    return validated_df\n</code></pre>"},{"location":"reference/ts_shape/features/cycles/cycles_extractor/#ts_shape.features.cycles.cycles_extractor.CycleExtractor.validate_cycles(cycle_df)","title":"<code>cycle_df</code>","text":"(<code>DataFrame</code>)           \u2013            <p>DataFrame with cycle data (output from process_* methods)</p>"},{"location":"reference/ts_shape/features/cycles/cycles_extractor/#ts_shape.features.cycles.cycles_extractor.CycleExtractor.validate_cycles(min_duration)","title":"<code>min_duration</code>","text":"(<code>str</code>, default:                   <code>'1s'</code> )           \u2013            <p>Minimum acceptable cycle duration (default: '1s')</p>"},{"location":"reference/ts_shape/features/cycles/cycles_extractor/#ts_shape.features.cycles.cycles_extractor.CycleExtractor.validate_cycles(max_duration)","title":"<code>max_duration</code>","text":"(<code>str</code>, default:                   <code>'1h'</code> )           \u2013            <p>Maximum acceptable cycle duration (default: '1h')</p>"},{"location":"reference/ts_shape/features/cycles/cycles_extractor/#ts_shape.features.cycles.cycles_extractor.CycleExtractor.validate_cycles(warn)","title":"<code>warn</code>","text":"(<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to log warnings for invalid cycles (default: True)</p>"},{"location":"reference/ts_shape/features/stats/__init__/","title":"init","text":""},{"location":"reference/ts_shape/features/stats/__init__/#ts_shape.features.stats","title":"ts_shape.features.stats","text":"<p>Statistics</p> <p>Descriptive statistics for numeric, string, boolean, and timestamp columns.</p> <ul> <li>NumericStatistics: Descriptive metrics for numeric columns.</li> <li>column_mean: Mean of a column.</li> <li>column_median: Median of a column.</li> <li>column_std: Standard deviation of a column.</li> <li>column_variance: Variance of a column.</li> <li>column_min: Minimum value.</li> <li>column_max: Maximum value.</li> <li>column_sum: Sum of values.</li> <li>column_kurtosis: Kurtosis of values.</li> <li>column_skewness: Skewness of values.</li> <li>column_quantile: Quantile of a column.</li> <li>column_iqr: Interquartile range.</li> <li>column_range: Range (max - min).</li> <li>column_mad: Mean absolute deviation.</li> <li>coefficient_of_variation: Standard deviation divided by mean (guarded).</li> <li>standard_error_mean: Standard error of the mean.</li> <li>describe: Pandas describe wrapper.</li> <li>summary_as_dict: Comprehensive numeric summary as dict.</li> <li> <p>summary_as_dataframe: Comprehensive numeric summary as DataFrame.</p> </li> <li> <p>StringStatistics: Metrics for string/categorical columns.</p> </li> <li>count_unique: Number of unique strings.</li> <li>most_frequent: Most frequent string.</li> <li>count_most_frequent: Count of the most frequent string.</li> <li>count_null: Number of nulls.</li> <li>average_string_length: Average length of non-null strings.</li> <li>longest_string: Longest string.</li> <li>shortest_string: Shortest string.</li> <li>string_length_summary: Summary of lengths.</li> <li>most_common_n_strings: Top-N most frequent strings.</li> <li>contains_substring_count: Count of strings containing a substring.</li> <li>starts_with_count: Count of strings starting with a prefix.</li> <li>ends_with_count: Count of strings ending with a suffix.</li> <li>uppercase_percentage: Percentage of uppercase strings.</li> <li>lowercase_percentage: Percentage of lowercase strings.</li> <li>contains_digit_count: Count of strings containing digits.</li> <li>summary_as_dict: Comprehensive string summary as dict.</li> <li> <p>summary_as_dataframe: Comprehensive string summary as DataFrame.</p> </li> <li> <p>BooleanStatistics: Metrics for boolean columns.</p> </li> <li>count_true: Count of True values.</li> <li>count_false: Count of False values.</li> <li>count_null: Count of nulls.</li> <li>count_not_null: Count of non-nulls.</li> <li>true_percentage: Percentage True.</li> <li>false_percentage: Percentage False.</li> <li>mode: Most common boolean value.</li> <li>is_balanced: Whether distribution is 50/50.</li> <li>summary_as_dict: Summary as dict.</li> <li> <p>summary_as_dataframe: Summary as DataFrame.</p> </li> <li> <p>TimestampStatistics: Metrics for timestamp columns.</p> </li> <li>count_null: Count of null timestamps.</li> <li>count_not_null: Count of non-null timestamps.</li> <li>earliest_timestamp: Earliest timestamp.</li> <li>latest_timestamp: Latest timestamp.</li> <li>timestamp_range: Time range (latest - earliest).</li> <li>most_frequent_timestamp: Most frequent timestamp.</li> <li>count_most_frequent_timestamp: Count of the modal timestamp.</li> <li>year_distribution: Distribution by year.</li> <li>month_distribution: Distribution by month.</li> <li>weekday_distribution: Distribution by weekday.</li> <li>hour_distribution: Distribution by hour.</li> <li>most_frequent_day: Most frequent weekday.</li> <li>most_frequent_hour: Most frequent hour.</li> <li>average_time_gap: Average gap between consecutive timestamps.</li> <li>median_timestamp: Median timestamp.</li> <li>standard_deviation_timestamps: Standard deviation of consecutive differences.</li> <li>timestamp_quartiles: 25th/50th/75th percentiles.</li> <li>days_with_most_activity: Top-N active days.</li> </ul> <p>Modules:</p> <ul> <li> <code>boolean_stats</code>           \u2013            </li> <li> <code>feature_table</code>           \u2013            </li> <li> <code>numeric_stats</code>           \u2013            </li> <li> <code>string_stats</code>           \u2013            </li> <li> <code>timestamp_stats</code>           \u2013            </li> </ul>"},{"location":"reference/ts_shape/features/stats/boolean_stats/","title":"boolean_stats","text":""},{"location":"reference/ts_shape/features/stats/boolean_stats/#ts_shape.features.stats.boolean_stats","title":"ts_shape.features.stats.boolean_stats","text":"<p>Classes:</p> <ul> <li> <code>BooleanStatistics</code>           \u2013            <p>Provides class methods to calculate statistics on a boolean column in a pandas DataFrame.</p> </li> </ul>"},{"location":"reference/ts_shape/features/stats/boolean_stats/#ts_shape.features.stats.boolean_stats.BooleanStatistics","title":"BooleanStatistics","text":"<pre><code>BooleanStatistics(dataframe: DataFrame, column_name: str = 'systime')\n</code></pre> <p>               Bases: <code>Base</code></p> <p>Provides class methods to calculate statistics on a boolean column in a pandas DataFrame.</p> <p>Parameters:</p> <p>Methods:</p> <ul> <li> <code>count_false</code>             \u2013              <p>Returns the count of False values in the boolean column.</p> </li> <li> <code>count_not_null</code>             \u2013              <p>Returns the count of non-null (True or False) values in the boolean column.</p> </li> <li> <code>count_null</code>             \u2013              <p>Returns the count of null (NaN) values in the boolean column.</p> </li> <li> <code>count_true</code>             \u2013              <p>Returns the count of True values in the boolean column.</p> </li> <li> <code>false_percentage</code>             \u2013              <p>Returns the percentage of False values in the boolean column.</p> </li> <li> <code>get_dataframe</code>             \u2013              <p>Returns the processed DataFrame.</p> </li> <li> <code>is_balanced</code>             \u2013              <p>Indicates if the distribution is balanced (50% True and False) in the specified boolean column.</p> </li> <li> <code>mode</code>             \u2013              <p>Returns the mode (most common value) of the specified boolean column.</p> </li> <li> <code>summary_as_dataframe</code>             \u2013              <p>Returns a summary of boolean statistics for the specified column as a DataFrame.</p> </li> <li> <code>summary_as_dict</code>             \u2013              <p>Returns a summary of boolean statistics for the specified column as a dictionary.</p> </li> <li> <code>true_percentage</code>             \u2013              <p>Returns the percentage of True values in the boolean column.</p> </li> </ul> Source code in <code>src/ts_shape/utils/base.py</code> <pre><code>def __init__(self, dataframe: pd.DataFrame, column_name: str = 'systime') -&gt; None:\n    \"\"\"\n    Initializes the Base with a DataFrame, detects time columns, converts them to datetime,\n    and sorts the DataFrame by the specified column (or the detected time column if applicable).\n\n    Args:\n        dataframe (pd.DataFrame): The DataFrame to be processed.\n        column_name (str): The column to sort by. Default is 'systime'. If the column is not found or is not a time column, the class will attempt to detect other time columns.\n    \"\"\"\n    self.dataframe = dataframe.copy()\n\n    # Attempt to convert the specified column_name to datetime if it exists\n    if column_name in self.dataframe.columns:\n        self.dataframe[column_name] = pd.to_datetime(self.dataframe[column_name], errors='coerce')\n    else:\n        # If the column_name is not in the DataFrame, fallback to automatic time detection\n        time_columns = [col for col in self.dataframe.columns if 'time' in col.lower() or 'date' in col.lower()]\n\n        # Convert all detected time columns to datetime, if any\n        for col in time_columns:\n            self.dataframe[col] = pd.to_datetime(self.dataframe[col], errors='coerce')\n\n        # If any time columns are detected, sort by the first one; otherwise, do nothing\n        if time_columns:\n            column_name = time_columns[0]\n\n    # Sort by the datetime column (either specified or detected)\n    if column_name in self.dataframe.columns:\n        self.dataframe = self.dataframe.sort_values(by=column_name)\n</code></pre>"},{"location":"reference/ts_shape/features/stats/boolean_stats/#ts_shape.features.stats.boolean_stats.BooleanStatistics(dataframe)","title":"<code>dataframe</code>","text":"(<code>DataFrame</code>)           \u2013            <p>The DataFrame to be processed.</p>"},{"location":"reference/ts_shape/features/stats/boolean_stats/#ts_shape.features.stats.boolean_stats.BooleanStatistics(column_name)","title":"<code>column_name</code>","text":"(<code>str</code>, default:                   <code>'systime'</code> )           \u2013            <p>The column to sort by. Default is 'systime'. If the column is not found or is not a time column, the class will attempt to detect other time columns.</p>"},{"location":"reference/ts_shape/features/stats/boolean_stats/#ts_shape.features.stats.boolean_stats.BooleanStatistics.count_false","title":"count_false  <code>classmethod</code>","text":"<pre><code>count_false(dataframe: DataFrame, column_name: str = 'value_bool') -&gt; int\n</code></pre> <p>Returns the count of False values in the boolean column.</p> Source code in <code>src/ts_shape/features/stats/boolean_stats.py</code> <pre><code>@classmethod\ndef count_false(cls, dataframe: pd.DataFrame, column_name: str = 'value_bool') -&gt; int:\n    \"\"\"Returns the count of False values in the boolean column.\"\"\"\n    return (dataframe[column_name] == False).sum()\n</code></pre>"},{"location":"reference/ts_shape/features/stats/boolean_stats/#ts_shape.features.stats.boolean_stats.BooleanStatistics.count_not_null","title":"count_not_null  <code>classmethod</code>","text":"<pre><code>count_not_null(dataframe: DataFrame, column_name: str = 'value_bool') -&gt; int\n</code></pre> <p>Returns the count of non-null (True or False) values in the boolean column.</p> Source code in <code>src/ts_shape/features/stats/boolean_stats.py</code> <pre><code>@classmethod\ndef count_not_null(cls, dataframe: pd.DataFrame, column_name: str = 'value_bool') -&gt; int:\n    \"\"\"Returns the count of non-null (True or False) values in the boolean column.\"\"\"\n    return dataframe[column_name].notna().sum()\n</code></pre>"},{"location":"reference/ts_shape/features/stats/boolean_stats/#ts_shape.features.stats.boolean_stats.BooleanStatistics.count_null","title":"count_null  <code>classmethod</code>","text":"<pre><code>count_null(dataframe: DataFrame, column_name: str = 'value_bool') -&gt; int\n</code></pre> <p>Returns the count of null (NaN) values in the boolean column.</p> Source code in <code>src/ts_shape/features/stats/boolean_stats.py</code> <pre><code>@classmethod\ndef count_null(cls, dataframe: pd.DataFrame, column_name: str = 'value_bool') -&gt; int:\n    \"\"\"Returns the count of null (NaN) values in the boolean column.\"\"\"\n    return dataframe[column_name].isna().sum()\n</code></pre>"},{"location":"reference/ts_shape/features/stats/boolean_stats/#ts_shape.features.stats.boolean_stats.BooleanStatistics.count_true","title":"count_true  <code>classmethod</code>","text":"<pre><code>count_true(dataframe: DataFrame, column_name: str = 'value_bool') -&gt; int\n</code></pre> <p>Returns the count of True values in the boolean column.</p> Source code in <code>src/ts_shape/features/stats/boolean_stats.py</code> <pre><code>@classmethod\ndef count_true(cls, dataframe: pd.DataFrame, column_name: str = 'value_bool') -&gt; int:\n    \"\"\"Returns the count of True values in the boolean column.\"\"\"\n    return dataframe[column_name].sum()\n</code></pre>"},{"location":"reference/ts_shape/features/stats/boolean_stats/#ts_shape.features.stats.boolean_stats.BooleanStatistics.false_percentage","title":"false_percentage  <code>classmethod</code>","text":"<pre><code>false_percentage(dataframe: DataFrame, column_name: str = 'value_bool') -&gt; float\n</code></pre> <p>Returns the percentage of False values in the boolean column.</p> Source code in <code>src/ts_shape/features/stats/boolean_stats.py</code> <pre><code>@classmethod\ndef false_percentage(cls, dataframe: pd.DataFrame, column_name: str = 'value_bool') -&gt; float:\n    \"\"\"Returns the percentage of False values in the boolean column.\"\"\"\n    false_count = cls.count_false(dataframe, column_name)\n    total_count = cls.count_not_null(dataframe, column_name)\n    return (false_count / total_count) * 100 if total_count &gt; 0 else 0.0\n</code></pre>"},{"location":"reference/ts_shape/features/stats/boolean_stats/#ts_shape.features.stats.boolean_stats.BooleanStatistics.get_dataframe","title":"get_dataframe","text":"<pre><code>get_dataframe() -&gt; DataFrame\n</code></pre> <p>Returns the processed DataFrame.</p> Source code in <code>src/ts_shape/utils/base.py</code> <pre><code>def get_dataframe(self) -&gt; pd.DataFrame:\n    \"\"\"Returns the processed DataFrame.\"\"\"\n    return self.dataframe\n</code></pre>"},{"location":"reference/ts_shape/features/stats/boolean_stats/#ts_shape.features.stats.boolean_stats.BooleanStatistics.is_balanced","title":"is_balanced  <code>classmethod</code>","text":"<pre><code>is_balanced(dataframe: DataFrame, column_name: str) -&gt; bool\n</code></pre> <p>Indicates if the distribution is balanced (50% True and False) in the specified boolean column.</p> Source code in <code>src/ts_shape/features/stats/boolean_stats.py</code> <pre><code>@classmethod\ndef is_balanced(cls, dataframe: pd.DataFrame, column_name: str) -&gt; bool:\n    \"\"\"Indicates if the distribution is balanced (50% True and False) in the specified boolean column.\"\"\"\n    true_percentage = dataframe[column_name].mean()\n    return true_percentage == 0.5\n</code></pre>"},{"location":"reference/ts_shape/features/stats/boolean_stats/#ts_shape.features.stats.boolean_stats.BooleanStatistics.mode","title":"mode  <code>classmethod</code>","text":"<pre><code>mode(dataframe: DataFrame, column_name: str) -&gt; bool\n</code></pre> <p>Returns the mode (most common value) of the specified boolean column.</p> Source code in <code>src/ts_shape/features/stats/boolean_stats.py</code> <pre><code>@classmethod\ndef mode(cls, dataframe: pd.DataFrame, column_name: str) -&gt; bool:\n    \"\"\"Returns the mode (most common value) of the specified boolean column.\"\"\"\n    return dataframe[column_name].mode()[0]\n</code></pre>"},{"location":"reference/ts_shape/features/stats/boolean_stats/#ts_shape.features.stats.boolean_stats.BooleanStatistics.summary_as_dataframe","title":"summary_as_dataframe  <code>classmethod</code>","text":"<pre><code>summary_as_dataframe(dataframe: DataFrame, column_name: str) -&gt; DataFrame\n</code></pre> <p>Returns a summary of boolean statistics for the specified column as a DataFrame.</p> Source code in <code>src/ts_shape/features/stats/boolean_stats.py</code> <pre><code>@classmethod\ndef summary_as_dataframe(cls, dataframe: pd.DataFrame, column_name: str) -&gt; pd.DataFrame:\n    \"\"\"Returns a summary of boolean statistics for the specified column as a DataFrame.\"\"\"\n    summary_data = cls.summary_as_dict(dataframe, column_name)\n    return pd.DataFrame([summary_data])\n</code></pre>"},{"location":"reference/ts_shape/features/stats/boolean_stats/#ts_shape.features.stats.boolean_stats.BooleanStatistics.summary_as_dict","title":"summary_as_dict  <code>classmethod</code>","text":"<pre><code>summary_as_dict(dataframe: DataFrame, column_name: str) -&gt; Dict[str, Union[int, float, bool]]\n</code></pre> <p>Returns a summary of boolean statistics for the specified column as a dictionary.</p> Source code in <code>src/ts_shape/features/stats/boolean_stats.py</code> <pre><code>@classmethod\ndef summary_as_dict(cls, dataframe: pd.DataFrame, column_name: str) -&gt; Dict[str, Union[int, float, bool]]:\n    \"\"\"Returns a summary of boolean statistics for the specified column as a dictionary.\"\"\"\n    return {\n        'true_count': cls.count_true(dataframe, column_name),\n        'false_count': cls.count_false(dataframe, column_name),\n        'true_percentage': cls.true_percentage(dataframe, column_name),\n        'false_percentage': cls.false_percentage(dataframe, column_name),\n        'mode': cls.mode(dataframe, column_name),\n        'is_balanced': cls.is_balanced(dataframe, column_name)\n    }\n</code></pre>"},{"location":"reference/ts_shape/features/stats/boolean_stats/#ts_shape.features.stats.boolean_stats.BooleanStatistics.true_percentage","title":"true_percentage  <code>classmethod</code>","text":"<pre><code>true_percentage(dataframe: DataFrame, column_name: str = 'value_bool') -&gt; float\n</code></pre> <p>Returns the percentage of True values in the boolean column.</p> Source code in <code>src/ts_shape/features/stats/boolean_stats.py</code> <pre><code>@classmethod\ndef true_percentage(cls, dataframe: pd.DataFrame, column_name: str = 'value_bool') -&gt; float:\n    \"\"\"Returns the percentage of True values in the boolean column.\"\"\"\n    true_count = cls.count_true(dataframe, column_name)\n    total_count = cls.count_not_null(dataframe, column_name)\n    return (true_count / total_count) * 100 if total_count &gt; 0 else 0.0\n</code></pre>"},{"location":"reference/ts_shape/features/stats/feature_table/","title":"feature_table","text":""},{"location":"reference/ts_shape/features/stats/feature_table/#ts_shape.features.stats.feature_table","title":"ts_shape.features.stats.feature_table","text":"<p>Classes:</p> <ul> <li> <code>DescriptiveFeatures</code>           \u2013            <p>A class used to compute descriptive statistics for a DataFrame, grouped by UUID.</p> </li> </ul>"},{"location":"reference/ts_shape/features/stats/feature_table/#ts_shape.features.stats.feature_table.DescriptiveFeatures","title":"DescriptiveFeatures","text":"<pre><code>DescriptiveFeatures(dataframe: DataFrame)\n</code></pre> <p>A class used to compute descriptive statistics for a DataFrame, grouped by UUID.</p>"},{"location":"reference/ts_shape/features/stats/feature_table/#ts_shape.features.stats.feature_table.DescriptiveFeatures--attributes","title":"Attributes","text":"<p>data : pandas.DataFrame     DataFrame containing the data</p>"},{"location":"reference/ts_shape/features/stats/feature_table/#ts_shape.features.stats.feature_table.DescriptiveFeatures--methods","title":"Methods","text":"<p>compute():     Compute and return descriptive statistics for each UUID in the DataFrame.</p> <p>dataframe : pandas.DataFrame     DataFrame containing the data</p> <p>Methods:</p> <ul> <li> <code>compute</code>             \u2013              <p>Compute and return descriptive statistics for each UUID in the DataFrame.</p> </li> <li> <code>compute_per_group</code>             \u2013              <p>Compute and return statistics for each column in the DataFrame group.</p> </li> <li> <code>overall_stats</code>             \u2013              <p>Compute and return overall statistics for the DataFrame group.</p> </li> </ul> Source code in <code>src/ts_shape/features/stats/feature_table.py</code> <pre><code>def __init__(self, dataframe: pd.DataFrame):\n    \"\"\"\n    Parameters\n    ----------\n    dataframe : pandas.DataFrame\n        DataFrame containing the data\n    \"\"\"\n    self.data = dataframe\n</code></pre>"},{"location":"reference/ts_shape/features/stats/feature_table/#ts_shape.features.stats.feature_table.DescriptiveFeatures.compute","title":"compute","text":"<pre><code>compute(output_format: str = 'dict') -&gt; Union[DataFrame, Dict[str, Dict[str, Dict[str, Union[int, float, str, bool]]]]]\n</code></pre> <p>Compute and return descriptive statistics for each UUID in the DataFrame.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>Union[DataFrame, Dict[str, Dict[str, Dict[str, Union[int, float, str, bool]]]]]</code>           \u2013            <p>Union[DataFrame, dict]: A DataFrame or a nested dictionary with the UUID as the key and specific statistics related to that UUID's data type.</p> </li> </ul> Source code in <code>src/ts_shape/features/stats/feature_table.py</code> <pre><code>def compute(self, output_format: str = 'dict') -&gt; Union[pd.DataFrame, Dict[str, Dict[str, Dict[str, Union[int, float, str, bool]]]]]:\n    \"\"\"Compute and return descriptive statistics for each UUID in the DataFrame.\n\n    Args:\n        output_format (str, optional): The desired output format ('dict' or 'dataframe'). Defaults to 'dict'.\n\n    Returns:\n        Union[DataFrame, dict]: A DataFrame or a nested dictionary with the UUID as the key and specific statistics related to that UUID's data type.\n    \"\"\"\n    if output_format == 'dataframe':\n        rows_list = []\n\n        # Iterate through each group of UUID\n        for uuid, group in self.data.groupby('uuid'):\n            stats_per_group = self.compute_per_group(group)\n\n            # Iterate through the nested stats and create flat columns\n            row_dict = {}\n            for section, stats in stats_per_group.items():\n                if isinstance(stats, dict):\n                    for key, value in stats.items():\n                        if isinstance(value, dict):\n                            for sub_key, sub_value in value.items():\n                                column_name = f'{uuid}::{section}::{key}::{sub_key}'\n                                row_dict[column_name] = sub_value\n                        else:\n                            column_name = f'{uuid}::{section}::{key}'\n                            row_dict[column_name] = value\n                else:\n                    column_name = f'{uuid}::{section}'\n                    row_dict[column_name] = stats\n\n            rows_list.append(row_dict)\n\n        return pd.DataFrame(rows_list)\n\n    elif output_format == 'dict':\n        return self.data.groupby('uuid').apply(self.compute_per_group).to_dict()\n\n    else:\n        raise ValueError(\"Invalid output format. Choose either 'dict' or 'dataframe'.\")\n</code></pre>"},{"location":"reference/ts_shape/features/stats/feature_table/#ts_shape.features.stats.feature_table.DescriptiveFeatures.compute(output_format)","title":"<code>output_format</code>","text":"(<code>str</code>, default:                   <code>'dict'</code> )           \u2013            <p>The desired output format ('dict' or 'dataframe'). Defaults to 'dict'.</p>"},{"location":"reference/ts_shape/features/stats/feature_table/#ts_shape.features.stats.feature_table.DescriptiveFeatures.compute_per_group","title":"compute_per_group","text":"<pre><code>compute_per_group(group: DataFrame) -&gt; Dict[str, Dict[str, Union[int, float, str, bool]]]\n</code></pre> <p>Compute and return statistics for each column in the DataFrame group.</p> <p>Returns:</p> <ul> <li> <code>dict</code> (              <code>Dict[str, Dict[str, Union[int, float, str, bool]]]</code> )          \u2013            <p>A dictionary with overall statistics, and string, numeric, and boolean statistics per column.</p> </li> </ul> Source code in <code>src/ts_shape/features/stats/feature_table.py</code> <pre><code>def compute_per_group(self, group: pd.DataFrame) -&gt; Dict[str, Dict[str, Union[int, float, str, bool]]]:\n    \"\"\"Compute and return statistics for each column in the DataFrame group.\n\n    Returns:\n        dict: A dictionary with overall statistics, and string, numeric, and boolean statistics per column.\n    \"\"\"\n    results = {\n        'overall': self.overall_stats(group)\n    }\n    for col in group.columns:\n        if col == 'uuid':\n            continue\n        elif is_bool_dtype(group[col]):\n            # Use BooleanStatistics for boolean columns\n            results[col] = {'boolean_stats': BooleanStatistics.summary_as_dict(group, col)}\n        elif is_numeric_dtype(group[col]):\n            # Use NumericStatistics for numeric columns\n            results[col] = {'numeric_stats': NumericStatistics.summary_as_dict(group, col)}\n        elif is_object_dtype(group[col]):\n            # Use StringStatistics for string columns\n            results[col] = {'string_stats': StringStatistics.summary_as_dict(group, col)}\n\n    return results\n</code></pre>"},{"location":"reference/ts_shape/features/stats/feature_table/#ts_shape.features.stats.feature_table.DescriptiveFeatures.overall_stats","title":"overall_stats","text":"<pre><code>overall_stats(group: DataFrame) -&gt; Dict[str, Union[int, float]]\n</code></pre> <p>Compute and return overall statistics for the DataFrame group.</p> <ul> <li>total_rows: Total number of rows in the group.</li> <li>total_time: Total time difference from max and min of 'systime' column.</li> <li>is_delta_sum: Sum of the 'is_delta' column.</li> <li>is_delta_avg: Mean of the 'is_delta' column.</li> <li>is_delta_std: Standard deviation of the 'is_delta' column.</li> </ul> <p>Returns:</p> <ul> <li> <code>dict</code> (              <code>Dict[str, Union[int, float]]</code> )          \u2013            <p>A dictionary with overall statistics.</p> </li> </ul> Source code in <code>src/ts_shape/features/stats/feature_table.py</code> <pre><code>def overall_stats(self, group: pd.DataFrame) -&gt; Dict[str, Union[int, float]]:\n    \"\"\"Compute and return overall statistics for the DataFrame group.\n\n    - **total_rows**: Total number of rows in the group.\n    - **total_time**: Total time difference from max and min of 'systime' column.\n    - **is_delta_sum**: Sum of the 'is_delta' column.\n    - **is_delta_avg**: Mean of the 'is_delta' column.\n    - **is_delta_std**: Standard deviation of the 'is_delta' column.\n\n    Returns:\n        dict: A dictionary with overall statistics.\n    \"\"\"\n    statistics = {\n        'total_rows': len(group),\n        'total_time': group['systime'].max() - group['systime'].min(),\n        'is_delta_sum': group['is_delta'].sum(),\n        'is_delta_avg': group['is_delta'].mean(),\n        'is_delta_std': group['is_delta'].std(),\n    }\n    return statistics\n</code></pre>"},{"location":"reference/ts_shape/features/stats/numeric_stats/","title":"numeric_stats","text":""},{"location":"reference/ts_shape/features/stats/numeric_stats/#ts_shape.features.stats.numeric_stats","title":"ts_shape.features.stats.numeric_stats","text":"<p>Classes:</p> <ul> <li> <code>NumericStatistics</code>           \u2013            <p>Provides class methods to calculate statistics on numeric columns in a pandas DataFrame.</p> </li> </ul>"},{"location":"reference/ts_shape/features/stats/numeric_stats/#ts_shape.features.stats.numeric_stats.NumericStatistics","title":"NumericStatistics","text":"<pre><code>NumericStatistics(dataframe: DataFrame, column_name: str = 'systime')\n</code></pre> <p>               Bases: <code>Base</code></p> <p>Provides class methods to calculate statistics on numeric columns in a pandas DataFrame.</p> <p>Parameters:</p> <p>Methods:</p> <ul> <li> <code>coefficient_of_variation</code>             \u2013              <p>Calculate the coefficient of variation of the column.</p> </li> <li> <code>column_iqr</code>             \u2013              <p>Calculate the interquartile range of the column.</p> </li> <li> <code>column_kurtosis</code>             \u2013              <p>Calculate the kurtosis of a specified column.</p> </li> <li> <code>column_mad</code>             \u2013              <p>Calculate the mean absolute deviation of the column.</p> </li> <li> <code>column_max</code>             \u2013              <p>Calculate the maximum value of a specified column.</p> </li> <li> <code>column_mean</code>             \u2013              <p>Calculate the mean of a specified column.</p> </li> <li> <code>column_median</code>             \u2013              <p>Calculate the median of a specified column.</p> </li> <li> <code>column_min</code>             \u2013              <p>Calculate the minimum value of a specified column.</p> </li> <li> <code>column_quantile</code>             \u2013              <p>Calculate a specific quantile of the column.</p> </li> <li> <code>column_range</code>             \u2013              <p>Calculate the range of the column.</p> </li> <li> <code>column_skewness</code>             \u2013              <p>Calculate the skewness of a specified column.</p> </li> <li> <code>column_std</code>             \u2013              <p>Calculate the standard deviation of a specified column.</p> </li> <li> <code>column_sum</code>             \u2013              <p>Calculate the sum of a specified column.</p> </li> <li> <code>column_variance</code>             \u2013              <p>Calculate the variance of a specified column.</p> </li> <li> <code>describe</code>             \u2013              <p>Provide a statistical summary for numeric columns in the DataFrame.</p> </li> <li> <code>get_dataframe</code>             \u2013              <p>Returns the processed DataFrame.</p> </li> <li> <code>standard_error_mean</code>             \u2013              <p>Calculate the standard error of the mean for the column.</p> </li> <li> <code>summary_as_dataframe</code>             \u2013              <p>Returns a DataFrame with comprehensive numeric statistics for the specified column.</p> </li> <li> <code>summary_as_dict</code>             \u2013              <p>Returns a dictionary with comprehensive numeric statistics for the specified column.</p> </li> </ul> Source code in <code>src/ts_shape/utils/base.py</code> <pre><code>def __init__(self, dataframe: pd.DataFrame, column_name: str = 'systime') -&gt; None:\n    \"\"\"\n    Initializes the Base with a DataFrame, detects time columns, converts them to datetime,\n    and sorts the DataFrame by the specified column (or the detected time column if applicable).\n\n    Args:\n        dataframe (pd.DataFrame): The DataFrame to be processed.\n        column_name (str): The column to sort by. Default is 'systime'. If the column is not found or is not a time column, the class will attempt to detect other time columns.\n    \"\"\"\n    self.dataframe = dataframe.copy()\n\n    # Attempt to convert the specified column_name to datetime if it exists\n    if column_name in self.dataframe.columns:\n        self.dataframe[column_name] = pd.to_datetime(self.dataframe[column_name], errors='coerce')\n    else:\n        # If the column_name is not in the DataFrame, fallback to automatic time detection\n        time_columns = [col for col in self.dataframe.columns if 'time' in col.lower() or 'date' in col.lower()]\n\n        # Convert all detected time columns to datetime, if any\n        for col in time_columns:\n            self.dataframe[col] = pd.to_datetime(self.dataframe[col], errors='coerce')\n\n        # If any time columns are detected, sort by the first one; otherwise, do nothing\n        if time_columns:\n            column_name = time_columns[0]\n\n    # Sort by the datetime column (either specified or detected)\n    if column_name in self.dataframe.columns:\n        self.dataframe = self.dataframe.sort_values(by=column_name)\n</code></pre>"},{"location":"reference/ts_shape/features/stats/numeric_stats/#ts_shape.features.stats.numeric_stats.NumericStatistics(dataframe)","title":"<code>dataframe</code>","text":"(<code>DataFrame</code>)           \u2013            <p>The DataFrame to be processed.</p>"},{"location":"reference/ts_shape/features/stats/numeric_stats/#ts_shape.features.stats.numeric_stats.NumericStatistics(column_name)","title":"<code>column_name</code>","text":"(<code>str</code>, default:                   <code>'systime'</code> )           \u2013            <p>The column to sort by. Default is 'systime'. If the column is not found or is not a time column, the class will attempt to detect other time columns.</p>"},{"location":"reference/ts_shape/features/stats/numeric_stats/#ts_shape.features.stats.numeric_stats.NumericStatistics.coefficient_of_variation","title":"coefficient_of_variation  <code>classmethod</code>","text":"<pre><code>coefficient_of_variation(dataframe: DataFrame, column_name: str) -&gt; float\n</code></pre> <p>Calculate the coefficient of variation of the column.</p> Source code in <code>src/ts_shape/features/stats/numeric_stats.py</code> <pre><code>@classmethod\ndef coefficient_of_variation(cls, dataframe: pd.DataFrame, column_name: str) -&gt; float:\n    \"\"\"Calculate the coefficient of variation of the column.\"\"\"\n    mean = cls.column_mean(dataframe, column_name)\n    return cls.column_std(dataframe, column_name) / mean if mean != 0 else None\n</code></pre>"},{"location":"reference/ts_shape/features/stats/numeric_stats/#ts_shape.features.stats.numeric_stats.NumericStatistics.column_iqr","title":"column_iqr  <code>classmethod</code>","text":"<pre><code>column_iqr(dataframe: DataFrame, column_name: str) -&gt; float\n</code></pre> <p>Calculate the interquartile range of the column.</p> Source code in <code>src/ts_shape/features/stats/numeric_stats.py</code> <pre><code>@classmethod\ndef column_iqr(cls, dataframe: pd.DataFrame, column_name: str) -&gt; float:\n    \"\"\"Calculate the interquartile range of the column.\"\"\"\n    return stats.iqr(dataframe[column_name])\n</code></pre>"},{"location":"reference/ts_shape/features/stats/numeric_stats/#ts_shape.features.stats.numeric_stats.NumericStatistics.column_kurtosis","title":"column_kurtosis  <code>classmethod</code>","text":"<pre><code>column_kurtosis(dataframe: DataFrame, column_name: str) -&gt; float\n</code></pre> <p>Calculate the kurtosis of a specified column.</p> Source code in <code>src/ts_shape/features/stats/numeric_stats.py</code> <pre><code>@classmethod\ndef column_kurtosis(cls, dataframe: pd.DataFrame, column_name: str) -&gt; float:\n    \"\"\"Calculate the kurtosis of a specified column.\"\"\"\n    return dataframe[column_name].kurt()\n</code></pre>"},{"location":"reference/ts_shape/features/stats/numeric_stats/#ts_shape.features.stats.numeric_stats.NumericStatistics.column_mad","title":"column_mad  <code>classmethod</code>","text":"<pre><code>column_mad(dataframe: DataFrame, column_name: str) -&gt; float\n</code></pre> <p>Calculate the mean absolute deviation of the column.</p> Source code in <code>src/ts_shape/features/stats/numeric_stats.py</code> <pre><code>@classmethod\ndef column_mad(cls, dataframe: pd.DataFrame, column_name: str) -&gt; float:\n    \"\"\"Calculate the mean absolute deviation of the column.\"\"\"\n    return dataframe[column_name].mad()\n</code></pre>"},{"location":"reference/ts_shape/features/stats/numeric_stats/#ts_shape.features.stats.numeric_stats.NumericStatistics.column_max","title":"column_max  <code>classmethod</code>","text":"<pre><code>column_max(dataframe: DataFrame, column_name: str) -&gt; float\n</code></pre> <p>Calculate the maximum value of a specified column.</p> Source code in <code>src/ts_shape/features/stats/numeric_stats.py</code> <pre><code>@classmethod\ndef column_max(cls, dataframe: pd.DataFrame, column_name: str) -&gt; float:\n    \"\"\"Calculate the maximum value of a specified column.\"\"\"\n    return dataframe[column_name].max()\n</code></pre>"},{"location":"reference/ts_shape/features/stats/numeric_stats/#ts_shape.features.stats.numeric_stats.NumericStatistics.column_mean","title":"column_mean  <code>classmethod</code>","text":"<pre><code>column_mean(dataframe: DataFrame, column_name: str) -&gt; float\n</code></pre> <p>Calculate the mean of a specified column.</p> Source code in <code>src/ts_shape/features/stats/numeric_stats.py</code> <pre><code>@classmethod\ndef column_mean(cls, dataframe: pd.DataFrame, column_name: str) -&gt; float:\n    \"\"\"Calculate the mean of a specified column.\"\"\"\n    return dataframe[column_name].mean()\n</code></pre>"},{"location":"reference/ts_shape/features/stats/numeric_stats/#ts_shape.features.stats.numeric_stats.NumericStatistics.column_median","title":"column_median  <code>classmethod</code>","text":"<pre><code>column_median(dataframe: DataFrame, column_name: str) -&gt; float\n</code></pre> <p>Calculate the median of a specified column.</p> Source code in <code>src/ts_shape/features/stats/numeric_stats.py</code> <pre><code>@classmethod\ndef column_median(cls, dataframe: pd.DataFrame, column_name: str) -&gt; float:\n    \"\"\"Calculate the median of a specified column.\"\"\"\n    return dataframe[column_name].median()\n</code></pre>"},{"location":"reference/ts_shape/features/stats/numeric_stats/#ts_shape.features.stats.numeric_stats.NumericStatistics.column_min","title":"column_min  <code>classmethod</code>","text":"<pre><code>column_min(dataframe: DataFrame, column_name: str) -&gt; float\n</code></pre> <p>Calculate the minimum value of a specified column.</p> Source code in <code>src/ts_shape/features/stats/numeric_stats.py</code> <pre><code>@classmethod\ndef column_min(cls, dataframe: pd.DataFrame, column_name: str) -&gt; float:\n    \"\"\"Calculate the minimum value of a specified column.\"\"\"\n    return dataframe[column_name].min()\n</code></pre>"},{"location":"reference/ts_shape/features/stats/numeric_stats/#ts_shape.features.stats.numeric_stats.NumericStatistics.column_quantile","title":"column_quantile  <code>classmethod</code>","text":"<pre><code>column_quantile(dataframe: DataFrame, column_name: str, quantile: float) -&gt; float\n</code></pre> <p>Calculate a specific quantile of the column.</p> Source code in <code>src/ts_shape/features/stats/numeric_stats.py</code> <pre><code>@classmethod\ndef column_quantile(cls, dataframe: pd.DataFrame, column_name: str, quantile: float) -&gt; float:\n    \"\"\"Calculate a specific quantile of the column.\"\"\"\n    return dataframe[column_name].quantile(quantile)\n</code></pre>"},{"location":"reference/ts_shape/features/stats/numeric_stats/#ts_shape.features.stats.numeric_stats.NumericStatistics.column_range","title":"column_range  <code>classmethod</code>","text":"<pre><code>column_range(dataframe: DataFrame, column_name: str) -&gt; float\n</code></pre> <p>Calculate the range of the column.</p> Source code in <code>src/ts_shape/features/stats/numeric_stats.py</code> <pre><code>@classmethod\ndef column_range(cls, dataframe: pd.DataFrame, column_name: str) -&gt; float:\n    \"\"\"Calculate the range of the column.\"\"\"\n    return cls.column_max(dataframe, column_name) - cls.column_min(dataframe, column_name)\n</code></pre>"},{"location":"reference/ts_shape/features/stats/numeric_stats/#ts_shape.features.stats.numeric_stats.NumericStatistics.column_skewness","title":"column_skewness  <code>classmethod</code>","text":"<pre><code>column_skewness(dataframe: DataFrame, column_name: str) -&gt; float\n</code></pre> <p>Calculate the skewness of a specified column.</p> Source code in <code>src/ts_shape/features/stats/numeric_stats.py</code> <pre><code>@classmethod\ndef column_skewness(cls, dataframe: pd.DataFrame, column_name: str) -&gt; float:\n    \"\"\"Calculate the skewness of a specified column.\"\"\"\n    return dataframe[column_name].skew()\n</code></pre>"},{"location":"reference/ts_shape/features/stats/numeric_stats/#ts_shape.features.stats.numeric_stats.NumericStatistics.column_std","title":"column_std  <code>classmethod</code>","text":"<pre><code>column_std(dataframe: DataFrame, column_name: str) -&gt; float\n</code></pre> <p>Calculate the standard deviation of a specified column.</p> Source code in <code>src/ts_shape/features/stats/numeric_stats.py</code> <pre><code>@classmethod\ndef column_std(cls, dataframe: pd.DataFrame, column_name: str) -&gt; float:\n    \"\"\"Calculate the standard deviation of a specified column.\"\"\"\n    return dataframe[column_name].std()\n</code></pre>"},{"location":"reference/ts_shape/features/stats/numeric_stats/#ts_shape.features.stats.numeric_stats.NumericStatistics.column_sum","title":"column_sum  <code>classmethod</code>","text":"<pre><code>column_sum(dataframe: DataFrame, column_name: str) -&gt; float\n</code></pre> <p>Calculate the sum of a specified column.</p> Source code in <code>src/ts_shape/features/stats/numeric_stats.py</code> <pre><code>@classmethod\ndef column_sum(cls, dataframe: pd.DataFrame, column_name: str) -&gt; float:\n    \"\"\"Calculate the sum of a specified column.\"\"\"\n    return dataframe[column_name].sum()\n</code></pre>"},{"location":"reference/ts_shape/features/stats/numeric_stats/#ts_shape.features.stats.numeric_stats.NumericStatistics.column_variance","title":"column_variance  <code>classmethod</code>","text":"<pre><code>column_variance(dataframe: DataFrame, column_name: str) -&gt; float\n</code></pre> <p>Calculate the variance of a specified column.</p> Source code in <code>src/ts_shape/features/stats/numeric_stats.py</code> <pre><code>@classmethod\ndef column_variance(cls, dataframe: pd.DataFrame, column_name: str) -&gt; float:\n    \"\"\"Calculate the variance of a specified column.\"\"\"\n    return dataframe[column_name].var()\n</code></pre>"},{"location":"reference/ts_shape/features/stats/numeric_stats/#ts_shape.features.stats.numeric_stats.NumericStatistics.describe","title":"describe  <code>classmethod</code>","text":"<pre><code>describe(dataframe: DataFrame) -&gt; DataFrame\n</code></pre> <p>Provide a statistical summary for numeric columns in the DataFrame.</p> Source code in <code>src/ts_shape/features/stats/numeric_stats.py</code> <pre><code>@classmethod\ndef describe(cls, dataframe: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Provide a statistical summary for numeric columns in the DataFrame.\"\"\"\n    return dataframe.describe()\n</code></pre>"},{"location":"reference/ts_shape/features/stats/numeric_stats/#ts_shape.features.stats.numeric_stats.NumericStatistics.get_dataframe","title":"get_dataframe","text":"<pre><code>get_dataframe() -&gt; DataFrame\n</code></pre> <p>Returns the processed DataFrame.</p> Source code in <code>src/ts_shape/utils/base.py</code> <pre><code>def get_dataframe(self) -&gt; pd.DataFrame:\n    \"\"\"Returns the processed DataFrame.\"\"\"\n    return self.dataframe\n</code></pre>"},{"location":"reference/ts_shape/features/stats/numeric_stats/#ts_shape.features.stats.numeric_stats.NumericStatistics.standard_error_mean","title":"standard_error_mean  <code>classmethod</code>","text":"<pre><code>standard_error_mean(dataframe: DataFrame, column_name: str) -&gt; float\n</code></pre> <p>Calculate the standard error of the mean for the column.</p> Source code in <code>src/ts_shape/features/stats/numeric_stats.py</code> <pre><code>@classmethod\ndef standard_error_mean(cls, dataframe: pd.DataFrame, column_name: str) -&gt; float:\n    \"\"\"Calculate the standard error of the mean for the column.\"\"\"\n    return dataframe[column_name].sem()\n</code></pre>"},{"location":"reference/ts_shape/features/stats/numeric_stats/#ts_shape.features.stats.numeric_stats.NumericStatistics.summary_as_dataframe","title":"summary_as_dataframe  <code>classmethod</code>","text":"<pre><code>summary_as_dataframe(dataframe: DataFrame, column_name: str) -&gt; DataFrame\n</code></pre> <p>Returns a DataFrame with comprehensive numeric statistics for the specified column.</p> Source code in <code>src/ts_shape/features/stats/numeric_stats.py</code> <pre><code>@classmethod\ndef summary_as_dataframe(cls, dataframe: pd.DataFrame, column_name: str) -&gt; pd.DataFrame:\n    \"\"\"Returns a DataFrame with comprehensive numeric statistics for the specified column.\"\"\"\n    summary_data = cls.summary_as_dict(dataframe, column_name)\n    return pd.DataFrame([summary_data])\n</code></pre>"},{"location":"reference/ts_shape/features/stats/numeric_stats/#ts_shape.features.stats.numeric_stats.NumericStatistics.summary_as_dict","title":"summary_as_dict  <code>classmethod</code>","text":"<pre><code>summary_as_dict(dataframe: DataFrame, column_name: str) -&gt; Dict[str, Union[float, int]]\n</code></pre> <p>Returns a dictionary with comprehensive numeric statistics for the specified column.</p> Source code in <code>src/ts_shape/features/stats/numeric_stats.py</code> <pre><code>@classmethod\ndef summary_as_dict(cls, dataframe: pd.DataFrame, column_name: str) -&gt; Dict[str, Union[float, int]]:\n    \"\"\"Returns a dictionary with comprehensive numeric statistics for the specified column.\"\"\"\n    series = dataframe[column_name]\n    return {\n        'min': cls.column_min(dataframe, column_name),\n        'max': cls.column_max(dataframe, column_name),\n        'mean': cls.column_mean(dataframe, column_name),\n        'median': cls.column_median(dataframe, column_name),\n        'std': cls.column_std(dataframe, column_name),\n        'var': cls.column_variance(dataframe, column_name),\n        'sum': cls.column_sum(dataframe, column_name),\n        'kurtosis': cls.column_kurtosis(dataframe, column_name),\n        'skewness': cls.column_skewness(dataframe, column_name),\n        'q1': cls.column_quantile(dataframe, column_name, 0.25),\n        'q3': cls.column_quantile(dataframe, column_name, 0.75),\n        'iqr': cls.column_iqr(dataframe, column_name),\n        'range': cls.column_range(dataframe, column_name),\n        'mad': cls.column_mad(dataframe, column_name),\n        'coeff_var': cls.coefficient_of_variation(dataframe, column_name),\n        'sem': cls.standard_error_mean(dataframe, column_name),\n        'mode': cls.column_mode(dataframe, column_name),\n        'percentile_90': cls.column_quantile(dataframe, column_name, 0.90),\n        'percentile_10': cls.column_quantile(dataframe, column_name, 0.10),\n    }\n</code></pre>"},{"location":"reference/ts_shape/features/stats/string_stats/","title":"string_stats","text":""},{"location":"reference/ts_shape/features/stats/string_stats/#ts_shape.features.stats.string_stats","title":"ts_shape.features.stats.string_stats","text":"<p>Classes:</p> <ul> <li> <code>StringStatistics</code>           \u2013            <p>Provides class methods to calculate statistics on string columns in a pandas DataFrame.</p> </li> </ul>"},{"location":"reference/ts_shape/features/stats/string_stats/#ts_shape.features.stats.string_stats.StringStatistics","title":"StringStatistics","text":"<pre><code>StringStatistics(dataframe: DataFrame, column_name: str = 'systime')\n</code></pre> <p>               Bases: <code>Base</code></p> <p>Provides class methods to calculate statistics on string columns in a pandas DataFrame.</p> <p>Parameters:</p> <p>Methods:</p> <ul> <li> <code>average_string_length</code>             \u2013              <p>Returns the average length of strings in the column, excluding null values.</p> </li> <li> <code>contains_digit_count</code>             \u2013              <p>Counts how many strings contain digits.</p> </li> <li> <code>contains_substring_count</code>             \u2013              <p>Counts how many strings contain the specified substring.</p> </li> <li> <code>count_most_frequent</code>             \u2013              <p>Returns the count of the most frequent string in the column.</p> </li> <li> <code>count_null</code>             \u2013              <p>Returns the number of null (NaN) values in the column.</p> </li> <li> <code>count_unique</code>             \u2013              <p>Returns the number of unique strings in the column.</p> </li> <li> <code>ends_with_count</code>             \u2013              <p>Counts how many strings end with the specified suffix.</p> </li> <li> <code>get_dataframe</code>             \u2013              <p>Returns the processed DataFrame.</p> </li> <li> <code>longest_string</code>             \u2013              <p>Returns the longest string in the column.</p> </li> <li> <code>lowercase_percentage</code>             \u2013              <p>Returns the percentage of strings that are fully lowercase.</p> </li> <li> <code>most_common_n_strings</code>             \u2013              <p>Returns the top N most frequent strings in the column.</p> </li> <li> <code>most_frequent</code>             \u2013              <p>Returns the most frequent string in the column.</p> </li> <li> <code>shortest_string</code>             \u2013              <p>Returns the shortest string in the column.</p> </li> <li> <code>starts_with_count</code>             \u2013              <p>Counts how many strings start with the specified prefix.</p> </li> <li> <code>string_length_summary</code>             \u2013              <p>Returns a summary of string lengths, including min, max, and average lengths.</p> </li> <li> <code>summary_as_dataframe</code>             \u2013              <p>Returns a DataFrame with comprehensive string statistics for the specified column.</p> </li> <li> <code>summary_as_dict</code>             \u2013              <p>Returns a dictionary with comprehensive string statistics for the specified column.</p> </li> <li> <code>uppercase_percentage</code>             \u2013              <p>Returns the percentage of strings that are fully uppercase.</p> </li> </ul> Source code in <code>src/ts_shape/utils/base.py</code> <pre><code>def __init__(self, dataframe: pd.DataFrame, column_name: str = 'systime') -&gt; None:\n    \"\"\"\n    Initializes the Base with a DataFrame, detects time columns, converts them to datetime,\n    and sorts the DataFrame by the specified column (or the detected time column if applicable).\n\n    Args:\n        dataframe (pd.DataFrame): The DataFrame to be processed.\n        column_name (str): The column to sort by. Default is 'systime'. If the column is not found or is not a time column, the class will attempt to detect other time columns.\n    \"\"\"\n    self.dataframe = dataframe.copy()\n\n    # Attempt to convert the specified column_name to datetime if it exists\n    if column_name in self.dataframe.columns:\n        self.dataframe[column_name] = pd.to_datetime(self.dataframe[column_name], errors='coerce')\n    else:\n        # If the column_name is not in the DataFrame, fallback to automatic time detection\n        time_columns = [col for col in self.dataframe.columns if 'time' in col.lower() or 'date' in col.lower()]\n\n        # Convert all detected time columns to datetime, if any\n        for col in time_columns:\n            self.dataframe[col] = pd.to_datetime(self.dataframe[col], errors='coerce')\n\n        # If any time columns are detected, sort by the first one; otherwise, do nothing\n        if time_columns:\n            column_name = time_columns[0]\n\n    # Sort by the datetime column (either specified or detected)\n    if column_name in self.dataframe.columns:\n        self.dataframe = self.dataframe.sort_values(by=column_name)\n</code></pre>"},{"location":"reference/ts_shape/features/stats/string_stats/#ts_shape.features.stats.string_stats.StringStatistics(dataframe)","title":"<code>dataframe</code>","text":"(<code>DataFrame</code>)           \u2013            <p>The DataFrame to be processed.</p>"},{"location":"reference/ts_shape/features/stats/string_stats/#ts_shape.features.stats.string_stats.StringStatistics(column_name)","title":"<code>column_name</code>","text":"(<code>str</code>, default:                   <code>'systime'</code> )           \u2013            <p>The column to sort by. Default is 'systime'. If the column is not found or is not a time column, the class will attempt to detect other time columns.</p>"},{"location":"reference/ts_shape/features/stats/string_stats/#ts_shape.features.stats.string_stats.StringStatistics.average_string_length","title":"average_string_length  <code>classmethod</code>","text":"<pre><code>average_string_length(dataframe: DataFrame, column_name: str = 'value_string') -&gt; float\n</code></pre> <p>Returns the average length of strings in the column, excluding null values.</p> Source code in <code>src/ts_shape/features/stats/string_stats.py</code> <pre><code>@classmethod\ndef average_string_length(cls, dataframe: pd.DataFrame, column_name: str = 'value_string') -&gt; float:\n    \"\"\"Returns the average length of strings in the column, excluding null values.\"\"\"\n    return dataframe[column_name].dropna().str.len().mean()\n</code></pre>"},{"location":"reference/ts_shape/features/stats/string_stats/#ts_shape.features.stats.string_stats.StringStatistics.contains_digit_count","title":"contains_digit_count  <code>classmethod</code>","text":"<pre><code>contains_digit_count(dataframe: DataFrame, column_name: str = 'value_string') -&gt; int\n</code></pre> <p>Counts how many strings contain digits.</p> Source code in <code>src/ts_shape/features/stats/string_stats.py</code> <pre><code>@classmethod\ndef contains_digit_count(cls, dataframe: pd.DataFrame, column_name: str = 'value_string') -&gt; int:\n    \"\"\"Counts how many strings contain digits.\"\"\"\n    return dataframe[column_name].dropna().str.contains(r'\\d').sum()\n</code></pre>"},{"location":"reference/ts_shape/features/stats/string_stats/#ts_shape.features.stats.string_stats.StringStatistics.contains_substring_count","title":"contains_substring_count  <code>classmethod</code>","text":"<pre><code>contains_substring_count(dataframe: DataFrame, substring: str, column_name: str = 'value_string') -&gt; int\n</code></pre> <p>Counts how many strings contain the specified substring.</p> Source code in <code>src/ts_shape/features/stats/string_stats.py</code> <pre><code>@classmethod\ndef contains_substring_count(cls, dataframe: pd.DataFrame, substring: str, column_name: str = 'value_string') -&gt; int:\n    \"\"\"Counts how many strings contain the specified substring.\"\"\"\n    return dataframe[column_name].dropna().str.contains(substring).sum()\n</code></pre>"},{"location":"reference/ts_shape/features/stats/string_stats/#ts_shape.features.stats.string_stats.StringStatistics.count_most_frequent","title":"count_most_frequent  <code>classmethod</code>","text":"<pre><code>count_most_frequent(dataframe: DataFrame, column_name: str = 'value_string') -&gt; int\n</code></pre> <p>Returns the count of the most frequent string in the column.</p> Source code in <code>src/ts_shape/features/stats/string_stats.py</code> <pre><code>@classmethod\ndef count_most_frequent(cls, dataframe: pd.DataFrame, column_name: str = 'value_string') -&gt; int:\n    \"\"\"Returns the count of the most frequent string in the column.\"\"\"\n    most_frequent_value = cls.most_frequent(dataframe, column_name)\n    return dataframe[column_name].value_counts().loc[most_frequent_value]\n</code></pre>"},{"location":"reference/ts_shape/features/stats/string_stats/#ts_shape.features.stats.string_stats.StringStatistics.count_null","title":"count_null  <code>classmethod</code>","text":"<pre><code>count_null(dataframe: DataFrame, column_name: str = 'value_string') -&gt; int\n</code></pre> <p>Returns the number of null (NaN) values in the column.</p> Source code in <code>src/ts_shape/features/stats/string_stats.py</code> <pre><code>@classmethod\ndef count_null(cls, dataframe: pd.DataFrame, column_name: str = 'value_string') -&gt; int:\n    \"\"\"Returns the number of null (NaN) values in the column.\"\"\"\n    return dataframe[column_name].isna().sum()\n</code></pre>"},{"location":"reference/ts_shape/features/stats/string_stats/#ts_shape.features.stats.string_stats.StringStatistics.count_unique","title":"count_unique  <code>classmethod</code>","text":"<pre><code>count_unique(dataframe: DataFrame, column_name: str = 'value_string') -&gt; int\n</code></pre> <p>Returns the number of unique strings in the column.</p> Source code in <code>src/ts_shape/features/stats/string_stats.py</code> <pre><code>@classmethod\ndef count_unique(cls, dataframe: pd.DataFrame, column_name: str = 'value_string') -&gt; int:\n    \"\"\"Returns the number of unique strings in the column.\"\"\"\n    return dataframe[column_name].nunique()\n</code></pre>"},{"location":"reference/ts_shape/features/stats/string_stats/#ts_shape.features.stats.string_stats.StringStatistics.ends_with_count","title":"ends_with_count  <code>classmethod</code>","text":"<pre><code>ends_with_count(dataframe: DataFrame, suffix: str, column_name: str = 'value_string') -&gt; int\n</code></pre> <p>Counts how many strings end with the specified suffix.</p> Source code in <code>src/ts_shape/features/stats/string_stats.py</code> <pre><code>@classmethod\ndef ends_with_count(cls, dataframe: pd.DataFrame, suffix: str, column_name: str = 'value_string') -&gt; int:\n    \"\"\"Counts how many strings end with the specified suffix.\"\"\"\n    return dataframe[column_name].dropna().str.endswith(suffix).sum()\n</code></pre>"},{"location":"reference/ts_shape/features/stats/string_stats/#ts_shape.features.stats.string_stats.StringStatistics.get_dataframe","title":"get_dataframe","text":"<pre><code>get_dataframe() -&gt; DataFrame\n</code></pre> <p>Returns the processed DataFrame.</p> Source code in <code>src/ts_shape/utils/base.py</code> <pre><code>def get_dataframe(self) -&gt; pd.DataFrame:\n    \"\"\"Returns the processed DataFrame.\"\"\"\n    return self.dataframe\n</code></pre>"},{"location":"reference/ts_shape/features/stats/string_stats/#ts_shape.features.stats.string_stats.StringStatistics.longest_string","title":"longest_string  <code>classmethod</code>","text":"<pre><code>longest_string(dataframe: DataFrame, column_name: str = 'value_string') -&gt; str\n</code></pre> <p>Returns the longest string in the column.</p> Source code in <code>src/ts_shape/features/stats/string_stats.py</code> <pre><code>@classmethod\ndef longest_string(cls, dataframe: pd.DataFrame, column_name: str = 'value_string') -&gt; str:\n    \"\"\"Returns the longest string in the column.\"\"\"\n    return dataframe[column_name].dropna().loc[dataframe[column_name].dropna().str.len().idxmax()]\n</code></pre>"},{"location":"reference/ts_shape/features/stats/string_stats/#ts_shape.features.stats.string_stats.StringStatistics.lowercase_percentage","title":"lowercase_percentage  <code>classmethod</code>","text":"<pre><code>lowercase_percentage(dataframe: DataFrame, column_name: str = 'value_string') -&gt; float\n</code></pre> <p>Returns the percentage of strings that are fully lowercase.</p> Source code in <code>src/ts_shape/features/stats/string_stats.py</code> <pre><code>@classmethod\ndef lowercase_percentage(cls, dataframe: pd.DataFrame, column_name: str = 'value_string') -&gt; float:\n    \"\"\"Returns the percentage of strings that are fully lowercase.\"\"\"\n    total_non_null = dataframe[column_name].notna().sum()\n    if total_non_null == 0:\n        return 0.0\n    lowercase_count = dataframe[column_name].dropna().str.islower().sum()\n    return (lowercase_count / total_non_null) * 100\n</code></pre>"},{"location":"reference/ts_shape/features/stats/string_stats/#ts_shape.features.stats.string_stats.StringStatistics.most_common_n_strings","title":"most_common_n_strings  <code>classmethod</code>","text":"<pre><code>most_common_n_strings(dataframe: DataFrame, n: int, column_name: str = 'value_string') -&gt; Series\n</code></pre> <p>Returns the top N most frequent strings in the column.</p> Source code in <code>src/ts_shape/features/stats/string_stats.py</code> <pre><code>@classmethod\ndef most_common_n_strings(cls, dataframe: pd.DataFrame, n: int, column_name: str = 'value_string') -&gt; pd.Series:\n    \"\"\"Returns the top N most frequent strings in the column.\"\"\"\n    return dataframe[column_name].value_counts().head(n)\n</code></pre>"},{"location":"reference/ts_shape/features/stats/string_stats/#ts_shape.features.stats.string_stats.StringStatistics.most_frequent","title":"most_frequent  <code>classmethod</code>","text":"<pre><code>most_frequent(dataframe: DataFrame, column_name: str = 'value_string') -&gt; str\n</code></pre> <p>Returns the most frequent string in the column.</p> Source code in <code>src/ts_shape/features/stats/string_stats.py</code> <pre><code>@classmethod\ndef most_frequent(cls, dataframe: pd.DataFrame, column_name: str = 'value_string') -&gt; str:\n    \"\"\"Returns the most frequent string in the column.\"\"\"\n    return dataframe[column_name].mode().iloc[0]\n</code></pre>"},{"location":"reference/ts_shape/features/stats/string_stats/#ts_shape.features.stats.string_stats.StringStatistics.shortest_string","title":"shortest_string  <code>classmethod</code>","text":"<pre><code>shortest_string(dataframe: DataFrame, column_name: str = 'value_string') -&gt; str\n</code></pre> <p>Returns the shortest string in the column.</p> Source code in <code>src/ts_shape/features/stats/string_stats.py</code> <pre><code>@classmethod\ndef shortest_string(cls, dataframe: pd.DataFrame, column_name: str = 'value_string') -&gt; str:\n    \"\"\"Returns the shortest string in the column.\"\"\"\n    return dataframe[column_name].dropna().loc[dataframe[column_name].dropna().str.len().idxmin()]\n</code></pre>"},{"location":"reference/ts_shape/features/stats/string_stats/#ts_shape.features.stats.string_stats.StringStatistics.starts_with_count","title":"starts_with_count  <code>classmethod</code>","text":"<pre><code>starts_with_count(dataframe: DataFrame, prefix: str, column_name: str = 'value_string') -&gt; int\n</code></pre> <p>Counts how many strings start with the specified prefix.</p> Source code in <code>src/ts_shape/features/stats/string_stats.py</code> <pre><code>@classmethod\ndef starts_with_count(cls, dataframe: pd.DataFrame, prefix: str, column_name: str = 'value_string') -&gt; int:\n    \"\"\"Counts how many strings start with the specified prefix.\"\"\"\n    return dataframe[column_name].dropna().str.startswith(prefix).sum()\n</code></pre>"},{"location":"reference/ts_shape/features/stats/string_stats/#ts_shape.features.stats.string_stats.StringStatistics.string_length_summary","title":"string_length_summary  <code>classmethod</code>","text":"<pre><code>string_length_summary(dataframe: DataFrame, column_name: str = 'value_string') -&gt; DataFrame\n</code></pre> <p>Returns a summary of string lengths, including min, max, and average lengths.</p> Source code in <code>src/ts_shape/features/stats/string_stats.py</code> <pre><code>@classmethod\ndef string_length_summary(cls, dataframe: pd.DataFrame, column_name: str = 'value_string') -&gt; pd.DataFrame:\n    \"\"\"Returns a summary of string lengths, including min, max, and average lengths.\"\"\"\n    lengths = dataframe[column_name].dropna().str.len()\n    return pd.DataFrame({\n        'Min Length': [lengths.min()],\n        'Max Length': [lengths.max()],\n        'Average Length': [lengths.mean()]\n    })\n</code></pre>"},{"location":"reference/ts_shape/features/stats/string_stats/#ts_shape.features.stats.string_stats.StringStatistics.summary_as_dataframe","title":"summary_as_dataframe  <code>classmethod</code>","text":"<pre><code>summary_as_dataframe(dataframe: DataFrame, column_name: str) -&gt; DataFrame\n</code></pre> <p>Returns a DataFrame with comprehensive string statistics for the specified column.</p> Source code in <code>src/ts_shape/features/stats/string_stats.py</code> <pre><code>@classmethod\ndef summary_as_dataframe(cls, dataframe: pd.DataFrame, column_name: str) -&gt; pd.DataFrame:\n    \"\"\"Returns a DataFrame with comprehensive string statistics for the specified column.\"\"\"\n    summary_data = cls.summary_as_dict(dataframe, column_name)\n    return pd.DataFrame([summary_data])\n</code></pre>"},{"location":"reference/ts_shape/features/stats/string_stats/#ts_shape.features.stats.string_stats.StringStatistics.summary_as_dict","title":"summary_as_dict  <code>classmethod</code>","text":"<pre><code>summary_as_dict(dataframe: DataFrame, column_name: str) -&gt; Dict[str, Union[int, str, float]]\n</code></pre> <p>Returns a dictionary with comprehensive string statistics for the specified column.</p> Source code in <code>src/ts_shape/features/stats/string_stats.py</code> <pre><code>@classmethod\ndef summary_as_dict(cls, dataframe: pd.DataFrame, column_name: str) -&gt; Dict[str, Union[int, str, float]]:\n    \"\"\"Returns a dictionary with comprehensive string statistics for the specified column.\"\"\"\n    most_frequent = cls.most_frequent(dataframe, column_name)\n    value_counts = dataframe[column_name].value_counts()\n\n    return {\n        'unique_values': cls.count_unique(dataframe, column_name),\n        'most_frequent': most_frequent,\n        'count_most_frequent': cls.count_most_frequent(dataframe, column_name),\n        'count_null': cls.count_null(dataframe, column_name),\n        'average_string_length': cls.average_string_length(dataframe, column_name),\n        'longest_string': cls.longest_string(dataframe, column_name),\n        'shortest_string': cls.shortest_string(dataframe, column_name),\n        'uppercase_percentage': cls.uppercase_percentage(dataframe, column_name),\n        'lowercase_percentage': cls.lowercase_percentage(dataframe, column_name),\n        'contains_digit_count': cls.contains_digit_count(dataframe, column_name),\n        'least_common': value_counts.idxmin() if not value_counts.empty else None,\n        'frequency_least_common': value_counts.min() if not value_counts.empty else 0\n    }\n</code></pre>"},{"location":"reference/ts_shape/features/stats/string_stats/#ts_shape.features.stats.string_stats.StringStatistics.uppercase_percentage","title":"uppercase_percentage  <code>classmethod</code>","text":"<pre><code>uppercase_percentage(dataframe: DataFrame, column_name: str = 'value_string') -&gt; float\n</code></pre> <p>Returns the percentage of strings that are fully uppercase.</p> Source code in <code>src/ts_shape/features/stats/string_stats.py</code> <pre><code>@classmethod\ndef uppercase_percentage(cls, dataframe: pd.DataFrame, column_name: str = 'value_string') -&gt; float:\n    \"\"\"Returns the percentage of strings that are fully uppercase.\"\"\"\n    total_non_null = dataframe[column_name].notna().sum()\n    if total_non_null == 0:\n        return 0.0\n    uppercase_count = dataframe[column_name].dropna().str.isupper().sum()\n    return (uppercase_count / total_non_null) * 100\n</code></pre>"},{"location":"reference/ts_shape/features/stats/timestamp_stats/","title":"timestamp_stats","text":""},{"location":"reference/ts_shape/features/stats/timestamp_stats/#ts_shape.features.stats.timestamp_stats","title":"ts_shape.features.stats.timestamp_stats","text":"<p>Classes:</p> <ul> <li> <code>TimestampStatistics</code>           \u2013            <p>Provides class methods to calculate statistics on timestamp columns in a pandas DataFrame.</p> </li> </ul>"},{"location":"reference/ts_shape/features/stats/timestamp_stats/#ts_shape.features.stats.timestamp_stats.TimestampStatistics","title":"TimestampStatistics","text":"<pre><code>TimestampStatistics(dataframe: DataFrame, column_name: str = 'systime')\n</code></pre> <p>               Bases: <code>Base</code></p> <p>Provides class methods to calculate statistics on timestamp columns in a pandas DataFrame. The default column for calculations is 'systime'.</p> <p>Parameters:</p> <p>Methods:</p> <ul> <li> <code>average_time_gap</code>             \u2013              <p>Returns the average time gap between consecutive timestamps.</p> </li> <li> <code>count_most_frequent_timestamp</code>             \u2013              <p>Returns the count of the most frequent timestamp in the column.</p> </li> <li> <code>count_not_null</code>             \u2013              <p>Returns the number of non-null (valid) timestamps in the column.</p> </li> <li> <code>count_null</code>             \u2013              <p>Returns the number of null (NaN) values in the timestamp column.</p> </li> <li> <code>days_with_most_activity</code>             \u2013              <p>Returns the top N days with the most timestamp activity.</p> </li> <li> <code>earliest_timestamp</code>             \u2013              <p>Returns the earliest timestamp in the column.</p> </li> <li> <code>get_dataframe</code>             \u2013              <p>Returns the processed DataFrame.</p> </li> <li> <code>hour_distribution</code>             \u2013              <p>Returns the distribution of timestamps per hour of the day.</p> </li> <li> <code>latest_timestamp</code>             \u2013              <p>Returns the latest timestamp in the column.</p> </li> <li> <code>median_timestamp</code>             \u2013              <p>Returns the median timestamp in the column.</p> </li> <li> <code>month_distribution</code>             \u2013              <p>Returns the distribution of timestamps per month.</p> </li> <li> <code>most_frequent_day</code>             \u2013              <p>Returns the most frequent day of the week (0=Monday, 6=Sunday).</p> </li> <li> <code>most_frequent_hour</code>             \u2013              <p>Returns the most frequent hour of the day (0-23).</p> </li> <li> <code>most_frequent_timestamp</code>             \u2013              <p>Returns the most frequent timestamp in the column.</p> </li> <li> <code>standard_deviation_timestamps</code>             \u2013              <p>Returns the standard deviation of the time differences between consecutive timestamps.</p> </li> <li> <code>timestamp_quartiles</code>             \u2013              <p>Returns the 25th, 50th (median), and 75th percentiles of the timestamps.</p> </li> <li> <code>timestamp_range</code>             \u2013              <p>Returns the time range (difference) between the earliest and latest timestamps.</p> </li> <li> <code>weekday_distribution</code>             \u2013              <p>Returns the distribution of timestamps per weekday.</p> </li> <li> <code>year_distribution</code>             \u2013              <p>Returns the distribution of timestamps per year.</p> </li> </ul> Source code in <code>src/ts_shape/utils/base.py</code> <pre><code>def __init__(self, dataframe: pd.DataFrame, column_name: str = 'systime') -&gt; None:\n    \"\"\"\n    Initializes the Base with a DataFrame, detects time columns, converts them to datetime,\n    and sorts the DataFrame by the specified column (or the detected time column if applicable).\n\n    Args:\n        dataframe (pd.DataFrame): The DataFrame to be processed.\n        column_name (str): The column to sort by. Default is 'systime'. If the column is not found or is not a time column, the class will attempt to detect other time columns.\n    \"\"\"\n    self.dataframe = dataframe.copy()\n\n    # Attempt to convert the specified column_name to datetime if it exists\n    if column_name in self.dataframe.columns:\n        self.dataframe[column_name] = pd.to_datetime(self.dataframe[column_name], errors='coerce')\n    else:\n        # If the column_name is not in the DataFrame, fallback to automatic time detection\n        time_columns = [col for col in self.dataframe.columns if 'time' in col.lower() or 'date' in col.lower()]\n\n        # Convert all detected time columns to datetime, if any\n        for col in time_columns:\n            self.dataframe[col] = pd.to_datetime(self.dataframe[col], errors='coerce')\n\n        # If any time columns are detected, sort by the first one; otherwise, do nothing\n        if time_columns:\n            column_name = time_columns[0]\n\n    # Sort by the datetime column (either specified or detected)\n    if column_name in self.dataframe.columns:\n        self.dataframe = self.dataframe.sort_values(by=column_name)\n</code></pre>"},{"location":"reference/ts_shape/features/stats/timestamp_stats/#ts_shape.features.stats.timestamp_stats.TimestampStatistics(dataframe)","title":"<code>dataframe</code>","text":"(<code>DataFrame</code>)           \u2013            <p>The DataFrame to be processed.</p>"},{"location":"reference/ts_shape/features/stats/timestamp_stats/#ts_shape.features.stats.timestamp_stats.TimestampStatistics(column_name)","title":"<code>column_name</code>","text":"(<code>str</code>, default:                   <code>'systime'</code> )           \u2013            <p>The column to sort by. Default is 'systime'. If the column is not found or is not a time column, the class will attempt to detect other time columns.</p>"},{"location":"reference/ts_shape/features/stats/timestamp_stats/#ts_shape.features.stats.timestamp_stats.TimestampStatistics.average_time_gap","title":"average_time_gap  <code>classmethod</code>","text":"<pre><code>average_time_gap(dataframe: DataFrame, column_name: str = 'systime') -&gt; Timedelta\n</code></pre> <p>Returns the average time gap between consecutive timestamps.</p> Source code in <code>src/ts_shape/features/stats/timestamp_stats.py</code> <pre><code>@classmethod\ndef average_time_gap(cls, dataframe: pd.DataFrame, column_name: str = 'systime') -&gt; pd.Timedelta:\n    \"\"\"Returns the average time gap between consecutive timestamps.\"\"\"\n    sorted_times = dataframe[column_name].dropna().sort_values()\n    time_deltas = sorted_times.diff().dropna()\n    return time_deltas.mean()\n</code></pre>"},{"location":"reference/ts_shape/features/stats/timestamp_stats/#ts_shape.features.stats.timestamp_stats.TimestampStatistics.count_most_frequent_timestamp","title":"count_most_frequent_timestamp  <code>classmethod</code>","text":"<pre><code>count_most_frequent_timestamp(dataframe: DataFrame, column_name: str = 'systime') -&gt; int\n</code></pre> <p>Returns the count of the most frequent timestamp in the column.</p> Source code in <code>src/ts_shape/features/stats/timestamp_stats.py</code> <pre><code>@classmethod\ndef count_most_frequent_timestamp(cls, dataframe: pd.DataFrame, column_name: str = 'systime') -&gt; int:\n    \"\"\"Returns the count of the most frequent timestamp in the column.\"\"\"\n    most_frequent_value = cls.most_frequent_timestamp(dataframe, column_name)\n    return dataframe[column_name].value_counts().loc[most_frequent_value]\n</code></pre>"},{"location":"reference/ts_shape/features/stats/timestamp_stats/#ts_shape.features.stats.timestamp_stats.TimestampStatistics.count_not_null","title":"count_not_null  <code>classmethod</code>","text":"<pre><code>count_not_null(dataframe: DataFrame, column_name: str = 'systime') -&gt; int\n</code></pre> <p>Returns the number of non-null (valid) timestamps in the column.</p> Source code in <code>src/ts_shape/features/stats/timestamp_stats.py</code> <pre><code>@classmethod\ndef count_not_null(cls, dataframe: pd.DataFrame, column_name: str = 'systime') -&gt; int:\n    \"\"\"Returns the number of non-null (valid) timestamps in the column.\"\"\"\n    return dataframe[column_name].notna().sum()\n</code></pre>"},{"location":"reference/ts_shape/features/stats/timestamp_stats/#ts_shape.features.stats.timestamp_stats.TimestampStatistics.count_null","title":"count_null  <code>classmethod</code>","text":"<pre><code>count_null(dataframe: DataFrame, column_name: str = 'systime') -&gt; int\n</code></pre> <p>Returns the number of null (NaN) values in the timestamp column.</p> Source code in <code>src/ts_shape/features/stats/timestamp_stats.py</code> <pre><code>@classmethod\ndef count_null(cls, dataframe: pd.DataFrame, column_name: str = 'systime') -&gt; int:\n    \"\"\"Returns the number of null (NaN) values in the timestamp column.\"\"\"\n    return dataframe[column_name].isna().sum()\n</code></pre>"},{"location":"reference/ts_shape/features/stats/timestamp_stats/#ts_shape.features.stats.timestamp_stats.TimestampStatistics.days_with_most_activity","title":"days_with_most_activity  <code>classmethod</code>","text":"<pre><code>days_with_most_activity(dataframe: DataFrame, column_name: str = 'systime', n: int = 3) -&gt; Series\n</code></pre> <p>Returns the top N days with the most timestamp activity.</p> Source code in <code>src/ts_shape/features/stats/timestamp_stats.py</code> <pre><code>@classmethod\ndef days_with_most_activity(cls, dataframe: pd.DataFrame, column_name: str = 'systime', n: int = 3) -&gt; pd.Series:\n    \"\"\"Returns the top N days with the most timestamp activity.\"\"\"\n    return dataframe[column_name].dt.date.value_counts().head(n)\n</code></pre>"},{"location":"reference/ts_shape/features/stats/timestamp_stats/#ts_shape.features.stats.timestamp_stats.TimestampStatistics.earliest_timestamp","title":"earliest_timestamp  <code>classmethod</code>","text":"<pre><code>earliest_timestamp(dataframe: DataFrame, column_name: str = 'systime')\n</code></pre> <p>Returns the earliest timestamp in the column.</p> Source code in <code>src/ts_shape/features/stats/timestamp_stats.py</code> <pre><code>@classmethod\ndef earliest_timestamp(cls, dataframe: pd.DataFrame, column_name: str = 'systime'):\n    \"\"\"Returns the earliest timestamp in the column.\"\"\"\n    return dataframe[column_name].min()\n</code></pre>"},{"location":"reference/ts_shape/features/stats/timestamp_stats/#ts_shape.features.stats.timestamp_stats.TimestampStatistics.get_dataframe","title":"get_dataframe","text":"<pre><code>get_dataframe() -&gt; DataFrame\n</code></pre> <p>Returns the processed DataFrame.</p> Source code in <code>src/ts_shape/utils/base.py</code> <pre><code>def get_dataframe(self) -&gt; pd.DataFrame:\n    \"\"\"Returns the processed DataFrame.\"\"\"\n    return self.dataframe\n</code></pre>"},{"location":"reference/ts_shape/features/stats/timestamp_stats/#ts_shape.features.stats.timestamp_stats.TimestampStatistics.hour_distribution","title":"hour_distribution  <code>classmethod</code>","text":"<pre><code>hour_distribution(dataframe: DataFrame, column_name: str = 'systime') -&gt; Series\n</code></pre> <p>Returns the distribution of timestamps per hour of the day.</p> Source code in <code>src/ts_shape/features/stats/timestamp_stats.py</code> <pre><code>@classmethod\ndef hour_distribution(cls, dataframe: pd.DataFrame, column_name: str = 'systime') -&gt; pd.Series:\n    \"\"\"Returns the distribution of timestamps per hour of the day.\"\"\"\n    return dataframe[column_name].dt.hour.value_counts()\n</code></pre>"},{"location":"reference/ts_shape/features/stats/timestamp_stats/#ts_shape.features.stats.timestamp_stats.TimestampStatistics.latest_timestamp","title":"latest_timestamp  <code>classmethod</code>","text":"<pre><code>latest_timestamp(dataframe: DataFrame, column_name: str = 'systime')\n</code></pre> <p>Returns the latest timestamp in the column.</p> Source code in <code>src/ts_shape/features/stats/timestamp_stats.py</code> <pre><code>@classmethod\ndef latest_timestamp(cls, dataframe: pd.DataFrame, column_name: str = 'systime'):\n    \"\"\"Returns the latest timestamp in the column.\"\"\"\n    return dataframe[column_name].max()\n</code></pre>"},{"location":"reference/ts_shape/features/stats/timestamp_stats/#ts_shape.features.stats.timestamp_stats.TimestampStatistics.median_timestamp","title":"median_timestamp  <code>classmethod</code>","text":"<pre><code>median_timestamp(dataframe: DataFrame, column_name: str = 'systime')\n</code></pre> <p>Returns the median timestamp in the column.</p> Source code in <code>src/ts_shape/features/stats/timestamp_stats.py</code> <pre><code>@classmethod\ndef median_timestamp(cls, dataframe: pd.DataFrame, column_name: str = 'systime'):\n    \"\"\"Returns the median timestamp in the column.\"\"\"\n    return dataframe[column_name].median()\n</code></pre>"},{"location":"reference/ts_shape/features/stats/timestamp_stats/#ts_shape.features.stats.timestamp_stats.TimestampStatistics.month_distribution","title":"month_distribution  <code>classmethod</code>","text":"<pre><code>month_distribution(dataframe: DataFrame, column_name: str = 'systime') -&gt; Series\n</code></pre> <p>Returns the distribution of timestamps per month.</p> Source code in <code>src/ts_shape/features/stats/timestamp_stats.py</code> <pre><code>@classmethod\ndef month_distribution(cls, dataframe: pd.DataFrame, column_name: str = 'systime') -&gt; pd.Series:\n    \"\"\"Returns the distribution of timestamps per month.\"\"\"\n    return dataframe[column_name].dt.month.value_counts()\n</code></pre>"},{"location":"reference/ts_shape/features/stats/timestamp_stats/#ts_shape.features.stats.timestamp_stats.TimestampStatistics.most_frequent_day","title":"most_frequent_day  <code>classmethod</code>","text":"<pre><code>most_frequent_day(dataframe: DataFrame, column_name: str = 'systime') -&gt; int\n</code></pre> <p>Returns the most frequent day of the week (0=Monday, 6=Sunday).</p> Source code in <code>src/ts_shape/features/stats/timestamp_stats.py</code> <pre><code>@classmethod\ndef most_frequent_day(cls, dataframe: pd.DataFrame, column_name: str = 'systime') -&gt; int:\n    \"\"\"Returns the most frequent day of the week (0=Monday, 6=Sunday).\"\"\"\n    return dataframe[column_name].dt.weekday.mode().iloc[0]\n</code></pre>"},{"location":"reference/ts_shape/features/stats/timestamp_stats/#ts_shape.features.stats.timestamp_stats.TimestampStatistics.most_frequent_hour","title":"most_frequent_hour  <code>classmethod</code>","text":"<pre><code>most_frequent_hour(dataframe: DataFrame, column_name: str = 'systime') -&gt; int\n</code></pre> <p>Returns the most frequent hour of the day (0-23).</p> Source code in <code>src/ts_shape/features/stats/timestamp_stats.py</code> <pre><code>@classmethod\ndef most_frequent_hour(cls, dataframe: pd.DataFrame, column_name: str = 'systime') -&gt; int:\n    \"\"\"Returns the most frequent hour of the day (0-23).\"\"\"\n    return dataframe[column_name].dt.hour.mode().iloc[0]\n</code></pre>"},{"location":"reference/ts_shape/features/stats/timestamp_stats/#ts_shape.features.stats.timestamp_stats.TimestampStatistics.most_frequent_timestamp","title":"most_frequent_timestamp  <code>classmethod</code>","text":"<pre><code>most_frequent_timestamp(dataframe: DataFrame, column_name: str = 'systime')\n</code></pre> <p>Returns the most frequent timestamp in the column.</p> Source code in <code>src/ts_shape/features/stats/timestamp_stats.py</code> <pre><code>@classmethod\ndef most_frequent_timestamp(cls, dataframe: pd.DataFrame, column_name: str = 'systime'):\n    \"\"\"Returns the most frequent timestamp in the column.\"\"\"\n    return dataframe[column_name].mode().iloc[0]\n</code></pre>"},{"location":"reference/ts_shape/features/stats/timestamp_stats/#ts_shape.features.stats.timestamp_stats.TimestampStatistics.standard_deviation_timestamps","title":"standard_deviation_timestamps  <code>classmethod</code>","text":"<pre><code>standard_deviation_timestamps(dataframe: DataFrame, column_name: str = 'systime') -&gt; Timedelta\n</code></pre> <p>Returns the standard deviation of the time differences between consecutive timestamps.</p> Source code in <code>src/ts_shape/features/stats/timestamp_stats.py</code> <pre><code>@classmethod\ndef standard_deviation_timestamps(cls, dataframe: pd.DataFrame, column_name: str = 'systime') -&gt; pd.Timedelta:\n    \"\"\"Returns the standard deviation of the time differences between consecutive timestamps.\"\"\"\n    sorted_times = dataframe[column_name].dropna().sort_values()\n    time_deltas = sorted_times.diff().dropna()\n    return time_deltas.std()\n</code></pre>"},{"location":"reference/ts_shape/features/stats/timestamp_stats/#ts_shape.features.stats.timestamp_stats.TimestampStatistics.timestamp_quartiles","title":"timestamp_quartiles  <code>classmethod</code>","text":"<pre><code>timestamp_quartiles(dataframe: DataFrame, column_name: str = 'systime') -&gt; Series\n</code></pre> <p>Returns the 25th, 50th (median), and 75th percentiles of the timestamps.</p> Source code in <code>src/ts_shape/features/stats/timestamp_stats.py</code> <pre><code>@classmethod\ndef timestamp_quartiles(cls, dataframe: pd.DataFrame, column_name: str = 'systime') -&gt; pd.Series:\n    \"\"\"Returns the 25th, 50th (median), and 75th percentiles of the timestamps.\"\"\"\n    return dataframe[column_name].quantile([0.25, 0.5, 0.75])\n</code></pre>"},{"location":"reference/ts_shape/features/stats/timestamp_stats/#ts_shape.features.stats.timestamp_stats.TimestampStatistics.timestamp_range","title":"timestamp_range  <code>classmethod</code>","text":"<pre><code>timestamp_range(dataframe: DataFrame, column_name: str = 'systime')\n</code></pre> <p>Returns the time range (difference) between the earliest and latest timestamps.</p> Source code in <code>src/ts_shape/features/stats/timestamp_stats.py</code> <pre><code>@classmethod\ndef timestamp_range(cls, dataframe: pd.DataFrame, column_name: str = 'systime'):\n    \"\"\"Returns the time range (difference) between the earliest and latest timestamps.\"\"\"\n    return cls.latest_timestamp(dataframe, column_name) - cls.earliest_timestamp(dataframe, column_name)\n</code></pre>"},{"location":"reference/ts_shape/features/stats/timestamp_stats/#ts_shape.features.stats.timestamp_stats.TimestampStatistics.weekday_distribution","title":"weekday_distribution  <code>classmethod</code>","text":"<pre><code>weekday_distribution(dataframe: DataFrame, column_name: str = 'systime') -&gt; Series\n</code></pre> <p>Returns the distribution of timestamps per weekday.</p> Source code in <code>src/ts_shape/features/stats/timestamp_stats.py</code> <pre><code>@classmethod\ndef weekday_distribution(cls, dataframe: pd.DataFrame, column_name: str = 'systime') -&gt; pd.Series:\n    \"\"\"Returns the distribution of timestamps per weekday.\"\"\"\n    return dataframe[column_name].dt.weekday.value_counts()\n</code></pre>"},{"location":"reference/ts_shape/features/stats/timestamp_stats/#ts_shape.features.stats.timestamp_stats.TimestampStatistics.year_distribution","title":"year_distribution  <code>classmethod</code>","text":"<pre><code>year_distribution(dataframe: DataFrame, column_name: str = 'systime') -&gt; Series\n</code></pre> <p>Returns the distribution of timestamps per year.</p> Source code in <code>src/ts_shape/features/stats/timestamp_stats.py</code> <pre><code>@classmethod\ndef year_distribution(cls, dataframe: pd.DataFrame, column_name: str = 'systime') -&gt; pd.Series:\n    \"\"\"Returns the distribution of timestamps per year.\"\"\"\n    return dataframe[column_name].dt.year.value_counts()\n</code></pre>"},{"location":"reference/ts_shape/features/time_stats/__init__/","title":"init","text":""},{"location":"reference/ts_shape/features/time_stats/__init__/#ts_shape.features.time_stats","title":"ts_shape.features.time_stats","text":"<p>Time-Grouped Statistics</p> <p>Aggregations over fixed time windows (e.g., hourly/daily) for numeric values.</p> <ul> <li>TimeGroupedStatistics: Time-windowed aggregations for numeric series.</li> <li>calculate_statistic: Single aggregation per window (mean/sum/min/max/diff/range).</li> <li>calculate_statistics: Multiple aggregations merged.</li> <li>calculate_custom_func: Apply a custom aggregation per window.</li> </ul> <p>Modules:</p> <ul> <li> <code>time_stats_numeric</code>           \u2013            </li> </ul>"},{"location":"reference/ts_shape/features/time_stats/time_stats_numeric/","title":"time_stats_numeric","text":""},{"location":"reference/ts_shape/features/time_stats/time_stats_numeric/#ts_shape.features.time_stats.time_stats_numeric","title":"ts_shape.features.time_stats.time_stats_numeric","text":"<p>Classes:</p> <ul> <li> <code>TimeGroupedStatistics</code>           \u2013            <p>A class for calculating time-grouped statistics on numeric data, with class methods to apply various statistical functions.</p> </li> </ul>"},{"location":"reference/ts_shape/features/time_stats/time_stats_numeric/#ts_shape.features.time_stats.time_stats_numeric.TimeGroupedStatistics","title":"TimeGroupedStatistics","text":"<pre><code>TimeGroupedStatistics(dataframe: DataFrame, column_name: str = 'systime')\n</code></pre> <p>               Bases: <code>Base</code></p> <p>A class for calculating time-grouped statistics on numeric data, with class methods to apply various statistical functions.</p> <p>Parameters:</p> <p>Methods:</p> <ul> <li> <code>calculate_custom_func</code>             \u2013              <p>Apply a custom aggregation function on the value column over the grouped time intervals.</p> </li> <li> <code>calculate_statistic</code>             \u2013              <p>Calculate a specified statistic on the value column over the grouped time intervals.</p> </li> <li> <code>calculate_statistics</code>             \u2013              <p>Calculate multiple specified statistics on the value column over the grouped time intervals.</p> </li> <li> <code>get_dataframe</code>             \u2013              <p>Returns the processed DataFrame.</p> </li> </ul> Source code in <code>src/ts_shape/utils/base.py</code> <pre><code>def __init__(self, dataframe: pd.DataFrame, column_name: str = 'systime') -&gt; None:\n    \"\"\"\n    Initializes the Base with a DataFrame, detects time columns, converts them to datetime,\n    and sorts the DataFrame by the specified column (or the detected time column if applicable).\n\n    Args:\n        dataframe (pd.DataFrame): The DataFrame to be processed.\n        column_name (str): The column to sort by. Default is 'systime'. If the column is not found or is not a time column, the class will attempt to detect other time columns.\n    \"\"\"\n    self.dataframe = dataframe.copy()\n\n    # Attempt to convert the specified column_name to datetime if it exists\n    if column_name in self.dataframe.columns:\n        self.dataframe[column_name] = pd.to_datetime(self.dataframe[column_name], errors='coerce')\n    else:\n        # If the column_name is not in the DataFrame, fallback to automatic time detection\n        time_columns = [col for col in self.dataframe.columns if 'time' in col.lower() or 'date' in col.lower()]\n\n        # Convert all detected time columns to datetime, if any\n        for col in time_columns:\n            self.dataframe[col] = pd.to_datetime(self.dataframe[col], errors='coerce')\n\n        # If any time columns are detected, sort by the first one; otherwise, do nothing\n        if time_columns:\n            column_name = time_columns[0]\n\n    # Sort by the datetime column (either specified or detected)\n    if column_name in self.dataframe.columns:\n        self.dataframe = self.dataframe.sort_values(by=column_name)\n</code></pre>"},{"location":"reference/ts_shape/features/time_stats/time_stats_numeric/#ts_shape.features.time_stats.time_stats_numeric.TimeGroupedStatistics(dataframe)","title":"<code>dataframe</code>","text":"(<code>DataFrame</code>)           \u2013            <p>The DataFrame to be processed.</p>"},{"location":"reference/ts_shape/features/time_stats/time_stats_numeric/#ts_shape.features.time_stats.time_stats_numeric.TimeGroupedStatistics(column_name)","title":"<code>column_name</code>","text":"(<code>str</code>, default:                   <code>'systime'</code> )           \u2013            <p>The column to sort by. Default is 'systime'. If the column is not found or is not a time column, the class will attempt to detect other time columns.</p>"},{"location":"reference/ts_shape/features/time_stats/time_stats_numeric/#ts_shape.features.time_stats.time_stats_numeric.TimeGroupedStatistics.calculate_custom_func","title":"calculate_custom_func  <code>classmethod</code>","text":"<pre><code>calculate_custom_func(dataframe: DataFrame, time_column: str, value_column: str, freq: str, func) -&gt; DataFrame\n</code></pre> <p>Apply a custom aggregation function on the value column over the grouped time intervals.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>pd.DataFrame: A DataFrame with the custom calculated statistics.</p> </li> </ul> Source code in <code>src/ts_shape/features/time_stats/time_stats_numeric.py</code> <pre><code>@classmethod\ndef calculate_custom_func(cls, dataframe: pd.DataFrame, time_column: str, value_column: str, freq: str, func) -&gt; pd.DataFrame:\n    \"\"\"\n    Apply a custom aggregation function on the value column over the grouped time intervals.\n\n    Args:\n        dataframe (pd.DataFrame): The DataFrame containing the data.\n        time_column (str): The name of the time column to group and sort by.\n        value_column (str): The name of the numeric column to calculate statistics on.\n        freq (str): Frequency string for time grouping (e.g., 'H' for hourly, 'D' for daily).\n        func (callable): Custom function to apply to each group.\n\n    Returns:\n        pd.DataFrame: A DataFrame with the custom calculated statistics.\n    \"\"\"\n    grouped_df = dataframe.set_index(time_column).resample(freq)\n    result = grouped_df[value_column].apply(func).to_frame('custom')\n    return result\n</code></pre>"},{"location":"reference/ts_shape/features/time_stats/time_stats_numeric/#ts_shape.features.time_stats.time_stats_numeric.TimeGroupedStatistics.calculate_custom_func(dataframe)","title":"<code>dataframe</code>","text":"(<code>DataFrame</code>)           \u2013            <p>The DataFrame containing the data.</p>"},{"location":"reference/ts_shape/features/time_stats/time_stats_numeric/#ts_shape.features.time_stats.time_stats_numeric.TimeGroupedStatistics.calculate_custom_func(time_column)","title":"<code>time_column</code>","text":"(<code>str</code>)           \u2013            <p>The name of the time column to group and sort by.</p>"},{"location":"reference/ts_shape/features/time_stats/time_stats_numeric/#ts_shape.features.time_stats.time_stats_numeric.TimeGroupedStatistics.calculate_custom_func(value_column)","title":"<code>value_column</code>","text":"(<code>str</code>)           \u2013            <p>The name of the numeric column to calculate statistics on.</p>"},{"location":"reference/ts_shape/features/time_stats/time_stats_numeric/#ts_shape.features.time_stats.time_stats_numeric.TimeGroupedStatistics.calculate_custom_func(freq)","title":"<code>freq</code>","text":"(<code>str</code>)           \u2013            <p>Frequency string for time grouping (e.g., 'H' for hourly, 'D' for daily).</p>"},{"location":"reference/ts_shape/features/time_stats/time_stats_numeric/#ts_shape.features.time_stats.time_stats_numeric.TimeGroupedStatistics.calculate_custom_func(func)","title":"<code>func</code>","text":"(<code>callable</code>)           \u2013            <p>Custom function to apply to each group.</p>"},{"location":"reference/ts_shape/features/time_stats/time_stats_numeric/#ts_shape.features.time_stats.time_stats_numeric.TimeGroupedStatistics.calculate_statistic","title":"calculate_statistic  <code>classmethod</code>","text":"<pre><code>calculate_statistic(dataframe: DataFrame, time_column: str, value_column: str, freq: str, stat_method: str) -&gt; DataFrame\n</code></pre> <p>Calculate a specified statistic on the value column over the grouped time intervals.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>pd.DataFrame: A DataFrame with the time intervals and the calculated statistics.</p> </li> </ul> Source code in <code>src/ts_shape/features/time_stats/time_stats_numeric.py</code> <pre><code>@classmethod\ndef calculate_statistic(cls, dataframe: pd.DataFrame, time_column: str, value_column: str, freq: str, stat_method: str) -&gt; pd.DataFrame:\n    \"\"\"\n    Calculate a specified statistic on the value column over the grouped time intervals.\n\n    Args:\n        dataframe (pd.DataFrame): The DataFrame containing the data.\n        time_column (str): The name of the time column to group and sort by.\n        value_column (str): The name of the numeric column to calculate statistics on.\n        freq (str): Frequency string for time grouping (e.g., 'H' for hourly, 'D' for daily).\n        stat_method (str): The statistical method to apply ('mean', 'sum', 'min', 'max', 'diff', 'range').\n\n    Returns:\n        pd.DataFrame: A DataFrame with the time intervals and the calculated statistics.\n    \"\"\"\n    # Set the DataFrame index to the time column and resample to the specified frequency\n    grouped_df = dataframe.set_index(time_column).resample(freq)\n\n    # Select the calculation method\n    if stat_method == 'mean':\n        result = grouped_df[value_column].mean().to_frame('mean')\n    elif stat_method == 'sum':\n        result = grouped_df[value_column].sum().to_frame('sum')\n    elif stat_method == 'min':\n        result = grouped_df[value_column].min().to_frame('min')\n    elif stat_method == 'max':\n        result = grouped_df[value_column].max().to_frame('max')\n    elif stat_method == 'diff':\n        # Improved diff: last value - first value within each interval\n        result = (grouped_df[value_column].last() - grouped_df[value_column].first()).to_frame('difference')\n    elif stat_method == 'range':\n        # Range: max value - min value within each interval\n        result = (grouped_df[value_column].max() - grouped_df[value_column].min()).to_frame('range')\n    else:\n        raise ValueError(\"Invalid stat_method. Choose from 'mean', 'sum', 'min', 'max', 'diff', 'range'.\")\n\n    return result\n</code></pre>"},{"location":"reference/ts_shape/features/time_stats/time_stats_numeric/#ts_shape.features.time_stats.time_stats_numeric.TimeGroupedStatistics.calculate_statistic(dataframe)","title":"<code>dataframe</code>","text":"(<code>DataFrame</code>)           \u2013            <p>The DataFrame containing the data.</p>"},{"location":"reference/ts_shape/features/time_stats/time_stats_numeric/#ts_shape.features.time_stats.time_stats_numeric.TimeGroupedStatistics.calculate_statistic(time_column)","title":"<code>time_column</code>","text":"(<code>str</code>)           \u2013            <p>The name of the time column to group and sort by.</p>"},{"location":"reference/ts_shape/features/time_stats/time_stats_numeric/#ts_shape.features.time_stats.time_stats_numeric.TimeGroupedStatistics.calculate_statistic(value_column)","title":"<code>value_column</code>","text":"(<code>str</code>)           \u2013            <p>The name of the numeric column to calculate statistics on.</p>"},{"location":"reference/ts_shape/features/time_stats/time_stats_numeric/#ts_shape.features.time_stats.time_stats_numeric.TimeGroupedStatistics.calculate_statistic(freq)","title":"<code>freq</code>","text":"(<code>str</code>)           \u2013            <p>Frequency string for time grouping (e.g., 'H' for hourly, 'D' for daily).</p>"},{"location":"reference/ts_shape/features/time_stats/time_stats_numeric/#ts_shape.features.time_stats.time_stats_numeric.TimeGroupedStatistics.calculate_statistic(stat_method)","title":"<code>stat_method</code>","text":"(<code>str</code>)           \u2013            <p>The statistical method to apply ('mean', 'sum', 'min', 'max', 'diff', 'range').</p>"},{"location":"reference/ts_shape/features/time_stats/time_stats_numeric/#ts_shape.features.time_stats.time_stats_numeric.TimeGroupedStatistics.calculate_statistics","title":"calculate_statistics  <code>classmethod</code>","text":"<pre><code>calculate_statistics(dataframe: DataFrame, time_column: str, value_column: str, freq: str, stat_methods: list) -&gt; DataFrame\n</code></pre> <p>Calculate multiple specified statistics on the value column over the grouped time intervals.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>pd.DataFrame: A DataFrame with the time intervals and the calculated statistics for each method.</p> </li> </ul> Source code in <code>src/ts_shape/features/time_stats/time_stats_numeric.py</code> <pre><code>@classmethod\ndef calculate_statistics(cls, dataframe: pd.DataFrame, time_column: str, value_column: str, freq: str, stat_methods: list) -&gt; pd.DataFrame:\n    \"\"\"\n    Calculate multiple specified statistics on the value column over the grouped time intervals.\n\n    Args:\n        dataframe (pd.DataFrame): The DataFrame containing the data.\n        time_column (str): The name of the time column to group and sort by.\n        value_column (str): The name of the numeric column to calculate statistics on.\n        freq (str): Frequency string for time grouping (e.g., 'H' for hourly, 'D' for daily).\n        stat_methods (list): A list of statistical methods to apply (e.g., ['mean', 'sum', 'diff', 'range']).\n\n    Returns:\n        pd.DataFrame: A DataFrame with the time intervals and the calculated statistics for each method.\n    \"\"\"\n    # Initialize an empty DataFrame for combining results\n    result_df = pd.DataFrame()\n\n    # Calculate each requested statistic and join to the result DataFrame\n    for method in stat_methods:\n        stat_df = cls.calculate_statistic(dataframe, time_column, value_column, freq, method)\n        result_df = result_df.join(stat_df, how='outer')\n\n    return result_df\n</code></pre>"},{"location":"reference/ts_shape/features/time_stats/time_stats_numeric/#ts_shape.features.time_stats.time_stats_numeric.TimeGroupedStatistics.calculate_statistics(dataframe)","title":"<code>dataframe</code>","text":"(<code>DataFrame</code>)           \u2013            <p>The DataFrame containing the data.</p>"},{"location":"reference/ts_shape/features/time_stats/time_stats_numeric/#ts_shape.features.time_stats.time_stats_numeric.TimeGroupedStatistics.calculate_statistics(time_column)","title":"<code>time_column</code>","text":"(<code>str</code>)           \u2013            <p>The name of the time column to group and sort by.</p>"},{"location":"reference/ts_shape/features/time_stats/time_stats_numeric/#ts_shape.features.time_stats.time_stats_numeric.TimeGroupedStatistics.calculate_statistics(value_column)","title":"<code>value_column</code>","text":"(<code>str</code>)           \u2013            <p>The name of the numeric column to calculate statistics on.</p>"},{"location":"reference/ts_shape/features/time_stats/time_stats_numeric/#ts_shape.features.time_stats.time_stats_numeric.TimeGroupedStatistics.calculate_statistics(freq)","title":"<code>freq</code>","text":"(<code>str</code>)           \u2013            <p>Frequency string for time grouping (e.g., 'H' for hourly, 'D' for daily).</p>"},{"location":"reference/ts_shape/features/time_stats/time_stats_numeric/#ts_shape.features.time_stats.time_stats_numeric.TimeGroupedStatistics.calculate_statistics(stat_methods)","title":"<code>stat_methods</code>","text":"(<code>list</code>)           \u2013            <p>A list of statistical methods to apply (e.g., ['mean', 'sum', 'diff', 'range']).</p>"},{"location":"reference/ts_shape/features/time_stats/time_stats_numeric/#ts_shape.features.time_stats.time_stats_numeric.TimeGroupedStatistics.get_dataframe","title":"get_dataframe","text":"<pre><code>get_dataframe() -&gt; DataFrame\n</code></pre> <p>Returns the processed DataFrame.</p> Source code in <code>src/ts_shape/utils/base.py</code> <pre><code>def get_dataframe(self) -&gt; pd.DataFrame:\n    \"\"\"Returns the processed DataFrame.\"\"\"\n    return self.dataframe\n</code></pre>"},{"location":"reference/ts_shape/loader/__init__/","title":"init","text":""},{"location":"reference/ts_shape/loader/__init__/#ts_shape.loader","title":"ts_shape.loader","text":"<p>Loaders</p> <p>Load timeseries and metadata from various backends and combine them.</p> <ul> <li>DataIntegratorHybrid: Combine timeseries and metadata from DataFrames or source objects.</li> <li> <p>combine_data: Merge sources on a join key with optional UUID filter.</p> </li> <li> <p>ParquetLoader: Read parquet files from local folder structures.</p> </li> <li>load_all_files: Load all parquet under a base path.</li> <li>load_by_time_range: Load files within YYYY/MM/DD/HH path range.</li> <li>load_by_uuid_list: Load files matching UUIDs in filenames.</li> <li> <p>load_files_by_time_range_and_uuids: Combine time range and UUID filters.</p> </li> <li> <p>S3ProxyDataAccess: Retrieve parquet via an S3-compatible proxy.</p> </li> <li>fetch_data_as_parquet: Save parquet files to a local folder structure.</li> <li> <p>fetch_data_as_dataframe: Return a combined DataFrame.</p> </li> <li> <p>AzureBlobParquetLoader: Load parquet from Azure Blob Storage.</p> </li> <li>load_all_files: Load all parquet under an optional prefix.</li> <li>load_by_time_range: Load hourly folders between start and end.</li> <li>load_files_by_time_range_and_uuids: Load per-hour per-UUID parquet files.</li> <li> <p>list_structure: List folders and files under a prefix.</p> </li> <li> <p>TimescaleDBDataAccess: Stream timeseries from TimescaleDB.</p> </li> <li>fetch_data_as_parquet: Partition-by-hour and write parquet.</li> <li> <p>fetch_data_as_dataframe: Return a combined DataFrame.</p> </li> <li> <p>MetadataJsonLoader: Ingest JSON metadata and flatten config.</p> </li> <li>from_file: Create from file.</li> <li>from_str: Create from string.</li> <li>to_df: Return DataFrame view.</li> <li>head: Preview top rows.</li> <li>get_by_uuid: Access row by UUID.</li> <li>get_by_label: Access row by label.</li> <li>join_with: Join with other DataFrames.</li> <li>filter_by_uuid: Filter by UUID set.</li> <li>filter_by_label: Filter by label set.</li> <li>list_uuids: List UUIDs.</li> <li> <p>list_labels: List non-null labels.</p> </li> <li> <p>DatapointAPI: Retrieve datapoint metadata from a REST API.</p> </li> <li>get_all_uuids: UUIDs per device.</li> <li>get_all_metadata: Metadata per device.</li> <li> <p>display_dataframe: Print DataFrames for devices.</p> </li> <li> <p>DatapointDB: Retrieve datapoint metadata from PostgreSQL.</p> </li> <li>get_all_uuids: UUIDs per device.</li> <li>get_all_metadata: Metadata per device.</li> <li>display_dataframe: Print DataFrames for devices.</li> </ul> <p>Modules:</p> <ul> <li> <code>combine</code>           \u2013            <p>Combine</p> </li> <li> <code>context</code>           \u2013            <p>Context Loaders</p> </li> <li> <code>metadata</code>           \u2013            <p>Metadata Loaders</p> </li> <li> <code>timeseries</code>           \u2013            <p>Timeseries Loaders</p> </li> </ul>"},{"location":"reference/ts_shape/loader/combine/__init__/","title":"init","text":""},{"location":"reference/ts_shape/loader/combine/__init__/#ts_shape.loader.combine","title":"ts_shape.loader.combine","text":"<p>Combine</p> <p>Utilities to combine timeseries and metadata from multiple sources.</p> <ul> <li>DataIntegratorHybrid: Combine timeseries and metadata from DataFrames or source objects.</li> <li>combine_data: Merge sources on a join key with optional UUID filter.</li> </ul> <p>Modules:</p> <ul> <li> <code>integrator</code>           \u2013            </li> </ul>"},{"location":"reference/ts_shape/loader/combine/integrator/","title":"integrator","text":""},{"location":"reference/ts_shape/loader/combine/integrator/#ts_shape.loader.combine.integrator","title":"ts_shape.loader.combine.integrator","text":"<p>Classes:</p> <ul> <li> <code>DataIntegratorHybrid</code>           \u2013            <p>A flexible utility class to integrate data from various sources, including:</p> </li> </ul>"},{"location":"reference/ts_shape/loader/combine/integrator/#ts_shape.loader.combine.integrator.DataIntegratorHybrid","title":"DataIntegratorHybrid","text":"<p>A flexible utility class to integrate data from various sources, including: - API instances (e.g., DatapointAPI) - Direct raw data (e.g., UUID list, metadata, timeseries DataFrame) - Hybrid approaches (combination of instances and raw data)</p> <p>Methods:</p> <ul> <li> <code>combine_data</code>             \u2013              <p>Combine timeseries and metadata from various sources.</p> </li> </ul>"},{"location":"reference/ts_shape/loader/combine/integrator/#ts_shape.loader.combine.integrator.DataIntegratorHybrid.combine_data","title":"combine_data  <code>classmethod</code>","text":"<pre><code>combine_data(timeseries_sources: Optional[List[Union[DataFrame, object]]] = None, metadata_sources: Optional[List[Union[DataFrame, object]]] = None, uuids: Optional[List[str]] = None, join_key: str = 'uuid', merge_how: str = 'left') -&gt; DataFrame\n</code></pre> <p>Combine timeseries and metadata from various sources.</p> <p>:param timeseries_sources: List of timeseries sources (DataFrame or instances with <code>fetch_data_as_dataframe</code>). :param metadata_sources: List of metadata sources (DataFrame or instances with <code>fetch_metadata</code>). :param uuids: Optional list of UUIDs to filter the combined data. :param join_key: Key column to use for merging, default is \"uuid\". :param merge_how: Merge strategy ('left', 'inner', etc.), default is \"left\". :return: A combined DataFrame.</p> Source code in <code>src/ts_shape/loader/combine/integrator.py</code> <pre><code>@classmethod\ndef combine_data(\n    cls,\n    timeseries_sources: Optional[List[Union[pd.DataFrame, object]]] = None,\n    metadata_sources: Optional[List[Union[pd.DataFrame, object]]] = None,\n    uuids: Optional[List[str]] = None,\n    join_key: str = \"uuid\",\n    merge_how: str = \"left\",\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Combine timeseries and metadata from various sources.\n\n    :param timeseries_sources: List of timeseries sources (DataFrame or instances with `fetch_data_as_dataframe`).\n    :param metadata_sources: List of metadata sources (DataFrame or instances with `fetch_metadata`).\n    :param uuids: Optional list of UUIDs to filter the combined data.\n    :param join_key: Key column to use for merging, default is \"uuid\".\n    :param merge_how: Merge strategy ('left', 'inner', etc.), default is \"left\".\n    :return: A combined DataFrame.\n    \"\"\"\n    # Retrieve and combine timeseries data\n    timeseries_data = cls._combine_timeseries(timeseries_sources, join_key)\n\n    if timeseries_data.empty:\n        print(\"No timeseries data found.\")\n        return pd.DataFrame()\n\n    # Retrieve and combine metadata\n    metadata = cls._combine_metadata(metadata_sources, join_key)\n\n    if metadata.empty:\n        print(\"No metadata found.\")\n        return timeseries_data\n\n    missing_timeseries_key = join_key not in timeseries_data.columns\n    missing_metadata_key = join_key not in metadata.columns\n\n    if missing_timeseries_key or missing_metadata_key:\n        missing_parts = []\n        if missing_timeseries_key:\n            missing_parts.append(\"timeseries data\")\n        if missing_metadata_key:\n            missing_parts.append(\"metadata\")\n        print(\n            f\"Cannot merge because join key '{join_key}' is missing in \"\n            f\"{', '.join(missing_parts)}.\"\n        )\n        return timeseries_data\n\n    # Merge timeseries data with metadata\n    combined_data = pd.merge(timeseries_data, metadata, on=join_key, how=merge_how)\n\n    # Optionally filter the combined data by UUIDs\n    if uuids:\n        combined_data = combined_data[combined_data[join_key].isin(uuids)]\n\n    return combined_data\n</code></pre>"},{"location":"reference/ts_shape/loader/context/__init__/","title":"context","text":""},{"location":"reference/ts_shape/loader/context/__init__/#ts_shape.loader.context","title":"ts_shape.loader.context","text":"<p>Context Loaders</p> <p>Helpers for context loading under the loader namespace.</p> <p>Classes: - None yet: Placeholder module for future context loader utilities.</p>"},{"location":"reference/ts_shape/loader/metadata/__init__/","title":"init","text":""},{"location":"reference/ts_shape/loader/metadata/__init__/#ts_shape.loader.metadata","title":"ts_shape.loader.metadata","text":"<p>Metadata Loaders</p> <p>Load and normalize datapoint metadata from JSON, REST APIs, or databases.</p> <ul> <li>MetadataJsonLoader: Normalize metadata JSONs into a UUID-indexed DataFrame.</li> <li>from_file: Create from a JSON file.</li> <li>from_str: Create from a JSON string.</li> <li>to_df: Return DataFrame view.</li> <li>head: Preview top rows.</li> <li>get_by_uuid: Access row by UUID.</li> <li>get_by_label: Access row by label.</li> <li>join_with: Join with other DataFrames.</li> <li>filter_by_uuid: Filter by UUID set.</li> <li>filter_by_label: Filter by label set.</li> <li>list_uuids: List UUIDs.</li> <li> <p>list_labels: List non-null labels.</p> </li> <li> <p>DatapointAPI: Retrieve datapoint metadata from a REST API.</p> </li> <li>get_all_uuids: UUIDs per device.</li> <li>get_all_metadata: Metadata per device.</li> <li> <p>display_dataframe: Print DataFrames for devices.</p> </li> <li> <p>DatapointDB: Retrieve datapoint metadata from PostgreSQL.</p> </li> <li>get_all_uuids: UUIDs per device.</li> <li>get_all_metadata: Metadata per device.</li> <li>display_dataframe: Print DataFrames for devices.</li> </ul> <p>Modules:</p> <ul> <li> <code>metadata_api_loader</code>           \u2013            </li> <li> <code>metadata_db_loader</code>           \u2013            </li> <li> <code>metadata_json_loader</code>           \u2013            </li> </ul>"},{"location":"reference/ts_shape/loader/metadata/metadata_api_loader/","title":"metadata_api_loader","text":""},{"location":"reference/ts_shape/loader/metadata/metadata_api_loader/#ts_shape.loader.metadata.metadata_api_loader","title":"ts_shape.loader.metadata.metadata_api_loader","text":"<p>Classes:</p> <ul> <li> <code>DatapointAPI</code>           \u2013            <p>Class for accessing datapoints for multiple devices via an API.</p> </li> </ul>"},{"location":"reference/ts_shape/loader/metadata/metadata_api_loader/#ts_shape.loader.metadata.metadata_api_loader.DatapointAPI","title":"DatapointAPI","text":"<pre><code>DatapointAPI(device_names: List[str], base_url: str, api_token: str, output_path: str = 'data', required_uuid_list: List[str] = None, filter_enabled: bool = True)\n</code></pre> <p>Class for accessing datapoints for multiple devices via an API.</p> <p>:param device_names: List of device names to retrieve metadata for. :param base_url: Base URL of the API. :param api_token: API token for authentication. :param output_path: Directory to save the data points JSON files. :param required_uuid_list: Mixed list of UUIDs to filter the metadata across devices (optional). :param filter_enabled: Whether to filter metadata by \"enabled == True\" (default is True).</p> <p>Methods:</p> <ul> <li> <code>display_dataframe</code>             \u2013              <p>Print the metadata DataFrame for a specific device or all devices.</p> </li> <li> <code>get_all_metadata</code>             \u2013              <p>Return a dictionary of metadata for each device.</p> </li> <li> <code>get_all_uuids</code>             \u2013              <p>Return a dictionary of UUIDs for each device.</p> </li> </ul> Source code in <code>src/ts_shape/loader/metadata/metadata_api_loader.py</code> <pre><code>def __init__(self, device_names: List[str], base_url: str, api_token: str, output_path: str = \"data\", required_uuid_list: List[str] = None, filter_enabled: bool = True):\n    \"\"\"\n    Initialize the DatapointAPI class.\n\n    :param device_names: List of device names to retrieve metadata for.\n    :param base_url: Base URL of the API.\n    :param api_token: API token for authentication.\n    :param output_path: Directory to save the data points JSON files.\n    :param required_uuid_list: Mixed list of UUIDs to filter the metadata across devices (optional).\n    :param filter_enabled: Whether to filter metadata by \"enabled == True\" (default is True).\n    \"\"\"\n    self.device_names = device_names\n    self.base_url = base_url\n    self.api_token = api_token\n    self.output_path = output_path\n    self.required_uuid_list = required_uuid_list or []  # Defaults to an empty list if None\n    self.filter_enabled = filter_enabled\n    self.device_metadata: Dict[str, pd.DataFrame] = {}  # Store metadata for each device\n    self.device_uuids: Dict[str, List[str]] = {}  # Store UUIDs for each device\n    self._api_access()\n</code></pre>"},{"location":"reference/ts_shape/loader/metadata/metadata_api_loader/#ts_shape.loader.metadata.metadata_api_loader.DatapointAPI.display_dataframe","title":"display_dataframe","text":"<pre><code>display_dataframe(device_name: str = None) -&gt; None\n</code></pre> <p>Print the metadata DataFrame for a specific device or all devices.</p> <p>:param device_name: Name of the device to display metadata for (optional).                     If None, displays metadata for all devices.</p> Source code in <code>src/ts_shape/loader/metadata/metadata_api_loader.py</code> <pre><code>def display_dataframe(self, device_name: str = None) -&gt; None:\n    \"\"\"\n    Print the metadata DataFrame for a specific device or all devices.\n\n    :param device_name: Name of the device to display metadata for (optional).\n                        If None, displays metadata for all devices.\n    \"\"\"\n    if device_name:\n        # Display metadata for a specific device\n        if device_name in self.device_metadata:\n            print(f\"Metadata for device: {device_name}\")\n            print(self.device_metadata[device_name])\n        else:\n            print(f\"No metadata found for device: {device_name}\")\n    else:\n        # Display metadata for all devices\n        for device, metadata in self.device_metadata.items():\n            print(f\"\\nMetadata for device: {device}\")\n            print(metadata)\n</code></pre>"},{"location":"reference/ts_shape/loader/metadata/metadata_api_loader/#ts_shape.loader.metadata.metadata_api_loader.DatapointAPI.get_all_metadata","title":"get_all_metadata","text":"<pre><code>get_all_metadata() -&gt; Dict[str, List[Dict[str, str]]]\n</code></pre> <p>Return a dictionary of metadata for each device.</p> Source code in <code>src/ts_shape/loader/metadata/metadata_api_loader.py</code> <pre><code>def get_all_metadata(self) -&gt; Dict[str, List[Dict[str, str]]]:\n    \"\"\"Return a dictionary of metadata for each device.\"\"\"\n    return {device: metadata.to_dict(orient=\"records\") for device, metadata in self.device_metadata.items()}\n</code></pre>"},{"location":"reference/ts_shape/loader/metadata/metadata_api_loader/#ts_shape.loader.metadata.metadata_api_loader.DatapointAPI.get_all_uuids","title":"get_all_uuids","text":"<pre><code>get_all_uuids() -&gt; Dict[str, List[str]]\n</code></pre> <p>Return a dictionary of UUIDs for each device.</p> Source code in <code>src/ts_shape/loader/metadata/metadata_api_loader.py</code> <pre><code>def get_all_uuids(self) -&gt; Dict[str, List[str]]:\n    \"\"\"Return a dictionary of UUIDs for each device.\"\"\"\n    return self.device_uuids\n</code></pre>"},{"location":"reference/ts_shape/loader/metadata/metadata_db_loader/","title":"metadata_db_loader","text":""},{"location":"reference/ts_shape/loader/metadata/metadata_db_loader/#ts_shape.loader.metadata.metadata_db_loader","title":"ts_shape.loader.metadata.metadata_db_loader","text":"<p>Classes:</p> <ul> <li> <code>DatapointDB</code>           \u2013            <p>Class for accessing datapoints via a database.</p> </li> </ul>"},{"location":"reference/ts_shape/loader/metadata/metadata_db_loader/#ts_shape.loader.metadata.metadata_db_loader.DatapointDB","title":"DatapointDB","text":"<pre><code>DatapointDB(device_names: List[str], db_user: str, db_pass: str, db_host: str, output_path: str = 'data', required_uuid_list: List[str] = None, filter_enabled: bool = True)\n</code></pre> <p>Class for accessing datapoints via a database.</p> <p>:param device_names: List of device names to retrieve metadata for. :param db_user: Database user. :param db_pass: Database password. :param db_host: Database host. :param output_path: Directory to save JSON files. :param required_uuid_list: List of UUIDs to filter the metadata (optional). :param filter_enabled: Whether to filter metadata by \"enabled == True\" and \"archived == False\" (default is True).</p> <p>Methods:</p> <ul> <li> <code>display_dataframe</code>             \u2013              <p>Display metadata as a DataFrame for a specific device or all devices.</p> </li> <li> <code>get_all_metadata</code>             \u2013              <p>Return a dictionary of metadata for each device.</p> </li> <li> <code>get_all_uuids</code>             \u2013              <p>Return a dictionary of UUIDs for each device.</p> </li> </ul> Source code in <code>src/ts_shape/loader/metadata/metadata_db_loader.py</code> <pre><code>def __init__(self, device_names: List[str], db_user: str, db_pass: str, db_host: str, output_path: str = \"data\", required_uuid_list: List[str] = None, filter_enabled: bool = True):\n    \"\"\"\n    Initialize the DatapointDB class.\n\n    :param device_names: List of device names to retrieve metadata for.\n    :param db_user: Database user.\n    :param db_pass: Database password.\n    :param db_host: Database host.\n    :param output_path: Directory to save JSON files.\n    :param required_uuid_list: List of UUIDs to filter the metadata (optional).\n    :param filter_enabled: Whether to filter metadata by \"enabled == True\" and \"archived == False\" (default is True).\n    \"\"\"\n    self.device_names = device_names\n    self.db_user = db_user\n    self.db_pass = db_pass\n    self.db_host = db_host\n    self.output_path = output_path\n    self.required_uuid_list = required_uuid_list or []\n    self.filter_enabled = filter_enabled\n    self.device_metadata: Dict[str, pd.DataFrame] = {}  # Store metadata for each device\n    self.device_uuids: Dict[str, List[str]] = {}  # Store UUIDs for each device\n    self._db_access()\n</code></pre>"},{"location":"reference/ts_shape/loader/metadata/metadata_db_loader/#ts_shape.loader.metadata.metadata_db_loader.DatapointDB.display_dataframe","title":"display_dataframe","text":"<pre><code>display_dataframe(device_name: str = None, aggregate: bool = False) -&gt; None\n</code></pre> <p>Display metadata as a DataFrame for a specific device or all devices.</p> <p>:param device_name: Name of the device to display metadata for (optional). :param aggregate: If True, combine metadata from all devices into a single DataFrame.</p> Source code in <code>src/ts_shape/loader/metadata/metadata_db_loader.py</code> <pre><code>def display_dataframe(self, device_name: str = None, aggregate: bool = False) -&gt; None:\n    \"\"\"\n    Display metadata as a DataFrame for a specific device or all devices.\n\n    :param device_name: Name of the device to display metadata for (optional).\n    :param aggregate: If True, combine metadata from all devices into a single DataFrame.\n    \"\"\"\n    if aggregate:\n        combined_df = pd.concat(self.device_metadata.values(), keys=self.device_metadata.keys())\n        print(\"Aggregated metadata for all devices:\")\n        print(combined_df)\n    elif device_name:\n        if device_name in self.device_metadata:\n            print(f\"Metadata for device: {device_name}\")\n            print(self.device_metadata[device_name])\n        else:\n            print(f\"No metadata found for device: {device_name}\")\n    else:\n        for device, metadata in self.device_metadata.items():\n            print(f\"\\nMetadata for device: {device}\")\n            print(metadata)\n</code></pre>"},{"location":"reference/ts_shape/loader/metadata/metadata_db_loader/#ts_shape.loader.metadata.metadata_db_loader.DatapointDB.get_all_metadata","title":"get_all_metadata","text":"<pre><code>get_all_metadata() -&gt; Dict[str, List[Dict[str, str]]]\n</code></pre> <p>Return a dictionary of metadata for each device.</p> Source code in <code>src/ts_shape/loader/metadata/metadata_db_loader.py</code> <pre><code>def get_all_metadata(self) -&gt; Dict[str, List[Dict[str, str]]]:\n    \"\"\"Return a dictionary of metadata for each device.\"\"\"\n    return {device: metadata.to_dict(orient=\"records\") for device, metadata in self.device_metadata.items()}\n</code></pre>"},{"location":"reference/ts_shape/loader/metadata/metadata_db_loader/#ts_shape.loader.metadata.metadata_db_loader.DatapointDB.get_all_uuids","title":"get_all_uuids","text":"<pre><code>get_all_uuids() -&gt; Dict[str, List[str]]\n</code></pre> <p>Return a dictionary of UUIDs for each device.</p> Source code in <code>src/ts_shape/loader/metadata/metadata_db_loader.py</code> <pre><code>def get_all_uuids(self) -&gt; Dict[str, List[str]]:\n    \"\"\"Return a dictionary of UUIDs for each device.\"\"\"\n    return self.device_uuids\n</code></pre>"},{"location":"reference/ts_shape/loader/metadata/metadata_json_loader/","title":"metadata_json_loader","text":""},{"location":"reference/ts_shape/loader/metadata/metadata_json_loader/#ts_shape.loader.metadata.metadata_json_loader","title":"ts_shape.loader.metadata.metadata_json_loader","text":"<p>Classes:</p> <ul> <li> <code>MetadataJsonLoader</code>           \u2013            <p>Load metadata JSON of shape:</p> </li> </ul>"},{"location":"reference/ts_shape/loader/metadata/metadata_json_loader/#ts_shape.loader.metadata.metadata_json_loader.MetadataJsonLoader","title":"MetadataJsonLoader","text":"<pre><code>MetadataJsonLoader(json_data: Any, *, strict: bool = True)\n</code></pre> Load metadata JSON of shape <p>{   \"uuid\":   {\"0\": \"...\", \"1\": \"...\", ...},   \"label\":  {\"0\": \"...\", \"1\": \"...\", ...},   \"config\": {\"0\": {...},  \"1\": {...},  ...} }</p> <p>into a pandas DataFrame with flattened config columns.</p> <p>Parameters:</p> <p>Methods:</p> <ul> <li> <code>filter_by_label</code>             \u2013              <p>Filter rows by a set or sequence of labels.</p> </li> <li> <code>filter_by_uuid</code>             \u2013              <p>Filter rows by a set or sequence of UUIDs.</p> </li> <li> <code>from_file</code>             \u2013              <p>Create a loader from a JSON file on disk.</p> </li> <li> <code>from_str</code>             \u2013              <p>Create a loader from a JSON string.</p> </li> <li> <code>get_by_label</code>             \u2013              <p>Retrieve the first row matching a label as a dictionary.</p> </li> <li> <code>get_by_uuid</code>             \u2013              <p>Retrieve a row by UUID as a dictionary.</p> </li> <li> <code>head</code>             \u2013              <p>Convenience wrapper for DataFrame.head.</p> </li> <li> <code>join_with</code>             \u2013              <p>Join the metadata DataFrame with another DataFrame on the 'uuid' index.</p> </li> <li> <code>list_labels</code>             \u2013              <p>Return all non-null labels.</p> </li> <li> <code>list_uuids</code>             \u2013              <p>Return a list of UUIDs present in the metadata index.</p> </li> <li> <code>to_df</code>             \u2013              <p>Return the underlying DataFrame.</p> </li> </ul> Source code in <code>src/ts_shape/loader/metadata/metadata_json_loader.py</code> <pre><code>def __init__(self, json_data: Any, *, strict: bool = True):\n    \"\"\"\n    Create a loader from JSON-like data.\n\n    Args:\n        json_data: Supported shapes:\n          - dict of columns with index-maps: {\"uuid\": {\"0\": ...}, \"label\": {...}, \"config\": {...}}\n          - dict of columns with lists: {\"uuid\": [...], \"label\": [...], \"config\": [...]}\n          - list of records: [{\"uuid\": ..., \"label\": ..., \"config\": {...}}, ...]\n        strict: If True, enforce presence of required keys and unique UUIDs.\n    \"\"\"\n    self.json_data = json_data\n    self.strict = strict\n    self.df = self._to_dataframe()\n</code></pre>"},{"location":"reference/ts_shape/loader/metadata/metadata_json_loader/#ts_shape.loader.metadata.metadata_json_loader.MetadataJsonLoader(json_data)","title":"<code>json_data</code>","text":"(<code>Any</code>)           \u2013            <p>Supported shapes: - dict of columns with index-maps: {\"uuid\": {\"0\": ...}, \"label\": {...}, \"config\": {...}} - dict of columns with lists: {\"uuid\": [...], \"label\": [...], \"config\": [...]} - list of records: [{\"uuid\": ..., \"label\": ..., \"config\": {...}}, ...]</p>"},{"location":"reference/ts_shape/loader/metadata/metadata_json_loader/#ts_shape.loader.metadata.metadata_json_loader.MetadataJsonLoader(strict)","title":"<code>strict</code>","text":"(<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, enforce presence of required keys and unique UUIDs.</p>"},{"location":"reference/ts_shape/loader/metadata/metadata_json_loader/#ts_shape.loader.metadata.metadata_json_loader.MetadataJsonLoader.filter_by_label","title":"filter_by_label","text":"<pre><code>filter_by_label(labels: Iterable[str]) -&gt; DataFrame\n</code></pre> <p>Filter rows by a set or sequence of labels.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>Filtered DataFrame.</p> </li> </ul> Source code in <code>src/ts_shape/loader/metadata/metadata_json_loader.py</code> <pre><code>def filter_by_label(self, labels: Iterable[str]) -&gt; pd.DataFrame:\n    \"\"\"\n    Filter rows by a set or sequence of labels.\n\n    Args:\n        labels: Iterable of label strings to retain.\n\n    Returns:\n        Filtered DataFrame.\n    \"\"\"\n    labels_set = set(labels)\n    return self.df[self.df[\"label\"].isin(labels_set)]\n</code></pre>"},{"location":"reference/ts_shape/loader/metadata/metadata_json_loader/#ts_shape.loader.metadata.metadata_json_loader.MetadataJsonLoader.filter_by_label(labels)","title":"<code>labels</code>","text":"(<code>Iterable[str]</code>)           \u2013            <p>Iterable of label strings to retain.</p>"},{"location":"reference/ts_shape/loader/metadata/metadata_json_loader/#ts_shape.loader.metadata.metadata_json_loader.MetadataJsonLoader.filter_by_uuid","title":"filter_by_uuid","text":"<pre><code>filter_by_uuid(uuids: Iterable[str]) -&gt; DataFrame\n</code></pre> <p>Filter rows by a set or sequence of UUIDs.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>Filtered DataFrame.</p> </li> </ul> Source code in <code>src/ts_shape/loader/metadata/metadata_json_loader.py</code> <pre><code>def filter_by_uuid(self, uuids: Iterable[str]) -&gt; pd.DataFrame:\n    \"\"\"\n    Filter rows by a set or sequence of UUIDs.\n\n    Args:\n        uuids: Iterable of UUID strings to retain.\n\n    Returns:\n        Filtered DataFrame.\n    \"\"\"\n    uuids_set = set(uuids)\n    return self.df[self.df.index.isin(uuids_set)]\n</code></pre>"},{"location":"reference/ts_shape/loader/metadata/metadata_json_loader/#ts_shape.loader.metadata.metadata_json_loader.MetadataJsonLoader.filter_by_uuid(uuids)","title":"<code>uuids</code>","text":"(<code>Iterable[str]</code>)           \u2013            <p>Iterable of UUID strings to retain.</p>"},{"location":"reference/ts_shape/loader/metadata/metadata_json_loader/#ts_shape.loader.metadata.metadata_json_loader.MetadataJsonLoader.from_file","title":"from_file  <code>classmethod</code>","text":"<pre><code>from_file(filepath: str, *, strict: bool = True) -&gt; MetadataJsonLoader\n</code></pre> <p>Create a loader from a JSON file on disk.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>MetadataJsonLoader</code>           \u2013            <p>MetadataJsonLoader instance.</p> </li> </ul> Source code in <code>src/ts_shape/loader/metadata/metadata_json_loader.py</code> <pre><code>@classmethod\ndef from_file(cls, filepath: str, *, strict: bool = True) -&gt; \"MetadataJsonLoader\":\n    \"\"\"\n    Create a loader from a JSON file on disk.\n\n    Args:\n        filepath: Path to the JSON file.\n        strict: Validation behavior; when True, enforces required fields and unique UUIDs.\n\n    Returns:\n        MetadataJsonLoader instance.\n    \"\"\"\n    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n        data = json.load(f)\n    return cls(data, strict=strict)\n</code></pre>"},{"location":"reference/ts_shape/loader/metadata/metadata_json_loader/#ts_shape.loader.metadata.metadata_json_loader.MetadataJsonLoader.from_file(filepath)","title":"<code>filepath</code>","text":"(<code>str</code>)           \u2013            <p>Path to the JSON file.</p>"},{"location":"reference/ts_shape/loader/metadata/metadata_json_loader/#ts_shape.loader.metadata.metadata_json_loader.MetadataJsonLoader.from_file(strict)","title":"<code>strict</code>","text":"(<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Validation behavior; when True, enforces required fields and unique UUIDs.</p>"},{"location":"reference/ts_shape/loader/metadata/metadata_json_loader/#ts_shape.loader.metadata.metadata_json_loader.MetadataJsonLoader.from_str","title":"from_str  <code>classmethod</code>","text":"<pre><code>from_str(json_str: str, *, strict: bool = True) -&gt; MetadataJsonLoader\n</code></pre> <p>Create a loader from a JSON string.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>MetadataJsonLoader</code>           \u2013            <p>MetadataJsonLoader instance.</p> </li> </ul> Source code in <code>src/ts_shape/loader/metadata/metadata_json_loader.py</code> <pre><code>@classmethod\ndef from_str(cls, json_str: str, *, strict: bool = True) -&gt; \"MetadataJsonLoader\":\n    \"\"\"\n    Create a loader from a JSON string.\n\n    Args:\n        json_str: Raw JSON content as a string.\n        strict: Validation behavior; when True, enforces required fields and unique UUIDs.\n\n    Returns:\n        MetadataJsonLoader instance.\n    \"\"\"\n    return cls(json.loads(json_str), strict=strict)\n</code></pre>"},{"location":"reference/ts_shape/loader/metadata/metadata_json_loader/#ts_shape.loader.metadata.metadata_json_loader.MetadataJsonLoader.from_str(json_str)","title":"<code>json_str</code>","text":"(<code>str</code>)           \u2013            <p>Raw JSON content as a string.</p>"},{"location":"reference/ts_shape/loader/metadata/metadata_json_loader/#ts_shape.loader.metadata.metadata_json_loader.MetadataJsonLoader.from_str(strict)","title":"<code>strict</code>","text":"(<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Validation behavior; when True, enforces required fields and unique UUIDs.</p>"},{"location":"reference/ts_shape/loader/metadata/metadata_json_loader/#ts_shape.loader.metadata.metadata_json_loader.MetadataJsonLoader.get_by_label","title":"get_by_label","text":"<pre><code>get_by_label(label: str) -&gt; Optional[Dict[str, Any]]\n</code></pre> <p>Retrieve the first row matching a label as a dictionary.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>Optional[Dict[str, Any]]</code>           \u2013            <p>Row as a dict, or None if not found.</p> </li> </ul> Source code in <code>src/ts_shape/loader/metadata/metadata_json_loader.py</code> <pre><code>def get_by_label(self, label: str) -&gt; Optional[Dict[str, Any]]:\n    \"\"\"\n    Retrieve the first row matching a label as a dictionary.\n\n    Args:\n        label: Label value to search for.\n\n    Returns:\n        Row as a dict, or None if not found.\n    \"\"\"\n    row = self.df[self.df[\"label\"] == label]\n    return None if row.empty else row.iloc[0].to_dict()\n</code></pre>"},{"location":"reference/ts_shape/loader/metadata/metadata_json_loader/#ts_shape.loader.metadata.metadata_json_loader.MetadataJsonLoader.get_by_label(label)","title":"<code>label</code>","text":"(<code>str</code>)           \u2013            <p>Label value to search for.</p>"},{"location":"reference/ts_shape/loader/metadata/metadata_json_loader/#ts_shape.loader.metadata.metadata_json_loader.MetadataJsonLoader.get_by_uuid","title":"get_by_uuid","text":"<pre><code>get_by_uuid(uuid: str) -&gt; Optional[Dict[str, Any]]\n</code></pre> <p>Retrieve a row by UUID as a dictionary.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>Optional[Dict[str, Any]]</code>           \u2013            <p>Row as a dict, or None if not present.</p> </li> </ul> Source code in <code>src/ts_shape/loader/metadata/metadata_json_loader.py</code> <pre><code>def get_by_uuid(self, uuid: str) -&gt; Optional[Dict[str, Any]]:\n    \"\"\"\n    Retrieve a row by UUID as a dictionary.\n\n    Args:\n        uuid: UUID key (index) to look up.\n\n    Returns:\n        Row as a dict, or None if not present.\n    \"\"\"\n    if uuid not in self.df.index:\n        return None\n    return self.df.loc[uuid].to_dict()\n</code></pre>"},{"location":"reference/ts_shape/loader/metadata/metadata_json_loader/#ts_shape.loader.metadata.metadata_json_loader.MetadataJsonLoader.get_by_uuid(uuid)","title":"<code>uuid</code>","text":"(<code>str</code>)           \u2013            <p>UUID key (index) to look up.</p>"},{"location":"reference/ts_shape/loader/metadata/metadata_json_loader/#ts_shape.loader.metadata.metadata_json_loader.MetadataJsonLoader.head","title":"head","text":"<pre><code>head(n: int = 5) -&gt; DataFrame\n</code></pre> <p>Convenience wrapper for DataFrame.head.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>Top n rows of the metadata DataFrame.</p> </li> </ul> Source code in <code>src/ts_shape/loader/metadata/metadata_json_loader.py</code> <pre><code>def head(self, n: int = 5) -&gt; pd.DataFrame:\n    \"\"\"\n    Convenience wrapper for DataFrame.head.\n\n    Args:\n        n: Number of rows to return.\n\n    Returns:\n        Top n rows of the metadata DataFrame.\n    \"\"\"\n    return self.df.head(n)\n</code></pre>"},{"location":"reference/ts_shape/loader/metadata/metadata_json_loader/#ts_shape.loader.metadata.metadata_json_loader.MetadataJsonLoader.head(n)","title":"<code>n</code>","text":"(<code>int</code>, default:                   <code>5</code> )           \u2013            <p>Number of rows to return.</p>"},{"location":"reference/ts_shape/loader/metadata/metadata_json_loader/#ts_shape.loader.metadata.metadata_json_loader.MetadataJsonLoader.join_with","title":"join_with","text":"<pre><code>join_with(other_df: DataFrame, how: str = 'inner') -&gt; DataFrame\n</code></pre> <p>Join the metadata DataFrame with another DataFrame on the 'uuid' index.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>Joined pandas DataFrame.</p> </li> </ul> Source code in <code>src/ts_shape/loader/metadata/metadata_json_loader.py</code> <pre><code>def join_with(self, other_df: pd.DataFrame, how: str = \"inner\") -&gt; pd.DataFrame:\n    \"\"\"\n    Join the metadata DataFrame with another DataFrame on the 'uuid' index.\n\n    Args:\n        other_df: DataFrame to join with (must be indexed compatibly).\n        how: Join strategy (e.g., 'inner', 'left', 'outer').\n\n    Returns:\n        Joined pandas DataFrame.\n    \"\"\"\n    return self.df.join(other_df, how=how)\n</code></pre>"},{"location":"reference/ts_shape/loader/metadata/metadata_json_loader/#ts_shape.loader.metadata.metadata_json_loader.MetadataJsonLoader.join_with(other_df)","title":"<code>other_df</code>","text":"(<code>DataFrame</code>)           \u2013            <p>DataFrame to join with (must be indexed compatibly).</p>"},{"location":"reference/ts_shape/loader/metadata/metadata_json_loader/#ts_shape.loader.metadata.metadata_json_loader.MetadataJsonLoader.join_with(how)","title":"<code>how</code>","text":"(<code>str</code>, default:                   <code>'inner'</code> )           \u2013            <p>Join strategy (e.g., 'inner', 'left', 'outer').</p>"},{"location":"reference/ts_shape/loader/metadata/metadata_json_loader/#ts_shape.loader.metadata.metadata_json_loader.MetadataJsonLoader.list_labels","title":"list_labels","text":"<pre><code>list_labels() -&gt; List[str]\n</code></pre> <p>Return all non-null labels.</p> <p>Returns:</p> <ul> <li> <code>List[str]</code>           \u2013            <p>List of label strings.</p> </li> </ul> Source code in <code>src/ts_shape/loader/metadata/metadata_json_loader.py</code> <pre><code>def list_labels(self) -&gt; List[str]:\n    \"\"\"\n    Return all non-null labels.\n\n    Returns:\n        List of label strings.\n    \"\"\"\n    return list(self.df[\"label\"].dropna().astype(str))\n</code></pre>"},{"location":"reference/ts_shape/loader/metadata/metadata_json_loader/#ts_shape.loader.metadata.metadata_json_loader.MetadataJsonLoader.list_uuids","title":"list_uuids","text":"<pre><code>list_uuids() -&gt; List[str]\n</code></pre> <p>Return a list of UUIDs present in the metadata index.</p> <p>Returns:</p> <ul> <li> <code>List[str]</code>           \u2013            <p>List of UUID strings.</p> </li> </ul> Source code in <code>src/ts_shape/loader/metadata/metadata_json_loader.py</code> <pre><code>def list_uuids(self) -&gt; List[str]:\n    \"\"\"\n    Return a list of UUIDs present in the metadata index.\n\n    Returns:\n        List of UUID strings.\n    \"\"\"\n    return list(self.df.index.astype(str))\n</code></pre>"},{"location":"reference/ts_shape/loader/metadata/metadata_json_loader/#ts_shape.loader.metadata.metadata_json_loader.MetadataJsonLoader.to_df","title":"to_df","text":"<pre><code>to_df(copy: bool = True) -&gt; DataFrame\n</code></pre> <p>Return the underlying DataFrame.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>pandas DataFrame indexed by 'uuid'.</p> </li> </ul> Source code in <code>src/ts_shape/loader/metadata/metadata_json_loader.py</code> <pre><code>def to_df(self, copy: bool = True) -&gt; pd.DataFrame:\n    \"\"\"\n    Return the underlying DataFrame.\n\n    Args:\n        copy: When True, returns a copy; otherwise returns a view/reference.\n\n    Returns:\n        pandas DataFrame indexed by 'uuid'.\n    \"\"\"\n    return self.df.copy() if copy else self.df\n</code></pre>"},{"location":"reference/ts_shape/loader/metadata/metadata_json_loader/#ts_shape.loader.metadata.metadata_json_loader.MetadataJsonLoader.to_df(copy)","title":"<code>copy</code>","text":"(<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>When True, returns a copy; otherwise returns a view/reference.</p>"},{"location":"reference/ts_shape/loader/timeseries/__init__/","title":"init","text":""},{"location":"reference/ts_shape/loader/timeseries/__init__/#ts_shape.loader.timeseries","title":"ts_shape.loader.timeseries","text":"<p>Timeseries Loaders</p> <p>Load timeseries data from parquet folders, S3-compatible stores, Azure Blob, and TimescaleDB.</p> <ul> <li>ParquetLoader: Read parquet files from local folder structures.</li> <li>load_all_files: Load all parquet under a base path.</li> <li>load_by_time_range: Load files within YYYY/MM/DD/HH path range.</li> <li>load_by_uuid_list: Load files matching UUIDs in filenames.</li> <li> <p>load_files_by_time_range_and_uuids: Combine time range and UUID filters.</p> </li> <li> <p>S3ProxyDataAccess: Retrieve parquet via an S3-compatible proxy.</p> </li> <li>fetch_data_as_parquet: Save parquet files to a local folder structure.</li> <li> <p>fetch_data_as_dataframe: Return a combined DataFrame.</p> </li> <li> <p>AzureBlobParquetLoader: Load parquet from Azure Blob Storage.</p> </li> <li>load_all_files: Load all parquet under an optional prefix.</li> <li>load_by_time_range: Load hourly folders between start and end.</li> <li>stream_by_time_range: Yield (blob, DataFrame) incrementally.</li> <li>load_files_by_time_range_and_uuids: Load per-hour per-UUID parquet files.</li> <li>stream_files_by_time_range_and_uuids: Yield per-UUID frames incrementally.</li> <li> <p>list_structure: List folders and files under a prefix.</p> </li> <li> <p>AzureBlobFlexibleFileLoader: Load arbitrary file types from Azure Blob Storage.</p> </li> <li>list_files_by_time_range: List matching files (by extension) under hourly folders.</li> <li>iter_file_names_by_time_range: Generator of names without downloading.</li> <li>fetch_files_by_time_range: Download matching files as raw bytes or parsed objects.</li> <li>stream_files_by_time_range: Stream (blob, bytes/parsed) incrementally.</li> <li>fetch_files_by_time_range_and_basenames: Download by explicit basenames.</li> <li>stream_files_by_time_range_and_basenames: Stream by explicit basenames.</li> <li> <p>register_parser/unregister_parser: Plug-in parser functions per file extension.</p> </li> <li> <p>TimescaleDBDataAccess: Stream timeseries from TimescaleDB.</p> </li> <li>fetch_data_as_parquet: Partition-by-hour and write parquet.</li> <li>fetch_data_as_dataframe: Return a combined DataFrame.</li> </ul> <p>Modules:</p> <ul> <li> <code>azure_blob_loader</code>           \u2013            </li> <li> <code>parquet_loader</code>           \u2013            </li> <li> <code>s3proxy_parquet_loader</code>           \u2013            </li> <li> <code>timescale_loader</code>           \u2013            </li> </ul>"},{"location":"reference/ts_shape/loader/timeseries/azure_blob_loader/","title":"azure_blob_loader","text":""},{"location":"reference/ts_shape/loader/timeseries/azure_blob_loader/#ts_shape.loader.timeseries.azure_blob_loader","title":"ts_shape.loader.timeseries.azure_blob_loader","text":"<p>Classes:</p> <ul> <li> <code>AzureBlobFlexibleFileLoader</code>           \u2013            <p>Load arbitrary file types from Azure Blob Storage under time-structured folders.</p> </li> <li> <code>AzureBlobParquetLoader</code>           \u2013            <p>Load parquet files from an Azure Blob Storage container filtered by a list of UUIDs.</p> </li> </ul>"},{"location":"reference/ts_shape/loader/timeseries/azure_blob_loader/#ts_shape.loader.timeseries.azure_blob_loader.AzureBlobFlexibleFileLoader","title":"AzureBlobFlexibleFileLoader","text":"<pre><code>AzureBlobFlexibleFileLoader(container_name: str, *, connection_string: Optional[str] = None, account_url: Optional[str] = None, credential: Optional[object] = None, prefix: str = '', max_workers: int = 8, hour_pattern: str = '{Y}/{m}/{d}/{H}/')\n</code></pre> <p>Load arbitrary file types from Azure Blob Storage under time-structured folders.</p> <p>Designed for containers with paths like: prefix/YYYY/MM/DD/HH//file.ext This class lists by per-hour prefix and can filter by extensions and/or basenames, then downloads files concurrently as raw bytes. <p>Methods:</p> <ul> <li> <code>fetch_files_by_time_range</code>             \u2013              <p>Download files that match extensions within [start, end] hour prefixes.</p> </li> <li> <code>fetch_files_by_time_range_and_basenames</code>             \u2013              <p>Download files whose basename (final path segment) is in <code>basenames</code>,</p> </li> <li> <code>iter_file_names_by_time_range</code>             \u2013              <p>Yield blob names under each hourly prefix within [start, end].</p> </li> <li> <code>list_files_by_time_range</code>             \u2013              <p>List blob names under each hourly prefix within [start, end].</p> </li> <li> <code>stream_files_by_time_range</code>             \u2013              <p>Stream matching files as (blob_name, bytes-or-parsed) within [start, end].</p> </li> <li> <code>stream_files_by_time_range_and_basenames</code>             \u2013              <p>Stream files whose basename is in <code>basenames</code> within [start, end].</p> </li> </ul> Source code in <code>src/ts_shape/loader/timeseries/azure_blob_loader.py</code> <pre><code>def __init__(\n    self,\n    container_name: str,\n    *,\n    connection_string: Optional[str] = None,\n    account_url: Optional[str] = None,\n    credential: Optional[object] = None,\n    prefix: str = \"\",\n    max_workers: int = 8,\n    hour_pattern: str = \"{Y}/{m}/{d}/{H}/\",\n) -&gt; None:\n    try:\n        from azure.storage.blob import ContainerClient  # type: ignore\n    except Exception as exc:  # pragma: no cover - import guard\n        raise ImportError(\n            \"azure-storage-blob is required for AzureBlobFlexibleFileLoader. \"\n            \"Install with `pip install azure-storage-blob`.\"\n        ) from exc\n\n    # Prefer AAD credential path if account_url provided or credential is given\n    if account_url or (credential is not None and not connection_string):\n        if not account_url:\n            raise ValueError(\"account_url must be provided when using AAD credential auth\")\n        if credential is None:\n            raise ValueError(\"credential must be provided when using AAD credential auth\")\n        self.container_client = ContainerClient(account_url=account_url, container_name=container_name, credential=credential)\n    else:\n        if not connection_string:\n            raise ValueError(\"Either connection_string or (account_url + credential) must be provided\")\n        self.container_client = ContainerClient.from_connection_string(\n            conn_str=connection_string, container_name=container_name\n        )\n    self.prefix = prefix\n    self.max_workers = max_workers if max_workers &gt; 0 else 1\n    # Pattern for hour-level subpath; tokens: {Y} {m} {d} {H}\n    self.hour_pattern = hour_pattern\n    # Initialize parsers lazily once per process\n    if not AzureBlobFlexibleFileLoader._parsers_initialized:\n        self._enable_builtin_parsers()\n        AzureBlobFlexibleFileLoader._parsers_initialized = True\n</code></pre>"},{"location":"reference/ts_shape/loader/timeseries/azure_blob_loader/#ts_shape.loader.timeseries.azure_blob_loader.AzureBlobFlexibleFileLoader.fetch_files_by_time_range","title":"fetch_files_by_time_range","text":"<pre><code>fetch_files_by_time_range(start_timestamp: str | Timestamp, end_timestamp: str | Timestamp, *, extensions: Optional[Iterable[str]] = None, parse: bool = False) -&gt; Dict[str, Any]\n</code></pre> <p>Download files that match extensions within [start, end] hour prefixes. Returns a dict mapping blob_name -&gt; parsed object (if parse=True and a parser exists), otherwise raw bytes.</p> Source code in <code>src/ts_shape/loader/timeseries/azure_blob_loader.py</code> <pre><code>def fetch_files_by_time_range(\n    self,\n    start_timestamp: str | pd.Timestamp,\n    end_timestamp: str | pd.Timestamp,\n    *,\n    extensions: Optional[Iterable[str]] = None,\n    parse: bool = False,\n) -&gt; Dict[str, Any]:\n    \"\"\"\n    Download files that match extensions within [start, end] hour prefixes.\n    Returns a dict mapping blob_name -&gt; parsed object (if parse=True and a parser exists),\n    otherwise raw bytes.\n    \"\"\"\n    blob_names = self.list_files_by_time_range(start_timestamp, end_timestamp, extensions=extensions)\n    if not blob_names:\n        return {}\n    results: Dict[str, Any] = {}\n    with ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n        future_to_name = {executor.submit(self._download_bytes, n): n for n in blob_names}\n        for fut in as_completed(future_to_name):\n            name = future_to_name[fut]\n            content = fut.result()\n            if content is not None:\n                results[name] = self._parse_bytes(name, content) if parse else content\n    return results\n</code></pre>"},{"location":"reference/ts_shape/loader/timeseries/azure_blob_loader/#ts_shape.loader.timeseries.azure_blob_loader.AzureBlobFlexibleFileLoader.fetch_files_by_time_range_and_basenames","title":"fetch_files_by_time_range_and_basenames","text":"<pre><code>fetch_files_by_time_range_and_basenames(start_timestamp: str | Timestamp, end_timestamp: str | Timestamp, basenames: Iterable[str], *, extensions: Optional[Iterable[str]] = None, parse: bool = False) -&gt; Dict[str, Any]\n</code></pre> <p>Download files whose basename (final path segment) is in <code>basenames</code>, optionally filtered by extensions, within [start, end] hour prefixes. Returns blob_name -&gt; parsed object (if parse=True and a parser exists), otherwise raw bytes.</p> Source code in <code>src/ts_shape/loader/timeseries/azure_blob_loader.py</code> <pre><code>def fetch_files_by_time_range_and_basenames(\n    self,\n    start_timestamp: str | pd.Timestamp,\n    end_timestamp: str | pd.Timestamp,\n    basenames: Iterable[str],\n    *,\n    extensions: Optional[Iterable[str]] = None,\n    parse: bool = False,\n) -&gt; Dict[str, Any]:\n    \"\"\"\n    Download files whose basename (final path segment) is in `basenames`,\n    optionally filtered by extensions, within [start, end] hour prefixes.\n    Returns blob_name -&gt; parsed object (if parse=True and a parser exists), otherwise raw bytes.\n    \"\"\"\n    base_set = {str(b).strip() for b in basenames if str(b).strip()}\n    allowed_exts = self._normalize_exts(extensions)\n    candidates: List[str] = []\n    for pfx in (self._hour_prefix(ts) for ts in self._hourly_slots(start_timestamp, end_timestamp)):\n        blob_iter = self.container_client.list_blobs(name_starts_with=pfx)\n        for b in blob_iter:  # type: ignore[attr-defined]\n            name = str(b.name)\n            base = name.rsplit('/', 1)[-1]\n            if base not in base_set:\n                continue\n            if allowed_exts is not None and not any(name.lower().endswith(ext) for ext in allowed_exts):\n                continue\n            candidates.append(name)\n\n    if not candidates:\n        return {}\n\n    results: Dict[str, Any] = {}\n    with ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n        future_to_name = {executor.submit(self._download_bytes, n): n for n in candidates}\n        for fut in as_completed(future_to_name):\n            name = future_to_name[fut]\n            content = fut.result()\n            if content is not None:\n                results[name] = self._parse_bytes(name, content) if parse else content\n    return results\n</code></pre>"},{"location":"reference/ts_shape/loader/timeseries/azure_blob_loader/#ts_shape.loader.timeseries.azure_blob_loader.AzureBlobFlexibleFileLoader.iter_file_names_by_time_range","title":"iter_file_names_by_time_range","text":"<pre><code>iter_file_names_by_time_range(start_timestamp: str | Timestamp, end_timestamp: str | Timestamp, *, extensions: Optional[Iterable[str]] = None) -&gt; Iterator[str]\n</code></pre> <p>Yield blob names under each hourly prefix within [start, end]. Uses server-side prefix listing and client-side extension filtering.</p> Source code in <code>src/ts_shape/loader/timeseries/azure_blob_loader.py</code> <pre><code>def iter_file_names_by_time_range(\n    self,\n    start_timestamp: str | pd.Timestamp,\n    end_timestamp: str | pd.Timestamp,\n    *,\n    extensions: Optional[Iterable[str]] = None,\n) -&gt; Iterator[str]:\n    \"\"\"\n    Yield blob names under each hourly prefix within [start, end].\n    Uses server-side prefix listing and client-side extension filtering.\n    \"\"\"\n    allowed_exts = self._normalize_exts(extensions)\n    for pfx in (self._hour_prefix(ts) for ts in self._hourly_slots(start_timestamp, end_timestamp)):\n        blob_iter = self.container_client.list_blobs(name_starts_with=pfx)\n        for b in blob_iter:  # type: ignore[attr-defined]\n            name = str(b.name)\n            if allowed_exts is not None:\n                if not any(name.lower().endswith(ext) for ext in allowed_exts):\n                    continue\n            yield name\n</code></pre>"},{"location":"reference/ts_shape/loader/timeseries/azure_blob_loader/#ts_shape.loader.timeseries.azure_blob_loader.AzureBlobFlexibleFileLoader.list_files_by_time_range","title":"list_files_by_time_range","text":"<pre><code>list_files_by_time_range(start_timestamp: str | Timestamp, end_timestamp: str | Timestamp, *, extensions: Optional[Iterable[str]] = None, limit: Optional[int] = None) -&gt; List[str]\n</code></pre> <p>List blob names under each hourly prefix within [start, end].</p> <p>Parameters:</p> Source code in <code>src/ts_shape/loader/timeseries/azure_blob_loader.py</code> <pre><code>def list_files_by_time_range(\n    self,\n    start_timestamp: str | pd.Timestamp,\n    end_timestamp: str | pd.Timestamp,\n    *,\n    extensions: Optional[Iterable[str]] = None,\n    limit: Optional[int] = None,\n) -&gt; List[str]:\n    \"\"\"\n    List blob names under each hourly prefix within [start, end].\n\n    Args:\n        extensions: Optional set/list of file extensions (e.g., {\"json\", \".bmp\"}). Case-insensitive.\n        limit: Optional cap on number of files collected.\n    \"\"\"\n    allowed_exts = self._normalize_exts(extensions)\n    names: List[str] = []\n    collected = 0\n    for pfx in (self._hour_prefix(ts) for ts in self._hourly_slots(start_timestamp, end_timestamp)):\n        blob_iter = self.container_client.list_blobs(name_starts_with=pfx)\n        for b in blob_iter:  # type: ignore[attr-defined]\n            name = str(b.name)\n            if allowed_exts is not None:\n                lower_name = name.lower()\n                if not any(lower_name.endswith(ext) for ext in allowed_exts):\n                    continue\n            names.append(name)\n            collected += 1\n            if limit is not None and collected &gt;= limit:\n                return names\n    return names\n</code></pre>"},{"location":"reference/ts_shape/loader/timeseries/azure_blob_loader/#ts_shape.loader.timeseries.azure_blob_loader.AzureBlobFlexibleFileLoader.list_files_by_time_range(extensions)","title":"<code>extensions</code>","text":"(<code>Optional[Iterable[str]]</code>, default:                   <code>None</code> )           \u2013            <p>Optional set/list of file extensions (e.g., {\"json\", \".bmp\"}). Case-insensitive.</p>"},{"location":"reference/ts_shape/loader/timeseries/azure_blob_loader/#ts_shape.loader.timeseries.azure_blob_loader.AzureBlobFlexibleFileLoader.list_files_by_time_range(limit)","title":"<code>limit</code>","text":"(<code>Optional[int]</code>, default:                   <code>None</code> )           \u2013            <p>Optional cap on number of files collected.</p>"},{"location":"reference/ts_shape/loader/timeseries/azure_blob_loader/#ts_shape.loader.timeseries.azure_blob_loader.AzureBlobFlexibleFileLoader.stream_files_by_time_range","title":"stream_files_by_time_range","text":"<pre><code>stream_files_by_time_range(start_timestamp: str | Timestamp, end_timestamp: str | Timestamp, *, extensions: Optional[Iterable[str]] = None, parse: bool = False) -&gt; Iterator[Tuple[str, Any]]\n</code></pre> <p>Stream matching files as (blob_name, bytes-or-parsed) within [start, end]. Maintains up to <code>max_workers</code> concurrent downloads while yielding incrementally.</p> Source code in <code>src/ts_shape/loader/timeseries/azure_blob_loader.py</code> <pre><code>def stream_files_by_time_range(\n    self,\n    start_timestamp: str | pd.Timestamp,\n    end_timestamp: str | pd.Timestamp,\n    *,\n    extensions: Optional[Iterable[str]] = None,\n    parse: bool = False,\n) -&gt; Iterator[Tuple[str, Any]]:\n    \"\"\"\n    Stream matching files as (blob_name, bytes-or-parsed) within [start, end].\n    Maintains up to `max_workers` concurrent downloads while yielding incrementally.\n    \"\"\"\n    names_iter = self.iter_file_names_by_time_range(start_timestamp, end_timestamp, extensions=extensions)\n\n    with ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n        future_to_name: Dict[Any, str] = {}\n        # initial fill\n        try:\n            while len(future_to_name) &lt; self.max_workers:\n                n = next(names_iter)\n                future_to_name[executor.submit(self._download_bytes, n)] = n\n        except StopIteration:\n            pass\n\n        while future_to_name:\n            # Drain\n            for fut in as_completed(list(future_to_name.keys())):\n                name = future_to_name.pop(fut)\n                try:\n                    content = fut.result()\n                except Exception:\n                    content = None\n                if content is not None:\n                    yield (name, self._parse_bytes(name, content) if parse else content)\n\n            # Refill\n            try:\n                while len(future_to_name) &lt; self.max_workers:\n                    n = next(names_iter)\n                    future_to_name[executor.submit(self._download_bytes, n)] = n\n            except StopIteration:\n                pass\n</code></pre>"},{"location":"reference/ts_shape/loader/timeseries/azure_blob_loader/#ts_shape.loader.timeseries.azure_blob_loader.AzureBlobFlexibleFileLoader.stream_files_by_time_range_and_basenames","title":"stream_files_by_time_range_and_basenames","text":"<pre><code>stream_files_by_time_range_and_basenames(start_timestamp: str | Timestamp, end_timestamp: str | Timestamp, basenames: Iterable[str], *, extensions: Optional[Iterable[str]] = None, parse: bool = False) -&gt; Iterator[Tuple[str, Any]]\n</code></pre> <p>Stream files whose basename is in <code>basenames</code> within [start, end]. Yields (blob_name, bytes-or-parsed) incrementally with bounded concurrency.</p> Source code in <code>src/ts_shape/loader/timeseries/azure_blob_loader.py</code> <pre><code>def stream_files_by_time_range_and_basenames(\n    self,\n    start_timestamp: str | pd.Timestamp,\n    end_timestamp: str | pd.Timestamp,\n    basenames: Iterable[str],\n    *,\n    extensions: Optional[Iterable[str]] = None,\n    parse: bool = False,\n) -&gt; Iterator[Tuple[str, Any]]:\n    \"\"\"\n    Stream files whose basename is in `basenames` within [start, end].\n    Yields (blob_name, bytes-or-parsed) incrementally with bounded concurrency.\n    \"\"\"\n    base_set = {str(b).strip() for b in basenames if str(b).strip()}\n    allowed_exts = self._normalize_exts(extensions)\n\n    def _names_iter() -&gt; Iterator[str]:\n        for pfx in (self._hour_prefix(ts) for ts in self._hourly_slots(start_timestamp, end_timestamp)):\n            blob_iter = self.container_client.list_blobs(name_starts_with=pfx)\n            for b in blob_iter:  # type: ignore[attr-defined]\n                name = str(b.name)\n                base = name.rsplit('/', 1)[-1]\n                if base not in base_set:\n                    continue\n                if allowed_exts is not None and not any(name.lower().endswith(ext) for ext in allowed_exts):\n                    continue\n                yield name\n\n    names_iter = _names_iter()\n    with ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n        future_to_name: Dict[Any, str] = {}\n        # initial fill\n        try:\n            while len(future_to_name) &lt; self.max_workers:\n                n = next(names_iter)\n                future_to_name[executor.submit(self._download_bytes, n)] = n\n        except StopIteration:\n            pass\n\n        while future_to_name:\n            for fut in as_completed(list(future_to_name.keys())):\n                name = future_to_name.pop(fut)\n                try:\n                    content = fut.result()\n                except Exception:\n                    content = None\n                if content is not None:\n                    yield (name, self._parse_bytes(name, content) if parse else content)\n\n            try:\n                while len(future_to_name) &lt; self.max_workers:\n                    n = next(names_iter)\n                    future_to_name[executor.submit(self._download_bytes, n)] = n\n            except StopIteration:\n                pass\n</code></pre>"},{"location":"reference/ts_shape/loader/timeseries/azure_blob_loader/#ts_shape.loader.timeseries.azure_blob_loader.AzureBlobParquetLoader","title":"AzureBlobParquetLoader","text":"<pre><code>AzureBlobParquetLoader(container_name: str, *, connection_string: Optional[str] = None, account_url: Optional[str] = None, credential: Optional[object] = None, prefix: str = '', max_workers: int = 8, hour_pattern: str = '{Y}/{m}/{d}/{H}/')\n</code></pre> <p>Load parquet files from an Azure Blob Storage container filtered by a list of UUIDs.</p> <p>Optimized for speed by: - Using server-side prefix filtering when provided - Streaming blob listings and filtering client-side by UUID containment - Downloading and parsing parquet files concurrently</p> <p>Parameters:</p> <p>Methods:</p> <ul> <li> <code>from_account_name</code>             \u2013              <p>Construct a loader using AAD credentials with an account name.</p> </li> <li> <code>list_structure</code>             \u2013              <p>List folder prefixes (hours) and blob names under the configured <code>prefix</code>.</p> </li> <li> <code>load_all_files</code>             \u2013              <p>Load all parquet blobs in the container (optionally under <code>prefix</code>).</p> </li> <li> <code>load_by_time_range</code>             \u2013              <p>Load all parquet blobs under hourly folders within [start, end].</p> </li> <li> <code>load_files_by_time_range_and_uuids</code>             \u2013              <p>Load parquet blobs for given UUIDs within [start, end] hours.</p> </li> <li> <code>stream_by_time_range</code>             \u2013              <p>Stream parquet DataFrames under hourly folders within [start, end].</p> </li> <li> <code>stream_files_by_time_range_and_uuids</code>             \u2013              <p>Stream parquet DataFrames for given UUIDs within [start, end] hours.</p> </li> </ul> Source code in <code>src/ts_shape/loader/timeseries/azure_blob_loader.py</code> <pre><code>def __init__(\n    self,\n    container_name: str,\n    *,\n    connection_string: Optional[str] = None,\n    account_url: Optional[str] = None,\n    credential: Optional[object] = None,\n    prefix: str = \"\",\n    max_workers: int = 8,\n    hour_pattern: str = \"{Y}/{m}/{d}/{H}/\",\n) -&gt; None:\n    \"\"\"\n    Initialize the loader with Azure connection details.\n\n    Args:\n        connection_string: Azure Storage connection string.\n        container_name: Target container name.\n        prefix: Optional path prefix to narrow listing (e.g. \"year/month/\").\n        max_workers: Max concurrent downloads/reads.\n    \"\"\"\n    try:\n        from azure.storage.blob import ContainerClient  # type: ignore\n    except Exception as exc:  # pragma: no cover - import guard\n        raise ImportError(\n            \"azure-storage-blob is required for AzureBlobParquetLoader. \"\n            \"Install with `pip install azure-storage-blob`.\"\n        ) from exc\n\n    self._ContainerClient = ContainerClient\n    # Prefer AAD credential path if account_url provided or credential is given\n    if account_url or (credential is not None and not connection_string):\n        if not account_url:\n            raise ValueError(\"account_url must be provided when using AAD credential auth\")\n        if credential is None:\n            raise ValueError(\"credential must be provided when using AAD credential auth\")\n        self.container_client = ContainerClient(account_url=account_url, container_name=container_name, credential=credential)\n    else:\n        if not connection_string:\n            raise ValueError(\"Either connection_string or (account_url + credential) must be provided\")\n        self.container_client = ContainerClient.from_connection_string(\n            conn_str=connection_string, container_name=container_name\n        )\n    self.prefix = prefix\n    self.max_workers = max_workers if max_workers &gt; 0 else 1\n    # Pattern for hour-level subpath; tokens: {Y} {m} {d} {H}\n    # Default matches many data lake layouts: YYYY/MM/DD/HH/\n    self.hour_pattern = hour_pattern\n</code></pre>"},{"location":"reference/ts_shape/loader/timeseries/azure_blob_loader/#ts_shape.loader.timeseries.azure_blob_loader.AzureBlobParquetLoader(connection_string)","title":"<code>connection_string</code>","text":"(<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Azure Storage connection string.</p>"},{"location":"reference/ts_shape/loader/timeseries/azure_blob_loader/#ts_shape.loader.timeseries.azure_blob_loader.AzureBlobParquetLoader(container_name)","title":"<code>container_name</code>","text":"(<code>str</code>)           \u2013            <p>Target container name.</p>"},{"location":"reference/ts_shape/loader/timeseries/azure_blob_loader/#ts_shape.loader.timeseries.azure_blob_loader.AzureBlobParquetLoader(prefix)","title":"<code>prefix</code>","text":"(<code>str</code>, default:                   <code>''</code> )           \u2013            <p>Optional path prefix to narrow listing (e.g. \"year/month/\").</p>"},{"location":"reference/ts_shape/loader/timeseries/azure_blob_loader/#ts_shape.loader.timeseries.azure_blob_loader.AzureBlobParquetLoader(max_workers)","title":"<code>max_workers</code>","text":"(<code>int</code>, default:                   <code>8</code> )           \u2013            <p>Max concurrent downloads/reads.</p>"},{"location":"reference/ts_shape/loader/timeseries/azure_blob_loader/#ts_shape.loader.timeseries.azure_blob_loader.AzureBlobParquetLoader.from_account_name","title":"from_account_name  <code>classmethod</code>","text":"<pre><code>from_account_name(account_name: str, container_name: str, *, credential: Optional[object] = None, endpoint_suffix: str = 'blob.core.windows.net', prefix: str = '', max_workers: int = 8) -&gt; AzureBlobParquetLoader\n</code></pre> <p>Construct a loader using AAD credentials with an account name.</p> <p>Parameters:</p> Source code in <code>src/ts_shape/loader/timeseries/azure_blob_loader.py</code> <pre><code>@classmethod\ndef from_account_name(\n    cls,\n    account_name: str,\n    container_name: str,\n    *,\n    credential: Optional[object] = None,\n    endpoint_suffix: str = \"blob.core.windows.net\",\n    prefix: str = \"\",\n    max_workers: int = 8,\n) -&gt; \"AzureBlobParquetLoader\":\n    \"\"\"\n    Construct a loader using AAD credentials with an account name.\n\n    Args:\n        account_name: Storage account name.\n        container_name: Target container.\n        credential: Optional Azure credential (DefaultAzureCredential if None).\n        endpoint_suffix: DNS suffix for the blob endpoint (e.g., for sovereign clouds).\n        prefix: Optional listing prefix (e.g., \"parquet/\").\n        max_workers: Concurrency for downloads.\n    \"\"\"\n    account_url = f\"https://{account_name}.{endpoint_suffix}\"\n    if credential is None:\n        raise ValueError(\"credential must be provided when using AAD credential auth\")\n    return cls(\n        container_name=container_name,\n        account_url=account_url,\n        credential=credential,\n        prefix=prefix,\n        max_workers=max_workers,\n    )\n</code></pre>"},{"location":"reference/ts_shape/loader/timeseries/azure_blob_loader/#ts_shape.loader.timeseries.azure_blob_loader.AzureBlobParquetLoader.from_account_name(account_name)","title":"<code>account_name</code>","text":"(<code>str</code>)           \u2013            <p>Storage account name.</p>"},{"location":"reference/ts_shape/loader/timeseries/azure_blob_loader/#ts_shape.loader.timeseries.azure_blob_loader.AzureBlobParquetLoader.from_account_name(container_name)","title":"<code>container_name</code>","text":"(<code>str</code>)           \u2013            <p>Target container.</p>"},{"location":"reference/ts_shape/loader/timeseries/azure_blob_loader/#ts_shape.loader.timeseries.azure_blob_loader.AzureBlobParquetLoader.from_account_name(credential)","title":"<code>credential</code>","text":"(<code>Optional[object]</code>, default:                   <code>None</code> )           \u2013            <p>Optional Azure credential (DefaultAzureCredential if None).</p>"},{"location":"reference/ts_shape/loader/timeseries/azure_blob_loader/#ts_shape.loader.timeseries.azure_blob_loader.AzureBlobParquetLoader.from_account_name(endpoint_suffix)","title":"<code>endpoint_suffix</code>","text":"(<code>str</code>, default:                   <code>'blob.core.windows.net'</code> )           \u2013            <p>DNS suffix for the blob endpoint (e.g., for sovereign clouds).</p>"},{"location":"reference/ts_shape/loader/timeseries/azure_blob_loader/#ts_shape.loader.timeseries.azure_blob_loader.AzureBlobParquetLoader.from_account_name(prefix)","title":"<code>prefix</code>","text":"(<code>str</code>, default:                   <code>''</code> )           \u2013            <p>Optional listing prefix (e.g., \"parquet/\").</p>"},{"location":"reference/ts_shape/loader/timeseries/azure_blob_loader/#ts_shape.loader.timeseries.azure_blob_loader.AzureBlobParquetLoader.from_account_name(max_workers)","title":"<code>max_workers</code>","text":"(<code>int</code>, default:                   <code>8</code> )           \u2013            <p>Concurrency for downloads.</p>"},{"location":"reference/ts_shape/loader/timeseries/azure_blob_loader/#ts_shape.loader.timeseries.azure_blob_loader.AzureBlobParquetLoader.list_structure","title":"list_structure","text":"<pre><code>list_structure(parquet_only: bool = True, limit: Optional[int] = None) -&gt; Dict[str, List[str]]\n</code></pre> <p>List folder prefixes (hours) and blob names under the configured <code>prefix</code>.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>Dict[str, List[str]]</code>           \u2013            <p>A dict with:</p> </li> <li> <code>Dict[str, List[str]]</code>           \u2013            <ul> <li>folders: Sorted unique hour-level prefixes like 'parquet/YYYY/MM/DD/HH/'</li> </ul> </li> <li> <code>Dict[str, List[str]]</code>           \u2013            <ul> <li>files: Sorted blob names (full paths) matching the filter</li> </ul> </li> </ul> Source code in <code>src/ts_shape/loader/timeseries/azure_blob_loader.py</code> <pre><code>def list_structure(self, parquet_only: bool = True, limit: Optional[int] = None) -&gt; Dict[str, List[str]]:\n    \"\"\"\n    List folder prefixes (hours) and blob names under the configured `prefix`.\n\n    Args:\n        parquet_only: If True, only include blobs ending with .parquet.\n        limit: Optional cap on number of files collected for quick inspection.\n\n    Returns:\n        A dict with:\n        - folders: Sorted unique hour-level prefixes like 'parquet/YYYY/MM/DD/HH/'\n        - files: Sorted blob names (full paths) matching the filter\n    \"\"\"\n    folders: Set[str] = set()\n    files: List[str] = []\n    collected = 0\n\n    blob_iter = self.container_client.list_blobs(name_starts_with=self.prefix or None)\n    for b in blob_iter:\n        name = str(b.name)\n        if parquet_only and not name.endswith(\".parquet\"):\n            continue\n        files.append(name)\n        # Derive hour-level folder prefix\n        if \"/\" in name:\n            folders.add(name.rsplit(\"/\", 1)[0].rstrip(\"/\") + \"/\")\n        collected += 1\n        if limit is not None and collected &gt;= limit:\n            break\n\n    return {\n        \"folders\": sorted(folders),\n        \"files\": sorted(files),\n    }\n</code></pre>"},{"location":"reference/ts_shape/loader/timeseries/azure_blob_loader/#ts_shape.loader.timeseries.azure_blob_loader.AzureBlobParquetLoader.list_structure(parquet_only)","title":"<code>parquet_only</code>","text":"(<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, only include blobs ending with .parquet.</p>"},{"location":"reference/ts_shape/loader/timeseries/azure_blob_loader/#ts_shape.loader.timeseries.azure_blob_loader.AzureBlobParquetLoader.list_structure(limit)","title":"<code>limit</code>","text":"(<code>Optional[int]</code>, default:                   <code>None</code> )           \u2013            <p>Optional cap on number of files collected for quick inspection.</p>"},{"location":"reference/ts_shape/loader/timeseries/azure_blob_loader/#ts_shape.loader.timeseries.azure_blob_loader.AzureBlobParquetLoader.load_all_files","title":"load_all_files","text":"<pre><code>load_all_files() -&gt; DataFrame\n</code></pre> <p>Load all parquet blobs in the container (optionally under <code>prefix</code>).</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>A concatenated DataFrame of all parquet blobs. Returns an empty DataFrame</p> </li> <li> <code>DataFrame</code>           \u2013            <p>if none are found.</p> </li> </ul> Source code in <code>src/ts_shape/loader/timeseries/azure_blob_loader.py</code> <pre><code>def load_all_files(self) -&gt; pd.DataFrame:\n    \"\"\"\n    Load all parquet blobs in the container (optionally under `prefix`).\n\n    Returns:\n        A concatenated DataFrame of all parquet blobs. Returns an empty DataFrame\n        if none are found.\n    \"\"\"\n    # List all parquet blob names using optional prefix for server-side filtering\n    blob_iter = self.container_client.list_blobs(name_starts_with=self.prefix or None)\n    blob_names = [b.name for b in blob_iter if str(b.name).endswith(\".parquet\")]  # type: ignore[attr-defined]\n    if not blob_names:\n        return pd.DataFrame()\n\n    frames: List[pd.DataFrame] = []\n    with ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n        future_to_name = {executor.submit(self._download_parquet, name): name for name in blob_names}\n        for future in as_completed(future_to_name):\n            df = future.result()\n            if df is not None and not df.empty:\n                frames.append(df)\n\n    return pd.concat(frames, ignore_index=True) if frames else pd.DataFrame()\n</code></pre>"},{"location":"reference/ts_shape/loader/timeseries/azure_blob_loader/#ts_shape.loader.timeseries.azure_blob_loader.AzureBlobParquetLoader.load_by_time_range","title":"load_by_time_range","text":"<pre><code>load_by_time_range(start_timestamp: str | Timestamp, end_timestamp: str | Timestamp) -&gt; DataFrame\n</code></pre> <p>Load all parquet blobs under hourly folders within [start, end].</p> <p>Assumes container structure: prefix/year/month/day/hour/{file}.parquet Listing is constrained per-hour for speed.</p> Source code in <code>src/ts_shape/loader/timeseries/azure_blob_loader.py</code> <pre><code>def load_by_time_range(self, start_timestamp: str | pd.Timestamp, end_timestamp: str | pd.Timestamp) -&gt; pd.DataFrame:\n    \"\"\"\n    Load all parquet blobs under hourly folders within [start, end].\n\n    Assumes container structure: prefix/year/month/day/hour/{file}.parquet\n    Listing is constrained per-hour for speed.\n    \"\"\"\n    hour_prefixes = [self._hour_prefix(ts) for ts in self._hourly_slots(start_timestamp, end_timestamp)]\n    blob_names: List[str] = []\n    for pfx in hour_prefixes:\n        blob_iter = self.container_client.list_blobs(name_starts_with=pfx)\n        blob_names.extend([b.name for b in blob_iter if str(b.name).endswith(\".parquet\")])  # type: ignore[attr-defined]\n\n    if not blob_names:\n        return pd.DataFrame()\n\n    frames: List[pd.DataFrame] = []\n    with ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n        future_to_name = {executor.submit(self._download_parquet, name): name for name in blob_names}\n        for future in as_completed(future_to_name):\n            df = future.result()\n            if df is not None and not df.empty:\n                frames.append(df)\n\n    return pd.concat(frames, ignore_index=True) if frames else pd.DataFrame()\n</code></pre>"},{"location":"reference/ts_shape/loader/timeseries/azure_blob_loader/#ts_shape.loader.timeseries.azure_blob_loader.AzureBlobParquetLoader.load_files_by_time_range_and_uuids","title":"load_files_by_time_range_and_uuids","text":"<pre><code>load_files_by_time_range_and_uuids(start_timestamp: str | Timestamp, end_timestamp: str | Timestamp, uuid_list: List[str]) -&gt; DataFrame\n</code></pre> <p>Load parquet blobs for given UUIDs within [start, end] hours.</p> <p>Strategy: 1) Construct direct blob paths assuming pattern prefix/YYYY/MM/DD/HH/{uuid}.parquet    (fast path, no listing). 2) For robustness, also list each hour prefix and include any blob whose basename    equals one of the requested UUID variants (handles case differences and extra    subfolders below the hour level).</p> Source code in <code>src/ts_shape/loader/timeseries/azure_blob_loader.py</code> <pre><code>def load_files_by_time_range_and_uuids(\n    self,\n    start_timestamp: str | pd.Timestamp,\n    end_timestamp: str | pd.Timestamp,\n    uuid_list: List[str],\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Load parquet blobs for given UUIDs within [start, end] hours.\n\n    Strategy:\n    1) Construct direct blob paths assuming pattern prefix/YYYY/MM/DD/HH/{uuid}.parquet\n       (fast path, no listing).\n    2) For robustness, also list each hour prefix and include any blob whose basename\n       equals one of the requested UUID variants (handles case differences and extra\n       subfolders below the hour level).\n    \"\"\"\n    if not uuid_list:\n        return pd.DataFrame()\n\n    # Sanitize and deduplicate UUIDs while preserving order\n    def _clean_uuid(u: object) -&gt; str:\n        s = str(u).strip().strip(\"{}\").strip()\n        return s\n\n    raw = [_clean_uuid(u) for u in uuid_list]\n    # Include lowercase variants to be tolerant of case differences in filenames\n    variants_ordered: List[str] = []\n    seen: Set[str] = set()\n    for u in raw:\n        for v in (u, u.lower()):\n            if v and v not in seen:\n                seen.add(v)\n                variants_ordered.append(v)\n\n    hour_prefixes = [self._hour_prefix(ts) for ts in self._hourly_slots(start_timestamp, end_timestamp)]\n\n    # 1) Fast path: build direct blob names\n    direct_names = [f\"{pfx}{u}.parquet\" for pfx in hour_prefixes for u in variants_ordered]\n\n    # 2) Robust path: list each hour prefix and filter by basename match\n    basenames = {f\"{u}.parquet\" for u in variants_ordered}\n    listed_names: List[str] = []\n    try:\n        for pfx in hour_prefixes:\n            blob_iter = self.container_client.list_blobs(name_starts_with=pfx)\n            for b in blob_iter:  # type: ignore[attr-defined]\n                name = str(b.name)\n                if not name.endswith(\".parquet\"):\n                    continue\n                base = name.rsplit(\"/\", 1)[-1]\n                if base in basenames:\n                    listed_names.append(name)\n    except Exception:\n        # If listing fails for any reason, continue with direct names only\n        pass\n\n    # Merge and preserve order, avoid duplicates\n    all_blob_names = list(dict.fromkeys([*direct_names, *listed_names]))\n\n    if not all_blob_names:\n        return pd.DataFrame()\n\n    frames: List[pd.DataFrame] = []\n    with ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n        future_to_name = {executor.submit(self._download_parquet, name): name for name in all_blob_names}\n        for future in as_completed(future_to_name):\n            df = future.result()\n            if df is not None and not df.empty:\n                frames.append(df)\n\n    return pd.concat(frames, ignore_index=True) if frames else pd.DataFrame()\n</code></pre>"},{"location":"reference/ts_shape/loader/timeseries/azure_blob_loader/#ts_shape.loader.timeseries.azure_blob_loader.AzureBlobParquetLoader.stream_by_time_range","title":"stream_by_time_range","text":"<pre><code>stream_by_time_range(start_timestamp: str | Timestamp, end_timestamp: str | Timestamp) -&gt; Iterator[Tuple[str, DataFrame]]\n</code></pre> <p>Stream parquet DataFrames under hourly folders within [start, end].</p> <p>Yields (blob_name, DataFrame) one by one to avoid holding everything in memory.</p> Source code in <code>src/ts_shape/loader/timeseries/azure_blob_loader.py</code> <pre><code>def stream_by_time_range(self, start_timestamp: str | pd.Timestamp, end_timestamp: str | pd.Timestamp) -&gt; Iterator[Tuple[str, pd.DataFrame]]:\n    \"\"\"\n    Stream parquet DataFrames under hourly folders within [start, end].\n\n    Yields (blob_name, DataFrame) one by one to avoid holding everything in memory.\n    \"\"\"\n    hour_prefixes = [self._hour_prefix(ts) for ts in self._hourly_slots(start_timestamp, end_timestamp)]\n\n    def _names_iter() -&gt; Iterator[str]:\n        for pfx in hour_prefixes:\n            blob_iter = self.container_client.list_blobs(name_starts_with=pfx)\n            for b in blob_iter:  # type: ignore[attr-defined]\n                name = str(b.name)\n                if name.endswith(\".parquet\"):\n                    yield name\n\n    names_iter = _names_iter()\n    with ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n        futures: Dict[Any, str] = {}\n        # initial fill\n        try:\n            while len(futures) &lt; self.max_workers:\n                n = next(names_iter)\n                futures[executor.submit(self._download_parquet, n)] = n\n        except StopIteration:\n            pass\n\n        while futures:\n            # Drain current batch\n            for fut in as_completed(list(futures.keys())):\n                name = futures.pop(fut)\n                try:\n                    df = fut.result()\n                except Exception:\n                    df = None\n                if df is not None and not df.empty:\n                    yield (name, df)\n\n            # Refill\n            try:\n                while len(futures) &lt; self.max_workers:\n                    n = next(names_iter)\n                    futures[executor.submit(self._download_parquet, n)] = n\n            except StopIteration:\n                pass\n</code></pre>"},{"location":"reference/ts_shape/loader/timeseries/azure_blob_loader/#ts_shape.loader.timeseries.azure_blob_loader.AzureBlobParquetLoader.stream_files_by_time_range_and_uuids","title":"stream_files_by_time_range_and_uuids","text":"<pre><code>stream_files_by_time_range_and_uuids(start_timestamp: str | Timestamp, end_timestamp: str | Timestamp, uuid_list: List[str]) -&gt; Iterator[Tuple[str, DataFrame]]\n</code></pre> <p>Stream parquet DataFrames for given UUIDs within [start, end] hours.</p> <p>Yields (blob_name, DataFrame) as they arrive. Uses direct names plus per-hour listing fallback.</p> Source code in <code>src/ts_shape/loader/timeseries/azure_blob_loader.py</code> <pre><code>def stream_files_by_time_range_and_uuids(\n    self,\n    start_timestamp: str | pd.Timestamp,\n    end_timestamp: str | pd.Timestamp,\n    uuid_list: List[str],\n) -&gt; Iterator[Tuple[str, pd.DataFrame]]:\n    \"\"\"\n    Stream parquet DataFrames for given UUIDs within [start, end] hours.\n\n    Yields (blob_name, DataFrame) as they arrive. Uses direct names plus per-hour listing fallback.\n    \"\"\"\n    if not uuid_list:\n        return iter(())\n\n    def _clean_uuid(u: object) -&gt; str:\n        return str(u).strip().strip(\"{}\").strip()\n\n    raw = [_clean_uuid(u) for u in uuid_list]\n    variants_ordered: List[str] = []\n    seen: Set[str] = set()\n    for u in raw:\n        for v in (u, u.lower()):\n            if v and v not in seen:\n                seen.add(v)\n                variants_ordered.append(v)\n\n    hour_prefixes = [self._hour_prefix(ts) for ts in self._hourly_slots(start_timestamp, end_timestamp)]\n    direct_names = [f\"{pfx}{u}.parquet\" for pfx in hour_prefixes for u in variants_ordered]\n\n    basenames = {f\"{u}.parquet\" for u in variants_ordered}\n\n    def _names_iter() -&gt; Iterator[str]:\n        # yield direct first\n        yielded: Set[str] = set()\n        for n in direct_names:\n            yielded.add(n)\n            yield n\n        # then list per-hour\n        for pfx in hour_prefixes:\n            blob_iter = self.container_client.list_blobs(name_starts_with=pfx)\n            for b in blob_iter:  # type: ignore[attr-defined]\n                name = str(b.name)\n                if not name.endswith(\".parquet\"):\n                    continue\n                base = name.rsplit(\"/\", 1)[-1]\n                if base in basenames and name not in yielded:\n                    yielded.add(name)\n                    yield name\n\n    names_iter = _names_iter()\n    with ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n        futures: Dict[Any, str] = {}\n        try:\n            while len(futures) &lt; self.max_workers:\n                n = next(names_iter)\n                futures[executor.submit(self._download_parquet, n)] = n\n        except StopIteration:\n            pass\n\n        while futures:\n            for fut in as_completed(list(futures.keys())):\n                name = futures.pop(fut)\n                try:\n                    df = fut.result()\n                except Exception:\n                    df = None\n                if df is not None and not df.empty:\n                    yield (name, df)\n\n            try:\n                while len(futures) &lt; self.max_workers:\n                    n = next(names_iter)\n                    futures[executor.submit(self._download_parquet, n)] = n\n            except StopIteration:\n                pass\n</code></pre>"},{"location":"reference/ts_shape/loader/timeseries/energy_api_loader/","title":"energy_api_loader","text":""},{"location":"reference/ts_shape/loader/timeseries/energy_api_loader/#ts_shape.loader.timeseries.energy_api_loader","title":"ts_shape.loader.timeseries.energy_api_loader","text":""},{"location":"reference/ts_shape/loader/timeseries/parquet_loader/","title":"parquet_loader","text":""},{"location":"reference/ts_shape/loader/timeseries/parquet_loader/#ts_shape.loader.timeseries.parquet_loader","title":"ts_shape.loader.timeseries.parquet_loader","text":"<p>Classes:</p> <ul> <li> <code>ParquetLoader</code>           \u2013            <p>This class provides class methods to load parquet files from a specified directory structure.</p> </li> </ul>"},{"location":"reference/ts_shape/loader/timeseries/parquet_loader/#ts_shape.loader.timeseries.parquet_loader.ParquetLoader","title":"ParquetLoader","text":"<pre><code>ParquetLoader(base_path: str)\n</code></pre> <p>This class provides class methods to load parquet files from a specified directory structure.</p> <p>Parameters:</p> <p>Methods:</p> <ul> <li> <code>load_all_files</code>             \u2013              <p>Loads all parquet files in the specified base directory into a single pandas DataFrame.</p> </li> <li> <code>load_by_time_range</code>             \u2013              <p>Loads parquet files that fall within a specified time range based on the directory structure.</p> </li> <li> <code>load_by_uuid_list</code>             \u2013              <p>Loads parquet files that match any UUID in the specified list.</p> </li> <li> <code>load_files_by_time_range_and_uuids</code>             \u2013              <p>Loads parquet files that fall within a specified time range and match any UUID in the list.</p> </li> </ul> Source code in <code>src/ts_shape/loader/timeseries/parquet_loader.py</code> <pre><code>def __init__(self, base_path: str):\n    \"\"\"\n    Initialize the ParquetLoader with the base directory path.\n\n    Args:\n        base_path (str): The base directory where parquet files are stored.\n    \"\"\"\n    self.base_path = Path(base_path)\n</code></pre>"},{"location":"reference/ts_shape/loader/timeseries/parquet_loader/#ts_shape.loader.timeseries.parquet_loader.ParquetLoader(base_path)","title":"<code>base_path</code>","text":"(<code>str</code>)           \u2013            <p>The base directory where parquet files are stored.</p>"},{"location":"reference/ts_shape/loader/timeseries/parquet_loader/#ts_shape.loader.timeseries.parquet_loader.ParquetLoader.load_all_files","title":"load_all_files  <code>classmethod</code>","text":"<pre><code>load_all_files(base_path: str) -&gt; DataFrame\n</code></pre> <p>Loads all parquet files in the specified base directory into a single pandas DataFrame.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>pd.DataFrame: A DataFrame containing all the data from the parquet files.</p> </li> </ul> Source code in <code>src/ts_shape/loader/timeseries/parquet_loader.py</code> <pre><code>@classmethod\ndef load_all_files(cls, base_path: str) -&gt; pd.DataFrame:\n    \"\"\"\n    Loads all parquet files in the specified base directory into a single pandas DataFrame.\n\n    Args:\n        base_path (str): The base directory where parquet files are stored.\n\n    Returns:\n        pd.DataFrame: A DataFrame containing all the data from the parquet files.\n    \"\"\"\n    # Convert base path to a Path object\n    base_path = Path(base_path)\n    # Get all parquet files in the directory\n    parquet_files = cls._get_parquet_files(base_path)\n    # Load all files into pandas DataFrames\n    dataframes = [pd.read_parquet(file) for file in parquet_files]\n\n    # Concatenate all DataFrames into a single DataFrame\n    return pd.concat(dataframes, ignore_index=True)\n</code></pre>"},{"location":"reference/ts_shape/loader/timeseries/parquet_loader/#ts_shape.loader.timeseries.parquet_loader.ParquetLoader.load_all_files(base_path)","title":"<code>base_path</code>","text":"(<code>str</code>)           \u2013            <p>The base directory where parquet files are stored.</p>"},{"location":"reference/ts_shape/loader/timeseries/parquet_loader/#ts_shape.loader.timeseries.parquet_loader.ParquetLoader.load_by_time_range","title":"load_by_time_range  <code>classmethod</code>","text":"<pre><code>load_by_time_range(base_path: str, start_time: Timestamp, end_time: Timestamp) -&gt; DataFrame\n</code></pre> <p>Loads parquet files that fall within a specified time range based on the directory structure.</p> <p>The directory structure is expected to be in the format YYYY/MM/DD/HH.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>pd.DataFrame: A DataFrame containing the data from the parquet files within the time range.</p> </li> </ul> Source code in <code>src/ts_shape/loader/timeseries/parquet_loader.py</code> <pre><code>@classmethod\ndef load_by_time_range(cls, base_path: str, start_time: pd.Timestamp, end_time: pd.Timestamp) -&gt; pd.DataFrame:\n    \"\"\"\n    Loads parquet files that fall within a specified time range based on the directory structure.\n\n    The directory structure is expected to be in the format YYYY/MM/DD/HH.\n\n    Args:\n        base_path (str): The base directory where parquet files are stored.\n        start_time (pd.Timestamp): The start timestamp.\n        end_time (pd.Timestamp): The end timestamp.\n\n    Returns:\n        pd.DataFrame: A DataFrame containing the data from the parquet files within the time range.\n    \"\"\"\n    # Convert base path to a Path object\n    base_path = Path(base_path)\n    # Get all parquet files in the directory\n    parquet_files = cls._get_parquet_files(base_path)\n    valid_files = []\n\n    for file in parquet_files:\n        try:\n            # Extract the timestamp from the file's relative path\n            folder_parts = file.relative_to(base_path).parts[:4]  # Extract YYYY/MM/DD/HH parts\n            folder_time_str = \"/\".join(folder_parts)\n            file_time = pd.to_datetime(folder_time_str, format=\"%Y/%m/%d/%H\")\n\n            # Check if the file's timestamp falls within the specified time range\n            if start_time &lt;= file_time &lt;= end_time:\n                valid_files.append(file)\n        except ValueError:\n            # Skip files that do not follow the expected folder structure\n            continue\n\n    # Load all valid files into pandas DataFrames\n    dataframes = [pd.read_parquet(file) for file in valid_files]\n    return pd.concat(dataframes, ignore_index=True)\n</code></pre>"},{"location":"reference/ts_shape/loader/timeseries/parquet_loader/#ts_shape.loader.timeseries.parquet_loader.ParquetLoader.load_by_time_range(base_path)","title":"<code>base_path</code>","text":"(<code>str</code>)           \u2013            <p>The base directory where parquet files are stored.</p>"},{"location":"reference/ts_shape/loader/timeseries/parquet_loader/#ts_shape.loader.timeseries.parquet_loader.ParquetLoader.load_by_time_range(start_time)","title":"<code>start_time</code>","text":"(<code>Timestamp</code>)           \u2013            <p>The start timestamp.</p>"},{"location":"reference/ts_shape/loader/timeseries/parquet_loader/#ts_shape.loader.timeseries.parquet_loader.ParquetLoader.load_by_time_range(end_time)","title":"<code>end_time</code>","text":"(<code>Timestamp</code>)           \u2013            <p>The end timestamp.</p>"},{"location":"reference/ts_shape/loader/timeseries/parquet_loader/#ts_shape.loader.timeseries.parquet_loader.ParquetLoader.load_by_uuid_list","title":"load_by_uuid_list  <code>classmethod</code>","text":"<pre><code>load_by_uuid_list(base_path: str, uuid_list: list) -&gt; DataFrame\n</code></pre> <p>Loads parquet files that match any UUID in the specified list.</p> <p>The UUIDs are expected to be part of the file names.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>pd.DataFrame: A DataFrame containing the data from the parquet files with matching UUIDs.</p> </li> </ul> Source code in <code>src/ts_shape/loader/timeseries/parquet_loader.py</code> <pre><code>@classmethod\ndef load_by_uuid_list(cls, base_path: str, uuid_list: list) -&gt; pd.DataFrame:\n    \"\"\"\n    Loads parquet files that match any UUID in the specified list.\n\n    The UUIDs are expected to be part of the file names.\n\n    Args:\n        base_path (str): The base directory where parquet files are stored.\n        uuid_list (list): A list of UUIDs to filter the files.\n\n    Returns:\n        pd.DataFrame: A DataFrame containing the data from the parquet files with matching UUIDs.\n    \"\"\"\n    # Convert base path to a Path object\n    base_path = Path(base_path)\n    # Get all parquet files in the directory\n    parquet_files = cls._get_parquet_files(base_path)\n    valid_files = []\n\n    for file in parquet_files:\n        # Extract the file name without extension\n        file_name = file.stem\n        # Check if the file name contains any of the UUIDs in the list\n        for uuid in uuid_list:\n            if uuid in file_name:\n                valid_files.append(file)\n                break  # Stop checking other UUIDs for this file\n\n    # Load all valid files into pandas DataFrames\n    dataframes = [pd.read_parquet(file) for file in valid_files]\n    return pd.concat(dataframes, ignore_index=True)\n</code></pre>"},{"location":"reference/ts_shape/loader/timeseries/parquet_loader/#ts_shape.loader.timeseries.parquet_loader.ParquetLoader.load_by_uuid_list(base_path)","title":"<code>base_path</code>","text":"(<code>str</code>)           \u2013            <p>The base directory where parquet files are stored.</p>"},{"location":"reference/ts_shape/loader/timeseries/parquet_loader/#ts_shape.loader.timeseries.parquet_loader.ParquetLoader.load_by_uuid_list(uuid_list)","title":"<code>uuid_list</code>","text":"(<code>list</code>)           \u2013            <p>A list of UUIDs to filter the files.</p>"},{"location":"reference/ts_shape/loader/timeseries/parquet_loader/#ts_shape.loader.timeseries.parquet_loader.ParquetLoader.load_files_by_time_range_and_uuids","title":"load_files_by_time_range_and_uuids  <code>classmethod</code>","text":"<pre><code>load_files_by_time_range_and_uuids(base_path: str, start_time: Timestamp, end_time: Timestamp, uuid_list: list) -&gt; DataFrame\n</code></pre> <p>Loads parquet files that fall within a specified time range and match any UUID in the list.</p> <p>The directory structure is expected to be in the format YYYY/MM/DD/HH, and UUIDs are part of the file names.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>pd.DataFrame: A DataFrame containing the data from the parquet files that meet both criteria.</p> </li> </ul> Source code in <code>src/ts_shape/loader/timeseries/parquet_loader.py</code> <pre><code>@classmethod\ndef load_files_by_time_range_and_uuids(cls, base_path: str, start_time: pd.Timestamp, end_time: pd.Timestamp, uuid_list: list) -&gt; pd.DataFrame:\n    \"\"\"\n    Loads parquet files that fall within a specified time range and match any UUID in the list.\n\n    The directory structure is expected to be in the format YYYY/MM/DD/HH, and UUIDs are part of the file names.\n\n    Args:\n        base_path (str): The base directory where parquet files are stored.\n        start_time (pd.Timestamp): The start timestamp.\n        end_time (pd.Timestamp): The end timestamp.\n        uuid_list (list): A list of UUIDs to filter the files.\n\n    Returns:\n        pd.DataFrame: A DataFrame containing the data from the parquet files that meet both criteria.\n    \"\"\"\n    # Convert base path to a Path object\n    base_path = Path(base_path)\n    # Get all parquet files in the directory\n    parquet_files = cls._get_parquet_files(base_path)\n    valid_files = []\n\n    for file in parquet_files:\n        try:\n            # Extract the timestamp from the file's relative path\n            folder_parts = file.relative_to(base_path).parts[:4]  # Extract YYYY/MM/DD/HH parts\n            folder_time_str = \"/\".join(folder_parts)\n            file_time = pd.to_datetime(folder_time_str, format=\"%Y/%m/%d/%H\")\n\n            # Check if the file's timestamp falls within the specified time range\n            if start_time &lt;= file_time &lt;= end_time:\n                # Extract the file name without extension\n                file_name = file.stem\n                # Check if the file name contains any of the UUIDs in the list\n                for uuid in uuid_list:\n                    if uuid in file_name:\n                        valid_files.append(file)\n                        break  # Stop checking other UUIDs for this file\n        except ValueError:\n            # Skip files that do not follow the expected folder structure\n            continue\n\n    # Load all valid files into pandas DataFrames\n    dataframes = [pd.read_parquet(file) for file in valid_files]\n    return pd.concat(dataframes, ignore_index=True)\n</code></pre>"},{"location":"reference/ts_shape/loader/timeseries/parquet_loader/#ts_shape.loader.timeseries.parquet_loader.ParquetLoader.load_files_by_time_range_and_uuids(base_path)","title":"<code>base_path</code>","text":"(<code>str</code>)           \u2013            <p>The base directory where parquet files are stored.</p>"},{"location":"reference/ts_shape/loader/timeseries/parquet_loader/#ts_shape.loader.timeseries.parquet_loader.ParquetLoader.load_files_by_time_range_and_uuids(start_time)","title":"<code>start_time</code>","text":"(<code>Timestamp</code>)           \u2013            <p>The start timestamp.</p>"},{"location":"reference/ts_shape/loader/timeseries/parquet_loader/#ts_shape.loader.timeseries.parquet_loader.ParquetLoader.load_files_by_time_range_and_uuids(end_time)","title":"<code>end_time</code>","text":"(<code>Timestamp</code>)           \u2013            <p>The end timestamp.</p>"},{"location":"reference/ts_shape/loader/timeseries/parquet_loader/#ts_shape.loader.timeseries.parquet_loader.ParquetLoader.load_files_by_time_range_and_uuids(uuid_list)","title":"<code>uuid_list</code>","text":"(<code>list</code>)           \u2013            <p>A list of UUIDs to filter the files.</p>"},{"location":"reference/ts_shape/loader/timeseries/s3proxy_parquet_loader/","title":"s3proxy_parquet_loader","text":""},{"location":"reference/ts_shape/loader/timeseries/s3proxy_parquet_loader/#ts_shape.loader.timeseries.s3proxy_parquet_loader","title":"ts_shape.loader.timeseries.s3proxy_parquet_loader","text":"<p>Classes:</p> <ul> <li> <code>S3ProxyDataAccess</code>           \u2013            <p>A class to access timeseries data via an S3 proxy. This class retrieves </p> </li> </ul>"},{"location":"reference/ts_shape/loader/timeseries/s3proxy_parquet_loader/#ts_shape.loader.timeseries.s3proxy_parquet_loader.S3ProxyDataAccess","title":"S3ProxyDataAccess","text":"<pre><code>S3ProxyDataAccess(start_timestamp: str, end_timestamp: str, uuids: List[str], s3_config: Dict[str, str])\n</code></pre> <p>A class to access timeseries data via an S3 proxy. This class retrieves  data for specified UUIDs within a defined time range, with the option to  output data as Parquet files or as a single combined DataFrame.</p> <p>:param end_timestamp: End timestamp in \"Year-Month-Day Hour:Minute:Second\" format. :param uuids: List of UUIDs to retrieve data for. :param s3_config: Configuration dictionary for S3 connection.</p> <p>Methods:</p> <ul> <li> <code>fetch_data_as_dataframe</code>             \u2013              <p>Retrieves timeseries data from S3 and returns it as a single DataFrame.</p> </li> <li> <code>fetch_data_as_parquet</code>             \u2013              <p>Retrieves timeseries data from S3 and saves it as Parquet files.</p> </li> </ul> Source code in <code>src/ts_shape/loader/timeseries/s3proxy_parquet_loader.py</code> <pre><code>def __init__(self, start_timestamp: str, end_timestamp: str, uuids: List[str], s3_config: Dict[str, str]):\n    \"\"\"\n    Initialize the S3ProxyDataAccess object.\n    :param start_timestamp: Start timestamp in \"Year-Month-Day Hour:Minute:Second\" format.\n    :param end_timestamp: End timestamp in \"Year-Month-Day Hour:Minute:Second\" format.\n    :param uuids: List of UUIDs to retrieve data for.\n    :param s3_config: Configuration dictionary for S3 connection.\n    \"\"\"\n    self.start_timestamp = start_timestamp\n    self.end_timestamp = end_timestamp\n    self.uuids = uuids\n    self.s3_config = s3_config\n\n    # Establish connection to S3 using provided configuration\n    self.s3 = s3fs.S3FileSystem(\n        endpoint_url=s3_config[\"endpoint_url\"],\n        key=s3_config[\"key\"],\n        secret=s3_config[\"secret\"],\n        use_ssl=s3_config[\"use_ssl\"],\n        version_aware=s3_config[\"version_aware\"]\n    )\n    self.s3_path_base = s3_config[\"s3_path_base\"]\n</code></pre>"},{"location":"reference/ts_shape/loader/timeseries/s3proxy_parquet_loader/#ts_shape.loader.timeseries.s3proxy_parquet_loader.S3ProxyDataAccess.fetch_data_as_dataframe","title":"fetch_data_as_dataframe","text":"<pre><code>fetch_data_as_dataframe() -&gt; DataFrame\n</code></pre> <p>Retrieves timeseries data from S3 and returns it as a single DataFrame. :return: A combined DataFrame with data for all specified UUIDs and time slots.</p> Source code in <code>src/ts_shape/loader/timeseries/s3proxy_parquet_loader.py</code> <pre><code>def fetch_data_as_dataframe(self) -&gt; pd.DataFrame:\n    \"\"\"\n    Retrieves timeseries data from S3 and returns it as a single DataFrame.\n    :return: A combined DataFrame with data for all specified UUIDs and time slots.\n    \"\"\"\n    data_frames = [self._fetch_parquet(uuid, timeslot_dir) \n                   for timeslot_dir in self._generate_timeslot_paths()\n                   for uuid in set(self.uuids)]\n    return pd.concat([df for df in data_frames if df is not None], ignore_index=True) if data_frames else pd.DataFrame()\n</code></pre>"},{"location":"reference/ts_shape/loader/timeseries/s3proxy_parquet_loader/#ts_shape.loader.timeseries.s3proxy_parquet_loader.S3ProxyDataAccess.fetch_data_as_parquet","title":"fetch_data_as_parquet","text":"<pre><code>fetch_data_as_parquet(output_dir: str)\n</code></pre> <p>Retrieves timeseries data from S3 and saves it as Parquet files. Each file is saved in a directory structure of UUID/year/month/day/hour. :param output_dir: Base directory to save the Parquet files.</p> Source code in <code>src/ts_shape/loader/timeseries/s3proxy_parquet_loader.py</code> <pre><code>def fetch_data_as_parquet(self, output_dir: str):\n    \"\"\"\n    Retrieves timeseries data from S3 and saves it as Parquet files.\n    Each file is saved in a directory structure of UUID/year/month/day/hour.\n    :param output_dir: Base directory to save the Parquet files.\n    \"\"\"\n    for timeslot_dir in self._generate_timeslot_paths():\n        for uuid in set(self.uuids):\n            df = self._fetch_parquet(uuid, timeslot_dir)\n            if df is not None:\n                output_path = Path(output_dir, timeslot_dir)\n                output_path.mkdir(parents=True, exist_ok=True)\n                df.to_parquet(output_path / f\"{uuid}.parquet\")\n</code></pre>"},{"location":"reference/ts_shape/loader/timeseries/timescale_loader/","title":"timescale_loader","text":""},{"location":"reference/ts_shape/loader/timeseries/timescale_loader/#ts_shape.loader.timeseries.timescale_loader","title":"ts_shape.loader.timeseries.timescale_loader","text":"<p>Classes:</p> <ul> <li> <code>TimescaleDBDataAccess</code>           \u2013            </li> </ul>"},{"location":"reference/ts_shape/loader/timeseries/timescale_loader/#ts_shape.loader.timeseries.timescale_loader.TimescaleDBDataAccess","title":"TimescaleDBDataAccess","text":"<pre><code>TimescaleDBDataAccess(start_timestamp: str, end_timestamp: str, uuids: List[str], db_config: Dict[str, str])\n</code></pre> <p>Methods:</p> <ul> <li> <code>fetch_data_as_dataframe</code>             \u2013              <p>Retrieves timeseries data from TimescaleDB and returns it as a single DataFrame.</p> </li> </ul> Source code in <code>src/ts_shape/loader/timeseries/timescale_loader.py</code> <pre><code>def __init__(self, start_timestamp: str, end_timestamp: str, uuids: List[str], db_config: Dict[str, str]):\n    self.start_timestamp = start_timestamp\n    self.end_timestamp = end_timestamp\n    self.uuids = uuids\n    self.db_config = db_config\n    self.engine = create_engine(\n        f'postgresql+psycopg2://{db_config[\"db_user\"]}:{db_config[\"db_pass\"]}@{db_config[\"db_host\"]}/{db_config[\"db_name\"]}'\n    )\n</code></pre>"},{"location":"reference/ts_shape/loader/timeseries/timescale_loader/#ts_shape.loader.timeseries.timescale_loader.TimescaleDBDataAccess.fetch_data_as_dataframe","title":"fetch_data_as_dataframe","text":"<pre><code>fetch_data_as_dataframe() -&gt; DataFrame\n</code></pre> <p>Retrieves timeseries data from TimescaleDB and returns it as a single DataFrame. :return: A combined DataFrame with data for all specified UUIDs within the time range.</p> Source code in <code>src/ts_shape/loader/timeseries/timescale_loader.py</code> <pre><code>def fetch_data_as_dataframe(self) -&gt; pd.DataFrame:\n    \"\"\"\n    Retrieves timeseries data from TimescaleDB and returns it as a single DataFrame.\n    :return: A combined DataFrame with data for all specified UUIDs within the time range.\n    \"\"\"\n    df_list = [chunk for uuid in self.uuids for chunk in self._fetch_data(uuid)]\n    return pd.concat(df_list, ignore_index=True) if df_list else pd.DataFrame()\n</code></pre>"},{"location":"reference/ts_shape/transform/__init__/","title":"init","text":""},{"location":"reference/ts_shape/transform/__init__/#ts_shape.transform","title":"ts_shape.transform","text":"<p>Transform</p> <p>Row-level filtering, column transformations, and timestamp utilities.</p> <ul> <li>IntegerFilter: Equality/inequality/range filters for integer columns.</li> <li>filter_value_integer_match: Select rows equal to a value.</li> <li>filter_value_integer_not_match: Select rows not equal to a value.</li> <li> <p>filter_value_integer_between: Select rows within [min, max].</p> </li> <li> <p>DoubleFilter: NaN removal and numeric ranges for floating-point columns.</p> </li> <li>filter_nan_value_double: Drop rows with NaN in the column.</li> <li> <p>filter_value_double_between: Select rows within [min, max].</p> </li> <li> <p>StringFilter: Equality, contains, regex cleaning, and change detection.</p> </li> <li>filter_na_value_string: Drop rows with NA values.</li> <li>filter_value_string_match: Select rows equal to a string.</li> <li>filter_value_string_not_match: Select rows not equal to a string.</li> <li>filter_string_contains: Select rows containing a substring.</li> <li>regex_clean_value_string: Regex-based cleaning or replacement.</li> <li> <p>detect_changes_in_string: Detect row-to-row changes in a string column.</p> </li> <li> <p>BooleanFilter: Detect raising/falling edges of boolean states.</p> </li> <li>filter_falling_value_bool: True\u2192False transitions.</li> <li> <p>filter_raising_value_bool: False\u2192True transitions.</p> </li> <li> <p>IsDeltaFilter: Select rows by the is_delta flag.</p> </li> <li>filter_is_delta_true: Only True.</li> <li> <p>filter_is_delta_false: Only False.</p> </li> <li> <p>DateTimeFilter: Before/after/between filters for timestamps.</p> </li> <li>filter_after_date: After a given date.</li> <li>filter_before_date: Before a given date.</li> <li>filter_between_dates: Between start and end dates.</li> <li>filter_after_datetime: After a given datetime.</li> <li>filter_before_datetime: Before a given datetime.</li> <li> <p>filter_between_datetimes: Between start and end datetimes.</p> </li> <li> <p>CustomFilter: Free-form DataFrame.query string conditions.</p> </li> <li> <p>filter_custom_conditions: Apply a query string to filter rows.</p> </li> <li> <p>IntegerCalc: Numeric column calculations.</p> </li> <li>scale_column: Multiply by a factor.</li> <li>offset_column: Add a constant.</li> <li>divide_column: Divide by a constant.</li> <li>subtract_column: Subtract a constant.</li> <li>calculate_with_fixed_factors: Multiply then add.</li> <li>mod_column: Modulo operation.</li> <li> <p>power_column: Raise to a power.</p> </li> <li> <p>LambdaProcessor: Apply vectorized callables to columns.</p> </li> <li> <p>apply_function: Apply a Python callable over a column's values.</p> </li> <li> <p>TimestampConverter: Convert integer timestamps to tz-aware datetimes.</p> </li> <li> <p>convert_to_datetime: Convert s/ms/us/ns to datetime in a timezone.</p> </li> <li> <p>TimezoneShift: Timezone localization/conversion helpers.</p> </li> <li>shift_timezone: Convert timezones in-place.</li> <li>add_timezone_column: Add a converted timestamp column.</li> <li>detect_timezone_awareness: Check tz-awareness of a column.</li> <li>revert_to_original_timezone: Convert back to original tz.</li> <li>calculate_time_difference: Difference between two timestamp columns.</li> </ul> <p>Modules:</p> <ul> <li> <code>calculator</code>           \u2013            <p>Calculator</p> </li> <li> <code>filter</code>           \u2013            <p>Filters</p> </li> <li> <code>functions</code>           \u2013            <p>Functions</p> </li> <li> <code>time_functions</code>           \u2013            <p>Time Functions</p> </li> </ul>"},{"location":"reference/ts_shape/transform/calculator/__init__/","title":"init","text":""},{"location":"reference/ts_shape/transform/calculator/__init__/#ts_shape.transform.calculator","title":"ts_shape.transform.calculator","text":"<p>Calculator</p> <p>Numeric column calculations for engineered features.</p> <ul> <li>IntegerCalc: Operations for integer-like numeric columns.</li> <li>scale_column: Multiply by a factor.</li> <li>offset_column: Add a constant.</li> <li>divide_column: Divide by a constant.</li> <li>subtract_column: Subtract a constant.</li> <li>calculate_with_fixed_factors: Multiply then add.</li> <li>mod_column: Modulo operation.</li> <li>power_column: Raise to a power.</li> </ul> <p>Modules:</p> <ul> <li> <code>numeric_calc</code>           \u2013            </li> </ul>"},{"location":"reference/ts_shape/transform/calculator/numeric_calc/","title":"numeric_calc","text":""},{"location":"reference/ts_shape/transform/calculator/numeric_calc/#ts_shape.transform.calculator.numeric_calc","title":"ts_shape.transform.calculator.numeric_calc","text":"<p>Classes:</p> <ul> <li> <code>IntegerCalc</code>           \u2013            <p>Provides class methods for performing calculations on integer columns in a pandas DataFrame.</p> </li> </ul>"},{"location":"reference/ts_shape/transform/calculator/numeric_calc/#ts_shape.transform.calculator.numeric_calc.IntegerCalc","title":"IntegerCalc","text":"<pre><code>IntegerCalc(dataframe: DataFrame, column_name: str = 'systime')\n</code></pre> <p>               Bases: <code>Base</code></p> <p>Provides class methods for performing calculations on integer columns in a pandas DataFrame.</p> <p>Parameters:</p> <p>Methods:</p> <ul> <li> <code>calculate_with_fixed_factors</code>             \u2013              <p>Performs a calculation by multiplying with a factor and then adding an additional factor.</p> </li> <li> <code>divide_column</code>             \u2013              <p>Divides each value in the integer column by the given divisor.</p> </li> <li> <code>get_dataframe</code>             \u2013              <p>Returns the processed DataFrame.</p> </li> <li> <code>mod_column</code>             \u2013              <p>Performs a modulus operation on the integer column with a specified value.</p> </li> <li> <code>offset_column</code>             \u2013              <p>Offsets the integer column by the given value.</p> </li> <li> <code>power_column</code>             \u2013              <p>Raises each value in the integer column to the power of a specified value.</p> </li> <li> <code>scale_column</code>             \u2013              <p>Scales the integer column by the given factor.</p> </li> <li> <code>subtract_column</code>             \u2013              <p>Subtracts a given value from each element in the integer column.</p> </li> </ul> Source code in <code>src/ts_shape/utils/base.py</code> <pre><code>def __init__(self, dataframe: pd.DataFrame, column_name: str = 'systime') -&gt; None:\n    \"\"\"\n    Initializes the Base with a DataFrame, detects time columns, converts them to datetime,\n    and sorts the DataFrame by the specified column (or the detected time column if applicable).\n\n    Args:\n        dataframe (pd.DataFrame): The DataFrame to be processed.\n        column_name (str): The column to sort by. Default is 'systime'. If the column is not found or is not a time column, the class will attempt to detect other time columns.\n    \"\"\"\n    self.dataframe = dataframe.copy()\n\n    # Attempt to convert the specified column_name to datetime if it exists\n    if column_name in self.dataframe.columns:\n        self.dataframe[column_name] = pd.to_datetime(self.dataframe[column_name], errors='coerce')\n    else:\n        # If the column_name is not in the DataFrame, fallback to automatic time detection\n        time_columns = [col for col in self.dataframe.columns if 'time' in col.lower() or 'date' in col.lower()]\n\n        # Convert all detected time columns to datetime, if any\n        for col in time_columns:\n            self.dataframe[col] = pd.to_datetime(self.dataframe[col], errors='coerce')\n\n        # If any time columns are detected, sort by the first one; otherwise, do nothing\n        if time_columns:\n            column_name = time_columns[0]\n\n    # Sort by the datetime column (either specified or detected)\n    if column_name in self.dataframe.columns:\n        self.dataframe = self.dataframe.sort_values(by=column_name)\n</code></pre>"},{"location":"reference/ts_shape/transform/calculator/numeric_calc/#ts_shape.transform.calculator.numeric_calc.IntegerCalc(dataframe)","title":"<code>dataframe</code>","text":"(<code>DataFrame</code>)           \u2013            <p>The DataFrame to be processed.</p>"},{"location":"reference/ts_shape/transform/calculator/numeric_calc/#ts_shape.transform.calculator.numeric_calc.IntegerCalc(column_name)","title":"<code>column_name</code>","text":"(<code>str</code>, default:                   <code>'systime'</code> )           \u2013            <p>The column to sort by. Default is 'systime'. If the column is not found or is not a time column, the class will attempt to detect other time columns.</p>"},{"location":"reference/ts_shape/transform/calculator/numeric_calc/#ts_shape.transform.calculator.numeric_calc.IntegerCalc.calculate_with_fixed_factors","title":"calculate_with_fixed_factors  <code>classmethod</code>","text":"<pre><code>calculate_with_fixed_factors(dataframe: DataFrame, column_name: str = 'value_integer', multiply_factor: float = 1, add_factor: float = 0) -&gt; DataFrame\n</code></pre> <p>Performs a calculation by multiplying with a factor and then adding an additional factor.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>pd.DataFrame: The DataFrame after applying the calculations.</p> </li> </ul> Source code in <code>src/ts_shape/transform/calculator/numeric_calc.py</code> <pre><code>@classmethod\ndef calculate_with_fixed_factors(cls, dataframe: pd.DataFrame, column_name: str = 'value_integer', multiply_factor: float = 1, add_factor: float = 0) -&gt; pd.DataFrame:\n    \"\"\"\n    Performs a calculation by multiplying with a factor and then adding an additional factor.\n\n    Args:\n        dataframe (pd.DataFrame): The DataFrame to perform the operation on.\n        column_name (str): The column to apply the calculations to.\n        multiply_factor (float): The factor to multiply each value by. Defaults to 1 (no scaling).\n        add_factor (float): The value to add after multiplication. Defaults to 0 (no offset).\n\n    Returns:\n        pd.DataFrame: The DataFrame after applying the calculations.\n    \"\"\"\n    dataframe[column_name] = (dataframe[column_name] * multiply_factor) + add_factor\n    return dataframe\n</code></pre>"},{"location":"reference/ts_shape/transform/calculator/numeric_calc/#ts_shape.transform.calculator.numeric_calc.IntegerCalc.calculate_with_fixed_factors(dataframe)","title":"<code>dataframe</code>","text":"(<code>DataFrame</code>)           \u2013            <p>The DataFrame to perform the operation on.</p>"},{"location":"reference/ts_shape/transform/calculator/numeric_calc/#ts_shape.transform.calculator.numeric_calc.IntegerCalc.calculate_with_fixed_factors(column_name)","title":"<code>column_name</code>","text":"(<code>str</code>, default:                   <code>'value_integer'</code> )           \u2013            <p>The column to apply the calculations to.</p>"},{"location":"reference/ts_shape/transform/calculator/numeric_calc/#ts_shape.transform.calculator.numeric_calc.IntegerCalc.calculate_with_fixed_factors(multiply_factor)","title":"<code>multiply_factor</code>","text":"(<code>float</code>, default:                   <code>1</code> )           \u2013            <p>The factor to multiply each value by. Defaults to 1 (no scaling).</p>"},{"location":"reference/ts_shape/transform/calculator/numeric_calc/#ts_shape.transform.calculator.numeric_calc.IntegerCalc.calculate_with_fixed_factors(add_factor)","title":"<code>add_factor</code>","text":"(<code>float</code>, default:                   <code>0</code> )           \u2013            <p>The value to add after multiplication. Defaults to 0 (no offset).</p>"},{"location":"reference/ts_shape/transform/calculator/numeric_calc/#ts_shape.transform.calculator.numeric_calc.IntegerCalc.divide_column","title":"divide_column  <code>classmethod</code>","text":"<pre><code>divide_column(dataframe: DataFrame, column_name: str = 'value_integer', divisor: float = 1) -&gt; DataFrame\n</code></pre> <p>Divides each value in the integer column by the given divisor.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>pd.DataFrame: The DataFrame with the divided column.</p> </li> </ul> Source code in <code>src/ts_shape/transform/calculator/numeric_calc.py</code> <pre><code>@classmethod\ndef divide_column(cls, dataframe: pd.DataFrame, column_name: str = 'value_integer', divisor: float = 1) -&gt; pd.DataFrame:\n    \"\"\"\n    Divides each value in the integer column by the given divisor.\n\n    Args:\n        dataframe (pd.DataFrame): The DataFrame to perform the operation on.\n        column_name (str): The column to apply the division to.\n        divisor (float): The value by which to divide each element.\n\n    Returns:\n        pd.DataFrame: The DataFrame with the divided column.\n    \"\"\"\n    dataframe[column_name] = dataframe[column_name] / divisor\n    return dataframe\n</code></pre>"},{"location":"reference/ts_shape/transform/calculator/numeric_calc/#ts_shape.transform.calculator.numeric_calc.IntegerCalc.divide_column(dataframe)","title":"<code>dataframe</code>","text":"(<code>DataFrame</code>)           \u2013            <p>The DataFrame to perform the operation on.</p>"},{"location":"reference/ts_shape/transform/calculator/numeric_calc/#ts_shape.transform.calculator.numeric_calc.IntegerCalc.divide_column(column_name)","title":"<code>column_name</code>","text":"(<code>str</code>, default:                   <code>'value_integer'</code> )           \u2013            <p>The column to apply the division to.</p>"},{"location":"reference/ts_shape/transform/calculator/numeric_calc/#ts_shape.transform.calculator.numeric_calc.IntegerCalc.divide_column(divisor)","title":"<code>divisor</code>","text":"(<code>float</code>, default:                   <code>1</code> )           \u2013            <p>The value by which to divide each element.</p>"},{"location":"reference/ts_shape/transform/calculator/numeric_calc/#ts_shape.transform.calculator.numeric_calc.IntegerCalc.get_dataframe","title":"get_dataframe","text":"<pre><code>get_dataframe() -&gt; DataFrame\n</code></pre> <p>Returns the processed DataFrame.</p> Source code in <code>src/ts_shape/utils/base.py</code> <pre><code>def get_dataframe(self) -&gt; pd.DataFrame:\n    \"\"\"Returns the processed DataFrame.\"\"\"\n    return self.dataframe\n</code></pre>"},{"location":"reference/ts_shape/transform/calculator/numeric_calc/#ts_shape.transform.calculator.numeric_calc.IntegerCalc.mod_column","title":"mod_column  <code>classmethod</code>","text":"<pre><code>mod_column(dataframe: DataFrame, column_name: str = 'value_integer', mod_value: int = 1) -&gt; DataFrame\n</code></pre> <p>Performs a modulus operation on the integer column with a specified value.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>pd.DataFrame: The DataFrame with the modulus operation applied.</p> </li> </ul> Source code in <code>src/ts_shape/transform/calculator/numeric_calc.py</code> <pre><code>@classmethod\ndef mod_column(cls, dataframe: pd.DataFrame, column_name: str = 'value_integer', mod_value: int = 1) -&gt; pd.DataFrame:\n    \"\"\"\n    Performs a modulus operation on the integer column with a specified value.\n\n    Args:\n        dataframe (pd.DataFrame): The DataFrame to perform the operation on.\n        column_name (str): The column to apply the modulus operation to.\n        mod_value (int): The value to perform the modulus operation with.\n\n    Returns:\n        pd.DataFrame: The DataFrame with the modulus operation applied.\n    \"\"\"\n    dataframe[column_name] = dataframe[column_name] % mod_value\n    return dataframe\n</code></pre>"},{"location":"reference/ts_shape/transform/calculator/numeric_calc/#ts_shape.transform.calculator.numeric_calc.IntegerCalc.mod_column(dataframe)","title":"<code>dataframe</code>","text":"(<code>DataFrame</code>)           \u2013            <p>The DataFrame to perform the operation on.</p>"},{"location":"reference/ts_shape/transform/calculator/numeric_calc/#ts_shape.transform.calculator.numeric_calc.IntegerCalc.mod_column(column_name)","title":"<code>column_name</code>","text":"(<code>str</code>, default:                   <code>'value_integer'</code> )           \u2013            <p>The column to apply the modulus operation to.</p>"},{"location":"reference/ts_shape/transform/calculator/numeric_calc/#ts_shape.transform.calculator.numeric_calc.IntegerCalc.mod_column(mod_value)","title":"<code>mod_value</code>","text":"(<code>int</code>, default:                   <code>1</code> )           \u2013            <p>The value to perform the modulus operation with.</p>"},{"location":"reference/ts_shape/transform/calculator/numeric_calc/#ts_shape.transform.calculator.numeric_calc.IntegerCalc.offset_column","title":"offset_column  <code>classmethod</code>","text":"<pre><code>offset_column(dataframe: DataFrame, column_name: str = 'value_integer', offset_value: float = 0) -&gt; DataFrame\n</code></pre> <p>Offsets the integer column by the given value.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>pd.DataFrame: The DataFrame with the offset column.</p> </li> </ul> Source code in <code>src/ts_shape/transform/calculator/numeric_calc.py</code> <pre><code>@classmethod\ndef offset_column(cls, dataframe: pd.DataFrame, column_name: str = 'value_integer', offset_value: float = 0) -&gt; pd.DataFrame:\n    \"\"\"\n    Offsets the integer column by the given value.\n\n    Args:\n        dataframe (pd.DataFrame): The DataFrame to perform the operation on.\n        column_name (str): The column to apply the offset to.\n        offset_value (float): The value to add (positive) or subtract (negative) from each element in the column.\n\n    Returns:\n        pd.DataFrame: The DataFrame with the offset column.\n    \"\"\"\n    dataframe[column_name] = dataframe[column_name] + offset_value\n    return dataframe\n</code></pre>"},{"location":"reference/ts_shape/transform/calculator/numeric_calc/#ts_shape.transform.calculator.numeric_calc.IntegerCalc.offset_column(dataframe)","title":"<code>dataframe</code>","text":"(<code>DataFrame</code>)           \u2013            <p>The DataFrame to perform the operation on.</p>"},{"location":"reference/ts_shape/transform/calculator/numeric_calc/#ts_shape.transform.calculator.numeric_calc.IntegerCalc.offset_column(column_name)","title":"<code>column_name</code>","text":"(<code>str</code>, default:                   <code>'value_integer'</code> )           \u2013            <p>The column to apply the offset to.</p>"},{"location":"reference/ts_shape/transform/calculator/numeric_calc/#ts_shape.transform.calculator.numeric_calc.IntegerCalc.offset_column(offset_value)","title":"<code>offset_value</code>","text":"(<code>float</code>, default:                   <code>0</code> )           \u2013            <p>The value to add (positive) or subtract (negative) from each element in the column.</p>"},{"location":"reference/ts_shape/transform/calculator/numeric_calc/#ts_shape.transform.calculator.numeric_calc.IntegerCalc.power_column","title":"power_column  <code>classmethod</code>","text":"<pre><code>power_column(dataframe: DataFrame, column_name: str = 'value_integer', power_value: float = 1) -&gt; DataFrame\n</code></pre> <p>Raises each value in the integer column to the power of a specified value.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>pd.DataFrame: The DataFrame with the power operation applied.</p> </li> </ul> Source code in <code>src/ts_shape/transform/calculator/numeric_calc.py</code> <pre><code>@classmethod\ndef power_column(cls, dataframe: pd.DataFrame, column_name: str = 'value_integer', power_value: float = 1) -&gt; pd.DataFrame:\n    \"\"\"\n    Raises each value in the integer column to the power of a specified value.\n\n    Args:\n        dataframe (pd.DataFrame): The DataFrame to perform the operation on.\n        column_name (str): The column to apply the power operation to.\n        power_value (float): The exponent to raise each element to.\n\n    Returns:\n        pd.DataFrame: The DataFrame with the power operation applied.\n    \"\"\"\n    dataframe[column_name] = dataframe[column_name] ** power_value\n    return dataframe\n</code></pre>"},{"location":"reference/ts_shape/transform/calculator/numeric_calc/#ts_shape.transform.calculator.numeric_calc.IntegerCalc.power_column(dataframe)","title":"<code>dataframe</code>","text":"(<code>DataFrame</code>)           \u2013            <p>The DataFrame to perform the operation on.</p>"},{"location":"reference/ts_shape/transform/calculator/numeric_calc/#ts_shape.transform.calculator.numeric_calc.IntegerCalc.power_column(column_name)","title":"<code>column_name</code>","text":"(<code>str</code>, default:                   <code>'value_integer'</code> )           \u2013            <p>The column to apply the power operation to.</p>"},{"location":"reference/ts_shape/transform/calculator/numeric_calc/#ts_shape.transform.calculator.numeric_calc.IntegerCalc.power_column(power_value)","title":"<code>power_value</code>","text":"(<code>float</code>, default:                   <code>1</code> )           \u2013            <p>The exponent to raise each element to.</p>"},{"location":"reference/ts_shape/transform/calculator/numeric_calc/#ts_shape.transform.calculator.numeric_calc.IntegerCalc.scale_column","title":"scale_column  <code>classmethod</code>","text":"<pre><code>scale_column(dataframe: DataFrame, column_name: str = 'value_integer', factor: float = 1) -&gt; DataFrame\n</code></pre> <p>Scales the integer column by the given factor.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>pd.DataFrame: The DataFrame with the scaled column.</p> </li> </ul> Source code in <code>src/ts_shape/transform/calculator/numeric_calc.py</code> <pre><code>@classmethod\ndef scale_column(cls, dataframe: pd.DataFrame, column_name: str = 'value_integer', factor: float = 1) -&gt; pd.DataFrame:\n    \"\"\"\n    Scales the integer column by the given factor.\n\n    Args:\n        dataframe (pd.DataFrame): The DataFrame to perform the operation on.\n        column_name (str): The column to apply the scaling to.\n        factor (float): The scaling factor.\n\n    Returns:\n        pd.DataFrame: The DataFrame with the scaled column.\n    \"\"\"\n    dataframe[column_name] = dataframe[column_name] * factor\n    return dataframe\n</code></pre>"},{"location":"reference/ts_shape/transform/calculator/numeric_calc/#ts_shape.transform.calculator.numeric_calc.IntegerCalc.scale_column(dataframe)","title":"<code>dataframe</code>","text":"(<code>DataFrame</code>)           \u2013            <p>The DataFrame to perform the operation on.</p>"},{"location":"reference/ts_shape/transform/calculator/numeric_calc/#ts_shape.transform.calculator.numeric_calc.IntegerCalc.scale_column(column_name)","title":"<code>column_name</code>","text":"(<code>str</code>, default:                   <code>'value_integer'</code> )           \u2013            <p>The column to apply the scaling to.</p>"},{"location":"reference/ts_shape/transform/calculator/numeric_calc/#ts_shape.transform.calculator.numeric_calc.IntegerCalc.scale_column(factor)","title":"<code>factor</code>","text":"(<code>float</code>, default:                   <code>1</code> )           \u2013            <p>The scaling factor.</p>"},{"location":"reference/ts_shape/transform/calculator/numeric_calc/#ts_shape.transform.calculator.numeric_calc.IntegerCalc.subtract_column","title":"subtract_column  <code>classmethod</code>","text":"<pre><code>subtract_column(dataframe: DataFrame, column_name: str = 'value_integer', subtract_value: float = 0) -&gt; DataFrame\n</code></pre> <p>Subtracts a given value from each element in the integer column.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>pd.DataFrame: The DataFrame with the subtracted column.</p> </li> </ul> Source code in <code>src/ts_shape/transform/calculator/numeric_calc.py</code> <pre><code>@classmethod\ndef subtract_column(cls, dataframe: pd.DataFrame, column_name: str = 'value_integer', subtract_value: float = 0) -&gt; pd.DataFrame:\n    \"\"\"\n    Subtracts a given value from each element in the integer column.\n\n    Args:\n        dataframe (pd.DataFrame): The DataFrame to perform the operation on.\n        column_name (str): The column to apply the subtraction to.\n        subtract_value (float): The value to subtract from each element.\n\n    Returns:\n        pd.DataFrame: The DataFrame with the subtracted column.\n    \"\"\"\n    dataframe[column_name] = dataframe[column_name] - subtract_value\n    return dataframe\n</code></pre>"},{"location":"reference/ts_shape/transform/calculator/numeric_calc/#ts_shape.transform.calculator.numeric_calc.IntegerCalc.subtract_column(dataframe)","title":"<code>dataframe</code>","text":"(<code>DataFrame</code>)           \u2013            <p>The DataFrame to perform the operation on.</p>"},{"location":"reference/ts_shape/transform/calculator/numeric_calc/#ts_shape.transform.calculator.numeric_calc.IntegerCalc.subtract_column(column_name)","title":"<code>column_name</code>","text":"(<code>str</code>, default:                   <code>'value_integer'</code> )           \u2013            <p>The column to apply the subtraction to.</p>"},{"location":"reference/ts_shape/transform/calculator/numeric_calc/#ts_shape.transform.calculator.numeric_calc.IntegerCalc.subtract_column(subtract_value)","title":"<code>subtract_value</code>","text":"(<code>float</code>, default:                   <code>0</code> )           \u2013            <p>The value to subtract from each element.</p>"},{"location":"reference/ts_shape/transform/filter/__init__/","title":"init","text":""},{"location":"reference/ts_shape/transform/filter/__init__/#ts_shape.transform.filter","title":"ts_shape.transform.filter","text":"<p>Filters</p> <p>Row-level predicates for common column types.</p> <ul> <li>IntegerFilter: Equality/inequality/range filters for integer columns.</li> <li>filter_value_integer_match: Select rows equal to a value.</li> <li>filter_value_integer_not_match: Select rows not equal to a value.</li> <li> <p>filter_value_integer_between: Select rows within [min, max].</p> </li> <li> <p>DoubleFilter: NaN removal and numeric ranges for floating-point columns.</p> </li> <li>filter_nan_value_double: Drop rows with NaN in the column.</li> <li> <p>filter_value_double_between: Select rows within [min, max].</p> </li> <li> <p>StringFilter: Equality, contains, regex cleaning, and change detection.</p> </li> <li>filter_na_value_string: Drop rows with NA values.</li> <li>filter_value_string_match: Select rows equal to a string.</li> <li>filter_value_string_not_match: Select rows not equal to a string.</li> <li>filter_string_contains: Select rows containing a substring.</li> <li>regex_clean_value_string: Regex-based cleaning or replacement.</li> <li> <p>detect_changes_in_string: Detect row-to-row changes in a string column.</p> </li> <li> <p>BooleanFilter: Detect raising/falling edges of boolean states.</p> </li> <li>filter_falling_value_bool: True\u2192False transitions.</li> <li> <p>filter_raising_value_bool: False\u2192True transitions.</p> </li> <li> <p>IsDeltaFilter: Select rows by the is_delta flag.</p> </li> <li>filter_is_delta_true: Only True.</li> <li> <p>filter_is_delta_false: Only False.</p> </li> <li> <p>DateTimeFilter: Before/after/between filters for timestamps.</p> </li> <li>filter_after_date: After a given date.</li> <li>filter_before_date: Before a given date.</li> <li>filter_between_dates: Between start and end dates.</li> <li>filter_after_datetime: After a given datetime.</li> <li>filter_before_datetime: Before a given datetime.</li> <li> <p>filter_between_datetimes: Between start and end datetimes.</p> </li> <li> <p>CustomFilter: Free-form DataFrame.query string conditions.</p> </li> <li>filter_custom_conditions: Apply a query string to filter rows.</li> </ul> <p>Modules:</p> <ul> <li> <code>boolean_filter</code>           \u2013            </li> <li> <code>custom_filter</code>           \u2013            </li> <li> <code>datetime_filter</code>           \u2013            </li> <li> <code>numeric_filter</code>           \u2013            </li> <li> <code>string_filter</code>           \u2013            </li> </ul>"},{"location":"reference/ts_shape/transform/filter/boolean_filter/","title":"boolean_filter","text":""},{"location":"reference/ts_shape/transform/filter/boolean_filter/#ts_shape.transform.filter.boolean_filter","title":"ts_shape.transform.filter.boolean_filter","text":"<p>Classes:</p> <ul> <li> <code>BooleanFilter</code>           \u2013            <p>Provides class methods for filtering boolean columns in a pandas DataFrame,</p> </li> <li> <code>IsDeltaFilter</code>           \u2013            <p>Provides class methods for filtering is_delta columns in a pandas DataFrame.</p> </li> </ul>"},{"location":"reference/ts_shape/transform/filter/boolean_filter/#ts_shape.transform.filter.boolean_filter.BooleanFilter","title":"BooleanFilter","text":"<pre><code>BooleanFilter(dataframe: DataFrame, column_name: str = 'systime')\n</code></pre> <p>               Bases: <code>Base</code></p> <p>Provides class methods for filtering boolean columns in a pandas DataFrame, particularly focusing on status changes.</p> <p>Parameters:</p> <p>Methods:</p> <ul> <li> <code>filter_falling_value_bool</code>             \u2013              <p>Filters rows where 'value_bool' changes from True to False.</p> </li> <li> <code>filter_raising_value_bool</code>             \u2013              <p>Filters rows where 'value_bool' changes from False to True.</p> </li> <li> <code>get_dataframe</code>             \u2013              <p>Returns the processed DataFrame.</p> </li> </ul> Source code in <code>src/ts_shape/utils/base.py</code> <pre><code>def __init__(self, dataframe: pd.DataFrame, column_name: str = 'systime') -&gt; None:\n    \"\"\"\n    Initializes the Base with a DataFrame, detects time columns, converts them to datetime,\n    and sorts the DataFrame by the specified column (or the detected time column if applicable).\n\n    Args:\n        dataframe (pd.DataFrame): The DataFrame to be processed.\n        column_name (str): The column to sort by. Default is 'systime'. If the column is not found or is not a time column, the class will attempt to detect other time columns.\n    \"\"\"\n    self.dataframe = dataframe.copy()\n\n    # Attempt to convert the specified column_name to datetime if it exists\n    if column_name in self.dataframe.columns:\n        self.dataframe[column_name] = pd.to_datetime(self.dataframe[column_name], errors='coerce')\n    else:\n        # If the column_name is not in the DataFrame, fallback to automatic time detection\n        time_columns = [col for col in self.dataframe.columns if 'time' in col.lower() or 'date' in col.lower()]\n\n        # Convert all detected time columns to datetime, if any\n        for col in time_columns:\n            self.dataframe[col] = pd.to_datetime(self.dataframe[col], errors='coerce')\n\n        # If any time columns are detected, sort by the first one; otherwise, do nothing\n        if time_columns:\n            column_name = time_columns[0]\n\n    # Sort by the datetime column (either specified or detected)\n    if column_name in self.dataframe.columns:\n        self.dataframe = self.dataframe.sort_values(by=column_name)\n</code></pre>"},{"location":"reference/ts_shape/transform/filter/boolean_filter/#ts_shape.transform.filter.boolean_filter.BooleanFilter(dataframe)","title":"<code>dataframe</code>","text":"(<code>DataFrame</code>)           \u2013            <p>The DataFrame to be processed.</p>"},{"location":"reference/ts_shape/transform/filter/boolean_filter/#ts_shape.transform.filter.boolean_filter.BooleanFilter(column_name)","title":"<code>column_name</code>","text":"(<code>str</code>, default:                   <code>'systime'</code> )           \u2013            <p>The column to sort by. Default is 'systime'. If the column is not found or is not a time column, the class will attempt to detect other time columns.</p>"},{"location":"reference/ts_shape/transform/filter/boolean_filter/#ts_shape.transform.filter.boolean_filter.BooleanFilter.filter_falling_value_bool","title":"filter_falling_value_bool  <code>classmethod</code>","text":"<pre><code>filter_falling_value_bool(dataframe: DataFrame, column_name: str = 'value_bool') -&gt; DataFrame\n</code></pre> <p>Filters rows where 'value_bool' changes from True to False.</p> Source code in <code>src/ts_shape/transform/filter/boolean_filter.py</code> <pre><code>@classmethod\ndef filter_falling_value_bool(cls, dataframe: pd.DataFrame, column_name: str = 'value_bool') -&gt; pd.DataFrame:\n    \"\"\"Filters rows where 'value_bool' changes from True to False.\"\"\"\n    dataframe['previous_value_bool'] = dataframe[column_name].shift(1)\n    return dataframe[(dataframe['previous_value_bool'] == True) &amp; (dataframe[column_name] == False)]\n</code></pre>"},{"location":"reference/ts_shape/transform/filter/boolean_filter/#ts_shape.transform.filter.boolean_filter.BooleanFilter.filter_raising_value_bool","title":"filter_raising_value_bool  <code>classmethod</code>","text":"<pre><code>filter_raising_value_bool(dataframe: DataFrame, column_name: str = 'value_bool') -&gt; DataFrame\n</code></pre> <p>Filters rows where 'value_bool' changes from False to True.</p> Source code in <code>src/ts_shape/transform/filter/boolean_filter.py</code> <pre><code>@classmethod\ndef filter_raising_value_bool(cls, dataframe: pd.DataFrame, column_name: str = 'value_bool') -&gt; pd.DataFrame:\n    \"\"\"Filters rows where 'value_bool' changes from False to True.\"\"\"\n    dataframe['previous_value_bool'] = dataframe[column_name].shift(1)\n    return dataframe[(dataframe['previous_value_bool'] == False) &amp; (dataframe[column_name] == True)]\n</code></pre>"},{"location":"reference/ts_shape/transform/filter/boolean_filter/#ts_shape.transform.filter.boolean_filter.BooleanFilter.get_dataframe","title":"get_dataframe","text":"<pre><code>get_dataframe() -&gt; DataFrame\n</code></pre> <p>Returns the processed DataFrame.</p> Source code in <code>src/ts_shape/utils/base.py</code> <pre><code>def get_dataframe(self) -&gt; pd.DataFrame:\n    \"\"\"Returns the processed DataFrame.\"\"\"\n    return self.dataframe\n</code></pre>"},{"location":"reference/ts_shape/transform/filter/boolean_filter/#ts_shape.transform.filter.boolean_filter.IsDeltaFilter","title":"IsDeltaFilter","text":"<pre><code>IsDeltaFilter(dataframe: DataFrame, column_name: str = 'systime')\n</code></pre> <p>               Bases: <code>Base</code></p> <p>Provides class methods for filtering is_delta columns in a pandas DataFrame.</p> <p>Parameters:</p> <p>Methods:</p> <ul> <li> <code>filter_is_delta_false</code>             \u2013              <p>Filters rows where 'is_delta' is False.</p> </li> <li> <code>filter_is_delta_true</code>             \u2013              <p>Filters rows where 'is_delta' is True.</p> </li> <li> <code>get_dataframe</code>             \u2013              <p>Returns the processed DataFrame.</p> </li> </ul> Source code in <code>src/ts_shape/utils/base.py</code> <pre><code>def __init__(self, dataframe: pd.DataFrame, column_name: str = 'systime') -&gt; None:\n    \"\"\"\n    Initializes the Base with a DataFrame, detects time columns, converts them to datetime,\n    and sorts the DataFrame by the specified column (or the detected time column if applicable).\n\n    Args:\n        dataframe (pd.DataFrame): The DataFrame to be processed.\n        column_name (str): The column to sort by. Default is 'systime'. If the column is not found or is not a time column, the class will attempt to detect other time columns.\n    \"\"\"\n    self.dataframe = dataframe.copy()\n\n    # Attempt to convert the specified column_name to datetime if it exists\n    if column_name in self.dataframe.columns:\n        self.dataframe[column_name] = pd.to_datetime(self.dataframe[column_name], errors='coerce')\n    else:\n        # If the column_name is not in the DataFrame, fallback to automatic time detection\n        time_columns = [col for col in self.dataframe.columns if 'time' in col.lower() or 'date' in col.lower()]\n\n        # Convert all detected time columns to datetime, if any\n        for col in time_columns:\n            self.dataframe[col] = pd.to_datetime(self.dataframe[col], errors='coerce')\n\n        # If any time columns are detected, sort by the first one; otherwise, do nothing\n        if time_columns:\n            column_name = time_columns[0]\n\n    # Sort by the datetime column (either specified or detected)\n    if column_name in self.dataframe.columns:\n        self.dataframe = self.dataframe.sort_values(by=column_name)\n</code></pre>"},{"location":"reference/ts_shape/transform/filter/boolean_filter/#ts_shape.transform.filter.boolean_filter.IsDeltaFilter(dataframe)","title":"<code>dataframe</code>","text":"(<code>DataFrame</code>)           \u2013            <p>The DataFrame to be processed.</p>"},{"location":"reference/ts_shape/transform/filter/boolean_filter/#ts_shape.transform.filter.boolean_filter.IsDeltaFilter(column_name)","title":"<code>column_name</code>","text":"(<code>str</code>, default:                   <code>'systime'</code> )           \u2013            <p>The column to sort by. Default is 'systime'. If the column is not found or is not a time column, the class will attempt to detect other time columns.</p>"},{"location":"reference/ts_shape/transform/filter/boolean_filter/#ts_shape.transform.filter.boolean_filter.IsDeltaFilter.filter_is_delta_false","title":"filter_is_delta_false  <code>classmethod</code>","text":"<pre><code>filter_is_delta_false(dataframe: DataFrame, column_name: str = 'is_delta') -&gt; DataFrame\n</code></pre> <p>Filters rows where 'is_delta' is False.</p> Source code in <code>src/ts_shape/transform/filter/boolean_filter.py</code> <pre><code>@classmethod\ndef filter_is_delta_false(cls, dataframe: pd.DataFrame, column_name: str = 'is_delta') -&gt; pd.DataFrame:\n    \"\"\"Filters rows where 'is_delta' is False.\"\"\"\n    return dataframe[dataframe[column_name] == False]\n</code></pre>"},{"location":"reference/ts_shape/transform/filter/boolean_filter/#ts_shape.transform.filter.boolean_filter.IsDeltaFilter.filter_is_delta_true","title":"filter_is_delta_true  <code>classmethod</code>","text":"<pre><code>filter_is_delta_true(dataframe: DataFrame, column_name: str = 'is_delta') -&gt; DataFrame\n</code></pre> <p>Filters rows where 'is_delta' is True.</p> Source code in <code>src/ts_shape/transform/filter/boolean_filter.py</code> <pre><code>@classmethod\ndef filter_is_delta_true(cls, dataframe: pd.DataFrame, column_name: str = 'is_delta') -&gt; pd.DataFrame:\n    \"\"\"Filters rows where 'is_delta' is True.\"\"\"\n    # No need for instance, working directly on class level\n    return dataframe[dataframe[column_name] == True]\n</code></pre>"},{"location":"reference/ts_shape/transform/filter/boolean_filter/#ts_shape.transform.filter.boolean_filter.IsDeltaFilter.get_dataframe","title":"get_dataframe","text":"<pre><code>get_dataframe() -&gt; DataFrame\n</code></pre> <p>Returns the processed DataFrame.</p> Source code in <code>src/ts_shape/utils/base.py</code> <pre><code>def get_dataframe(self) -&gt; pd.DataFrame:\n    \"\"\"Returns the processed DataFrame.\"\"\"\n    return self.dataframe\n</code></pre>"},{"location":"reference/ts_shape/transform/filter/custom_filter/","title":"custom_filter","text":""},{"location":"reference/ts_shape/transform/filter/custom_filter/#ts_shape.transform.filter.custom_filter","title":"ts_shape.transform.filter.custom_filter","text":"<p>Classes:</p> <ul> <li> <code>CustomFilter</code>           \u2013            </li> </ul>"},{"location":"reference/ts_shape/transform/filter/custom_filter/#ts_shape.transform.filter.custom_filter.CustomFilter","title":"CustomFilter","text":"<pre><code>CustomFilter(dataframe: DataFrame, column_name: str = 'systime')\n</code></pre> <p>               Bases: <code>Base</code></p> <p>Parameters:</p> <p>Methods:</p> <ul> <li> <code>filter_custom_conditions</code>             \u2013              <p>Filters the DataFrame based on a set of user-defined conditions passed as a string.</p> </li> <li> <code>get_dataframe</code>             \u2013              <p>Returns the processed DataFrame.</p> </li> </ul> Source code in <code>src/ts_shape/utils/base.py</code> <pre><code>def __init__(self, dataframe: pd.DataFrame, column_name: str = 'systime') -&gt; None:\n    \"\"\"\n    Initializes the Base with a DataFrame, detects time columns, converts them to datetime,\n    and sorts the DataFrame by the specified column (or the detected time column if applicable).\n\n    Args:\n        dataframe (pd.DataFrame): The DataFrame to be processed.\n        column_name (str): The column to sort by. Default is 'systime'. If the column is not found or is not a time column, the class will attempt to detect other time columns.\n    \"\"\"\n    self.dataframe = dataframe.copy()\n\n    # Attempt to convert the specified column_name to datetime if it exists\n    if column_name in self.dataframe.columns:\n        self.dataframe[column_name] = pd.to_datetime(self.dataframe[column_name], errors='coerce')\n    else:\n        # If the column_name is not in the DataFrame, fallback to automatic time detection\n        time_columns = [col for col in self.dataframe.columns if 'time' in col.lower() or 'date' in col.lower()]\n\n        # Convert all detected time columns to datetime, if any\n        for col in time_columns:\n            self.dataframe[col] = pd.to_datetime(self.dataframe[col], errors='coerce')\n\n        # If any time columns are detected, sort by the first one; otherwise, do nothing\n        if time_columns:\n            column_name = time_columns[0]\n\n    # Sort by the datetime column (either specified or detected)\n    if column_name in self.dataframe.columns:\n        self.dataframe = self.dataframe.sort_values(by=column_name)\n</code></pre>"},{"location":"reference/ts_shape/transform/filter/custom_filter/#ts_shape.transform.filter.custom_filter.CustomFilter(dataframe)","title":"<code>dataframe</code>","text":"(<code>DataFrame</code>)           \u2013            <p>The DataFrame to be processed.</p>"},{"location":"reference/ts_shape/transform/filter/custom_filter/#ts_shape.transform.filter.custom_filter.CustomFilter(column_name)","title":"<code>column_name</code>","text":"(<code>str</code>, default:                   <code>'systime'</code> )           \u2013            <p>The column to sort by. Default is 'systime'. If the column is not found or is not a time column, the class will attempt to detect other time columns.</p>"},{"location":"reference/ts_shape/transform/filter/custom_filter/#ts_shape.transform.filter.custom_filter.CustomFilter.filter_custom_conditions","title":"filter_custom_conditions  <code>classmethod</code>","text":"<pre><code>filter_custom_conditions(dataframe: DataFrame, conditions: str) -&gt; DataFrame\n</code></pre> <p>Filters the DataFrame based on a set of user-defined conditions passed as a string.</p> <p>This method allows for flexible data filtering by evaluating a condition or multiple conditions specified in the 'conditions' parameter. The conditions must be provided as a string that can be interpreted by pandas' DataFrame.query() method.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>pd.DataFrame: A DataFrame containing only the rows that meet the specified conditions.</p> </li> </ul>"},{"location":"reference/ts_shape/transform/filter/custom_filter/#ts_shape.transform.filter.custom_filter.CustomFilter.filter_custom_conditions(dataframe)","title":"<code>dataframe</code>","text":"(<code>DataFrame</code>)           \u2013            <p>The DataFrame to apply the filter on.</p>"},{"location":"reference/ts_shape/transform/filter/custom_filter/#ts_shape.transform.filter.custom_filter.CustomFilter.filter_custom_conditions(conditions)","title":"<code>conditions</code>","text":"(<code>str</code>)           \u2013            <p>A string representing the conditions to filter the DataFrame.             The string should be formatted according to pandas query syntax.</p>"},{"location":"reference/ts_shape/transform/filter/custom_filter/#ts_shape.transform.filter.custom_filter.CustomFilter.filter_custom_conditions--example","title":"Example:","text":""},{"location":"reference/ts_shape/transform/filter/custom_filter/#ts_shape.transform.filter.custom_filter.CustomFilter.filter_custom_conditions--given-a-dataframe-df-containing-columns-age-and-score","title":"Given a DataFrame 'df' containing columns 'age' and 'score':","text":"<p>filtered_data = CustomFilter.filter_custom_conditions(df, \"age &gt; 30 and score &gt; 80\") print(filtered_data)</p> Note <p>Ensure that the column names and values used in conditions match those in the DataFrame. Complex expressions and functions available in pandas query syntax can also be used.</p> Source code in <code>src/ts_shape/transform/filter/custom_filter.py</code> <pre><code>@classmethod\ndef filter_custom_conditions(cls, dataframe: pd.DataFrame, conditions: str) -&gt; pd.DataFrame:\n    \"\"\"\n    Filters the DataFrame based on a set of user-defined conditions passed as a string.\n\n    This method allows for flexible data filtering by evaluating a condition or multiple conditions\n    specified in the 'conditions' parameter. The conditions must be provided as a string\n    that can be interpreted by pandas' DataFrame.query() method.\n\n    Args:\n        dataframe (pd.DataFrame): The DataFrame to apply the filter on.\n        conditions (str): A string representing the conditions to filter the DataFrame.\n                        The string should be formatted according to pandas query syntax.\n\n    Returns:\n        pd.DataFrame: A DataFrame containing only the rows that meet the specified conditions.\n\n    Example:\n    --------\n    # Given a DataFrame 'df' containing columns 'age' and 'score':\n    &gt;&gt;&gt; filtered_data = CustomFilter.filter_custom_conditions(df, \"age &gt; 30 and score &gt; 80\")\n    &gt;&gt;&gt; print(filtered_data)\n\n    Note:\n        Ensure that the column names and values used in conditions match those in the DataFrame.\n        Complex expressions and functions available in pandas query syntax can also be used.\n    \"\"\"\n    return dataframe.query(conditions)\n</code></pre>"},{"location":"reference/ts_shape/transform/filter/custom_filter/#ts_shape.transform.filter.custom_filter.CustomFilter.get_dataframe","title":"get_dataframe","text":"<pre><code>get_dataframe() -&gt; DataFrame\n</code></pre> <p>Returns the processed DataFrame.</p> Source code in <code>src/ts_shape/utils/base.py</code> <pre><code>def get_dataframe(self) -&gt; pd.DataFrame:\n    \"\"\"Returns the processed DataFrame.\"\"\"\n    return self.dataframe\n</code></pre>"},{"location":"reference/ts_shape/transform/filter/datetime_filter/","title":"datetime_filter","text":""},{"location":"reference/ts_shape/transform/filter/datetime_filter/#ts_shape.transform.filter.datetime_filter","title":"ts_shape.transform.filter.datetime_filter","text":"<p>Classes:</p> <ul> <li> <code>DateTimeFilter</code>           \u2013            <p>Provides class methods for filtering time columns in a pandas DataFrame.</p> </li> </ul>"},{"location":"reference/ts_shape/transform/filter/datetime_filter/#ts_shape.transform.filter.datetime_filter.DateTimeFilter","title":"DateTimeFilter","text":"<pre><code>DateTimeFilter(dataframe: DataFrame, column_name: str = 'systime')\n</code></pre> <p>               Bases: <code>Base</code></p> <p>Provides class methods for filtering time columns in a pandas DataFrame. Allows specification of which column to operate on.</p> Inherits from <p>Base (class): Base class with common initializations for DataFrame handling.</p> <p>Parameters:</p> <p>Methods:</p> <ul> <li> <code>filter_after_date</code>             \u2013              <p>Filters the DataFrame to include only rows after the specified date.</p> </li> <li> <code>filter_after_datetime</code>             \u2013              <p>Filters the DataFrame to include only rows after the specified datetime.</p> </li> <li> <code>filter_before_date</code>             \u2013              <p>Filters the DataFrame to include only rows before the specified date.</p> </li> <li> <code>filter_before_datetime</code>             \u2013              <p>Filters the DataFrame to include only rows before the specified datetime.</p> </li> <li> <code>filter_between_dates</code>             \u2013              <p>Filters the DataFrame to include only rows between the specified start and end dates.</p> </li> <li> <code>filter_between_datetimes</code>             \u2013              <p>Filters the DataFrame to include only rows between the specified start and end datetimes.</p> </li> <li> <code>get_dataframe</code>             \u2013              <p>Returns the processed DataFrame.</p> </li> </ul> Source code in <code>src/ts_shape/utils/base.py</code> <pre><code>def __init__(self, dataframe: pd.DataFrame, column_name: str = 'systime') -&gt; None:\n    \"\"\"\n    Initializes the Base with a DataFrame, detects time columns, converts them to datetime,\n    and sorts the DataFrame by the specified column (or the detected time column if applicable).\n\n    Args:\n        dataframe (pd.DataFrame): The DataFrame to be processed.\n        column_name (str): The column to sort by. Default is 'systime'. If the column is not found or is not a time column, the class will attempt to detect other time columns.\n    \"\"\"\n    self.dataframe = dataframe.copy()\n\n    # Attempt to convert the specified column_name to datetime if it exists\n    if column_name in self.dataframe.columns:\n        self.dataframe[column_name] = pd.to_datetime(self.dataframe[column_name], errors='coerce')\n    else:\n        # If the column_name is not in the DataFrame, fallback to automatic time detection\n        time_columns = [col for col in self.dataframe.columns if 'time' in col.lower() or 'date' in col.lower()]\n\n        # Convert all detected time columns to datetime, if any\n        for col in time_columns:\n            self.dataframe[col] = pd.to_datetime(self.dataframe[col], errors='coerce')\n\n        # If any time columns are detected, sort by the first one; otherwise, do nothing\n        if time_columns:\n            column_name = time_columns[0]\n\n    # Sort by the datetime column (either specified or detected)\n    if column_name in self.dataframe.columns:\n        self.dataframe = self.dataframe.sort_values(by=column_name)\n</code></pre>"},{"location":"reference/ts_shape/transform/filter/datetime_filter/#ts_shape.transform.filter.datetime_filter.DateTimeFilter(dataframe)","title":"<code>dataframe</code>","text":"(<code>DataFrame</code>)           \u2013            <p>The DataFrame to be processed.</p>"},{"location":"reference/ts_shape/transform/filter/datetime_filter/#ts_shape.transform.filter.datetime_filter.DateTimeFilter(column_name)","title":"<code>column_name</code>","text":"(<code>str</code>, default:                   <code>'systime'</code> )           \u2013            <p>The column to sort by. Default is 'systime'. If the column is not found or is not a time column, the class will attempt to detect other time columns.</p>"},{"location":"reference/ts_shape/transform/filter/datetime_filter/#ts_shape.transform.filter.datetime_filter.DateTimeFilter.filter_after_date","title":"filter_after_date  <code>classmethod</code>","text":"<pre><code>filter_after_date(dataframe: DataFrame, column_name: str = 'systime', date: str = None) -&gt; DataFrame\n</code></pre> <p>Filters the DataFrame to include only rows after the specified date.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>pd.DataFrame: A DataFrame containing rows where the 'systime' is after the specified date.</p> </li> </ul>"},{"location":"reference/ts_shape/transform/filter/datetime_filter/#ts_shape.transform.filter.datetime_filter.DateTimeFilter.filter_after_date(date)","title":"<code>date</code>","text":"(<code>str</code>, default:                   <code>None</code> )           \u2013            <p>The cutoff date in 'YYYY-MM-DD' format.</p>"},{"location":"reference/ts_shape/transform/filter/datetime_filter/#ts_shape.transform.filter.datetime_filter.DateTimeFilter.filter_after_date--example","title":"Example:","text":"<p>filtered_data = DateTimeFilter.filter_after_date(df, \"systime\", \"2023-01-01\") print(filtered_data)</p> Source code in <code>src/ts_shape/transform/filter/datetime_filter.py</code> <pre><code>@classmethod\ndef filter_after_date(cls, dataframe: pd.DataFrame, column_name: str = 'systime', date: str = None) -&gt; pd.DataFrame:\n    \"\"\"\n    Filters the DataFrame to include only rows after the specified date.\n\n    Args:\n        date (str): The cutoff date in 'YYYY-MM-DD' format.\n\n    Returns:\n        pd.DataFrame: A DataFrame containing rows where the 'systime' is after the specified date.\n\n    Example:\n    --------\n    &gt;&gt;&gt; filtered_data = DateTimeFilter.filter_after_date(df, \"systime\", \"2023-01-01\")\n    &gt;&gt;&gt; print(filtered_data)\n    \"\"\"\n    return dataframe[dataframe[column_name] &gt; pd.to_datetime(date)]\n</code></pre>"},{"location":"reference/ts_shape/transform/filter/datetime_filter/#ts_shape.transform.filter.datetime_filter.DateTimeFilter.filter_after_datetime","title":"filter_after_datetime  <code>classmethod</code>","text":"<pre><code>filter_after_datetime(dataframe: DataFrame, column_name: str = 'systime', datetime: str = None) -&gt; DataFrame\n</code></pre> <p>Filters the DataFrame to include only rows after the specified datetime.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>pd.DataFrame: A DataFrame containing rows where the 'systime' is after the specified datetime.</p> </li> </ul>"},{"location":"reference/ts_shape/transform/filter/datetime_filter/#ts_shape.transform.filter.datetime_filter.DateTimeFilter.filter_after_datetime(datetime)","title":"<code>datetime</code>","text":"(<code>str</code>, default:                   <code>None</code> )           \u2013            <p>The cutoff datetime in 'YYYY-MM-DD HH:MM:SS' format.</p>"},{"location":"reference/ts_shape/transform/filter/datetime_filter/#ts_shape.transform.filter.datetime_filter.DateTimeFilter.filter_after_datetime--example","title":"Example:","text":"<p>filtered_data = DateTimeFilter.filter_after_datetime(df, \"systime\", \"2023-01-01 12:00:00\") print(filtered_data)</p> Source code in <code>src/ts_shape/transform/filter/datetime_filter.py</code> <pre><code>@classmethod\ndef filter_after_datetime(cls, dataframe: pd.DataFrame, column_name: str = 'systime', datetime: str = None) -&gt; pd.DataFrame:\n    \"\"\"\n    Filters the DataFrame to include only rows after the specified datetime.\n\n    Args:\n        datetime (str): The cutoff datetime in 'YYYY-MM-DD HH:MM:SS' format.\n\n    Returns:\n        pd.DataFrame: A DataFrame containing rows where the 'systime' is after the specified datetime.\n\n    Example:\n    --------\n    &gt;&gt;&gt; filtered_data = DateTimeFilter.filter_after_datetime(df, \"systime\", \"2023-01-01 12:00:00\")\n    &gt;&gt;&gt; print(filtered_data)\n    \"\"\"\n    return dataframe[dataframe[column_name] &gt; pd.to_datetime(datetime)]\n</code></pre>"},{"location":"reference/ts_shape/transform/filter/datetime_filter/#ts_shape.transform.filter.datetime_filter.DateTimeFilter.filter_before_date","title":"filter_before_date  <code>classmethod</code>","text":"<pre><code>filter_before_date(dataframe: DataFrame, column_name: str = 'systime', date: str = None) -&gt; DataFrame\n</code></pre> <p>Filters the DataFrame to include only rows before the specified date.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>pd.DataFrame: A DataFrame containing rows where the 'systime' is before the specified date.</p> </li> </ul>"},{"location":"reference/ts_shape/transform/filter/datetime_filter/#ts_shape.transform.filter.datetime_filter.DateTimeFilter.filter_before_date(date)","title":"<code>date</code>","text":"(<code>str</code>, default:                   <code>None</code> )           \u2013            <p>The cutoff date in 'YYYY-MM-DD' format.</p>"},{"location":"reference/ts_shape/transform/filter/datetime_filter/#ts_shape.transform.filter.datetime_filter.DateTimeFilter.filter_before_date--example","title":"Example:","text":"<p>filtered_data = DateTimeFilter.filter_before_date(df, \"systime\", \"2023-01-01\") print(filtered_data)</p> Source code in <code>src/ts_shape/transform/filter/datetime_filter.py</code> <pre><code>@classmethod\ndef filter_before_date(cls, dataframe: pd.DataFrame, column_name: str = 'systime', date: str = None) -&gt; pd.DataFrame:\n    \"\"\"\n    Filters the DataFrame to include only rows before the specified date.\n\n    Args:\n        date (str): The cutoff date in 'YYYY-MM-DD' format.\n\n    Returns:\n        pd.DataFrame: A DataFrame containing rows where the 'systime' is before the specified date.\n\n    Example:\n    --------\n    &gt;&gt;&gt; filtered_data = DateTimeFilter.filter_before_date(df, \"systime\", \"2023-01-01\")\n    &gt;&gt;&gt; print(filtered_data)\n    \"\"\"\n    return dataframe[dataframe[column_name] &lt; pd.to_datetime(date)]\n</code></pre>"},{"location":"reference/ts_shape/transform/filter/datetime_filter/#ts_shape.transform.filter.datetime_filter.DateTimeFilter.filter_before_datetime","title":"filter_before_datetime  <code>classmethod</code>","text":"<pre><code>filter_before_datetime(dataframe: DataFrame, column_name: str = 'systime', datetime: str = None) -&gt; DataFrame\n</code></pre> <p>Filters the DataFrame to include only rows before the specified datetime.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>pd.DataFrame: A DataFrame containing rows where the 'systime' is before the specified datetime.</p> </li> </ul>"},{"location":"reference/ts_shape/transform/filter/datetime_filter/#ts_shape.transform.filter.datetime_filter.DateTimeFilter.filter_before_datetime(datetime)","title":"<code>datetime</code>","text":"(<code>str</code>, default:                   <code>None</code> )           \u2013            <p>The cutoff datetime in 'YYYY-MM-DD HH:MM:SS' format.</p>"},{"location":"reference/ts_shape/transform/filter/datetime_filter/#ts_shape.transform.filter.datetime_filter.DateTimeFilter.filter_before_datetime--example","title":"Example:","text":"<p>filtered_data = DateTimeFilter.filter_before_datetime(df, \"systime\", \"2023-01-01 12:00:00\") print(filtered_data)</p> Source code in <code>src/ts_shape/transform/filter/datetime_filter.py</code> <pre><code>@classmethod\ndef filter_before_datetime(cls, dataframe: pd.DataFrame, column_name: str = 'systime', datetime: str = None) -&gt; pd.DataFrame:\n    \"\"\"\n    Filters the DataFrame to include only rows before the specified datetime.\n\n    Args:\n        datetime (str): The cutoff datetime in 'YYYY-MM-DD HH:MM:SS' format.\n\n    Returns:\n        pd.DataFrame: A DataFrame containing rows where the 'systime' is before the specified datetime.\n\n    Example:\n    --------\n    &gt;&gt;&gt; filtered_data = DateTimeFilter.filter_before_datetime(df, \"systime\", \"2023-01-01 12:00:00\")\n    &gt;&gt;&gt; print(filtered_data)\n    \"\"\"\n    return dataframe[dataframe[column_name] &lt; pd.to_datetime(datetime)]\n</code></pre>"},{"location":"reference/ts_shape/transform/filter/datetime_filter/#ts_shape.transform.filter.datetime_filter.DateTimeFilter.filter_between_dates","title":"filter_between_dates  <code>classmethod</code>","text":"<pre><code>filter_between_dates(dataframe: DataFrame, column_name: str = 'systime', start_date: str = None, end_date: str = None) -&gt; DataFrame\n</code></pre> <p>Filters the DataFrame to include only rows between the specified start and end dates.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>pd.DataFrame: A DataFrame containing rows where the 'systime' is between the specified dates.</p> </li> </ul>"},{"location":"reference/ts_shape/transform/filter/datetime_filter/#ts_shape.transform.filter.datetime_filter.DateTimeFilter.filter_between_dates(start_date)","title":"<code>start_date</code>","text":"(<code>str</code>, default:                   <code>None</code> )           \u2013            <p>The start date of the interval in 'YYYY-MM-DD' format.</p>"},{"location":"reference/ts_shape/transform/filter/datetime_filter/#ts_shape.transform.filter.datetime_filter.DateTimeFilter.filter_between_dates(end_date)","title":"<code>end_date</code>","text":"(<code>str</code>, default:                   <code>None</code> )           \u2013            <p>The end date of the interval in 'YYYY-MM-DD' format.</p>"},{"location":"reference/ts_shape/transform/filter/datetime_filter/#ts_shape.transform.filter.datetime_filter.DateTimeFilter.filter_between_dates--example","title":"Example:","text":"<p>filtered_data = DateTimeFilter.filter_between_dates(df, \"systime\", \"2023-01-01\", \"2023-02-01\") print(filtered_data)</p> Source code in <code>src/ts_shape/transform/filter/datetime_filter.py</code> <pre><code>@classmethod\ndef filter_between_dates(cls, dataframe: pd.DataFrame, column_name: str = 'systime', start_date: str = None, end_date: str = None) -&gt; pd.DataFrame:\n    \"\"\"\n    Filters the DataFrame to include only rows between the specified start and end dates.\n\n    Args:\n        start_date (str): The start date of the interval in 'YYYY-MM-DD' format.\n        end_date (str): The end date of the interval in 'YYYY-MM-DD' format.\n\n    Returns:\n        pd.DataFrame: A DataFrame containing rows where the 'systime' is between the specified dates.\n\n    Example:\n    --------\n    &gt;&gt;&gt; filtered_data = DateTimeFilter.filter_between_dates(df, \"systime\", \"2023-01-01\", \"2023-02-01\")\n    &gt;&gt;&gt; print(filtered_data)\n    \"\"\"\n    mask = (dataframe[column_name] &gt; pd.to_datetime(start_date)) &amp; (dataframe[column_name] &lt; pd.to_datetime(end_date))\n    return dataframe[mask]\n</code></pre>"},{"location":"reference/ts_shape/transform/filter/datetime_filter/#ts_shape.transform.filter.datetime_filter.DateTimeFilter.filter_between_datetimes","title":"filter_between_datetimes  <code>classmethod</code>","text":"<pre><code>filter_between_datetimes(dataframe: DataFrame, column_name: str = 'systime', start_datetime: str = None, end_datetime: str = None) -&gt; DataFrame\n</code></pre> <p>Filters the DataFrame to include only rows between the specified start and end datetimes.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>pd.DataFrame: A DataFrame containing rows where the 'systime' is between the specified datetimes.</p> </li> </ul>"},{"location":"reference/ts_shape/transform/filter/datetime_filter/#ts_shape.transform.filter.datetime_filter.DateTimeFilter.filter_between_datetimes(start_datetime)","title":"<code>start_datetime</code>","text":"(<code>str</code>, default:                   <code>None</code> )           \u2013            <p>The start datetime of the interval in 'YYYY-MM-DD HH:MM:SS' format.</p>"},{"location":"reference/ts_shape/transform/filter/datetime_filter/#ts_shape.transform.filter.datetime_filter.DateTimeFilter.filter_between_datetimes(end_datetime)","title":"<code>end_datetime</code>","text":"(<code>str</code>, default:                   <code>None</code> )           \u2013            <p>The end datetime of the interval in 'YYYY-MM-DD HH:MM:SS' format.</p>"},{"location":"reference/ts_shape/transform/filter/datetime_filter/#ts_shape.transform.filter.datetime_filter.DateTimeFilter.filter_between_datetimes--example","title":"Example:","text":"<p>filtered_data = DateTimeFilter.filter_between_datetimes(df, \"systime\", \"2023-01-01 12:00:00\", \"2023-02-01 12:00:00\") print(filtered_data)</p> Source code in <code>src/ts_shape/transform/filter/datetime_filter.py</code> <pre><code>@classmethod\ndef filter_between_datetimes(cls, dataframe: pd.DataFrame, column_name: str = 'systime', start_datetime: str = None, end_datetime: str = None) -&gt; pd.DataFrame:\n    \"\"\"\n    Filters the DataFrame to include only rows between the specified start and end datetimes.\n\n    Args:\n        start_datetime (str): The start datetime of the interval in 'YYYY-MM-DD HH:MM:SS' format.\n        end_datetime (str): The end datetime of the interval in 'YYYY-MM-DD HH:MM:SS' format.\n\n    Returns:\n        pd.DataFrame: A DataFrame containing rows where the 'systime' is between the specified datetimes.\n\n    Example:\n    --------\n    &gt;&gt;&gt; filtered_data = DateTimeFilter.filter_between_datetimes(df, \"systime\", \"2023-01-01 12:00:00\", \"2023-02-01 12:00:00\")\n    &gt;&gt;&gt; print(filtered_data)\n    \"\"\"\n    mask = (dataframe[column_name] &gt; pd.to_datetime(start_datetime)) &amp; (dataframe[column_name] &lt; pd.to_datetime(end_datetime))\n    return dataframe[mask]\n</code></pre>"},{"location":"reference/ts_shape/transform/filter/datetime_filter/#ts_shape.transform.filter.datetime_filter.DateTimeFilter.get_dataframe","title":"get_dataframe","text":"<pre><code>get_dataframe() -&gt; DataFrame\n</code></pre> <p>Returns the processed DataFrame.</p> Source code in <code>src/ts_shape/utils/base.py</code> <pre><code>def get_dataframe(self) -&gt; pd.DataFrame:\n    \"\"\"Returns the processed DataFrame.\"\"\"\n    return self.dataframe\n</code></pre>"},{"location":"reference/ts_shape/transform/filter/numeric_filter/","title":"numeric_filter","text":""},{"location":"reference/ts_shape/transform/filter/numeric_filter/#ts_shape.transform.filter.numeric_filter","title":"ts_shape.transform.filter.numeric_filter","text":"<p>Classes:</p> <ul> <li> <code>DoubleFilter</code>           \u2013            <p>Provides class methods for filtering double (floating-point) columns in a pandas DataFrame,</p> </li> <li> <code>IntegerFilter</code>           \u2013            <p>Provides class methods for filtering integer columns in a pandas DataFrame.</p> </li> </ul>"},{"location":"reference/ts_shape/transform/filter/numeric_filter/#ts_shape.transform.filter.numeric_filter.DoubleFilter","title":"DoubleFilter","text":"<pre><code>DoubleFilter(dataframe: DataFrame, column_name: str = 'systime')\n</code></pre> <p>               Bases: <code>Base</code></p> <p>Provides class methods for filtering double (floating-point) columns in a pandas DataFrame, particularly focusing on NaN values.</p> <p>Parameters:</p> <p>Methods:</p> <ul> <li> <code>filter_nan_value_double</code>             \u2013              <p>Filters out rows where 'value_double' is NaN.</p> </li> <li> <code>filter_value_double_between</code>             \u2013              <p>Filters rows where 'value_double' is between the specified min and max values (inclusive).</p> </li> <li> <code>get_dataframe</code>             \u2013              <p>Returns the processed DataFrame.</p> </li> </ul> Source code in <code>src/ts_shape/utils/base.py</code> <pre><code>def __init__(self, dataframe: pd.DataFrame, column_name: str = 'systime') -&gt; None:\n    \"\"\"\n    Initializes the Base with a DataFrame, detects time columns, converts them to datetime,\n    and sorts the DataFrame by the specified column (or the detected time column if applicable).\n\n    Args:\n        dataframe (pd.DataFrame): The DataFrame to be processed.\n        column_name (str): The column to sort by. Default is 'systime'. If the column is not found or is not a time column, the class will attempt to detect other time columns.\n    \"\"\"\n    self.dataframe = dataframe.copy()\n\n    # Attempt to convert the specified column_name to datetime if it exists\n    if column_name in self.dataframe.columns:\n        self.dataframe[column_name] = pd.to_datetime(self.dataframe[column_name], errors='coerce')\n    else:\n        # If the column_name is not in the DataFrame, fallback to automatic time detection\n        time_columns = [col for col in self.dataframe.columns if 'time' in col.lower() or 'date' in col.lower()]\n\n        # Convert all detected time columns to datetime, if any\n        for col in time_columns:\n            self.dataframe[col] = pd.to_datetime(self.dataframe[col], errors='coerce')\n\n        # If any time columns are detected, sort by the first one; otherwise, do nothing\n        if time_columns:\n            column_name = time_columns[0]\n\n    # Sort by the datetime column (either specified or detected)\n    if column_name in self.dataframe.columns:\n        self.dataframe = self.dataframe.sort_values(by=column_name)\n</code></pre>"},{"location":"reference/ts_shape/transform/filter/numeric_filter/#ts_shape.transform.filter.numeric_filter.DoubleFilter(dataframe)","title":"<code>dataframe</code>","text":"(<code>DataFrame</code>)           \u2013            <p>The DataFrame to be processed.</p>"},{"location":"reference/ts_shape/transform/filter/numeric_filter/#ts_shape.transform.filter.numeric_filter.DoubleFilter(column_name)","title":"<code>column_name</code>","text":"(<code>str</code>, default:                   <code>'systime'</code> )           \u2013            <p>The column to sort by. Default is 'systime'. If the column is not found or is not a time column, the class will attempt to detect other time columns.</p>"},{"location":"reference/ts_shape/transform/filter/numeric_filter/#ts_shape.transform.filter.numeric_filter.DoubleFilter.filter_nan_value_double","title":"filter_nan_value_double  <code>classmethod</code>","text":"<pre><code>filter_nan_value_double(dataframe: DataFrame, column_name: str = 'value_double') -&gt; DataFrame\n</code></pre> <p>Filters out rows where 'value_double' is NaN.</p> Source code in <code>src/ts_shape/transform/filter/numeric_filter.py</code> <pre><code>@classmethod\ndef filter_nan_value_double(cls, dataframe: pd.DataFrame, column_name: str = 'value_double') -&gt; pd.DataFrame:\n    \"\"\"Filters out rows where 'value_double' is NaN.\"\"\"\n    return dataframe[dataframe[column_name].notna()]\n</code></pre>"},{"location":"reference/ts_shape/transform/filter/numeric_filter/#ts_shape.transform.filter.numeric_filter.DoubleFilter.filter_value_double_between","title":"filter_value_double_between  <code>classmethod</code>","text":"<pre><code>filter_value_double_between(dataframe: DataFrame, column_name: str = 'value_double', min_value: float = 0.0, max_value: float = 100.0) -&gt; DataFrame\n</code></pre> <p>Filters rows where 'value_double' is between the specified min and max values (inclusive).</p> Source code in <code>src/ts_shape/transform/filter/numeric_filter.py</code> <pre><code>@classmethod\ndef filter_value_double_between(cls, dataframe: pd.DataFrame, column_name: str = 'value_double', min_value: float = 0.0, max_value: float = 100.0) -&gt; pd.DataFrame:\n    \"\"\"Filters rows where 'value_double' is between the specified min and max values (inclusive).\"\"\"\n    return dataframe[(dataframe[column_name] &gt;= min_value) &amp; (dataframe[column_name] &lt;= max_value)]\n</code></pre>"},{"location":"reference/ts_shape/transform/filter/numeric_filter/#ts_shape.transform.filter.numeric_filter.DoubleFilter.get_dataframe","title":"get_dataframe","text":"<pre><code>get_dataframe() -&gt; DataFrame\n</code></pre> <p>Returns the processed DataFrame.</p> Source code in <code>src/ts_shape/utils/base.py</code> <pre><code>def get_dataframe(self) -&gt; pd.DataFrame:\n    \"\"\"Returns the processed DataFrame.\"\"\"\n    return self.dataframe\n</code></pre>"},{"location":"reference/ts_shape/transform/filter/numeric_filter/#ts_shape.transform.filter.numeric_filter.IntegerFilter","title":"IntegerFilter","text":"<pre><code>IntegerFilter(dataframe: DataFrame, column_name: str = 'systime')\n</code></pre> <p>               Bases: <code>Base</code></p> <p>Provides class methods for filtering integer columns in a pandas DataFrame.</p> <p>Parameters:</p> <p>Methods:</p> <ul> <li> <code>filter_value_integer_between</code>             \u2013              <p>Filters rows where 'value_integer' is between the specified min and max values (inclusive).</p> </li> <li> <code>filter_value_integer_match</code>             \u2013              <p>Filters rows where 'value_integer' matches the specified integer.</p> </li> <li> <code>filter_value_integer_not_match</code>             \u2013              <p>Filters rows where 'value_integer' does not match the specified integer.</p> </li> <li> <code>get_dataframe</code>             \u2013              <p>Returns the processed DataFrame.</p> </li> </ul> Source code in <code>src/ts_shape/utils/base.py</code> <pre><code>def __init__(self, dataframe: pd.DataFrame, column_name: str = 'systime') -&gt; None:\n    \"\"\"\n    Initializes the Base with a DataFrame, detects time columns, converts them to datetime,\n    and sorts the DataFrame by the specified column (or the detected time column if applicable).\n\n    Args:\n        dataframe (pd.DataFrame): The DataFrame to be processed.\n        column_name (str): The column to sort by. Default is 'systime'. If the column is not found or is not a time column, the class will attempt to detect other time columns.\n    \"\"\"\n    self.dataframe = dataframe.copy()\n\n    # Attempt to convert the specified column_name to datetime if it exists\n    if column_name in self.dataframe.columns:\n        self.dataframe[column_name] = pd.to_datetime(self.dataframe[column_name], errors='coerce')\n    else:\n        # If the column_name is not in the DataFrame, fallback to automatic time detection\n        time_columns = [col for col in self.dataframe.columns if 'time' in col.lower() or 'date' in col.lower()]\n\n        # Convert all detected time columns to datetime, if any\n        for col in time_columns:\n            self.dataframe[col] = pd.to_datetime(self.dataframe[col], errors='coerce')\n\n        # If any time columns are detected, sort by the first one; otherwise, do nothing\n        if time_columns:\n            column_name = time_columns[0]\n\n    # Sort by the datetime column (either specified or detected)\n    if column_name in self.dataframe.columns:\n        self.dataframe = self.dataframe.sort_values(by=column_name)\n</code></pre>"},{"location":"reference/ts_shape/transform/filter/numeric_filter/#ts_shape.transform.filter.numeric_filter.IntegerFilter(dataframe)","title":"<code>dataframe</code>","text":"(<code>DataFrame</code>)           \u2013            <p>The DataFrame to be processed.</p>"},{"location":"reference/ts_shape/transform/filter/numeric_filter/#ts_shape.transform.filter.numeric_filter.IntegerFilter(column_name)","title":"<code>column_name</code>","text":"(<code>str</code>, default:                   <code>'systime'</code> )           \u2013            <p>The column to sort by. Default is 'systime'. If the column is not found or is not a time column, the class will attempt to detect other time columns.</p>"},{"location":"reference/ts_shape/transform/filter/numeric_filter/#ts_shape.transform.filter.numeric_filter.IntegerFilter.filter_value_integer_between","title":"filter_value_integer_between  <code>classmethod</code>","text":"<pre><code>filter_value_integer_between(dataframe: DataFrame, column_name: str = 'value_integer', min_value: int = 0, max_value: int = 100) -&gt; DataFrame\n</code></pre> <p>Filters rows where 'value_integer' is between the specified min and max values (inclusive).</p> Source code in <code>src/ts_shape/transform/filter/numeric_filter.py</code> <pre><code>@classmethod\ndef filter_value_integer_between(cls, dataframe: pd.DataFrame, column_name: str = 'value_integer', min_value: int = 0, max_value: int = 100) -&gt; pd.DataFrame:\n    \"\"\"Filters rows where 'value_integer' is between the specified min and max values (inclusive).\"\"\"\n    return dataframe[(dataframe[column_name] &gt;= min_value) &amp; (dataframe[column_name] &lt;= max_value)]\n</code></pre>"},{"location":"reference/ts_shape/transform/filter/numeric_filter/#ts_shape.transform.filter.numeric_filter.IntegerFilter.filter_value_integer_match","title":"filter_value_integer_match  <code>classmethod</code>","text":"<pre><code>filter_value_integer_match(dataframe: DataFrame, column_name: str = 'value_integer', integer_value: int = 0) -&gt; DataFrame\n</code></pre> <p>Filters rows where 'value_integer' matches the specified integer.</p> Source code in <code>src/ts_shape/transform/filter/numeric_filter.py</code> <pre><code>@classmethod\ndef filter_value_integer_match(cls, dataframe: pd.DataFrame, column_name: str = 'value_integer', integer_value: int = 0) -&gt; pd.DataFrame:\n    \"\"\"Filters rows where 'value_integer' matches the specified integer.\"\"\"\n    return dataframe[dataframe[column_name] == integer_value]\n</code></pre>"},{"location":"reference/ts_shape/transform/filter/numeric_filter/#ts_shape.transform.filter.numeric_filter.IntegerFilter.filter_value_integer_not_match","title":"filter_value_integer_not_match  <code>classmethod</code>","text":"<pre><code>filter_value_integer_not_match(dataframe: DataFrame, column_name: str = 'value_integer', integer_value: int = 0) -&gt; DataFrame\n</code></pre> <p>Filters rows where 'value_integer' does not match the specified integer.</p> Source code in <code>src/ts_shape/transform/filter/numeric_filter.py</code> <pre><code>@classmethod\ndef filter_value_integer_not_match(cls, dataframe: pd.DataFrame, column_name: str = 'value_integer', integer_value: int = 0) -&gt; pd.DataFrame:\n    \"\"\"Filters rows where 'value_integer' does not match the specified integer.\"\"\"\n    return dataframe[dataframe[column_name] != integer_value]\n</code></pre>"},{"location":"reference/ts_shape/transform/filter/numeric_filter/#ts_shape.transform.filter.numeric_filter.IntegerFilter.get_dataframe","title":"get_dataframe","text":"<pre><code>get_dataframe() -&gt; DataFrame\n</code></pre> <p>Returns the processed DataFrame.</p> Source code in <code>src/ts_shape/utils/base.py</code> <pre><code>def get_dataframe(self) -&gt; pd.DataFrame:\n    \"\"\"Returns the processed DataFrame.\"\"\"\n    return self.dataframe\n</code></pre>"},{"location":"reference/ts_shape/transform/filter/string_filter/","title":"string_filter","text":""},{"location":"reference/ts_shape/transform/filter/string_filter/#ts_shape.transform.filter.string_filter","title":"ts_shape.transform.filter.string_filter","text":"<p>Classes:</p> <ul> <li> <code>StringFilter</code>           \u2013            <p>A class for filtering operations on string columns within a pandas DataFrame.</p> </li> </ul>"},{"location":"reference/ts_shape/transform/filter/string_filter/#ts_shape.transform.filter.string_filter.StringFilter","title":"StringFilter","text":"<pre><code>StringFilter(dataframe: DataFrame, column_name: str = 'systime')\n</code></pre> <p>               Bases: <code>Base</code></p> <p>A class for filtering operations on string columns within a pandas DataFrame.</p> <p>Provides class methods for operations on string columns.</p> <p>Parameters:</p> <p>Methods:</p> <ul> <li> <code>detect_changes_in_string</code>             \u2013              <p>Detects changes from row to row in the specified string column.</p> </li> <li> <code>filter_na_value_string</code>             \u2013              <p>Filters out rows where the specified string column is NA.</p> </li> <li> <code>filter_string_contains</code>             \u2013              <p>Filters rows where the specified string column contains the provided substring.</p> </li> <li> <code>filter_value_string_match</code>             \u2013              <p>Filters rows where the specified string column matches the provided string.</p> </li> <li> <code>filter_value_string_not_match</code>             \u2013              <p>Filters rows where the specified string column does not match the provided string.</p> </li> <li> <code>get_dataframe</code>             \u2013              <p>Returns the processed DataFrame.</p> </li> <li> <code>regex_clean_value_string</code>             \u2013              <p>Applies a regex pattern to clean the specified string column.</p> </li> </ul> Source code in <code>src/ts_shape/utils/base.py</code> <pre><code>def __init__(self, dataframe: pd.DataFrame, column_name: str = 'systime') -&gt; None:\n    \"\"\"\n    Initializes the Base with a DataFrame, detects time columns, converts them to datetime,\n    and sorts the DataFrame by the specified column (or the detected time column if applicable).\n\n    Args:\n        dataframe (pd.DataFrame): The DataFrame to be processed.\n        column_name (str): The column to sort by. Default is 'systime'. If the column is not found or is not a time column, the class will attempt to detect other time columns.\n    \"\"\"\n    self.dataframe = dataframe.copy()\n\n    # Attempt to convert the specified column_name to datetime if it exists\n    if column_name in self.dataframe.columns:\n        self.dataframe[column_name] = pd.to_datetime(self.dataframe[column_name], errors='coerce')\n    else:\n        # If the column_name is not in the DataFrame, fallback to automatic time detection\n        time_columns = [col for col in self.dataframe.columns if 'time' in col.lower() or 'date' in col.lower()]\n\n        # Convert all detected time columns to datetime, if any\n        for col in time_columns:\n            self.dataframe[col] = pd.to_datetime(self.dataframe[col], errors='coerce')\n\n        # If any time columns are detected, sort by the first one; otherwise, do nothing\n        if time_columns:\n            column_name = time_columns[0]\n\n    # Sort by the datetime column (either specified or detected)\n    if column_name in self.dataframe.columns:\n        self.dataframe = self.dataframe.sort_values(by=column_name)\n</code></pre>"},{"location":"reference/ts_shape/transform/filter/string_filter/#ts_shape.transform.filter.string_filter.StringFilter(dataframe)","title":"<code>dataframe</code>","text":"(<code>DataFrame</code>)           \u2013            <p>The DataFrame to be processed.</p>"},{"location":"reference/ts_shape/transform/filter/string_filter/#ts_shape.transform.filter.string_filter.StringFilter(column_name)","title":"<code>column_name</code>","text":"(<code>str</code>, default:                   <code>'systime'</code> )           \u2013            <p>The column to sort by. Default is 'systime'. If the column is not found or is not a time column, the class will attempt to detect other time columns.</p>"},{"location":"reference/ts_shape/transform/filter/string_filter/#ts_shape.transform.filter.string_filter.StringFilter.detect_changes_in_string","title":"detect_changes_in_string  <code>classmethod</code>","text":"<pre><code>detect_changes_in_string(dataframe: DataFrame, column_name: str = 'value_string') -&gt; DataFrame\n</code></pre> <p>Detects changes from row to row in the specified string column.</p> Source code in <code>src/ts_shape/transform/filter/string_filter.py</code> <pre><code>@classmethod\ndef detect_changes_in_string(cls, dataframe: pd.DataFrame, column_name: str = 'value_string') -&gt; pd.DataFrame:\n    \"\"\"Detects changes from row to row in the specified string column.\"\"\"\n    changes_detected = dataframe[column_name].ne(dataframe[column_name].shift())\n    result = dataframe[changes_detected]\n    if result.empty:\n        print(f\"No changes detected in the '{column_name}' column between consecutive rows.\")\n    return result\n</code></pre>"},{"location":"reference/ts_shape/transform/filter/string_filter/#ts_shape.transform.filter.string_filter.StringFilter.filter_na_value_string","title":"filter_na_value_string  <code>classmethod</code>","text":"<pre><code>filter_na_value_string(dataframe: DataFrame, column_name: str = 'value_string') -&gt; DataFrame\n</code></pre> <p>Filters out rows where the specified string column is NA.</p> Source code in <code>src/ts_shape/transform/filter/string_filter.py</code> <pre><code>@classmethod\ndef filter_na_value_string(cls, dataframe: pd.DataFrame, column_name: str = 'value_string') -&gt; pd.DataFrame:\n    \"\"\"Filters out rows where the specified string column is NA.\"\"\"\n    return dataframe[dataframe[column_name].notna()]\n</code></pre>"},{"location":"reference/ts_shape/transform/filter/string_filter/#ts_shape.transform.filter.string_filter.StringFilter.filter_string_contains","title":"filter_string_contains  <code>classmethod</code>","text":"<pre><code>filter_string_contains(dataframe: DataFrame, substring: str, column_name: str = 'value_string') -&gt; DataFrame\n</code></pre> <p>Filters rows where the specified string column contains the provided substring.</p> Source code in <code>src/ts_shape/transform/filter/string_filter.py</code> <pre><code>@classmethod\ndef filter_string_contains(cls, dataframe: pd.DataFrame, substring: str, column_name: str = 'value_string') -&gt; pd.DataFrame:\n    \"\"\"Filters rows where the specified string column contains the provided substring.\"\"\"\n    return dataframe[dataframe[column_name].str.contains(substring, na=False)]\n</code></pre>"},{"location":"reference/ts_shape/transform/filter/string_filter/#ts_shape.transform.filter.string_filter.StringFilter.filter_value_string_match","title":"filter_value_string_match  <code>classmethod</code>","text":"<pre><code>filter_value_string_match(dataframe: DataFrame, string_value: str, column_name: str = 'value_string') -&gt; DataFrame\n</code></pre> <p>Filters rows where the specified string column matches the provided string.</p> Source code in <code>src/ts_shape/transform/filter/string_filter.py</code> <pre><code>@classmethod\ndef filter_value_string_match(cls, dataframe: pd.DataFrame, string_value: str, column_name: str = 'value_string') -&gt; pd.DataFrame:\n    \"\"\"Filters rows where the specified string column matches the provided string.\"\"\"\n    return dataframe[dataframe[column_name] == string_value]\n</code></pre>"},{"location":"reference/ts_shape/transform/filter/string_filter/#ts_shape.transform.filter.string_filter.StringFilter.filter_value_string_not_match","title":"filter_value_string_not_match  <code>classmethod</code>","text":"<pre><code>filter_value_string_not_match(dataframe: DataFrame, string_value: str, column_name: str = 'value_string') -&gt; DataFrame\n</code></pre> <p>Filters rows where the specified string column does not match the provided string.</p> Source code in <code>src/ts_shape/transform/filter/string_filter.py</code> <pre><code>@classmethod\ndef filter_value_string_not_match(cls, dataframe: pd.DataFrame, string_value: str, column_name: str = 'value_string') -&gt; pd.DataFrame:\n    \"\"\"Filters rows where the specified string column does not match the provided string.\"\"\"\n    return dataframe[dataframe[column_name] != string_value]\n</code></pre>"},{"location":"reference/ts_shape/transform/filter/string_filter/#ts_shape.transform.filter.string_filter.StringFilter.get_dataframe","title":"get_dataframe","text":"<pre><code>get_dataframe() -&gt; DataFrame\n</code></pre> <p>Returns the processed DataFrame.</p> Source code in <code>src/ts_shape/utils/base.py</code> <pre><code>def get_dataframe(self) -&gt; pd.DataFrame:\n    \"\"\"Returns the processed DataFrame.\"\"\"\n    return self.dataframe\n</code></pre>"},{"location":"reference/ts_shape/transform/filter/string_filter/#ts_shape.transform.filter.string_filter.StringFilter.regex_clean_value_string","title":"regex_clean_value_string  <code>classmethod</code>","text":"<pre><code>regex_clean_value_string(dataframe: DataFrame, column_name: str = 'value_string', regex_pattern: str = '(\\\\d+)\\\\s*([a-zA-Z]*)', replacement: str = '', regex: bool = True) -&gt; DataFrame\n</code></pre> <p>Applies a regex pattern to clean the specified string column.</p> Source code in <code>src/ts_shape/transform/filter/string_filter.py</code> <pre><code>@classmethod\ndef regex_clean_value_string(cls, dataframe: pd.DataFrame, column_name: str = 'value_string', regex_pattern: str = r'(\\d+)\\s*([a-zA-Z]*)', replacement: str = '', regex: bool = True) -&gt; pd.DataFrame:\n    \"\"\"Applies a regex pattern to clean the specified string column.\"\"\"\n    dataframe[column_name] = dataframe[column_name].str.replace(regex_pattern, replacement, regex=regex)\n    return dataframe\n</code></pre>"},{"location":"reference/ts_shape/transform/functions/__init__/","title":"init","text":""},{"location":"reference/ts_shape/transform/functions/__init__/#ts_shape.transform.functions","title":"ts_shape.transform.functions","text":"<p>Functions</p> <p>Column-wise function application helpers.</p> <ul> <li>LambdaProcessor: Apply vectorized callables to columns.</li> <li>apply_function: Apply a Python callable over a column's values.</li> </ul> <p>Modules:</p> <ul> <li> <code>lambda_func</code>           \u2013            </li> </ul>"},{"location":"reference/ts_shape/transform/functions/lambda_func/","title":"lambda_func","text":""},{"location":"reference/ts_shape/transform/functions/lambda_func/#ts_shape.transform.functions.lambda_func","title":"ts_shape.transform.functions.lambda_func","text":"<p>Classes:</p> <ul> <li> <code>LambdaProcessor</code>           \u2013            <p>Provides class methods for applying lambda or callable functions to columns in a pandas DataFrame.</p> </li> </ul>"},{"location":"reference/ts_shape/transform/functions/lambda_func/#ts_shape.transform.functions.lambda_func.LambdaProcessor","title":"LambdaProcessor","text":"<pre><code>LambdaProcessor(dataframe: DataFrame, column_name: str = 'systime')\n</code></pre> <p>               Bases: <code>Base</code></p> <p>Provides class methods for applying lambda or callable functions to columns in a pandas DataFrame. This class inherits from Base, ensuring consistency with other processors.</p> <p>Parameters:</p> <p>Methods:</p> <ul> <li> <code>apply_function</code>             \u2013              <p>Applies a lambda or callable function to a specified column in the DataFrame.</p> </li> <li> <code>get_dataframe</code>             \u2013              <p>Returns the processed DataFrame.</p> </li> </ul> Source code in <code>src/ts_shape/utils/base.py</code> <pre><code>def __init__(self, dataframe: pd.DataFrame, column_name: str = 'systime') -&gt; None:\n    \"\"\"\n    Initializes the Base with a DataFrame, detects time columns, converts them to datetime,\n    and sorts the DataFrame by the specified column (or the detected time column if applicable).\n\n    Args:\n        dataframe (pd.DataFrame): The DataFrame to be processed.\n        column_name (str): The column to sort by. Default is 'systime'. If the column is not found or is not a time column, the class will attempt to detect other time columns.\n    \"\"\"\n    self.dataframe = dataframe.copy()\n\n    # Attempt to convert the specified column_name to datetime if it exists\n    if column_name in self.dataframe.columns:\n        self.dataframe[column_name] = pd.to_datetime(self.dataframe[column_name], errors='coerce')\n    else:\n        # If the column_name is not in the DataFrame, fallback to automatic time detection\n        time_columns = [col for col in self.dataframe.columns if 'time' in col.lower() or 'date' in col.lower()]\n\n        # Convert all detected time columns to datetime, if any\n        for col in time_columns:\n            self.dataframe[col] = pd.to_datetime(self.dataframe[col], errors='coerce')\n\n        # If any time columns are detected, sort by the first one; otherwise, do nothing\n        if time_columns:\n            column_name = time_columns[0]\n\n    # Sort by the datetime column (either specified or detected)\n    if column_name in self.dataframe.columns:\n        self.dataframe = self.dataframe.sort_values(by=column_name)\n</code></pre>"},{"location":"reference/ts_shape/transform/functions/lambda_func/#ts_shape.transform.functions.lambda_func.LambdaProcessor(dataframe)","title":"<code>dataframe</code>","text":"(<code>DataFrame</code>)           \u2013            <p>The DataFrame to be processed.</p>"},{"location":"reference/ts_shape/transform/functions/lambda_func/#ts_shape.transform.functions.lambda_func.LambdaProcessor(column_name)","title":"<code>column_name</code>","text":"(<code>str</code>, default:                   <code>'systime'</code> )           \u2013            <p>The column to sort by. Default is 'systime'. If the column is not found or is not a time column, the class will attempt to detect other time columns.</p>"},{"location":"reference/ts_shape/transform/functions/lambda_func/#ts_shape.transform.functions.lambda_func.LambdaProcessor.apply_function","title":"apply_function  <code>classmethod</code>","text":"<pre><code>apply_function(dataframe: DataFrame, column_name: str, func: Callable[[Any], Any]) -&gt; DataFrame\n</code></pre> <p>Applies a lambda or callable function to a specified column in the DataFrame.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>pd.DataFrame: The DataFrame with the transformed column.</p> </li> </ul> Source code in <code>src/ts_shape/transform/functions/lambda_func.py</code> <pre><code>@classmethod\ndef apply_function(cls, dataframe: pd.DataFrame, column_name: str, func: Callable[[Any], Any]) -&gt; pd.DataFrame:\n    \"\"\"\n    Applies a lambda or callable function to a specified column in the DataFrame.\n\n    Args:\n        dataframe (pd.DataFrame): The DataFrame containing the data.\n        column_name (str): The name of the column to apply the function to.\n        func (Callable): The lambda function or callable to apply to the column.\n\n    Returns:\n        pd.DataFrame: The DataFrame with the transformed column.\n    \"\"\"\n    if column_name not in dataframe.columns:\n        raise ValueError(f\"Column '{column_name}' not found in DataFrame.\")\n\n    dataframe[column_name] = func(dataframe[column_name].values)\n    return dataframe\n</code></pre>"},{"location":"reference/ts_shape/transform/functions/lambda_func/#ts_shape.transform.functions.lambda_func.LambdaProcessor.apply_function(dataframe)","title":"<code>dataframe</code>","text":"(<code>DataFrame</code>)           \u2013            <p>The DataFrame containing the data.</p>"},{"location":"reference/ts_shape/transform/functions/lambda_func/#ts_shape.transform.functions.lambda_func.LambdaProcessor.apply_function(column_name)","title":"<code>column_name</code>","text":"(<code>str</code>)           \u2013            <p>The name of the column to apply the function to.</p>"},{"location":"reference/ts_shape/transform/functions/lambda_func/#ts_shape.transform.functions.lambda_func.LambdaProcessor.apply_function(func)","title":"<code>func</code>","text":"(<code>Callable</code>)           \u2013            <p>The lambda function or callable to apply to the column.</p>"},{"location":"reference/ts_shape/transform/functions/lambda_func/#ts_shape.transform.functions.lambda_func.LambdaProcessor.get_dataframe","title":"get_dataframe","text":"<pre><code>get_dataframe() -&gt; DataFrame\n</code></pre> <p>Returns the processed DataFrame.</p> Source code in <code>src/ts_shape/utils/base.py</code> <pre><code>def get_dataframe(self) -&gt; pd.DataFrame:\n    \"\"\"Returns the processed DataFrame.\"\"\"\n    return self.dataframe\n</code></pre>"},{"location":"reference/ts_shape/transform/time_functions/__init__/","title":"init","text":""},{"location":"reference/ts_shape/transform/time_functions/__init__/#ts_shape.transform.time_functions","title":"ts_shape.transform.time_functions","text":"<p>Time Functions</p> <p>Timestamp conversion and timezone operations.</p> <ul> <li>TimestampConverter: Convert integer timestamps to tz-aware datetimes.</li> <li> <p>convert_to_datetime: Convert s/ms/us/ns to datetime in a timezone.</p> </li> <li> <p>TimezoneShift: Timezone localization/conversion helpers.</p> </li> <li>shift_timezone: Convert timezones in-place.</li> <li>add_timezone_column: Add a converted timestamp column.</li> <li>detect_timezone_awareness: Check tz-awareness of a column.</li> <li>revert_to_original_timezone: Convert back to original tz.</li> <li>calculate_time_difference: Difference between two timestamp columns.</li> </ul> <p>Modules:</p> <ul> <li> <code>timestamp_converter</code>           \u2013            </li> <li> <code>timezone_shift</code>           \u2013            </li> </ul>"},{"location":"reference/ts_shape/transform/time_functions/timestamp_converter/","title":"timestamp_converter","text":""},{"location":"reference/ts_shape/transform/time_functions/timestamp_converter/#ts_shape.transform.time_functions.timestamp_converter","title":"ts_shape.transform.time_functions.timestamp_converter","text":"<p>Classes:</p> <ul> <li> <code>TimestampConverter</code>           \u2013            <p>A class dedicated to converting high-precision timestamp data (e.g., in seconds, milliseconds, microseconds, or nanoseconds)</p> </li> </ul>"},{"location":"reference/ts_shape/transform/time_functions/timestamp_converter/#ts_shape.transform.time_functions.timestamp_converter.TimestampConverter","title":"TimestampConverter","text":"<pre><code>TimestampConverter(dataframe: DataFrame, column_name: str = 'systime')\n</code></pre> <p>               Bases: <code>Base</code></p> <p>A class dedicated to converting high-precision timestamp data (e.g., in seconds, milliseconds, microseconds, or nanoseconds) to standard datetime formats with optional timezone adjustment.</p> <p>Parameters:</p> <p>Methods:</p> <ul> <li> <code>convert_to_datetime</code>             \u2013              <p>Converts specified columns from a given timestamp unit to datetime format in a target timezone.</p> </li> <li> <code>get_dataframe</code>             \u2013              <p>Returns the processed DataFrame.</p> </li> </ul> Source code in <code>src/ts_shape/utils/base.py</code> <pre><code>def __init__(self, dataframe: pd.DataFrame, column_name: str = 'systime') -&gt; None:\n    \"\"\"\n    Initializes the Base with a DataFrame, detects time columns, converts them to datetime,\n    and sorts the DataFrame by the specified column (or the detected time column if applicable).\n\n    Args:\n        dataframe (pd.DataFrame): The DataFrame to be processed.\n        column_name (str): The column to sort by. Default is 'systime'. If the column is not found or is not a time column, the class will attempt to detect other time columns.\n    \"\"\"\n    self.dataframe = dataframe.copy()\n\n    # Attempt to convert the specified column_name to datetime if it exists\n    if column_name in self.dataframe.columns:\n        self.dataframe[column_name] = pd.to_datetime(self.dataframe[column_name], errors='coerce')\n    else:\n        # If the column_name is not in the DataFrame, fallback to automatic time detection\n        time_columns = [col for col in self.dataframe.columns if 'time' in col.lower() or 'date' in col.lower()]\n\n        # Convert all detected time columns to datetime, if any\n        for col in time_columns:\n            self.dataframe[col] = pd.to_datetime(self.dataframe[col], errors='coerce')\n\n        # If any time columns are detected, sort by the first one; otherwise, do nothing\n        if time_columns:\n            column_name = time_columns[0]\n\n    # Sort by the datetime column (either specified or detected)\n    if column_name in self.dataframe.columns:\n        self.dataframe = self.dataframe.sort_values(by=column_name)\n</code></pre>"},{"location":"reference/ts_shape/transform/time_functions/timestamp_converter/#ts_shape.transform.time_functions.timestamp_converter.TimestampConverter(dataframe)","title":"<code>dataframe</code>","text":"(<code>DataFrame</code>)           \u2013            <p>The DataFrame to be processed.</p>"},{"location":"reference/ts_shape/transform/time_functions/timestamp_converter/#ts_shape.transform.time_functions.timestamp_converter.TimestampConverter(column_name)","title":"<code>column_name</code>","text":"(<code>str</code>, default:                   <code>'systime'</code> )           \u2013            <p>The column to sort by. Default is 'systime'. If the column is not found or is not a time column, the class will attempt to detect other time columns.</p>"},{"location":"reference/ts_shape/transform/time_functions/timestamp_converter/#ts_shape.transform.time_functions.timestamp_converter.TimestampConverter.convert_to_datetime","title":"convert_to_datetime  <code>classmethod</code>","text":"<pre><code>convert_to_datetime(dataframe: DataFrame, columns: list, unit: str = 'ns', timezone: str = 'UTC') -&gt; DataFrame\n</code></pre> <p>Converts specified columns from a given timestamp unit to datetime format in a target timezone.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>pd.DataFrame: A DataFrame with the converted datetime columns in the specified timezone.</p> </li> </ul> Source code in <code>src/ts_shape/transform/time_functions/timestamp_converter.py</code> <pre><code>@classmethod\ndef convert_to_datetime(cls, dataframe: pd.DataFrame, columns: list, unit: str = 'ns', timezone: str = 'UTC') -&gt; pd.DataFrame:\n    \"\"\"\n    Converts specified columns from a given timestamp unit to datetime format in a target timezone.\n\n    Args:\n        dataframe (pd.DataFrame): The DataFrame containing the data.\n        columns (list): A list of column names with timestamp data to convert.\n        unit (str): The unit of the timestamps ('s', 'ms', 'us', or 'ns').\n        timezone (str): The target timezone for the converted datetime (default is 'UTC').\n\n    Returns:\n        pd.DataFrame: A DataFrame with the converted datetime columns in the specified timezone.\n    \"\"\"\n    # Validate unit\n    valid_units = ['s', 'ms', 'us', 'ns']\n    if unit not in valid_units:\n        raise ValueError(f\"Invalid unit '{unit}'. Must be one of {valid_units}.\")\n\n    # Validate timezone\n    if timezone not in pytz.all_timezones:\n        raise ValueError(f\"Invalid timezone '{timezone}'. Use a valid timezone name from pytz.all_timezones.\")\n\n    df = dataframe.copy()\n    for col in columns:\n        # Convert timestamps to datetime in UTC first\n        df[col] = pd.to_datetime(df[col], unit=unit, utc=True)\n        # Adjust to the target timezone\n        df[col] = df[col].dt.tz_convert(timezone)\n\n    return df\n</code></pre>"},{"location":"reference/ts_shape/transform/time_functions/timestamp_converter/#ts_shape.transform.time_functions.timestamp_converter.TimestampConverter.convert_to_datetime(dataframe)","title":"<code>dataframe</code>","text":"(<code>DataFrame</code>)           \u2013            <p>The DataFrame containing the data.</p>"},{"location":"reference/ts_shape/transform/time_functions/timestamp_converter/#ts_shape.transform.time_functions.timestamp_converter.TimestampConverter.convert_to_datetime(columns)","title":"<code>columns</code>","text":"(<code>list</code>)           \u2013            <p>A list of column names with timestamp data to convert.</p>"},{"location":"reference/ts_shape/transform/time_functions/timestamp_converter/#ts_shape.transform.time_functions.timestamp_converter.TimestampConverter.convert_to_datetime(unit)","title":"<code>unit</code>","text":"(<code>str</code>, default:                   <code>'ns'</code> )           \u2013            <p>The unit of the timestamps ('s', 'ms', 'us', or 'ns').</p>"},{"location":"reference/ts_shape/transform/time_functions/timestamp_converter/#ts_shape.transform.time_functions.timestamp_converter.TimestampConverter.convert_to_datetime(timezone)","title":"<code>timezone</code>","text":"(<code>str</code>, default:                   <code>'UTC'</code> )           \u2013            <p>The target timezone for the converted datetime (default is 'UTC').</p>"},{"location":"reference/ts_shape/transform/time_functions/timestamp_converter/#ts_shape.transform.time_functions.timestamp_converter.TimestampConverter.get_dataframe","title":"get_dataframe","text":"<pre><code>get_dataframe() -&gt; DataFrame\n</code></pre> <p>Returns the processed DataFrame.</p> Source code in <code>src/ts_shape/utils/base.py</code> <pre><code>def get_dataframe(self) -&gt; pd.DataFrame:\n    \"\"\"Returns the processed DataFrame.\"\"\"\n    return self.dataframe\n</code></pre>"},{"location":"reference/ts_shape/transform/time_functions/timezone_shift/","title":"timezone_shift","text":""},{"location":"reference/ts_shape/transform/time_functions/timezone_shift/#ts_shape.transform.time_functions.timezone_shift","title":"ts_shape.transform.time_functions.timezone_shift","text":"<p>Classes:</p> <ul> <li> <code>TimezoneShift</code>           \u2013            <p>A class for shifting timestamps in a DataFrame to a different timezone, with methods to handle timezone localization and conversion.</p> </li> </ul>"},{"location":"reference/ts_shape/transform/time_functions/timezone_shift/#ts_shape.transform.time_functions.timezone_shift.TimezoneShift","title":"TimezoneShift","text":"<pre><code>TimezoneShift(dataframe: DataFrame, column_name: str = 'systime')\n</code></pre> <p>               Bases: <code>Base</code></p> <p>A class for shifting timestamps in a DataFrame to a different timezone, with methods to handle timezone localization and conversion.</p> <p>Parameters:</p> <p>Methods:</p> <ul> <li> <code>add_timezone_column</code>             \u2013              <p>Creates a new column with timestamps converted from an input timezone to a target timezone, without altering the original column.</p> </li> <li> <code>calculate_time_difference</code>             \u2013              <p>Calculates the time difference between two timestamp columns.</p> </li> <li> <code>detect_timezone_awareness</code>             \u2013              <p>Detects if a time column in a DataFrame is timezone-aware.</p> </li> <li> <code>get_dataframe</code>             \u2013              <p>Returns the processed DataFrame.</p> </li> <li> <code>list_available_timezones</code>             \u2013              <p>Returns a list of all available timezones.</p> </li> <li> <code>revert_to_original_timezone</code>             \u2013              <p>Reverts a timezone-shifted time column back to the original timezone.</p> </li> <li> <code>shift_timezone</code>             \u2013              <p>Shifts timestamps in the specified column of a DataFrame from a given timezone to a target timezone.</p> </li> </ul> Source code in <code>src/ts_shape/utils/base.py</code> <pre><code>def __init__(self, dataframe: pd.DataFrame, column_name: str = 'systime') -&gt; None:\n    \"\"\"\n    Initializes the Base with a DataFrame, detects time columns, converts them to datetime,\n    and sorts the DataFrame by the specified column (or the detected time column if applicable).\n\n    Args:\n        dataframe (pd.DataFrame): The DataFrame to be processed.\n        column_name (str): The column to sort by. Default is 'systime'. If the column is not found or is not a time column, the class will attempt to detect other time columns.\n    \"\"\"\n    self.dataframe = dataframe.copy()\n\n    # Attempt to convert the specified column_name to datetime if it exists\n    if column_name in self.dataframe.columns:\n        self.dataframe[column_name] = pd.to_datetime(self.dataframe[column_name], errors='coerce')\n    else:\n        # If the column_name is not in the DataFrame, fallback to automatic time detection\n        time_columns = [col for col in self.dataframe.columns if 'time' in col.lower() or 'date' in col.lower()]\n\n        # Convert all detected time columns to datetime, if any\n        for col in time_columns:\n            self.dataframe[col] = pd.to_datetime(self.dataframe[col], errors='coerce')\n\n        # If any time columns are detected, sort by the first one; otherwise, do nothing\n        if time_columns:\n            column_name = time_columns[0]\n\n    # Sort by the datetime column (either specified or detected)\n    if column_name in self.dataframe.columns:\n        self.dataframe = self.dataframe.sort_values(by=column_name)\n</code></pre>"},{"location":"reference/ts_shape/transform/time_functions/timezone_shift/#ts_shape.transform.time_functions.timezone_shift.TimezoneShift(dataframe)","title":"<code>dataframe</code>","text":"(<code>DataFrame</code>)           \u2013            <p>The DataFrame to be processed.</p>"},{"location":"reference/ts_shape/transform/time_functions/timezone_shift/#ts_shape.transform.time_functions.timezone_shift.TimezoneShift(column_name)","title":"<code>column_name</code>","text":"(<code>str</code>, default:                   <code>'systime'</code> )           \u2013            <p>The column to sort by. Default is 'systime'. If the column is not found or is not a time column, the class will attempt to detect other time columns.</p>"},{"location":"reference/ts_shape/transform/time_functions/timezone_shift/#ts_shape.transform.time_functions.timezone_shift.TimezoneShift.add_timezone_column","title":"add_timezone_column  <code>classmethod</code>","text":"<pre><code>add_timezone_column(dataframe: DataFrame, time_column: str, input_timezone: str, target_timezone: str) -&gt; DataFrame\n</code></pre> <p>Creates a new column with timestamps converted from an input timezone to a target timezone, without altering the original column.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>pd.DataFrame: A DataFrame with an additional column for the shifted timezone.</p> </li> </ul> Source code in <code>src/ts_shape/transform/time_functions/timezone_shift.py</code> <pre><code>@classmethod\ndef add_timezone_column(cls, dataframe: pd.DataFrame, time_column: str, input_timezone: str, target_timezone: str) -&gt; pd.DataFrame:\n    \"\"\"\n    Creates a new column with timestamps converted from an input timezone to a target timezone, without altering the original column.\n\n    Args:\n        dataframe (pd.DataFrame): The DataFrame containing the data.\n        time_column (str): The name of the time column to convert.\n        input_timezone (str): The timezone of the input timestamps.\n        target_timezone (str): The target timezone.\n\n    Returns:\n        pd.DataFrame: A DataFrame with an additional column for the shifted timezone.\n    \"\"\"\n    # Duplicate the DataFrame to prevent modifying the original column\n    df_copy = dataframe.copy()\n\n    # Create the new timezone-shifted column\n    new_column = f\"{time_column}_{target_timezone.replace('/', '_')}\"\n    df_copy[new_column] = df_copy[time_column]\n\n    # Apply the timezone shift to the new column\n    df_copy = cls.shift_timezone(df_copy, new_column, input_timezone, target_timezone)\n\n    return df_copy\n</code></pre>"},{"location":"reference/ts_shape/transform/time_functions/timezone_shift/#ts_shape.transform.time_functions.timezone_shift.TimezoneShift.add_timezone_column(dataframe)","title":"<code>dataframe</code>","text":"(<code>DataFrame</code>)           \u2013            <p>The DataFrame containing the data.</p>"},{"location":"reference/ts_shape/transform/time_functions/timezone_shift/#ts_shape.transform.time_functions.timezone_shift.TimezoneShift.add_timezone_column(time_column)","title":"<code>time_column</code>","text":"(<code>str</code>)           \u2013            <p>The name of the time column to convert.</p>"},{"location":"reference/ts_shape/transform/time_functions/timezone_shift/#ts_shape.transform.time_functions.timezone_shift.TimezoneShift.add_timezone_column(input_timezone)","title":"<code>input_timezone</code>","text":"(<code>str</code>)           \u2013            <p>The timezone of the input timestamps.</p>"},{"location":"reference/ts_shape/transform/time_functions/timezone_shift/#ts_shape.transform.time_functions.timezone_shift.TimezoneShift.add_timezone_column(target_timezone)","title":"<code>target_timezone</code>","text":"(<code>str</code>)           \u2013            <p>The target timezone.</p>"},{"location":"reference/ts_shape/transform/time_functions/timezone_shift/#ts_shape.transform.time_functions.timezone_shift.TimezoneShift.calculate_time_difference","title":"calculate_time_difference  <code>classmethod</code>","text":"<pre><code>calculate_time_difference(dataframe: DataFrame, start_column: str, end_column: str) -&gt; Series\n</code></pre> <p>Calculates the time difference between two timestamp columns.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>Series</code>           \u2013            <p>pd.Series: A Series with the time differences in seconds.</p> </li> </ul> Source code in <code>src/ts_shape/transform/time_functions/timezone_shift.py</code> <pre><code>@classmethod\ndef calculate_time_difference(cls, dataframe: pd.DataFrame, start_column: str, end_column: str) -&gt; pd.Series:\n    \"\"\"\n    Calculates the time difference between two timestamp columns.\n\n    Args:\n        dataframe (pd.DataFrame): The DataFrame containing the data.\n        start_column (str): The name of the start time column.\n        end_column (str): The name of the end time column.\n\n    Returns:\n        pd.Series: A Series with the time differences in seconds.\n    \"\"\"\n    # Check if both columns are timezone-aware or both are timezone-naive\n    start_is_aware = dataframe[start_column].dt.tz is not None\n    end_is_aware = dataframe[end_column].dt.tz is not None\n\n    if start_is_aware != end_is_aware:\n        raise ValueError(\"Both columns must be either timezone-aware or timezone-naive.\")\n\n    # If timezone-aware, convert both columns to UTC for comparison\n    if start_is_aware:\n        start_times = dataframe[start_column].dt.tz_convert('UTC')\n        end_times = dataframe[end_column].dt.tz_convert('UTC')\n    else:\n        start_times = dataframe[start_column]\n        end_times = dataframe[end_column]\n\n    # Calculate the difference in seconds\n    time_difference = (end_times - start_times).dt.total_seconds()\n\n    return time_difference\n</code></pre>"},{"location":"reference/ts_shape/transform/time_functions/timezone_shift/#ts_shape.transform.time_functions.timezone_shift.TimezoneShift.calculate_time_difference(dataframe)","title":"<code>dataframe</code>","text":"(<code>DataFrame</code>)           \u2013            <p>The DataFrame containing the data.</p>"},{"location":"reference/ts_shape/transform/time_functions/timezone_shift/#ts_shape.transform.time_functions.timezone_shift.TimezoneShift.calculate_time_difference(start_column)","title":"<code>start_column</code>","text":"(<code>str</code>)           \u2013            <p>The name of the start time column.</p>"},{"location":"reference/ts_shape/transform/time_functions/timezone_shift/#ts_shape.transform.time_functions.timezone_shift.TimezoneShift.calculate_time_difference(end_column)","title":"<code>end_column</code>","text":"(<code>str</code>)           \u2013            <p>The name of the end time column.</p>"},{"location":"reference/ts_shape/transform/time_functions/timezone_shift/#ts_shape.transform.time_functions.timezone_shift.TimezoneShift.detect_timezone_awareness","title":"detect_timezone_awareness  <code>classmethod</code>","text":"<pre><code>detect_timezone_awareness(dataframe: DataFrame, time_column: str) -&gt; bool\n</code></pre> <p>Detects if a time column in a DataFrame is timezone-aware.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>bool</code> (              <code>bool</code> )          \u2013            <p>True if the column is timezone-aware, False otherwise.</p> </li> </ul> Source code in <code>src/ts_shape/transform/time_functions/timezone_shift.py</code> <pre><code>@classmethod\ndef detect_timezone_awareness(cls, dataframe: pd.DataFrame, time_column: str) -&gt; bool:\n    \"\"\"\n    Detects if a time column in a DataFrame is timezone-aware.\n\n    Args:\n        dataframe (pd.DataFrame): The DataFrame containing the data.\n        time_column (str): The name of the time column to check.\n\n    Returns:\n        bool: True if the column is timezone-aware, False otherwise.\n    \"\"\"\n    return dataframe[time_column].dt.tz is not None\n</code></pre>"},{"location":"reference/ts_shape/transform/time_functions/timezone_shift/#ts_shape.transform.time_functions.timezone_shift.TimezoneShift.detect_timezone_awareness(dataframe)","title":"<code>dataframe</code>","text":"(<code>DataFrame</code>)           \u2013            <p>The DataFrame containing the data.</p>"},{"location":"reference/ts_shape/transform/time_functions/timezone_shift/#ts_shape.transform.time_functions.timezone_shift.TimezoneShift.detect_timezone_awareness(time_column)","title":"<code>time_column</code>","text":"(<code>str</code>)           \u2013            <p>The name of the time column to check.</p>"},{"location":"reference/ts_shape/transform/time_functions/timezone_shift/#ts_shape.transform.time_functions.timezone_shift.TimezoneShift.get_dataframe","title":"get_dataframe","text":"<pre><code>get_dataframe() -&gt; DataFrame\n</code></pre> <p>Returns the processed DataFrame.</p> Source code in <code>src/ts_shape/utils/base.py</code> <pre><code>def get_dataframe(self) -&gt; pd.DataFrame:\n    \"\"\"Returns the processed DataFrame.\"\"\"\n    return self.dataframe\n</code></pre>"},{"location":"reference/ts_shape/transform/time_functions/timezone_shift/#ts_shape.transform.time_functions.timezone_shift.TimezoneShift.list_available_timezones","title":"list_available_timezones  <code>classmethod</code>","text":"<pre><code>list_available_timezones() -&gt; list\n</code></pre> <p>Returns a list of all available timezones.</p> <p>Returns:</p> <ul> <li> <code>list</code> (              <code>list</code> )          \u2013            <p>A list of strings representing all available timezones.</p> </li> </ul> Source code in <code>src/ts_shape/transform/time_functions/timezone_shift.py</code> <pre><code>@classmethod\ndef list_available_timezones(cls) -&gt; list:\n    \"\"\"\n    Returns a list of all available timezones.\n\n    Returns:\n        list: A list of strings representing all available timezones.\n    \"\"\"\n    return pytz.all_timezones\n</code></pre>"},{"location":"reference/ts_shape/transform/time_functions/timezone_shift/#ts_shape.transform.time_functions.timezone_shift.TimezoneShift.revert_to_original_timezone","title":"revert_to_original_timezone  <code>classmethod</code>","text":"<pre><code>revert_to_original_timezone(dataframe: DataFrame, time_column: str, original_timezone: str) -&gt; DataFrame\n</code></pre> <p>Reverts a timezone-shifted time column back to the original timezone.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>pd.DataFrame: A DataFrame with timestamps reverted to the original timezone.</p> </li> </ul> Source code in <code>src/ts_shape/transform/time_functions/timezone_shift.py</code> <pre><code>@classmethod\ndef revert_to_original_timezone(cls, dataframe: pd.DataFrame, time_column: str, original_timezone: str) -&gt; pd.DataFrame:\n    \"\"\"\n    Reverts a timezone-shifted time column back to the original timezone.\n\n    Args:\n        dataframe (pd.DataFrame): The DataFrame containing the data.\n        time_column (str): The name of the time column to revert.\n        original_timezone (str): The original timezone to revert to.\n\n    Returns:\n        pd.DataFrame: A DataFrame with timestamps reverted to the original timezone.\n    \"\"\"\n    # Validate the original timezone\n    if original_timezone not in pytz.all_timezones:\n        raise ValueError(f\"Invalid original timezone: {original_timezone}\")\n\n    # Convert to the original timezone\n    dataframe[time_column] = dataframe[time_column].dt.tz_convert(original_timezone)\n\n    return dataframe\n</code></pre>"},{"location":"reference/ts_shape/transform/time_functions/timezone_shift/#ts_shape.transform.time_functions.timezone_shift.TimezoneShift.revert_to_original_timezone(dataframe)","title":"<code>dataframe</code>","text":"(<code>DataFrame</code>)           \u2013            <p>The DataFrame containing the data.</p>"},{"location":"reference/ts_shape/transform/time_functions/timezone_shift/#ts_shape.transform.time_functions.timezone_shift.TimezoneShift.revert_to_original_timezone(time_column)","title":"<code>time_column</code>","text":"(<code>str</code>)           \u2013            <p>The name of the time column to revert.</p>"},{"location":"reference/ts_shape/transform/time_functions/timezone_shift/#ts_shape.transform.time_functions.timezone_shift.TimezoneShift.revert_to_original_timezone(original_timezone)","title":"<code>original_timezone</code>","text":"(<code>str</code>)           \u2013            <p>The original timezone to revert to.</p>"},{"location":"reference/ts_shape/transform/time_functions/timezone_shift/#ts_shape.transform.time_functions.timezone_shift.TimezoneShift.shift_timezone","title":"shift_timezone  <code>classmethod</code>","text":"<pre><code>shift_timezone(dataframe: DataFrame, time_column: str, input_timezone: str, target_timezone: str) -&gt; DataFrame\n</code></pre> <p>Shifts timestamps in the specified column of a DataFrame from a given timezone to a target timezone.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>pd.DataFrame: A DataFrame with timestamps converted to the target timezone.</p> </li> </ul> Source code in <code>src/ts_shape/transform/time_functions/timezone_shift.py</code> <pre><code>@classmethod\ndef shift_timezone(cls, dataframe: pd.DataFrame, time_column: str, input_timezone: str, target_timezone: str) -&gt; pd.DataFrame:\n    \"\"\"\n    Shifts timestamps in the specified column of a DataFrame from a given timezone to a target timezone.\n\n    Args:\n        dataframe (pd.DataFrame): The DataFrame containing the data.\n        time_column (str): The name of the time column to convert.\n        input_timezone (str): The timezone of the input timestamps (e.g., 'UTC' or 'America/New_York').\n        target_timezone (str): The target timezone to shift to (e.g., 'America/New_York').\n\n    Returns:\n        pd.DataFrame: A DataFrame with timestamps converted to the target timezone.\n    \"\"\"\n    # Validate timezones\n    if input_timezone not in pytz.all_timezones:\n        raise ValueError(f\"Invalid input timezone: {input_timezone}\")\n    if target_timezone not in pytz.all_timezones:\n        raise ValueError(f\"Invalid target timezone: {target_timezone}\")\n\n    # Ensure the time column is in datetime format\n    if not pd.api.types.is_datetime64_any_dtype(dataframe[time_column]):\n        raise ValueError(f\"Column '{time_column}' must contain datetime values.\")\n\n    # Localize to the specified input timezone if timestamps are naive\n    dataframe[time_column] = pd.to_datetime(dataframe[time_column])\n    if dataframe[time_column].dt.tz is None:\n        dataframe[time_column] = dataframe[time_column].dt.tz_localize(input_timezone)\n    else:\n        # Convert from the existing timezone to the specified input timezone, if they differ\n        dataframe[time_column] = dataframe[time_column].dt.tz_convert(input_timezone)\n\n    # Convert to the target timezone\n    dataframe[time_column] = dataframe[time_column].dt.tz_convert(target_timezone)\n\n    return dataframe\n</code></pre>"},{"location":"reference/ts_shape/transform/time_functions/timezone_shift/#ts_shape.transform.time_functions.timezone_shift.TimezoneShift.shift_timezone(dataframe)","title":"<code>dataframe</code>","text":"(<code>DataFrame</code>)           \u2013            <p>The DataFrame containing the data.</p>"},{"location":"reference/ts_shape/transform/time_functions/timezone_shift/#ts_shape.transform.time_functions.timezone_shift.TimezoneShift.shift_timezone(time_column)","title":"<code>time_column</code>","text":"(<code>str</code>)           \u2013            <p>The name of the time column to convert.</p>"},{"location":"reference/ts_shape/transform/time_functions/timezone_shift/#ts_shape.transform.time_functions.timezone_shift.TimezoneShift.shift_timezone(input_timezone)","title":"<code>input_timezone</code>","text":"(<code>str</code>)           \u2013            <p>The timezone of the input timestamps (e.g., 'UTC' or 'America/New_York').</p>"},{"location":"reference/ts_shape/transform/time_functions/timezone_shift/#ts_shape.transform.time_functions.timezone_shift.TimezoneShift.shift_timezone(target_timezone)","title":"<code>target_timezone</code>","text":"(<code>str</code>)           \u2013            <p>The target timezone to shift to (e.g., 'America/New_York').</p>"},{"location":"reference/ts_shape/utils/__init__/","title":"init","text":""},{"location":"reference/ts_shape/utils/__init__/#ts_shape.utils","title":"ts_shape.utils","text":"<p>Utils</p> <p>Shared utilities used across modules.</p> <ul> <li>Base: Normalize and sort DataFrames by time columns.</li> <li>get_dataframe: Return the processed DataFrame copy.</li> </ul> <p>Modules:</p> <ul> <li> <code>base</code>           \u2013            </li> </ul>"},{"location":"reference/ts_shape/utils/base/","title":"base","text":""},{"location":"reference/ts_shape/utils/base/#ts_shape.utils.base","title":"ts_shape.utils.base","text":"<p>Classes:</p> <ul> <li> <code>Base</code>           \u2013            </li> </ul>"},{"location":"reference/ts_shape/utils/base/#ts_shape.utils.base.Base","title":"Base","text":"<pre><code>Base(dataframe: DataFrame, column_name: str = 'systime')\n</code></pre> <p>Parameters:</p> <p>Methods:</p> <ul> <li> <code>get_dataframe</code>             \u2013              <p>Returns the processed DataFrame.</p> </li> </ul> Source code in <code>src/ts_shape/utils/base.py</code> <pre><code>def __init__(self, dataframe: pd.DataFrame, column_name: str = 'systime') -&gt; None:\n    \"\"\"\n    Initializes the Base with a DataFrame, detects time columns, converts them to datetime,\n    and sorts the DataFrame by the specified column (or the detected time column if applicable).\n\n    Args:\n        dataframe (pd.DataFrame): The DataFrame to be processed.\n        column_name (str): The column to sort by. Default is 'systime'. If the column is not found or is not a time column, the class will attempt to detect other time columns.\n    \"\"\"\n    self.dataframe = dataframe.copy()\n\n    # Attempt to convert the specified column_name to datetime if it exists\n    if column_name in self.dataframe.columns:\n        self.dataframe[column_name] = pd.to_datetime(self.dataframe[column_name], errors='coerce')\n    else:\n        # If the column_name is not in the DataFrame, fallback to automatic time detection\n        time_columns = [col for col in self.dataframe.columns if 'time' in col.lower() or 'date' in col.lower()]\n\n        # Convert all detected time columns to datetime, if any\n        for col in time_columns:\n            self.dataframe[col] = pd.to_datetime(self.dataframe[col], errors='coerce')\n\n        # If any time columns are detected, sort by the first one; otherwise, do nothing\n        if time_columns:\n            column_name = time_columns[0]\n\n    # Sort by the datetime column (either specified or detected)\n    if column_name in self.dataframe.columns:\n        self.dataframe = self.dataframe.sort_values(by=column_name)\n</code></pre>"},{"location":"reference/ts_shape/utils/base/#ts_shape.utils.base.Base(dataframe)","title":"<code>dataframe</code>","text":"(<code>DataFrame</code>)           \u2013            <p>The DataFrame to be processed.</p>"},{"location":"reference/ts_shape/utils/base/#ts_shape.utils.base.Base(column_name)","title":"<code>column_name</code>","text":"(<code>str</code>, default:                   <code>'systime'</code> )           \u2013            <p>The column to sort by. Default is 'systime'. If the column is not found or is not a time column, the class will attempt to detect other time columns.</p>"},{"location":"reference/ts_shape/utils/base/#ts_shape.utils.base.Base.get_dataframe","title":"get_dataframe","text":"<pre><code>get_dataframe() -&gt; DataFrame\n</code></pre> <p>Returns the processed DataFrame.</p> Source code in <code>src/ts_shape/utils/base.py</code> <pre><code>def get_dataframe(self) -&gt; pd.DataFrame:\n    \"\"\"Returns the processed DataFrame.\"\"\"\n    return self.dataframe\n</code></pre>"},{"location":"usage/","title":"Quick Start Guide","text":"<p>This guide walks you through a simple usage example of the <code>ts-shape</code> library to load, transform, and analyze time series data.</p>"},{"location":"usage/#installation","title":"Installation","text":"<p>You can install the library via pip:</p> <pre><code>pip install ts-shape\n</code></pre>"},{"location":"usage/#example-workflow","title":"Example Workflow","text":""},{"location":"usage/#1-import-modules","title":"1. Import Modules","text":"<pre><code>from ts_shape.loader.timeseries import parquet_loader\nfrom ts_shape.transform.filter import boolean_filter\nfrom ts_shape.transform.time_functions import timestamp_converter, timezone_shift\nfrom ts_shape.features.stats import string_stats\n</code></pre>"},{"location":"usage/#2-load-time-series-data","title":"2. Load Time Series Data","text":"<p>Load all parquet files from a given directory:</p> <pre><code>base_path = 'path/to/your/parquet/files'\ndf_all = parquet_loader.ParquetLoader.load_all_files(base_path)\n</code></pre>"},{"location":"usage/#3-filter-data","title":"3. Filter Data","text":"<p>Filter rows where the column <code>is_delta</code> is <code>True</code>:</p> <pre><code>df_is_delta = boolean_filter.IsDeltaFilter.filter_is_delta_true(df_all)\n</code></pre>"},{"location":"usage/#4-convert-timestamps","title":"4. Convert Timestamps","text":"<p>Convert Unix nanosecond timestamps to timezone-aware datetime objects:</p> <pre><code>df_timestamp = timestamp_converter.TimestampConverter.convert_to_datetime(\n    dataframe=df_is_delta,\n    columns=['systime', 'plctime'],\n    unit='ns',\n    timezone='UTC'\n)\n</code></pre>"},{"location":"usage/#5-shift-timezone","title":"5. Shift Timezone","text":"<p>Convert timestamps from UTC to local time (e.g., Europe/Berlin):</p> <pre><code>df_timestamp_shift = timezone_shift.TimezoneShift.shift_timezone(\n    dataframe=df_timestamp,\n    time_column='systime',\n    input_timezone='UTC',\n    target_timezone='Europe/Berlin'\n)\n</code></pre>"},{"location":"usage/#next-steps","title":"Next Steps","text":"<ul> <li>Explore feature extraction with <code>ts_shape.features</code>.</li> <li>Chain multiple transformations into a pipeline.</li> </ul>"}]}