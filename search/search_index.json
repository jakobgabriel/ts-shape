{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"ts-shape | Timeseries Shaper","text":"<p>ts-shape is a lightweight, composable toolkit to load, shape, and analyze time series data. It embraces a simple DataFrame-in \u2192 DataFrame-out philosophy across loaders, transforms, feature extractors, and event detectors.</p> <p>Key ideas:</p> <ul> <li>Unified DataFrame workflow: Load timeseries + metadata, join on <code>uuid</code>, and process.</li> <li>Modular building blocks: Use only what you need; components are decoupled and easy to extend.</li> <li>Performance aware: Vectorized ops, chunked DB reads, and concurrent I/O for remote storage.</li> </ul>"},{"location":"#install","title":"Install","text":"<pre><code>pip install ts-shape\n# Parquet engine (recommended)\npip install pyarrow  # or: pip install fastparquet\n</code></pre> <p>Optional integrations:</p> <ul> <li>Azure Blob Storage: <code>pip install azure-storage-blob</code></li> <li>Azure AAD + management (optional): <code>pip install azure-identity azure-mgmt-storage</code></li> <li>S3 proxy access: already included via <code>s3fs</code></li> <li>TimescaleDB: <code>pip install sqlalchemy psycopg2-binary</code></li> </ul>"},{"location":"#whats-inside","title":"What\u2019s Inside","text":"<ul> <li>Loaders (timeseries):</li> <li>Parquet folders (local)</li> <li>S3 proxy parquet via <code>s3fs</code></li> <li>Azure Blob parquet (hourly layout, UUID filters, time range)</li> <li>TimescaleDB (chunked reads, parquet export by hour)</li> <li>Loaders (metadata):</li> <li>JSON metadata loader (robust input shapes, flattens config)</li> <li>Transformations:</li> <li>Filters (numeric/string/boolean/datetime), generic functions, time functions, calculators</li> <li>Features:</li> <li>Descriptive stats, time stats, cycles utilities</li> <li>Events:</li> <li>Quality (outlier detection, SPC, tolerance deviation), production/maintenance patterns</li> </ul> <p>See the extended concept overview in <code>docs/concept.md</code>.</p>"},{"location":"#license","title":"License","text":"<p>MIT \u2014 see <code>LICENSE.txt</code>.</p>"},{"location":"changelog/","title":"Sep 09, 2025","text":"<p>add: Azure blob storage container loader add: Metadata JSON loader added changed: development guide</p>"},{"location":"changelog/#apr-06-2025","title":"Apr 06, 2025","text":"<p>add: quickstart guide added to docs</p>"},{"location":"changelog/#dec-26-2024","title":"Dec 26, 2024","text":"<p>add: mkdocs material deployment with gh actions</p>"},{"location":"changelog/#dec-25-2024","title":"Dec 25, 2024","text":"<p>add: pdoc docs exchanged with mkdocs material autodoc</p>"},{"location":"changelog/#dec-23-2024","title":"Dec 23, 2024","text":"<p>add: library rename add: library structure change. loader, transform, feature, context, events</p>"},{"location":"changelog/#dec-20-2024","title":"Dec 20, 2024","text":"<p>fix: closes #7</p>"},{"location":"changelog/#nov-16-2024","title":"Nov 16, 2024","text":"<p>add: dev state docs for combine/integrator.py add: dev state for combine/integrator.py add: metadata loader improved</p>"},{"location":"changelog/#nov-4-2024","title":"Nov 4, 2024","text":"<p>add: stats classes improved + feature table class added</p>"},{"location":"changelog/#oct-29-2024","title":"Oct 29, 2024","text":"<p>add: timescaledb loader adjusted</p>"},{"location":"changelog/#oct-28-2024","title":"Oct 28, 2024","text":"<p>add: loader classes adjusted add: classes for timestamp_converter added add: classes for s3, timescaledb, timezone_shift added</p>"},{"location":"changelog/#oct-27-2024","title":"Oct 27, 2024","text":"<p>add: docs for time_stats added add: time_stats for numeric value columns added</p>"},{"location":"changelog/#sep-11-2024","title":"Sep 11, 2024","text":"<p>add: methods refactored to execute without an instance</p>"},{"location":"changelog/#sep-9-2024","title":"Sep 9, 2024","text":"<p>add: cycle methods refined and splitted to cycle processor and extractor</p>"},{"location":"changelog/#sep-8-2024","title":"Sep 8, 2024","text":"<p>add: cycle method and metadata loader including tests for numeric filters add: additional methods added</p>"},{"location":"changelog/#aug-25-2024","title":"Aug 25, 2024","text":"<p>fix: docs re-created fix: docs add: parquet loader, timestamp, string, numeric and boolean stats methods</p>"},{"location":"changelog/#may-4-2024","title":"May 4, 2024","text":"<p>change: typings added, first test added change: package folder moved init: pypi package structure for timeseries-shaper / internal project for my master thesis</p>"},{"location":"concept/","title":"Concept","text":"<p>ts-shape is a lightweight, composable toolkit for shaping time series data into analysis-ready DataFrames. It focuses on three pillars: loading, transforming, and extracting higher-level features/events \u2014 with a consistent, Pandas-first interface.</p>"},{"location":"concept/#architecture-overview","title":"Architecture Overview","text":"<pre><code>flowchart TD\n    A[Loaders: Timeseries + Metadata] --&gt; B[Combine: join on uuid]\n    B --&gt; C[Transform: Filters / Functions / Time Functions / Calculator]\n    C --&gt; D[Features: Stats / Time Stats / Cycles]\n    D --&gt; E[Events: Quality / Maintenance / Production / Engineering]</code></pre> <p>Core ideas:</p> <ul> <li>DataFrame-in, DataFrame-out: Every stage accepts and returns Pandas DataFrames for easy composition.</li> <li>Simple schema: Timeseries frames use a compact set of typed columns; metadata/enrichment joins on a stable <code>uuid</code> key.</li> <li>Modular blocks: Use only what you need \u2014 loaders, transforms, features, and events are decoupled.</li> </ul>"},{"location":"concept/#data-model","title":"Data Model","text":"<p>Timeseries DataFrame (typical columns):</p> <ul> <li>uuid: string identifier for a signal/series</li> <li>systime: timestamp (tz-aware recommended)</li> <li>value_double, value_integer, value_string, value_bool: value channels (one or more may be present)</li> <li>is_delta: boolean flag indicating delta semantics (optional)</li> </ul> <p>Metadata DataFrame:</p> <ul> <li>Indexed by uuid or has a <code>uuid</code> column</li> <li>Arbitrary columns describing the signal (label, unit, config.*)</li> </ul> <p>Conventions:</p> <ul> <li>Join key is <code>uuid</code> by default.</li> <li>Keep values narrow: prefer one type-specific value column where possible.</li> </ul>"},{"location":"concept/#loaders","title":"Loaders","text":"<p>Timeseries:</p> <ul> <li>Parquet folder loader: Recursively reads parquet files from local/remote mounts.</li> <li>S3 proxy parquet loader: Streams parquet via S3-compatible endpoints.</li> <li>Azure Blob parquet loader: Loads parquet files from containers; supports time-based folder structure (parquet/YYYY/MM/DD/HH) and UUID filters.</li> <li>TimescaleDB loader: Streams rows by UUID and time range; can emit parquet partitioned by hour.</li> </ul> <p>Metadata:</p> <ul> <li>JSON metadata loader: Robustly ingests JSON in multiple shapes (list-of-records, dicts of lists/dicts), flattens <code>config</code> into columns, and indexes by <code>uuid</code>.</li> </ul> <p>All loaders expose either a DataFrame-returning method (e.g., <code>fetch_data_as_dataframe</code>, <code>to_df</code>) or a parquet materialization method when desired.</p>"},{"location":"concept/#combination-layer","title":"Combination Layer","text":"<p>Use <code>DataIntegratorHybrid.combine_data(...)</code> to merge timeseries and metadata sources into one frame:</p> <ul> <li>Accepts DataFrames or source objects (with <code>fetch_data_as_dataframe</code>/<code>fetch_metadata</code>).</li> <li>Merges on <code>uuid</code> (configurable), supporting different join strategies (<code>left</code>, <code>inner</code>, ...).</li> </ul> <p>Example: <pre><code>from ts_shape.loader.combine.integrator import DataIntegratorHybrid\n\ncombined = DataIntegratorHybrid.combine_data(\n    timeseries_sources=[ts_df_or_loader],\n    metadata_sources=[meta_df_or_loader],\n    uuids=[\"id-1\", \"id-2\"],\n    join_key=\"uuid\",\n    merge_how=\"left\",\n)\n</code></pre></p>"},{"location":"concept/#transform","title":"Transform","text":"<p>Reusable blocks to reshape and clean data:</p> <ul> <li>Filters: datatype-specific predicates (numeric/string/boolean/datetime) to subset rows or fix values.</li> <li>Functions: arbitrary lambda-like transformations for columns.</li> <li>Time Functions: timestamp operations (timezone shift, conversion, resampling helpers).</li> <li>Calculator: numeric calculators to derive engineered columns.</li> </ul> <p>All transformations accept/return DataFrames to compose pipelines like small, testable steps.</p>"},{"location":"concept/#features","title":"Features","text":"<p>Feature extractors summarize series into compact descriptors:</p> <ul> <li>Stats: per-type descriptive stats (min/max/mean/std for numeric, frequency for strings, etc.).</li> <li>Time Stats: timestamp-specific stats (first/last timestamp, counts per window, coverage).</li> <li>Cycles: utilities to identify and process cycles in signals.</li> </ul> <p><code>DescriptiveFeatures.compute(...)</code> can emit a nested dict or a flat DataFrame for easy downstream analysis.</p>"},{"location":"concept/#events","title":"Events","text":"<p>Event detectors derive categorical flags and ranges from raw signals:</p> <ul> <li>Quality: outlier detection, SPC rules, tolerance deviations.</li> <li>Maintenance: downtime and other operational events.</li> <li>Production/Engineering: domain patterns extractable from the shaped series.</li> </ul> <p>Each detector takes a DataFrame and returns either annotated frames or event tables.</p>"},{"location":"concept/#typical-pipeline","title":"Typical Pipeline","text":"<ol> <li>Load</li> <li>Read timeseries (e.g., parquet or DB) into a DataFrame with <code>uuid</code>, <code>systime</code>, and values.</li> <li> <p>Load metadata JSON and convert to a <code>uuid</code>-indexed DataFrame.</p> </li> <li> <p>Combine</p> </li> <li> <p>Join timeseries with metadata on <code>uuid</code> to enrich context.</p> </li> <li> <p>Transform</p> </li> <li> <p>Apply filters/functions/time operations; compute engineered columns.</p> </li> <li> <p>Features &amp; Events</p> </li> <li> <p>Compute stats and time stats; identify domain events.</p> </li> <li> <p>Output</p> </li> <li>Keep as a DataFrame, write parquet/CSV, or feed to a model/BI tool.</li> </ol>"},{"location":"concept/#design-principles","title":"Design Principles","text":"<ul> <li>Minimal assumptions: Works with partial columns; you choose the value channel(s) in play.</li> <li>Composability: Small building blocks; pure DataFrame IO.</li> <li>Performance-aware: Vectorized Pandas ops; chunked DB reads; concurrent IO for remote storage.</li> <li>Extensible: Add new loaders, transforms, features, or events with simple, documented interfaces.</li> </ul>"},{"location":"concept/#extending-ts-shape","title":"Extending ts-shape","text":"<ul> <li>New loader: implement a class with <code>fetch_data_as_dataframe()</code> or an explicit <code>to_parquet()</code> flow.</li> <li>New transform: write a function that takes/returns a DataFrame; place under <code>transform/*</code>.</li> <li>New feature/event: follow existing patterns; accept a DataFrame and return a summary/event frame.</li> </ul>"},{"location":"concept/#when-to-use-ts-shape","title":"When to Use ts-shape","text":"<ul> <li>You need a quick, pythonic path from raw timeseries + context to analysis-ready tables.</li> <li>You want modular building blocks instead of a monolithic framework.</li> <li>You operate across storage backends (parquet, S3/Azure, SQL) and prefer a unified DataFrame API.</li> </ul>"},{"location":"license/","title":"License","text":"<p>MIT License</p> <p>Copyright (c) [2024] Jakob Gabriel</p> <p>Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:</p> <p>The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.</p> <p>THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</p>"},{"location":"insiders/development/","title":"Development Guide","text":""},{"location":"insiders/development/#1-install-package-locally","title":"1. Install Package Locally","text":"<pre><code>pip install -e .\n</code></pre> <p>Installs your package in editable mode for local development.</p>"},{"location":"insiders/development/#2-run-tests","title":"2. Run Tests","text":"<pre><code>pytest ./tests\n</code></pre> <p>Executes all tests to ensure your code is working as expected.</p>"},{"location":"insiders/development/#3-build-distribution-packages","title":"3. Build Distribution Packages","text":"<pre><code>python setup.py sdist bdist_wheel\n</code></pre> <p>Creates source and wheel distributions in the <code>dist/</code> directory.</p>"},{"location":"insiders/development/#4-publish-to-pypi","title":"4. Publish to PyPI","text":"<pre><code>twine upload dist/* --verbose --skip-existing\n</code></pre> <p>Uploads your package to PyPI. Tip: Ensure your credentials are set up in <code>~/.pypirc</code>.</p>"},{"location":"insiders/development/#5-automatic-version-bumping-and-publishing-ci","title":"5. Automatic Version Bumping and Publishing (CI)","text":"<p>This repo is configured to auto-bump the version in <code>setup.py</code>, create a Git tag, and publish to PyPI on pushes to <code>main</code>.</p> <ul> <li>Workflow: <code>.github/workflows/auto_bump_version.yml</code></li> <li>Publish on tags: <code>.github/workflows/pypi-packaging.yml</code> (triggers on <code>v*</code> tags)</li> </ul> <p>Keep the version declaration in <code>setup.py</code> in this exact form (the trailing comma is fine):</p> <pre><code>setuptools.setup(\n    name=\"ts_shape\",\n    version = \"0.0.0.24\",\n    # ...\n)\n</code></pre> <p>On every push to <code>main</code>, the auto-bump workflow reads that line and increments it based on the latest commit message:</p> <ul> <li>Major: include <code>BREAKING CHANGE</code>, <code>#major</code>, or the short <code>!:</code> in the subject</li> <li>Minor: start the subject with <code>feat</code> or include <code>#minor</code></li> <li>Patch: default for all other commits</li> </ul> <p>Examples:</p> <pre><code>feat: add new SPC rule 9\n# =&gt; bumps 0.0.0.24 -&gt; 0.1.0 and tags v0.1.0\n\nfix: handle NaNs in StringFilter\n# =&gt; bumps 0.1.0 -&gt; 0.1.1 and tags v0.1.1\n\nrefactor!: remove deprecated API (BREAKING CHANGE)\n# =&gt; bumps 0.1.1 -&gt; 1.0.0 and tags v1.0.0\n</code></pre> <p>The workflow then: - Commits the updated <code>setup.py</code> with <code>[skip ci]</code> to avoid loops - Creates and pushes a tag <code>vX.Y.Z</code> - The packaging workflow sees the tag and publishes the built artifacts to PyPI</p> <p>One\u2011time repo setting required: enable \u201cRead and write permissions\u201d for GitHub Actions under Settings \u2192 Actions \u2192 General \u2192 Workflow permissions.</p>"},{"location":"insiders/development/#6-manage-requirements-piptools","title":"6. Manage Requirements (pip\u2011tools)","text":"<p>Keep only direct dependencies in <code>requirements.in</code> and compile pinned versions into <code>requirements.txt</code>.</p> <p>Setup (once per environment):</p> <pre><code>python -m pip install --upgrade pip-tools\n</code></pre> <p>Compile/update pins:</p> <pre><code># Compile requirements.in -&gt; requirements.txt\npython scripts/requirements.py compile\n\n# Upgrade all pins to latest compatible versions\npython scripts/requirements.py upgrade\n</code></pre> <p>Sync your virtualenv exactly to <code>requirements.txt</code> (adds/removes packages):</p> <pre><code>python scripts/requirements.py sync\n</code></pre> <p>Notes: - Edit direct deps in <code>requirements.in</code> (not <code>requirements.txt</code>). - <code>pip-sync</code> will uninstall anything not listed in <code>requirements.txt</code>.</p>"},{"location":"insiders/installation/","title":"Installation Guide","text":"<p>This guide helps you install ts-shape for common environments, and optionally enable cloud/data-source integrations.</p>"},{"location":"insiders/installation/#requirements","title":"Requirements","text":"<ul> <li>Python 3.10 or newer</li> <li>pip 22+ recommended</li> <li>A virtual environment (recommended): <code>python -m venv .venv &amp;&amp; source .venv/bin/activate</code> (PowerShell: <code>.venv\\Scripts\\Activate.ps1</code>)</li> </ul>"},{"location":"insiders/installation/#quick-start-pypi","title":"Quick Start (PyPI)","text":"<ul> <li>Install the package:</li> <li><code>pip install ts-shape</code></li> <li>Verify import and version:</li> <li><code>python -c \"import ts_shape, importlib.metadata as m; print('ts_shape imported, version', m.version('ts-shape'))\"</code></li> </ul>"},{"location":"insiders/installation/#optional-parquet-engines","title":"Optional Parquet Engines","text":"<p>ts-shape uses pandas to read/write Parquet. Install one engine:</p> <ul> <li><code>pip install pyarrow</code>  (recommended)</li> <li>or <code>pip install fastparquet</code></li> </ul> <p>Without one of these, <code>pd.read_parquet</code> will raise an error.</p>"},{"location":"insiders/installation/#optional-integrations","title":"Optional Integrations","text":"<ul> <li>S3 access (S3 proxy loader):</li> <li> <p>Already included: <code>s3fs</code></p> </li> <li> <p>Azure Blob Storage (Azure parquet loader):</p> </li> <li><code>pip install azure-storage-blob pyarrow</code></li> <li> <p>For AAD auth and management APIs:</p> <ul> <li><code>pip install azure-identity azure-mgmt-storage</code></li> </ul> </li> <li> <p>TimescaleDB (Timescale loader):</p> </li> <li><code>pip install sqlalchemy psycopg2-binary</code></li> </ul>"},{"location":"insiders/installation/#from-source-editable","title":"From Source (Editable)","text":"<p>If you\u2019re developing or want the latest:</p> <p>1) Clone the repo and enter it:    - <code>git clone https://github.com/jakobgabriel/ts-shape.git</code>    - <code>cd ts-shape</code></p> <p>2) Create and activate a venv (recommended):    - <code>python -m venv .venv</code>    - Linux/macOS: <code>source .venv/bin/activate</code>    - Windows (PowerShell): <code>.venv\\Scripts\\Activate.ps1</code></p> <p>3) Install base deps and package:    - <code>pip install -r requirements.txt</code>    - <code>pip install -e .</code></p> <p>4) Add optional integrations as needed (see above).</p>"},{"location":"insiders/installation/#sanity-check","title":"Sanity Check","text":"<ul> <li>Minimal check to ensure functionality:</li> </ul> <pre><code>python - &lt;&lt; 'PY'\nimport pandas as pd\nfrom ts_shape.loader.metadata.metadata_json_loader import MetadataJsonLoader\ndata = {\"uuid\": {\"0\": \"u1\"}, \"label\": {\"0\": \"sensor-1\"}, \"config\": {\"0\": {\"unit\": \"C\"}}}\ndf = MetadataJsonLoader(data).to_df()\nprint(df)\nPY\n</code></pre>"},{"location":"insiders/installation/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>Parquet engine missing: Install <code>pyarrow</code> (or <code>fastparquet</code>).</li> <li>ImportError for Azure classes: Install <code>azure-storage-blob</code> (and optionally <code>azure-identity</code>, <code>azure-mgmt-storage</code>).</li> <li>Timescale connection errors: Verify <code>sqlalchemy</code>/<code>psycopg2-binary</code> installed and DSN is correct.</li> <li>Version conflicts: <code>pip install --upgrade pip setuptools wheel</code> then reinstall.</li> </ul>"},{"location":"insiders/installation/#uninstall","title":"Uninstall","text":"<ul> <li><code>pip uninstall ts-shape</code></li> </ul>"},{"location":"reference/SUMMARY/","title":"Summary","text":"<ul> <li>ts_shape<ul> <li>context<ul> <li>value_mapping</li> </ul> </li> <li>events<ul> <li>engineering<ul> <li>setpoint_events</li> <li>startup_events</li> </ul> </li> <li>maintenance</li> <li>production<ul> <li>changeover</li> <li>downtime</li> <li>flow_constraints</li> <li>line_throughput</li> <li>machine_state</li> </ul> </li> <li>quality<ul> <li>outlier_detection</li> <li>statistical_process_control</li> <li>tolerance_deviation</li> </ul> </li> <li>supplychain</li> </ul> </li> <li>features<ul> <li>cycles<ul> <li>cycle_processor</li> <li>cycles_extractor</li> </ul> </li> <li>stats<ul> <li>boolean_stats</li> <li>feature_table</li> <li>numeric_stats</li> <li>string_stats</li> <li>timestamp_stats</li> </ul> </li> <li>time_stats<ul> <li>time_stats_numeric</li> </ul> </li> </ul> </li> <li>loader<ul> <li>combine<ul> <li>integrator</li> </ul> </li> <li>context</li> <li>metadata<ul> <li>metadata_api_loader</li> <li>metadata_db_loader</li> <li>metadata_json_loader</li> </ul> </li> <li>timeseries<ul> <li>azure_blob_loader</li> <li>energy_api_loader</li> <li>parquet_loader</li> <li>s3proxy_parquet_loader</li> <li>timescale_loader</li> </ul> </li> </ul> </li> <li>transform<ul> <li>calculator<ul> <li>numeric_calc</li> </ul> </li> <li>filter<ul> <li>boolean_filter</li> <li>custom_filter</li> <li>datetime_filter</li> <li>numeric_filter</li> <li>string_filter</li> </ul> </li> <li>functions<ul> <li>lambda_func</li> </ul> </li> <li>time_functions<ul> <li>timestamp_converter</li> <li>timezone_shift</li> </ul> </li> </ul> </li> <li>utils<ul> <li>base</li> </ul> </li> </ul> </li> </ul>"},{"location":"reference/ts_shape/__init__/","title":"init","text":""},{"location":"reference/ts_shape/__init__/#ts_shape","title":"ts_shape","text":"<p>Modules:</p> <ul> <li> <code>context</code>           \u2013            <p>Context</p> </li> <li> <code>events</code>           \u2013            <p>Events</p> </li> <li> <code>features</code>           \u2013            <p>Features</p> </li> <li> <code>loader</code>           \u2013            <p>Loaders</p> </li> <li> <code>transform</code>           \u2013            <p>Transform</p> </li> <li> <code>utils</code>           \u2013            <p>Utils</p> </li> </ul>"},{"location":"reference/ts_shape/context/__init__/","title":"init","text":""},{"location":"reference/ts_shape/context/__init__/#ts_shape.context","title":"ts_shape.context","text":"<p>Context</p> <p>Utilities for enriching DataFrames with contextual information and mappings.</p> <ul> <li>ValueMapper: Map categorical codes to readable values from external files.</li> <li>map_values: Merge and replace a target column using a CSV/JSON mapping table.</li> <li>_load_mapping_table: Load a mapping table from CSV or JSON.</li> </ul> <p>Modules:</p> <ul> <li> <code>value_mapping</code>           \u2013            </li> </ul>"},{"location":"reference/ts_shape/context/value_mapping/","title":"value_mapping","text":""},{"location":"reference/ts_shape/context/value_mapping/#ts_shape.context.value_mapping","title":"ts_shape.context.value_mapping","text":"<p>Classes:</p> <ul> <li> <code>ValueMapper</code>           \u2013            <p>A class to map values from specified columns of a DataFrame using a mapping table (CSV or JSON file),</p> </li> </ul>"},{"location":"reference/ts_shape/context/value_mapping/#ts_shape.context.value_mapping.ValueMapper","title":"ValueMapper","text":"<pre><code>ValueMapper(dataframe: DataFrame, mapping_file: str, map_column: str, mapping_key_column: str, mapping_value_column: str, file_type: str = 'csv', sep: str = ',', encoding: str = 'utf-8', column_name: str = 'systime')\n</code></pre> <p>               Bases: <code>Base</code></p> <p>A class to map values from specified columns of a DataFrame using a mapping table (CSV or JSON file), inheriting from the Base class.</p> <p>Parameters:</p> <p>Methods:</p> <ul> <li> <code>get_dataframe</code>             \u2013              <p>Returns the processed DataFrame.</p> </li> <li> <code>map_values</code>             \u2013              <p>Maps values in the specified DataFrame column based on the mapping table.</p> </li> </ul> Source code in <code>src/ts_shape/context/value_mapping.py</code> <pre><code>def __init__(\n    self, \n    dataframe: pd.DataFrame, \n    mapping_file: str, \n    map_column: str, \n    mapping_key_column: str, \n    mapping_value_column: str, \n    file_type: str = 'csv', \n    sep: str = ',', \n    encoding: str = 'utf-8', \n    column_name: str = 'systime'\n) -&gt; None:\n    \"\"\"\n    Initializes ValueMapper and the base DataFrame from the Base class.\n\n    Args:\n        dataframe (pd.DataFrame): The DataFrame to be processed and mapped.\n        mapping_file (str): The file path of the mapping table (CSV or JSON).\n        map_column (str): The name of the column in the DataFrame that needs to be mapped.\n        mapping_key_column (str): The column in the mapping table to match with values from the DataFrame.\n        mapping_value_column (str): The column in the mapping table containing the values to map to.\n        file_type (str): The type of the mapping file ('csv' or 'json'). Defaults to 'csv'.\n        sep (str): The separator for CSV files. Defaults to ','.\n        encoding (str): The encoding to use for reading the file. Defaults to 'utf-8'.\n        column_name (str): The name of the column to sort the DataFrame by in the base class. Defaults to 'systime'.\n    \"\"\"\n    # Initialize the Base class with the sorted DataFrame\n    super().__init__(dataframe, column_name)\n\n    # Additional attributes for ValueMapper\n    self.map_column: str = map_column\n    self.mapping_key_column: str = mapping_key_column\n    self.mapping_value_column: str = mapping_value_column\n    self.sep: str = sep\n    self.encoding: str = encoding\n\n    # Load the mapping table based on file type\n    self.mapping_table: pd.DataFrame = self._load_mapping_table(mapping_file, file_type)\n</code></pre>"},{"location":"reference/ts_shape/context/value_mapping/#ts_shape.context.value_mapping.ValueMapper(dataframe)","title":"<code>dataframe</code>","text":"(<code>DataFrame</code>)           \u2013            <p>The DataFrame to be processed and mapped.</p>"},{"location":"reference/ts_shape/context/value_mapping/#ts_shape.context.value_mapping.ValueMapper(mapping_file)","title":"<code>mapping_file</code>","text":"(<code>str</code>)           \u2013            <p>The file path of the mapping table (CSV or JSON).</p>"},{"location":"reference/ts_shape/context/value_mapping/#ts_shape.context.value_mapping.ValueMapper(map_column)","title":"<code>map_column</code>","text":"(<code>str</code>)           \u2013            <p>The name of the column in the DataFrame that needs to be mapped.</p>"},{"location":"reference/ts_shape/context/value_mapping/#ts_shape.context.value_mapping.ValueMapper(mapping_key_column)","title":"<code>mapping_key_column</code>","text":"(<code>str</code>)           \u2013            <p>The column in the mapping table to match with values from the DataFrame.</p>"},{"location":"reference/ts_shape/context/value_mapping/#ts_shape.context.value_mapping.ValueMapper(mapping_value_column)","title":"<code>mapping_value_column</code>","text":"(<code>str</code>)           \u2013            <p>The column in the mapping table containing the values to map to.</p>"},{"location":"reference/ts_shape/context/value_mapping/#ts_shape.context.value_mapping.ValueMapper(file_type)","title":"<code>file_type</code>","text":"(<code>str</code>, default:                   <code>'csv'</code> )           \u2013            <p>The type of the mapping file ('csv' or 'json'). Defaults to 'csv'.</p>"},{"location":"reference/ts_shape/context/value_mapping/#ts_shape.context.value_mapping.ValueMapper(sep)","title":"<code>sep</code>","text":"(<code>str</code>, default:                   <code>','</code> )           \u2013            <p>The separator for CSV files. Defaults to ','.</p>"},{"location":"reference/ts_shape/context/value_mapping/#ts_shape.context.value_mapping.ValueMapper(encoding)","title":"<code>encoding</code>","text":"(<code>str</code>, default:                   <code>'utf-8'</code> )           \u2013            <p>The encoding to use for reading the file. Defaults to 'utf-8'.</p>"},{"location":"reference/ts_shape/context/value_mapping/#ts_shape.context.value_mapping.ValueMapper(column_name)","title":"<code>column_name</code>","text":"(<code>str</code>, default:                   <code>'systime'</code> )           \u2013            <p>The name of the column to sort the DataFrame by in the base class. Defaults to 'systime'.</p>"},{"location":"reference/ts_shape/context/value_mapping/#ts_shape.context.value_mapping.ValueMapper.get_dataframe","title":"get_dataframe","text":"<pre><code>get_dataframe() -&gt; DataFrame\n</code></pre> <p>Returns the processed DataFrame.</p> Source code in <code>src/ts_shape/utils/base.py</code> <pre><code>def get_dataframe(self) -&gt; pd.DataFrame:\n    \"\"\"Returns the processed DataFrame.\"\"\"\n    return self.dataframe\n</code></pre>"},{"location":"reference/ts_shape/context/value_mapping/#ts_shape.context.value_mapping.ValueMapper.map_values","title":"map_values","text":"<pre><code>map_values() -&gt; DataFrame\n</code></pre> <p>Maps values in the specified DataFrame column based on the mapping table.</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>pd.DataFrame: A new DataFrame with the mapped values.</p> </li> </ul> Source code in <code>src/ts_shape/context/value_mapping.py</code> <pre><code>def map_values(self) -&gt; pd.DataFrame:\n    \"\"\"\n    Maps values in the specified DataFrame column based on the mapping table.\n\n    Returns:\n        pd.DataFrame: A new DataFrame with the mapped values.\n    \"\"\"\n    # Merge the mapping table with the DataFrame based on the map_column and mapping_key_column\n    mapped_df = self.dataframe.merge(\n        self.mapping_table[[self.mapping_key_column, self.mapping_value_column]],\n        left_on=self.map_column,\n        right_on=self.mapping_key_column,\n        how='left'\n    )\n\n    # Replace the original column with the mapped values\n    mapped_df[self.map_column] = mapped_df[self.mapping_value_column]\n\n    # Drop unnecessary columns\n    mapped_df = mapped_df.drop([self.mapping_key_column, self.mapping_value_column], axis=1)\n\n    return mapped_df\n</code></pre>"},{"location":"reference/ts_shape/events/__init__/","title":"init","text":""},{"location":"reference/ts_shape/events/__init__/#ts_shape.events","title":"ts_shape.events","text":"<p>Events</p> <p>Extract events from shaped timeseries across quality, maintenance, and production domains.</p> <ul> <li>OutlierDetectionEvents: Detect and group outlier events in a time series.</li> <li>detect_outliers_zscore: Detect outliers using Z-score thresholding and group nearby points.</li> <li> <p>detect_outliers_iqr: Detect outliers using IQR bounds and group nearby points.</p> </li> <li> <p>StatisticalProcessControlRuleBased: Apply Western Electric rules to flag   control-limit violations on actual values using tolerance context.</p> </li> <li>calculate_control_limits: Compute mean and \u00b11/\u00b12/\u00b13 standard-deviation bands from tolerance rows.</li> <li> <p>process: Apply selected rules and emit event rows for violations.</p> </li> <li> <p>ToleranceDeviationEvents: Flag intervals where actual values cross configured   tolerance and group them into start/end events.</p> </li> <li>process_and_group_data_with_events: Build grouped deviation events with event UUIDs.</li> </ul> <p>Modules:</p> <ul> <li> <code>engineering</code>           \u2013            <p>Engineering Events</p> </li> <li> <code>maintenance</code>           \u2013            <p>Maintenance Events</p> </li> <li> <code>production</code>           \u2013            <p>Production Events</p> </li> <li> <code>quality</code>           \u2013            <p>Quality Events</p> </li> <li> <code>supplychain</code>           \u2013            <p>Supply Chain Events</p> </li> </ul>"},{"location":"reference/ts_shape/events/engineering/__init__/","title":"init","text":""},{"location":"reference/ts_shape/events/engineering/__init__/#ts_shape.events.engineering","title":"ts_shape.events.engineering","text":"<p>Engineering Events</p> <p>Detectors for engineering-related patterns over shaped timeseries.</p> <ul> <li>SetpointChangeEvents: Detect setpoint changes and compute response KPIs.</li> <li>detect_setpoint_steps: Point events where |\u0394setpoint| \u2265 min_delta and holds for min_hold.</li> <li>detect_setpoint_ramps: Intervals where |dS/dt| \u2265 min_rate for at least min_duration.</li> <li>detect_setpoint_changes: Unified table of steps and ramps with standardized columns.</li> <li>time_to_settle: Time until |actual \u2212 setpoint| \u2264 tol for a hold duration within a window.</li> <li> <p>overshoot_metrics: Peak overshoot magnitude/percent and time-to-peak after a change.</p> </li> <li> <p>StartupDetectionEvents: Detect startup intervals from thresholds or slope.</p> </li> <li>detect_startup_by_threshold: Rising threshold crossing with minimum dwell above threshold.</li> <li>detect_startup_by_slope: Intervals with sustained positive slope \u2265 min_slope for min_duration.</li> </ul> <p>Modules:</p> <ul> <li> <code>setpoint_events</code>           \u2013            </li> <li> <code>startup_events</code>           \u2013            </li> </ul> <p>Classes:</p> <ul> <li> <code>SetpointChangeEvents</code>           \u2013            <p>Detect step/ramp changes on a setpoint signal and compute follow-up KPIs</p> </li> <li> <code>StartupDetectionEvents</code>           \u2013            <p>Detect equipment startup intervals based on threshold crossings or</p> </li> </ul>"},{"location":"reference/ts_shape/events/engineering/__init__/#ts_shape.events.engineering.SetpointChangeEvents","title":"SetpointChangeEvents","text":"<pre><code>SetpointChangeEvents(dataframe: DataFrame, setpoint_uuid: str, *, event_uuid: str = 'setpoint_change_event', value_column: str = 'value_double', time_column: str = 'systime')\n</code></pre> <p>               Bases: <code>Base</code></p> <p>Detect step/ramp changes on a setpoint signal and compute follow-up KPIs like time-to-settle and overshoot based on an actual (process) value.</p> <p>Schema assumptions (columns): - uuid, sequence_number, systime, plctime, is_delta - value_integer, value_string, value_double, value_bool, value_bytes</p> <p>Methods:</p> <ul> <li> <code>detect_setpoint_changes</code>             \u2013              <p>Unified setpoint change table (steps + ramps) with standardized columns.</p> </li> <li> <code>detect_setpoint_ramps</code>             \u2013              <p>Interval events where |dS/dt| &gt;= min_rate for at least <code>min_duration</code>.</p> </li> <li> <code>detect_setpoint_steps</code>             \u2013              <p>Point events at times where the setpoint changes by &gt;= min_delta and the</p> </li> <li> <code>get_dataframe</code>             \u2013              <p>Returns the processed DataFrame.</p> </li> <li> <code>overshoot_metrics</code>             \u2013              <p>For each change, compute peak overshoot relative to the new setpoint</p> </li> <li> <code>time_to_settle</code>             \u2013              <p>For each setpoint change (any change), compute time until the actual signal</p> </li> </ul> Source code in <code>src/ts_shape/events/engineering/setpoint_events.py</code> <pre><code>def __init__(\n    self,\n    dataframe: pd.DataFrame,\n    setpoint_uuid: str,\n    *,\n    event_uuid: str = \"setpoint_change_event\",\n    value_column: str = \"value_double\",\n    time_column: str = \"systime\",\n) -&gt; None:\n    super().__init__(dataframe, column_name=time_column)\n    self.setpoint_uuid = setpoint_uuid\n    self.event_uuid = event_uuid\n    self.value_column = value_column\n    self.time_column = time_column\n\n    # isolate setpoint series and ensure proper dtypes/sort\n    self.sp = (\n        self.dataframe[self.dataframe[\"uuid\"] == self.setpoint_uuid]\n        .copy()\n        .sort_values(self.time_column)\n    )\n    self.sp[self.time_column] = pd.to_datetime(self.sp[self.time_column])\n</code></pre>"},{"location":"reference/ts_shape/events/engineering/__init__/#ts_shape.events.engineering.SetpointChangeEvents.detect_setpoint_changes","title":"detect_setpoint_changes","text":"<pre><code>detect_setpoint_changes(*, min_delta: float = 0.0, min_rate: Optional[float] = None, min_hold: str = '0s', min_duration: str = '0s') -&gt; DataFrame\n</code></pre> <p>Unified setpoint change table (steps + ramps) with standardized columns.</p> Source code in <code>src/ts_shape/events/engineering/setpoint_events.py</code> <pre><code>def detect_setpoint_changes(\n    self,\n    *,\n    min_delta: float = 0.0,\n    min_rate: Optional[float] = None,\n    min_hold: str = \"0s\",\n    min_duration: str = \"0s\",\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Unified setpoint change table (steps + ramps) with standardized columns.\n    \"\"\"\n    steps = self.detect_setpoint_steps(min_delta=min_delta, min_hold=min_hold)\n    ramps = (\n        self.detect_setpoint_ramps(min_rate=min_rate, min_duration=min_duration)\n        if min_rate is not None\n        else pd.DataFrame(columns=[\"start\", \"end\", \"uuid\", \"is_delta\", \"change_type\", \"avg_rate\", \"delta\"])\n    )\n    # ensure uniform columns\n    if not steps.empty:\n        steps = steps.assign(avg_rate=None, delta=None)[\n            [\n                \"start\",\n                \"end\",\n                \"uuid\",\n                \"is_delta\",\n                \"change_type\",\n                \"magnitude\",\n                \"prev_level\",\n                \"new_level\",\n                \"avg_rate\",\n                \"delta\",\n            ]\n        ]\n    if not ramps.empty:\n        ramps = ramps.assign(magnitude=None, prev_level=None, new_level=None)[\n            [\n                \"start\",\n                \"end\",\n                \"uuid\",\n                \"is_delta\",\n                \"change_type\",\n                \"magnitude\",\n                \"prev_level\",\n                \"new_level\",\n                \"avg_rate\",\n                \"delta\",\n            ]\n        ]\n    frames = [df for df in (steps, ramps) if not df.empty]\n    combined = pd.concat(frames, ignore_index=True) if frames else pd.DataFrame(\n        columns=[\n            \"start\",\n            \"end\",\n            \"uuid\",\n            \"is_delta\",\n            \"change_type\",\n            \"magnitude\",\n            \"prev_level\",\n            \"new_level\",\n            \"avg_rate\",\n            \"delta\",\n        ]\n    )\n    return combined.sort_values([\"start\", \"end\"]) if not combined.empty else combined\n</code></pre>"},{"location":"reference/ts_shape/events/engineering/__init__/#ts_shape.events.engineering.SetpointChangeEvents.detect_setpoint_ramps","title":"detect_setpoint_ramps","text":"<pre><code>detect_setpoint_ramps(min_rate: float, min_duration: str = '0s') -&gt; DataFrame\n</code></pre> <p>Interval events where |dS/dt| &gt;= min_rate for at least <code>min_duration</code>.</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>DataFrame with columns: start, end, uuid, is_delta, change_type='ramp',</p> </li> <li> <code>DataFrame</code>           \u2013            <p>avg_rate, delta.</p> </li> </ul> Source code in <code>src/ts_shape/events/engineering/setpoint_events.py</code> <pre><code>def detect_setpoint_ramps(self, min_rate: float, min_duration: str = \"0s\") -&gt; pd.DataFrame:\n    \"\"\"\n    Interval events where |dS/dt| &gt;= min_rate for at least `min_duration`.\n\n    Returns:\n        DataFrame with columns: start, end, uuid, is_delta, change_type='ramp',\n        avg_rate, delta.\n    \"\"\"\n    if self.sp.empty:\n        return pd.DataFrame(\n            columns=[\"start\", \"end\", \"uuid\", \"is_delta\", \"change_type\", \"avg_rate\", \"delta\"]\n        )\n\n    sp = self.sp[[self.time_column, self.value_column]].copy()\n    sp[\"dt_s\"] = sp[self.time_column].diff().dt.total_seconds()\n    sp[\"dv\"] = sp[self.value_column].diff()\n    sp[\"rate\"] = sp[\"dv\"] / sp[\"dt_s\"]\n    rate_mask = sp[\"rate\"].abs() &gt;= float(min_rate)\n\n    # group contiguous True segments\n    group_id = (rate_mask != rate_mask.shift()).cumsum()\n    events: List[Dict[str, Any]] = []\n    min_d = pd.to_timedelta(min_duration)\n    for gid, seg in sp.groupby(group_id):\n        seg_mask_true = rate_mask.loc[seg.index]\n        if not seg_mask_true.any():\n            continue\n        # boundaries\n        start_time = seg.loc[seg_mask_true, self.time_column].iloc[0]\n        end_time = seg.loc[seg_mask_true, self.time_column].iloc[-1]\n        if (end_time - start_time) &lt; min_d:\n            continue\n        avg_rate = seg.loc[seg_mask_true, \"rate\"].mean()\n        delta = seg.loc[seg_mask_true, \"dv\"].sum()\n        events.append(\n            {\n                \"start\": start_time,\n                \"end\": end_time,\n                \"uuid\": self.event_uuid,\n                \"is_delta\": True,\n                \"change_type\": \"ramp\",\n                \"avg_rate\": float(avg_rate) if pd.notna(avg_rate) else None,\n                \"delta\": float(delta) if pd.notna(delta) else None,\n            }\n        )\n\n    return pd.DataFrame(events)\n</code></pre>"},{"location":"reference/ts_shape/events/engineering/__init__/#ts_shape.events.engineering.SetpointChangeEvents.detect_setpoint_steps","title":"detect_setpoint_steps","text":"<pre><code>detect_setpoint_steps(min_delta: float, min_hold: str = '0s') -&gt; DataFrame\n</code></pre> <p>Point events at times where the setpoint changes by &gt;= min_delta and the new level holds for at least <code>min_hold</code> (no subsequent change within that time).</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>DataFrame with columns: start, end (== start), uuid, is_delta,</p> </li> <li> <code>DataFrame</code>           \u2013            <p>change_type='step', magnitude, prev_level, new_level.</p> </li> </ul> Source code in <code>src/ts_shape/events/engineering/setpoint_events.py</code> <pre><code>def detect_setpoint_steps(self, min_delta: float, min_hold: str = \"0s\") -&gt; pd.DataFrame:\n    \"\"\"\n    Point events at times where the setpoint changes by &gt;= min_delta and the\n    new level holds for at least `min_hold` (no subsequent change within that time).\n\n    Returns:\n        DataFrame with columns: start, end (== start), uuid, is_delta,\n        change_type='step', magnitude, prev_level, new_level.\n    \"\"\"\n    if self.sp.empty:\n        return pd.DataFrame(\n            columns=[\n                \"start\",\n                \"end\",\n                \"uuid\",\n                \"is_delta\",\n                \"change_type\",\n                \"magnitude\",\n                \"prev_level\",\n                \"new_level\",\n            ]\n        )\n\n    sp = self.sp[[self.time_column, self.value_column]].copy()\n    sp[\"prev\"] = sp[self.value_column].shift(1)\n    sp[\"delta\"] = sp[self.value_column] - sp[\"prev\"]\n    change_mask = sp[\"delta\"].abs() &gt;= float(min_delta)\n\n    # hold condition: next change must be after min_hold\n    change_times = sp.loc[change_mask, self.time_column]\n    min_hold_td = pd.to_timedelta(min_hold)\n    next_change_times = change_times.shift(-1)\n    hold_ok = (next_change_times - change_times &gt;= min_hold_td) | next_change_times.isna()\n    valid_change_times = change_times[hold_ok]\n\n    rows: List[Dict[str, Any]] = []\n    for t in valid_change_times:\n        row = sp.loc[sp[self.time_column] == t].iloc[0]\n        rows.append(\n            {\n                \"start\": t,\n                \"end\": t,\n                \"uuid\": self.event_uuid,\n                \"is_delta\": True,\n                \"change_type\": \"step\",\n                \"magnitude\": float(row[\"delta\"]),\n                \"prev_level\": float(row[\"prev\"]) if pd.notna(row[\"prev\"]) else None,\n                \"new_level\": float(row[self.value_column]),\n            }\n        )\n\n    return pd.DataFrame(rows)\n</code></pre>"},{"location":"reference/ts_shape/events/engineering/__init__/#ts_shape.events.engineering.SetpointChangeEvents.get_dataframe","title":"get_dataframe","text":"<pre><code>get_dataframe() -&gt; DataFrame\n</code></pre> <p>Returns the processed DataFrame.</p> Source code in <code>src/ts_shape/utils/base.py</code> <pre><code>def get_dataframe(self) -&gt; pd.DataFrame:\n    \"\"\"Returns the processed DataFrame.\"\"\"\n    return self.dataframe\n</code></pre>"},{"location":"reference/ts_shape/events/engineering/__init__/#ts_shape.events.engineering.SetpointChangeEvents.overshoot_metrics","title":"overshoot_metrics","text":"<pre><code>overshoot_metrics(actual_uuid: str, *, window: str = '10m') -&gt; DataFrame\n</code></pre> <p>For each change, compute peak overshoot relative to the new setpoint within a lookahead window.</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>DataFrame with columns: start, uuid, is_delta, overshoot_abs,</p> </li> <li> <code>DataFrame</code>           \u2013            <p>overshoot_pct, t_peak_seconds.</p> </li> </ul> Source code in <code>src/ts_shape/events/engineering/setpoint_events.py</code> <pre><code>def overshoot_metrics(\n    self,\n    actual_uuid: str,\n    *,\n    window: str = \"10m\",\n) -&gt; pd.DataFrame:\n    \"\"\"\n    For each change, compute peak overshoot relative to the new setpoint\n    within a lookahead window.\n\n    Returns:\n        DataFrame with columns: start, uuid, is_delta, overshoot_abs,\n        overshoot_pct, t_peak_seconds.\n    \"\"\"\n    if self.sp.empty:\n        return pd.DataFrame(columns=[\"start\", \"uuid\", \"is_delta\", \"overshoot_abs\", \"overshoot_pct\", \"t_peak_seconds\"])\n\n    actual = (\n        self.dataframe[self.dataframe[\"uuid\"] == actual_uuid]\n        .copy()\n        .sort_values(self.time_column)\n    )\n    actual[self.time_column] = pd.to_datetime(actual[self.time_column])\n    look_td = pd.to_timedelta(window)\n\n    sp = self.sp[[self.time_column, self.value_column]].copy()\n    sp[\"prev\"] = sp[self.value_column].shift(1)\n    sp[\"delta\"] = sp[self.value_column] - sp[\"prev\"]\n    changes = sp.loc[sp[\"delta\"].abs() &gt; 0, [self.time_column, self.value_column, \"delta\"]]\n\n    out_rows: List[Dict[str, Any]] = []\n    for _, r in changes.iterrows():\n        t0 = r[self.time_column]\n        s_new = float(r[self.value_column])\n        delta = float(r[\"delta\"]) if pd.notna(r[\"delta\"]) else 0.0\n        win = actual[(actual[self.time_column] &gt;= t0) &amp; (actual[self.time_column] &lt;= t0 + look_td)]\n        if win.empty:\n            out_rows.append(\n                {\n                    \"start\": t0,\n                    \"uuid\": self.event_uuid,\n                    \"is_delta\": True,\n                    \"overshoot_abs\": None,\n                    \"overshoot_pct\": None,\n                    \"t_peak_seconds\": None,\n                }\n            )\n            continue\n        err = win[self.value_column] - s_new\n        if delta &gt;= 0:\n            peak = err.max()\n            t_peak = win.loc[err.idxmax(), self.time_column]\n        else:\n            peak = -err.min()  # magnitude for downward step\n            t_peak = win.loc[err.idxmin(), self.time_column]\n        overshoot_abs = float(peak) if pd.notna(peak) else None\n        overshoot_pct = (overshoot_abs / abs(delta)) if (delta != 0 and overshoot_abs is not None) else None\n        out_rows.append(\n            {\n                \"start\": t0,\n                \"uuid\": self.event_uuid,\n                \"is_delta\": True,\n                \"overshoot_abs\": overshoot_abs,\n                \"overshoot_pct\": float(overshoot_pct) if overshoot_pct is not None else None,\n                \"t_peak_seconds\": (t_peak - t0).total_seconds() if pd.notna(t_peak) else None,\n            }\n        )\n\n    return pd.DataFrame(out_rows)\n</code></pre>"},{"location":"reference/ts_shape/events/engineering/__init__/#ts_shape.events.engineering.SetpointChangeEvents.time_to_settle","title":"time_to_settle","text":"<pre><code>time_to_settle(actual_uuid: str, *, tol: float, hold: str = '0s', lookahead: str = '10m') -&gt; DataFrame\n</code></pre> <p>For each setpoint change (any change), compute time until the actual signal is within \u00b1<code>tol</code> of the new setpoint for a continuous duration of <code>hold</code>.</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>DataFrame with columns: start, uuid, is_delta, t_settle_seconds, settled.</p> </li> </ul> Source code in <code>src/ts_shape/events/engineering/setpoint_events.py</code> <pre><code>def time_to_settle(\n    self,\n    actual_uuid: str,\n    *,\n    tol: float,\n    hold: str = \"0s\",\n    lookahead: str = \"10m\",\n) -&gt; pd.DataFrame:\n    \"\"\"\n    For each setpoint change (any change), compute time until the actual signal\n    is within \u00b1`tol` of the new setpoint for a continuous duration of `hold`.\n\n    Returns:\n        DataFrame with columns: start, uuid, is_delta, t_settle_seconds, settled.\n    \"\"\"\n    if self.sp.empty:\n        return pd.DataFrame(columns=[\"start\", \"uuid\", \"is_delta\", \"t_settle_seconds\", \"settled\"])\n\n    actual = (\n        self.dataframe[self.dataframe[\"uuid\"] == actual_uuid]\n        .copy()\n        .sort_values(self.time_column)\n    )\n    actual[self.time_column] = pd.to_datetime(actual[self.time_column])\n    hold_td = pd.to_timedelta(hold)\n    look_td = pd.to_timedelta(lookahead)\n\n    # change instants\n    sp = self.sp[[self.time_column, self.value_column]].copy()\n    sp[\"prev\"] = sp[self.value_column].shift(1)\n    sp[\"delta\"] = sp[self.value_column] - sp[\"prev\"]\n    change_times = sp.loc[sp[\"delta\"].abs() &gt; 0, [self.time_column, self.value_column]].reset_index(drop=True)\n\n    rows: List[Dict[str, Any]] = []\n    for _, c in change_times.iterrows():\n        t0 = c[self.time_column]\n        s_new = float(c[self.value_column])\n        window = actual[(actual[self.time_column] &gt;= t0) &amp; (actual[self.time_column] &lt;= t0 + look_td)]\n        if window.empty:\n            rows.append({\"start\": t0, \"uuid\": self.event_uuid, \"is_delta\": True, \"t_settle_seconds\": None, \"settled\": False})\n            continue\n        err = (window[self.value_column] - s_new).abs()\n        inside = err &lt;= tol\n\n        # time to first entry within tolerance (ignores hold)\n        if inside.any():\n            first_idx = inside[inside].index[0]\n            t_enter = window.loc[first_idx, self.time_column]\n        else:\n            t_enter = None\n\n        # determine if any contiguous inside segment satisfies hold duration\n        settled = False\n        if inside.any():\n            gid = (inside.ne(inside.shift())).cumsum()\n            for _, seg in window.groupby(gid):\n                seg_inside = inside.loc[seg.index]\n                if not seg_inside.iloc[0]:\n                    continue\n                start_seg = seg[self.time_column].iloc[0]\n                end_seg = seg[self.time_column].iloc[-1]\n                if (end_seg - start_seg) &gt;= hold_td:\n                    settled = True\n                    break\n\n        rows.append(\n            {\n                \"start\": t0,\n                \"uuid\": self.event_uuid,\n                \"is_delta\": True,\n                \"t_settle_seconds\": (t_enter - t0).total_seconds() if t_enter is not None else None,\n                \"settled\": bool(settled),\n            }\n        )\n\n    return pd.DataFrame(rows)\n</code></pre>"},{"location":"reference/ts_shape/events/engineering/__init__/#ts_shape.events.engineering.StartupDetectionEvents","title":"StartupDetectionEvents","text":"<pre><code>StartupDetectionEvents(dataframe: DataFrame, target_uuid: str, *, event_uuid: str = 'startup_event', value_column: str = 'value_double', time_column: str = 'systime')\n</code></pre> <p>               Bases: <code>Base</code></p> <p>Detect equipment startup intervals based on threshold crossings or sustained positive slope in a numeric metric (speed, temperature, etc.).</p> <p>Schema assumptions (columns): - uuid, sequence_number, systime, plctime, is_delta - value_integer, value_string, value_double, value_bool, value_bytes</p> <p>Methods:</p> <ul> <li> <code>detect_startup_by_slope</code>             \u2013              <p>Startup intervals where per-second slope &gt;= <code>min_slope</code> for at least</p> </li> <li> <code>detect_startup_by_threshold</code>             \u2013              <p>Startup begins at first crossing above <code>threshold</code> (or hysteresis enter)</p> </li> <li> <code>get_dataframe</code>             \u2013              <p>Returns the processed DataFrame.</p> </li> </ul> Source code in <code>src/ts_shape/events/engineering/startup_events.py</code> <pre><code>def __init__(\n    self,\n    dataframe: pd.DataFrame,\n    target_uuid: str,\n    *,\n    event_uuid: str = \"startup_event\",\n    value_column: str = \"value_double\",\n    time_column: str = \"systime\",\n) -&gt; None:\n    super().__init__(dataframe, column_name=time_column)\n    self.target_uuid = target_uuid\n    self.event_uuid = event_uuid\n    self.value_column = value_column\n    self.time_column = time_column\n\n    self.series = (\n        self.dataframe[self.dataframe[\"uuid\"] == self.target_uuid]\n        .copy()\n        .sort_values(self.time_column)\n    )\n    self.series[self.time_column] = pd.to_datetime(self.series[self.time_column])\n</code></pre>"},{"location":"reference/ts_shape/events/engineering/__init__/#ts_shape.events.engineering.StartupDetectionEvents.detect_startup_by_slope","title":"detect_startup_by_slope","text":"<pre><code>detect_startup_by_slope(*, min_slope: float, slope_window: str = '0s', min_duration: str = '0s') -&gt; DataFrame\n</code></pre> <p>Startup intervals where per-second slope &gt;= <code>min_slope</code> for at least <code>min_duration</code>. <code>slope_window</code> is accepted for API completeness but the current implementation uses instantaneous slope between samples.</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>DataFrame with columns: start, end, uuid, is_delta, method, min_slope, avg_slope.</p> </li> </ul> Source code in <code>src/ts_shape/events/engineering/startup_events.py</code> <pre><code>def detect_startup_by_slope(\n    self,\n    *,\n    min_slope: float,\n    slope_window: str = \"0s\",\n    min_duration: str = \"0s\",\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Startup intervals where per-second slope &gt;= `min_slope` for at least\n    `min_duration`. `slope_window` is accepted for API completeness but the\n    current implementation uses instantaneous slope between samples.\n\n    Returns:\n        DataFrame with columns: start, end, uuid, is_delta, method, min_slope, avg_slope.\n    \"\"\"\n    if self.series.empty:\n        return pd.DataFrame(columns=[\"start\", \"end\", \"uuid\", \"is_delta\", \"method\", \"min_slope\", \"avg_slope\"])\n\n    s = self.series[[self.time_column, self.value_column]].copy()\n    s[\"dt_s\"] = s[self.time_column].diff().dt.total_seconds()\n    s[\"dv\"] = s[self.value_column].diff()\n    s[\"slope\"] = s[\"dv\"] / s[\"dt_s\"]\n    mask = s[\"slope\"] &gt;= float(min_slope)\n\n    gid = (mask != mask.shift()).cumsum()\n    min_d = pd.to_timedelta(min_duration)\n    events: List[Dict[str, Any]] = []\n    for _, seg in s.groupby(gid):\n        seg_mask = mask.loc[seg.index]\n        if not seg_mask.any():\n            continue\n        start_t = seg.loc[seg_mask, self.time_column].iloc[0]\n        end_t = seg.loc[seg_mask, self.time_column].iloc[-1]\n        if (end_t - start_t) &lt; min_d:\n            continue\n        avg_slope = seg.loc[seg_mask, \"slope\"].mean()\n        events.append(\n            {\n                \"start\": start_t,\n                \"end\": end_t,\n                \"uuid\": self.event_uuid,\n                \"is_delta\": True,\n                \"method\": \"slope\",\n                \"min_slope\": float(min_slope),\n                \"avg_slope\": float(avg_slope) if pd.notna(avg_slope) else None,\n            }\n        )\n\n    return pd.DataFrame(events)\n</code></pre>"},{"location":"reference/ts_shape/events/engineering/__init__/#ts_shape.events.engineering.StartupDetectionEvents.detect_startup_by_threshold","title":"detect_startup_by_threshold","text":"<pre><code>detect_startup_by_threshold(*, threshold: float, hysteresis: tuple[float, float] | None = None, min_above: str = '0s') -&gt; DataFrame\n</code></pre> <p>Startup begins at first crossing above <code>threshold</code> (or hysteresis enter) and is valid only if the metric stays above the (exit) threshold for at least <code>min_above</code>.</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>DataFrame with columns: start, end, uuid, is_delta, method, threshold.</p> </li> </ul> Source code in <code>src/ts_shape/events/engineering/startup_events.py</code> <pre><code>def detect_startup_by_threshold(\n    self,\n    *,\n    threshold: float,\n    hysteresis: tuple[float, float] | None = None,\n    min_above: str = \"0s\",\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Startup begins at first crossing above `threshold` (or hysteresis enter)\n    and is valid only if the metric stays above the (exit) threshold for at\n    least `min_above`.\n\n    Returns:\n        DataFrame with columns: start, end, uuid, is_delta, method, threshold.\n    \"\"\"\n    if self.series.empty:\n        return pd.DataFrame(columns=[\"start\", \"end\", \"uuid\", \"is_delta\", \"method\", \"threshold\"])\n\n    enter_thr = threshold if hysteresis is None else hysteresis[0]\n    exit_thr = threshold if hysteresis is None else hysteresis[1]\n    min_above_td = pd.to_timedelta(min_above)\n\n    s = self.series[[self.time_column, self.value_column]].copy()\n    above_enter = s[self.value_column] &gt;= enter_thr\n    rising = (~above_enter.shift(fill_value=False)) &amp; above_enter\n    rise_times = s.loc[rising, self.time_column]\n\n    events: List[Dict[str, Any]] = []\n    for t0 in rise_times:\n        # ensure dwell above exit threshold for min_above\n        win = s[(s[self.time_column] &gt;= t0) &amp; (s[self.time_column] &lt;= t0 + min_above_td)]\n        if win.empty:\n            continue\n        if (win[self.value_column] &gt;= exit_thr).all():\n            events.append(\n                {\n                    \"start\": t0,\n                    \"end\": t0 + min_above_td,\n                    \"uuid\": self.event_uuid,\n                    \"is_delta\": True,\n                    \"method\": \"threshold\",\n                    \"threshold\": float(threshold),\n                }\n            )\n\n    return pd.DataFrame(events)\n</code></pre>"},{"location":"reference/ts_shape/events/engineering/__init__/#ts_shape.events.engineering.StartupDetectionEvents.get_dataframe","title":"get_dataframe","text":"<pre><code>get_dataframe() -&gt; DataFrame\n</code></pre> <p>Returns the processed DataFrame.</p> Source code in <code>src/ts_shape/utils/base.py</code> <pre><code>def get_dataframe(self) -&gt; pd.DataFrame:\n    \"\"\"Returns the processed DataFrame.\"\"\"\n    return self.dataframe\n</code></pre>"},{"location":"reference/ts_shape/events/engineering/setpoint_events/","title":"setpoint_events","text":""},{"location":"reference/ts_shape/events/engineering/setpoint_events/#ts_shape.events.engineering.setpoint_events","title":"ts_shape.events.engineering.setpoint_events","text":"<p>Classes:</p> <ul> <li> <code>SetpointChangeEvents</code>           \u2013            <p>Detect step/ramp changes on a setpoint signal and compute follow-up KPIs</p> </li> </ul>"},{"location":"reference/ts_shape/events/engineering/setpoint_events/#ts_shape.events.engineering.setpoint_events.SetpointChangeEvents","title":"SetpointChangeEvents","text":"<pre><code>SetpointChangeEvents(dataframe: DataFrame, setpoint_uuid: str, *, event_uuid: str = 'setpoint_change_event', value_column: str = 'value_double', time_column: str = 'systime')\n</code></pre> <p>               Bases: <code>Base</code></p> <p>Detect step/ramp changes on a setpoint signal and compute follow-up KPIs like time-to-settle and overshoot based on an actual (process) value.</p> <p>Schema assumptions (columns): - uuid, sequence_number, systime, plctime, is_delta - value_integer, value_string, value_double, value_bool, value_bytes</p> <p>Methods:</p> <ul> <li> <code>detect_setpoint_changes</code>             \u2013              <p>Unified setpoint change table (steps + ramps) with standardized columns.</p> </li> <li> <code>detect_setpoint_ramps</code>             \u2013              <p>Interval events where |dS/dt| &gt;= min_rate for at least <code>min_duration</code>.</p> </li> <li> <code>detect_setpoint_steps</code>             \u2013              <p>Point events at times where the setpoint changes by &gt;= min_delta and the</p> </li> <li> <code>get_dataframe</code>             \u2013              <p>Returns the processed DataFrame.</p> </li> <li> <code>overshoot_metrics</code>             \u2013              <p>For each change, compute peak overshoot relative to the new setpoint</p> </li> <li> <code>time_to_settle</code>             \u2013              <p>For each setpoint change (any change), compute time until the actual signal</p> </li> </ul> Source code in <code>src/ts_shape/events/engineering/setpoint_events.py</code> <pre><code>def __init__(\n    self,\n    dataframe: pd.DataFrame,\n    setpoint_uuid: str,\n    *,\n    event_uuid: str = \"setpoint_change_event\",\n    value_column: str = \"value_double\",\n    time_column: str = \"systime\",\n) -&gt; None:\n    super().__init__(dataframe, column_name=time_column)\n    self.setpoint_uuid = setpoint_uuid\n    self.event_uuid = event_uuid\n    self.value_column = value_column\n    self.time_column = time_column\n\n    # isolate setpoint series and ensure proper dtypes/sort\n    self.sp = (\n        self.dataframe[self.dataframe[\"uuid\"] == self.setpoint_uuid]\n        .copy()\n        .sort_values(self.time_column)\n    )\n    self.sp[self.time_column] = pd.to_datetime(self.sp[self.time_column])\n</code></pre>"},{"location":"reference/ts_shape/events/engineering/setpoint_events/#ts_shape.events.engineering.setpoint_events.SetpointChangeEvents.detect_setpoint_changes","title":"detect_setpoint_changes","text":"<pre><code>detect_setpoint_changes(*, min_delta: float = 0.0, min_rate: Optional[float] = None, min_hold: str = '0s', min_duration: str = '0s') -&gt; DataFrame\n</code></pre> <p>Unified setpoint change table (steps + ramps) with standardized columns.</p> Source code in <code>src/ts_shape/events/engineering/setpoint_events.py</code> <pre><code>def detect_setpoint_changes(\n    self,\n    *,\n    min_delta: float = 0.0,\n    min_rate: Optional[float] = None,\n    min_hold: str = \"0s\",\n    min_duration: str = \"0s\",\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Unified setpoint change table (steps + ramps) with standardized columns.\n    \"\"\"\n    steps = self.detect_setpoint_steps(min_delta=min_delta, min_hold=min_hold)\n    ramps = (\n        self.detect_setpoint_ramps(min_rate=min_rate, min_duration=min_duration)\n        if min_rate is not None\n        else pd.DataFrame(columns=[\"start\", \"end\", \"uuid\", \"is_delta\", \"change_type\", \"avg_rate\", \"delta\"])\n    )\n    # ensure uniform columns\n    if not steps.empty:\n        steps = steps.assign(avg_rate=None, delta=None)[\n            [\n                \"start\",\n                \"end\",\n                \"uuid\",\n                \"is_delta\",\n                \"change_type\",\n                \"magnitude\",\n                \"prev_level\",\n                \"new_level\",\n                \"avg_rate\",\n                \"delta\",\n            ]\n        ]\n    if not ramps.empty:\n        ramps = ramps.assign(magnitude=None, prev_level=None, new_level=None)[\n            [\n                \"start\",\n                \"end\",\n                \"uuid\",\n                \"is_delta\",\n                \"change_type\",\n                \"magnitude\",\n                \"prev_level\",\n                \"new_level\",\n                \"avg_rate\",\n                \"delta\",\n            ]\n        ]\n    frames = [df for df in (steps, ramps) if not df.empty]\n    combined = pd.concat(frames, ignore_index=True) if frames else pd.DataFrame(\n        columns=[\n            \"start\",\n            \"end\",\n            \"uuid\",\n            \"is_delta\",\n            \"change_type\",\n            \"magnitude\",\n            \"prev_level\",\n            \"new_level\",\n            \"avg_rate\",\n            \"delta\",\n        ]\n    )\n    return combined.sort_values([\"start\", \"end\"]) if not combined.empty else combined\n</code></pre>"},{"location":"reference/ts_shape/events/engineering/setpoint_events/#ts_shape.events.engineering.setpoint_events.SetpointChangeEvents.detect_setpoint_ramps","title":"detect_setpoint_ramps","text":"<pre><code>detect_setpoint_ramps(min_rate: float, min_duration: str = '0s') -&gt; DataFrame\n</code></pre> <p>Interval events where |dS/dt| &gt;= min_rate for at least <code>min_duration</code>.</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>DataFrame with columns: start, end, uuid, is_delta, change_type='ramp',</p> </li> <li> <code>DataFrame</code>           \u2013            <p>avg_rate, delta.</p> </li> </ul> Source code in <code>src/ts_shape/events/engineering/setpoint_events.py</code> <pre><code>def detect_setpoint_ramps(self, min_rate: float, min_duration: str = \"0s\") -&gt; pd.DataFrame:\n    \"\"\"\n    Interval events where |dS/dt| &gt;= min_rate for at least `min_duration`.\n\n    Returns:\n        DataFrame with columns: start, end, uuid, is_delta, change_type='ramp',\n        avg_rate, delta.\n    \"\"\"\n    if self.sp.empty:\n        return pd.DataFrame(\n            columns=[\"start\", \"end\", \"uuid\", \"is_delta\", \"change_type\", \"avg_rate\", \"delta\"]\n        )\n\n    sp = self.sp[[self.time_column, self.value_column]].copy()\n    sp[\"dt_s\"] = sp[self.time_column].diff().dt.total_seconds()\n    sp[\"dv\"] = sp[self.value_column].diff()\n    sp[\"rate\"] = sp[\"dv\"] / sp[\"dt_s\"]\n    rate_mask = sp[\"rate\"].abs() &gt;= float(min_rate)\n\n    # group contiguous True segments\n    group_id = (rate_mask != rate_mask.shift()).cumsum()\n    events: List[Dict[str, Any]] = []\n    min_d = pd.to_timedelta(min_duration)\n    for gid, seg in sp.groupby(group_id):\n        seg_mask_true = rate_mask.loc[seg.index]\n        if not seg_mask_true.any():\n            continue\n        # boundaries\n        start_time = seg.loc[seg_mask_true, self.time_column].iloc[0]\n        end_time = seg.loc[seg_mask_true, self.time_column].iloc[-1]\n        if (end_time - start_time) &lt; min_d:\n            continue\n        avg_rate = seg.loc[seg_mask_true, \"rate\"].mean()\n        delta = seg.loc[seg_mask_true, \"dv\"].sum()\n        events.append(\n            {\n                \"start\": start_time,\n                \"end\": end_time,\n                \"uuid\": self.event_uuid,\n                \"is_delta\": True,\n                \"change_type\": \"ramp\",\n                \"avg_rate\": float(avg_rate) if pd.notna(avg_rate) else None,\n                \"delta\": float(delta) if pd.notna(delta) else None,\n            }\n        )\n\n    return pd.DataFrame(events)\n</code></pre>"},{"location":"reference/ts_shape/events/engineering/setpoint_events/#ts_shape.events.engineering.setpoint_events.SetpointChangeEvents.detect_setpoint_steps","title":"detect_setpoint_steps","text":"<pre><code>detect_setpoint_steps(min_delta: float, min_hold: str = '0s') -&gt; DataFrame\n</code></pre> <p>Point events at times where the setpoint changes by &gt;= min_delta and the new level holds for at least <code>min_hold</code> (no subsequent change within that time).</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>DataFrame with columns: start, end (== start), uuid, is_delta,</p> </li> <li> <code>DataFrame</code>           \u2013            <p>change_type='step', magnitude, prev_level, new_level.</p> </li> </ul> Source code in <code>src/ts_shape/events/engineering/setpoint_events.py</code> <pre><code>def detect_setpoint_steps(self, min_delta: float, min_hold: str = \"0s\") -&gt; pd.DataFrame:\n    \"\"\"\n    Point events at times where the setpoint changes by &gt;= min_delta and the\n    new level holds for at least `min_hold` (no subsequent change within that time).\n\n    Returns:\n        DataFrame with columns: start, end (== start), uuid, is_delta,\n        change_type='step', magnitude, prev_level, new_level.\n    \"\"\"\n    if self.sp.empty:\n        return pd.DataFrame(\n            columns=[\n                \"start\",\n                \"end\",\n                \"uuid\",\n                \"is_delta\",\n                \"change_type\",\n                \"magnitude\",\n                \"prev_level\",\n                \"new_level\",\n            ]\n        )\n\n    sp = self.sp[[self.time_column, self.value_column]].copy()\n    sp[\"prev\"] = sp[self.value_column].shift(1)\n    sp[\"delta\"] = sp[self.value_column] - sp[\"prev\"]\n    change_mask = sp[\"delta\"].abs() &gt;= float(min_delta)\n\n    # hold condition: next change must be after min_hold\n    change_times = sp.loc[change_mask, self.time_column]\n    min_hold_td = pd.to_timedelta(min_hold)\n    next_change_times = change_times.shift(-1)\n    hold_ok = (next_change_times - change_times &gt;= min_hold_td) | next_change_times.isna()\n    valid_change_times = change_times[hold_ok]\n\n    rows: List[Dict[str, Any]] = []\n    for t in valid_change_times:\n        row = sp.loc[sp[self.time_column] == t].iloc[0]\n        rows.append(\n            {\n                \"start\": t,\n                \"end\": t,\n                \"uuid\": self.event_uuid,\n                \"is_delta\": True,\n                \"change_type\": \"step\",\n                \"magnitude\": float(row[\"delta\"]),\n                \"prev_level\": float(row[\"prev\"]) if pd.notna(row[\"prev\"]) else None,\n                \"new_level\": float(row[self.value_column]),\n            }\n        )\n\n    return pd.DataFrame(rows)\n</code></pre>"},{"location":"reference/ts_shape/events/engineering/setpoint_events/#ts_shape.events.engineering.setpoint_events.SetpointChangeEvents.get_dataframe","title":"get_dataframe","text":"<pre><code>get_dataframe() -&gt; DataFrame\n</code></pre> <p>Returns the processed DataFrame.</p> Source code in <code>src/ts_shape/utils/base.py</code> <pre><code>def get_dataframe(self) -&gt; pd.DataFrame:\n    \"\"\"Returns the processed DataFrame.\"\"\"\n    return self.dataframe\n</code></pre>"},{"location":"reference/ts_shape/events/engineering/setpoint_events/#ts_shape.events.engineering.setpoint_events.SetpointChangeEvents.overshoot_metrics","title":"overshoot_metrics","text":"<pre><code>overshoot_metrics(actual_uuid: str, *, window: str = '10m') -&gt; DataFrame\n</code></pre> <p>For each change, compute peak overshoot relative to the new setpoint within a lookahead window.</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>DataFrame with columns: start, uuid, is_delta, overshoot_abs,</p> </li> <li> <code>DataFrame</code>           \u2013            <p>overshoot_pct, t_peak_seconds.</p> </li> </ul> Source code in <code>src/ts_shape/events/engineering/setpoint_events.py</code> <pre><code>def overshoot_metrics(\n    self,\n    actual_uuid: str,\n    *,\n    window: str = \"10m\",\n) -&gt; pd.DataFrame:\n    \"\"\"\n    For each change, compute peak overshoot relative to the new setpoint\n    within a lookahead window.\n\n    Returns:\n        DataFrame with columns: start, uuid, is_delta, overshoot_abs,\n        overshoot_pct, t_peak_seconds.\n    \"\"\"\n    if self.sp.empty:\n        return pd.DataFrame(columns=[\"start\", \"uuid\", \"is_delta\", \"overshoot_abs\", \"overshoot_pct\", \"t_peak_seconds\"])\n\n    actual = (\n        self.dataframe[self.dataframe[\"uuid\"] == actual_uuid]\n        .copy()\n        .sort_values(self.time_column)\n    )\n    actual[self.time_column] = pd.to_datetime(actual[self.time_column])\n    look_td = pd.to_timedelta(window)\n\n    sp = self.sp[[self.time_column, self.value_column]].copy()\n    sp[\"prev\"] = sp[self.value_column].shift(1)\n    sp[\"delta\"] = sp[self.value_column] - sp[\"prev\"]\n    changes = sp.loc[sp[\"delta\"].abs() &gt; 0, [self.time_column, self.value_column, \"delta\"]]\n\n    out_rows: List[Dict[str, Any]] = []\n    for _, r in changes.iterrows():\n        t0 = r[self.time_column]\n        s_new = float(r[self.value_column])\n        delta = float(r[\"delta\"]) if pd.notna(r[\"delta\"]) else 0.0\n        win = actual[(actual[self.time_column] &gt;= t0) &amp; (actual[self.time_column] &lt;= t0 + look_td)]\n        if win.empty:\n            out_rows.append(\n                {\n                    \"start\": t0,\n                    \"uuid\": self.event_uuid,\n                    \"is_delta\": True,\n                    \"overshoot_abs\": None,\n                    \"overshoot_pct\": None,\n                    \"t_peak_seconds\": None,\n                }\n            )\n            continue\n        err = win[self.value_column] - s_new\n        if delta &gt;= 0:\n            peak = err.max()\n            t_peak = win.loc[err.idxmax(), self.time_column]\n        else:\n            peak = -err.min()  # magnitude for downward step\n            t_peak = win.loc[err.idxmin(), self.time_column]\n        overshoot_abs = float(peak) if pd.notna(peak) else None\n        overshoot_pct = (overshoot_abs / abs(delta)) if (delta != 0 and overshoot_abs is not None) else None\n        out_rows.append(\n            {\n                \"start\": t0,\n                \"uuid\": self.event_uuid,\n                \"is_delta\": True,\n                \"overshoot_abs\": overshoot_abs,\n                \"overshoot_pct\": float(overshoot_pct) if overshoot_pct is not None else None,\n                \"t_peak_seconds\": (t_peak - t0).total_seconds() if pd.notna(t_peak) else None,\n            }\n        )\n\n    return pd.DataFrame(out_rows)\n</code></pre>"},{"location":"reference/ts_shape/events/engineering/setpoint_events/#ts_shape.events.engineering.setpoint_events.SetpointChangeEvents.time_to_settle","title":"time_to_settle","text":"<pre><code>time_to_settle(actual_uuid: str, *, tol: float, hold: str = '0s', lookahead: str = '10m') -&gt; DataFrame\n</code></pre> <p>For each setpoint change (any change), compute time until the actual signal is within \u00b1<code>tol</code> of the new setpoint for a continuous duration of <code>hold</code>.</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>DataFrame with columns: start, uuid, is_delta, t_settle_seconds, settled.</p> </li> </ul> Source code in <code>src/ts_shape/events/engineering/setpoint_events.py</code> <pre><code>def time_to_settle(\n    self,\n    actual_uuid: str,\n    *,\n    tol: float,\n    hold: str = \"0s\",\n    lookahead: str = \"10m\",\n) -&gt; pd.DataFrame:\n    \"\"\"\n    For each setpoint change (any change), compute time until the actual signal\n    is within \u00b1`tol` of the new setpoint for a continuous duration of `hold`.\n\n    Returns:\n        DataFrame with columns: start, uuid, is_delta, t_settle_seconds, settled.\n    \"\"\"\n    if self.sp.empty:\n        return pd.DataFrame(columns=[\"start\", \"uuid\", \"is_delta\", \"t_settle_seconds\", \"settled\"])\n\n    actual = (\n        self.dataframe[self.dataframe[\"uuid\"] == actual_uuid]\n        .copy()\n        .sort_values(self.time_column)\n    )\n    actual[self.time_column] = pd.to_datetime(actual[self.time_column])\n    hold_td = pd.to_timedelta(hold)\n    look_td = pd.to_timedelta(lookahead)\n\n    # change instants\n    sp = self.sp[[self.time_column, self.value_column]].copy()\n    sp[\"prev\"] = sp[self.value_column].shift(1)\n    sp[\"delta\"] = sp[self.value_column] - sp[\"prev\"]\n    change_times = sp.loc[sp[\"delta\"].abs() &gt; 0, [self.time_column, self.value_column]].reset_index(drop=True)\n\n    rows: List[Dict[str, Any]] = []\n    for _, c in change_times.iterrows():\n        t0 = c[self.time_column]\n        s_new = float(c[self.value_column])\n        window = actual[(actual[self.time_column] &gt;= t0) &amp; (actual[self.time_column] &lt;= t0 + look_td)]\n        if window.empty:\n            rows.append({\"start\": t0, \"uuid\": self.event_uuid, \"is_delta\": True, \"t_settle_seconds\": None, \"settled\": False})\n            continue\n        err = (window[self.value_column] - s_new).abs()\n        inside = err &lt;= tol\n\n        # time to first entry within tolerance (ignores hold)\n        if inside.any():\n            first_idx = inside[inside].index[0]\n            t_enter = window.loc[first_idx, self.time_column]\n        else:\n            t_enter = None\n\n        # determine if any contiguous inside segment satisfies hold duration\n        settled = False\n        if inside.any():\n            gid = (inside.ne(inside.shift())).cumsum()\n            for _, seg in window.groupby(gid):\n                seg_inside = inside.loc[seg.index]\n                if not seg_inside.iloc[0]:\n                    continue\n                start_seg = seg[self.time_column].iloc[0]\n                end_seg = seg[self.time_column].iloc[-1]\n                if (end_seg - start_seg) &gt;= hold_td:\n                    settled = True\n                    break\n\n        rows.append(\n            {\n                \"start\": t0,\n                \"uuid\": self.event_uuid,\n                \"is_delta\": True,\n                \"t_settle_seconds\": (t_enter - t0).total_seconds() if t_enter is not None else None,\n                \"settled\": bool(settled),\n            }\n        )\n\n    return pd.DataFrame(rows)\n</code></pre>"},{"location":"reference/ts_shape/events/engineering/startup_events/","title":"startup_events","text":""},{"location":"reference/ts_shape/events/engineering/startup_events/#ts_shape.events.engineering.startup_events","title":"ts_shape.events.engineering.startup_events","text":"<p>Classes:</p> <ul> <li> <code>StartupDetectionEvents</code>           \u2013            <p>Detect equipment startup intervals based on threshold crossings or</p> </li> </ul>"},{"location":"reference/ts_shape/events/engineering/startup_events/#ts_shape.events.engineering.startup_events.StartupDetectionEvents","title":"StartupDetectionEvents","text":"<pre><code>StartupDetectionEvents(dataframe: DataFrame, target_uuid: str, *, event_uuid: str = 'startup_event', value_column: str = 'value_double', time_column: str = 'systime')\n</code></pre> <p>               Bases: <code>Base</code></p> <p>Detect equipment startup intervals based on threshold crossings or sustained positive slope in a numeric metric (speed, temperature, etc.).</p> <p>Schema assumptions (columns): - uuid, sequence_number, systime, plctime, is_delta - value_integer, value_string, value_double, value_bool, value_bytes</p> <p>Methods:</p> <ul> <li> <code>detect_startup_by_slope</code>             \u2013              <p>Startup intervals where per-second slope &gt;= <code>min_slope</code> for at least</p> </li> <li> <code>detect_startup_by_threshold</code>             \u2013              <p>Startup begins at first crossing above <code>threshold</code> (or hysteresis enter)</p> </li> <li> <code>get_dataframe</code>             \u2013              <p>Returns the processed DataFrame.</p> </li> </ul> Source code in <code>src/ts_shape/events/engineering/startup_events.py</code> <pre><code>def __init__(\n    self,\n    dataframe: pd.DataFrame,\n    target_uuid: str,\n    *,\n    event_uuid: str = \"startup_event\",\n    value_column: str = \"value_double\",\n    time_column: str = \"systime\",\n) -&gt; None:\n    super().__init__(dataframe, column_name=time_column)\n    self.target_uuid = target_uuid\n    self.event_uuid = event_uuid\n    self.value_column = value_column\n    self.time_column = time_column\n\n    self.series = (\n        self.dataframe[self.dataframe[\"uuid\"] == self.target_uuid]\n        .copy()\n        .sort_values(self.time_column)\n    )\n    self.series[self.time_column] = pd.to_datetime(self.series[self.time_column])\n</code></pre>"},{"location":"reference/ts_shape/events/engineering/startup_events/#ts_shape.events.engineering.startup_events.StartupDetectionEvents.detect_startup_by_slope","title":"detect_startup_by_slope","text":"<pre><code>detect_startup_by_slope(*, min_slope: float, slope_window: str = '0s', min_duration: str = '0s') -&gt; DataFrame\n</code></pre> <p>Startup intervals where per-second slope &gt;= <code>min_slope</code> for at least <code>min_duration</code>. <code>slope_window</code> is accepted for API completeness but the current implementation uses instantaneous slope between samples.</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>DataFrame with columns: start, end, uuid, is_delta, method, min_slope, avg_slope.</p> </li> </ul> Source code in <code>src/ts_shape/events/engineering/startup_events.py</code> <pre><code>def detect_startup_by_slope(\n    self,\n    *,\n    min_slope: float,\n    slope_window: str = \"0s\",\n    min_duration: str = \"0s\",\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Startup intervals where per-second slope &gt;= `min_slope` for at least\n    `min_duration`. `slope_window` is accepted for API completeness but the\n    current implementation uses instantaneous slope between samples.\n\n    Returns:\n        DataFrame with columns: start, end, uuid, is_delta, method, min_slope, avg_slope.\n    \"\"\"\n    if self.series.empty:\n        return pd.DataFrame(columns=[\"start\", \"end\", \"uuid\", \"is_delta\", \"method\", \"min_slope\", \"avg_slope\"])\n\n    s = self.series[[self.time_column, self.value_column]].copy()\n    s[\"dt_s\"] = s[self.time_column].diff().dt.total_seconds()\n    s[\"dv\"] = s[self.value_column].diff()\n    s[\"slope\"] = s[\"dv\"] / s[\"dt_s\"]\n    mask = s[\"slope\"] &gt;= float(min_slope)\n\n    gid = (mask != mask.shift()).cumsum()\n    min_d = pd.to_timedelta(min_duration)\n    events: List[Dict[str, Any]] = []\n    for _, seg in s.groupby(gid):\n        seg_mask = mask.loc[seg.index]\n        if not seg_mask.any():\n            continue\n        start_t = seg.loc[seg_mask, self.time_column].iloc[0]\n        end_t = seg.loc[seg_mask, self.time_column].iloc[-1]\n        if (end_t - start_t) &lt; min_d:\n            continue\n        avg_slope = seg.loc[seg_mask, \"slope\"].mean()\n        events.append(\n            {\n                \"start\": start_t,\n                \"end\": end_t,\n                \"uuid\": self.event_uuid,\n                \"is_delta\": True,\n                \"method\": \"slope\",\n                \"min_slope\": float(min_slope),\n                \"avg_slope\": float(avg_slope) if pd.notna(avg_slope) else None,\n            }\n        )\n\n    return pd.DataFrame(events)\n</code></pre>"},{"location":"reference/ts_shape/events/engineering/startup_events/#ts_shape.events.engineering.startup_events.StartupDetectionEvents.detect_startup_by_threshold","title":"detect_startup_by_threshold","text":"<pre><code>detect_startup_by_threshold(*, threshold: float, hysteresis: tuple[float, float] | None = None, min_above: str = '0s') -&gt; DataFrame\n</code></pre> <p>Startup begins at first crossing above <code>threshold</code> (or hysteresis enter) and is valid only if the metric stays above the (exit) threshold for at least <code>min_above</code>.</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>DataFrame with columns: start, end, uuid, is_delta, method, threshold.</p> </li> </ul> Source code in <code>src/ts_shape/events/engineering/startup_events.py</code> <pre><code>def detect_startup_by_threshold(\n    self,\n    *,\n    threshold: float,\n    hysteresis: tuple[float, float] | None = None,\n    min_above: str = \"0s\",\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Startup begins at first crossing above `threshold` (or hysteresis enter)\n    and is valid only if the metric stays above the (exit) threshold for at\n    least `min_above`.\n\n    Returns:\n        DataFrame with columns: start, end, uuid, is_delta, method, threshold.\n    \"\"\"\n    if self.series.empty:\n        return pd.DataFrame(columns=[\"start\", \"end\", \"uuid\", \"is_delta\", \"method\", \"threshold\"])\n\n    enter_thr = threshold if hysteresis is None else hysteresis[0]\n    exit_thr = threshold if hysteresis is None else hysteresis[1]\n    min_above_td = pd.to_timedelta(min_above)\n\n    s = self.series[[self.time_column, self.value_column]].copy()\n    above_enter = s[self.value_column] &gt;= enter_thr\n    rising = (~above_enter.shift(fill_value=False)) &amp; above_enter\n    rise_times = s.loc[rising, self.time_column]\n\n    events: List[Dict[str, Any]] = []\n    for t0 in rise_times:\n        # ensure dwell above exit threshold for min_above\n        win = s[(s[self.time_column] &gt;= t0) &amp; (s[self.time_column] &lt;= t0 + min_above_td)]\n        if win.empty:\n            continue\n        if (win[self.value_column] &gt;= exit_thr).all():\n            events.append(\n                {\n                    \"start\": t0,\n                    \"end\": t0 + min_above_td,\n                    \"uuid\": self.event_uuid,\n                    \"is_delta\": True,\n                    \"method\": \"threshold\",\n                    \"threshold\": float(threshold),\n                }\n            )\n\n    return pd.DataFrame(events)\n</code></pre>"},{"location":"reference/ts_shape/events/engineering/startup_events/#ts_shape.events.engineering.startup_events.StartupDetectionEvents.get_dataframe","title":"get_dataframe","text":"<pre><code>get_dataframe() -&gt; DataFrame\n</code></pre> <p>Returns the processed DataFrame.</p> Source code in <code>src/ts_shape/utils/base.py</code> <pre><code>def get_dataframe(self) -&gt; pd.DataFrame:\n    \"\"\"Returns the processed DataFrame.\"\"\"\n    return self.dataframe\n</code></pre>"},{"location":"reference/ts_shape/events/maintenance/__init__/","title":"maintenance","text":""},{"location":"reference/ts_shape/events/maintenance/__init__/#ts_shape.events.maintenance","title":"ts_shape.events.maintenance","text":"<p>Maintenance Events</p> <p>Detectors for maintenance-related patterns (e.g., downtime windows).</p> <p>Classes: - None yet: Placeholder module for future maintenance event detectors.</p>"},{"location":"reference/ts_shape/events/production/__init__/","title":"init","text":""},{"location":"reference/ts_shape/events/production/__init__/#ts_shape.events.production","title":"ts_shape.events.production","text":"<p>Production Events</p> <p>Detectors for production events in long-form timeseries (uuid-per-signal).</p> <ul> <li>MachineStateEvents: Run/idle intervals and transition points from a boolean state signal.</li> <li>detect_run_idle: Intervalize run/idle with optional min duration.</li> <li> <p>transition_events: Point events on idle\u2192run and run\u2192idle changes.</p> </li> <li> <p>LineThroughputEvents: Throughput metrics and takt adherence.</p> </li> <li>count_parts: Parts per fixed window from a counter uuid.</li> <li> <p>takt_adherence: Cycle time violations vs. a takt time.</p> </li> <li> <p>ChangeoverEvents: Product/recipe changes and end-of-changeover derivation.</p> </li> <li>detect_changeover: Point events at product value changes.</li> <li> <p>changeover_window: End via fixed window or stable band metrics.</p> </li> <li> <p>FlowConstraintEvents: Blocked/starved intervals between upstream/downstream run signals.</p> </li> <li>blocked_events: Upstream running while downstream not consuming.</li> <li>starved_events: Downstream running while upstream not supplying.</li> </ul> <p>Modules:</p> <ul> <li> <code>changeover</code>           \u2013            </li> <li> <code>flow_constraints</code>           \u2013            </li> <li> <code>line_throughput</code>           \u2013            </li> <li> <code>machine_state</code>           \u2013            </li> </ul> <p>Classes:</p> <ul> <li> <code>ChangeoverEvents</code>           \u2013            <p>Production: Changeover</p> </li> <li> <code>FlowConstraintEvents</code>           \u2013            <p>Production: Flow Constraints</p> </li> <li> <code>LineThroughputEvents</code>           \u2013            <p>Production: Line Throughput</p> </li> <li> <code>MachineStateEvents</code>           \u2013            <p>Production: Machine State</p> </li> </ul>"},{"location":"reference/ts_shape/events/production/__init__/#ts_shape.events.production.ChangeoverEvents","title":"ChangeoverEvents","text":"<pre><code>ChangeoverEvents(dataframe: DataFrame, *, event_uuid: str = 'prod:changeover', time_column: str = 'systime')\n</code></pre> <p>               Bases: <code>Base</code></p> <p>Production: Changeover</p> <p>Detect product/recipe changes and compute changeover windows without requiring a dedicated 'first good' signal.</p> <p>Methods: - detect_changeover: point events when product/recipe changes. - changeover_window: derive an end time via fixed window or 'stable_band' metrics.</p> <p>Methods:</p> <ul> <li> <code>changeover_window</code>             \u2013              <p>Compute changeover windows per product change.</p> </li> <li> <code>detect_changeover</code>             \u2013              <p>Emit point events when the product/recipe changes value.</p> </li> <li> <code>get_dataframe</code>             \u2013              <p>Returns the processed DataFrame.</p> </li> </ul> Source code in <code>src/ts_shape/events/production/changeover.py</code> <pre><code>def __init__(\n    self,\n    dataframe: pd.DataFrame,\n    *,\n    event_uuid: str = \"prod:changeover\",\n    time_column: str = \"systime\",\n) -&gt; None:\n    super().__init__(dataframe, column_name=time_column)\n    self.event_uuid = event_uuid\n    self.time_column = time_column\n</code></pre>"},{"location":"reference/ts_shape/events/production/__init__/#ts_shape.events.production.ChangeoverEvents.changeover_window","title":"changeover_window","text":"<pre><code>changeover_window(product_uuid: str, *, value_column: str = 'value_string', start_time: Optional[Timestamp] = None, until: str = 'fixed_window', config: Optional[Dict[str, Any]] = None, fallback: Optional[Dict[str, Any]] = None) -&gt; DataFrame\n</code></pre> <p>Compute changeover windows per product change.</p> until <ul> <li>fixed_window: end = start + config['duration'] (e.g., '10m')</li> <li>stable_band: end when all metrics stabilize within band for hold:       config = {         'metrics': [           {'uuid': 'm1', 'value_column': 'value_double', 'band': 0.2, 'hold': '2m'},           ...         ]       }</li> </ul> <p>fallback: {'default_duration': '10m', 'completed': False}</p> Source code in <code>src/ts_shape/events/production/changeover.py</code> <pre><code>def changeover_window(\n    self,\n    product_uuid: str,\n    *,\n    value_column: str = \"value_string\",\n    start_time: Optional[pd.Timestamp] = None,\n    until: str = \"fixed_window\",\n    config: Optional[Dict[str, Any]] = None,\n    fallback: Optional[Dict[str, Any]] = None,\n) -&gt; pd.DataFrame:\n    \"\"\"Compute changeover windows per product change.\n\n    until:\n      - fixed_window: end = start + config['duration'] (e.g., '10m')\n      - stable_band: end when all metrics stabilize within band for hold:\n            config = {\n              'metrics': [\n                {'uuid': 'm1', 'value_column': 'value_double', 'band': 0.2, 'hold': '2m'},\n                ...\n              ]\n            }\n    fallback: {'default_duration': '10m', 'completed': False}\n    \"\"\"\n    config = config or {}\n    fallback = fallback or {\"default_duration\": \"10m\", \"completed\": False}\n\n    changes = self.detect_changeover(product_uuid, value_column=value_column, min_hold=config.get(\"min_hold\", \"0s\"))\n    if start_time is not None:\n        changes = changes[changes[\"systime\"] &gt;= pd.to_datetime(start_time)]\n    if changes.empty:\n        return pd.DataFrame(\n            columns=[\"start\", \"end\", \"uuid\", \"source_uuid\", \"is_delta\", \"method\", \"completed\"]\n        )\n\n    rows: List[Dict[str, Any]] = []\n    for _, r in changes.iterrows():\n        t0 = pd.to_datetime(r[\"systime\"])\n        if until == \"fixed_window\":\n            duration = pd.to_timedelta(config.get(\"duration\", \"10m\"))\n            end = t0 + duration\n            rows.append(\n                {\n                    \"start\": t0,\n                    \"end\": end,\n                    \"uuid\": self.event_uuid,\n                    \"source_uuid\": product_uuid,\n                    \"is_delta\": True,\n                    \"method\": \"fixed_window\",\n                    \"completed\": True,\n                }\n            )\n            continue\n\n        if until == \"stable_band\":\n            metric_defs = config.get(\"metrics\", [])\n            metric_ends: List[pd.Timestamp] = []\n            for mdef in metric_defs:\n                uid = mdef[\"uuid\"]\n                vcol = mdef.get(\"value_column\", \"value_double\")\n                band = float(mdef.get(\"band\", 0.0))\n                hold_td = pd.to_timedelta(mdef.get(\"hold\", \"0s\"))\n                s = (\n                    self.dataframe[self.dataframe[\"uuid\"] == uid]\n                    .copy()\n                    .sort_values(self.time_column)\n                )\n                s[self.time_column] = pd.to_datetime(s[self.time_column])\n                s = s[s[self.time_column] &gt;= t0]\n                if s.empty:\n                    continue\n                # Rolling median reference and band mask\n                # Use expanding median to be robust soon after change\n                ref = s[vcol].expanding(min_periods=3).median()\n                inside = (s[vcol] - ref).abs() &lt;= band\n                if not inside.any():\n                    continue\n                gid = (inside.ne(inside.shift())).cumsum()\n                end_found: Optional[pd.Timestamp] = None\n                for _, seg in s.groupby(gid):\n                    seg_inside = inside.loc[seg.index]\n                    if not seg_inside.iloc[0]:\n                        continue\n                    start_seg = seg[self.time_column].iloc[0]\n                    end_seg = seg[self.time_column].iloc[-1]\n                    if (end_seg - start_seg) &gt;= hold_td:\n                        end_found = start_seg\n                        break\n                if end_found is not None:\n                    metric_ends.append(end_found)\n            if metric_defs and len(metric_ends) == len(metric_defs):\n                end = max(metric_ends)\n                rows.append(\n                    {\n                        \"start\": t0,\n                        \"end\": end,\n                        \"uuid\": self.event_uuid,\n                        \"source_uuid\": product_uuid,\n                        \"is_delta\": True,\n                        \"method\": \"stable_band\",\n                        \"completed\": True,\n                    }\n                )\n                continue\n\n        # fallback\n        end = t0 + pd.to_timedelta(fallback.get(\"default_duration\", \"10m\"))\n        rows.append(\n            {\n                \"start\": t0,\n                \"end\": end,\n                \"uuid\": self.event_uuid,\n                \"source_uuid\": product_uuid,\n                \"is_delta\": True,\n                \"method\": until,\n                \"completed\": bool(fallback.get(\"completed\", False)),\n            }\n        )\n\n    return pd.DataFrame(rows)\n</code></pre>"},{"location":"reference/ts_shape/events/production/__init__/#ts_shape.events.production.ChangeoverEvents.detect_changeover","title":"detect_changeover","text":"<pre><code>detect_changeover(product_uuid: str, *, value_column: str = 'value_string', min_hold: str = '0s') -&gt; DataFrame\n</code></pre> <p>Emit point events when the product/recipe changes value.</p> <p>Uses a hold check: the new product must persist for at least min_hold until the next change.</p> Source code in <code>src/ts_shape/events/production/changeover.py</code> <pre><code>def detect_changeover(\n    self,\n    product_uuid: str,\n    *,\n    value_column: str = \"value_string\",\n    min_hold: str = \"0s\",\n) -&gt; pd.DataFrame:\n    \"\"\"Emit point events when the product/recipe changes value.\n\n    Uses a hold check: the new product must persist for at least min_hold\n    until the next change.\n    \"\"\"\n    p = (\n        self.dataframe[self.dataframe[\"uuid\"] == product_uuid]\n        .copy()\n        .sort_values(self.time_column)\n    )\n    if p.empty:\n        return pd.DataFrame(\n            columns=[\"systime\", \"uuid\", \"source_uuid\", \"is_delta\", \"new_value\"]\n        )\n    p[self.time_column] = pd.to_datetime(p[self.time_column])\n    series = p[value_column]\n    changed = series.ne(series.shift())\n    change_times = p.loc[changed, self.time_column]\n    min_td = pd.to_timedelta(min_hold)\n    next_change = change_times.shift(-1)\n    ok = (next_change - change_times &gt;= min_td) | next_change.isna()\n    change_times = change_times[ok]\n    out = p[p[self.time_column].isin(change_times)][\n        [self.time_column, value_column]\n    ].rename(columns={self.time_column: \"systime\", value_column: \"new_value\"})\n    out[\"uuid\"] = self.event_uuid\n    out[\"source_uuid\"] = product_uuid\n    out[\"is_delta\"] = True\n    return out[[\"systime\", \"uuid\", \"source_uuid\", \"is_delta\", \"new_value\"]]\n</code></pre>"},{"location":"reference/ts_shape/events/production/__init__/#ts_shape.events.production.ChangeoverEvents.get_dataframe","title":"get_dataframe","text":"<pre><code>get_dataframe() -&gt; DataFrame\n</code></pre> <p>Returns the processed DataFrame.</p> Source code in <code>src/ts_shape/utils/base.py</code> <pre><code>def get_dataframe(self) -&gt; pd.DataFrame:\n    \"\"\"Returns the processed DataFrame.\"\"\"\n    return self.dataframe\n</code></pre>"},{"location":"reference/ts_shape/events/production/__init__/#ts_shape.events.production.FlowConstraintEvents","title":"FlowConstraintEvents","text":"<pre><code>FlowConstraintEvents(dataframe: DataFrame, *, time_column: str = 'systime', event_uuid: str = 'prod:flow')\n</code></pre> <p>               Bases: <code>Base</code></p> <p>Production: Flow Constraints</p> <ul> <li>blocked_events: upstream running while downstream not consuming.</li> <li>starved_events: downstream idle due to lack of upstream supply.</li> </ul> <p>Methods:</p> <ul> <li> <code>blocked_events</code>             \u2013              <p>Blocked: upstream_run=True while downstream_run=False.</p> </li> <li> <code>get_dataframe</code>             \u2013              <p>Returns the processed DataFrame.</p> </li> <li> <code>starved_events</code>             \u2013              <p>Starved: downstream_run=True while upstream_run=False.</p> </li> </ul> Source code in <code>src/ts_shape/events/production/flow_constraints.py</code> <pre><code>def __init__(\n    self,\n    dataframe: pd.DataFrame,\n    *,\n    time_column: str = \"systime\",\n    event_uuid: str = \"prod:flow\",\n) -&gt; None:\n    super().__init__(dataframe, column_name=time_column)\n    self.time_column = time_column\n    self.event_uuid = event_uuid\n</code></pre>"},{"location":"reference/ts_shape/events/production/__init__/#ts_shape.events.production.FlowConstraintEvents.blocked_events","title":"blocked_events","text":"<pre><code>blocked_events(*, roles: Dict[str, str], tolerance: str = '200ms', min_duration: str = '0s') -&gt; DataFrame\n</code></pre> <p>Blocked: upstream_run=True while downstream_run=False.</p> <p>roles = {'upstream_run': uuid, 'downstream_run': uuid}</p> Source code in <code>src/ts_shape/events/production/flow_constraints.py</code> <pre><code>def blocked_events(\n    self,\n    *,\n    roles: Dict[str, str],\n    tolerance: str = \"200ms\",\n    min_duration: str = \"0s\",\n) -&gt; pd.DataFrame:\n    \"\"\"Blocked: upstream_run=True while downstream_run=False.\n\n    roles = {'upstream_run': uuid, 'downstream_run': uuid}\n    \"\"\"\n    up = self._align_bool(roles[\"upstream_run\"])  # time, state\n    dn = self._align_bool(roles[\"downstream_run\"])  # time, state\n    if up.empty or dn.empty:\n        return pd.DataFrame(\n            columns=[\"start\", \"end\", \"uuid\", \"source_uuid\", \"is_delta\", \"type\"]\n        )\n    tol = pd.to_timedelta(tolerance)\n    merged = pd.merge_asof(up, dn, on=self.time_column, suffixes=(\"_up\", \"_dn\"), tolerance=tol, direction=\"nearest\")\n    cond = merged[\"state_up\"] &amp; (~merged[\"state_dn\"].fillna(False))\n    gid = (cond.ne(cond.shift())).cumsum()\n    min_td = pd.to_timedelta(min_duration)\n    rows: List[Dict[str, Any]] = []\n    for _, seg in merged.groupby(gid):\n        m = cond.loc[seg.index]\n        if not m.any():\n            continue\n        start = seg[self.time_column].iloc[0]\n        end = seg[self.time_column].iloc[-1]\n        if (end - start) &lt; min_td:\n            continue\n        rows.append(\n            {\n                \"start\": start,\n                \"end\": end,\n                \"uuid\": self.event_uuid,\n                \"source_uuid\": roles[\"upstream_run\"],\n                \"is_delta\": True,\n                \"type\": \"blocked\",\n            }\n        )\n    return pd.DataFrame(rows)\n</code></pre>"},{"location":"reference/ts_shape/events/production/__init__/#ts_shape.events.production.FlowConstraintEvents.get_dataframe","title":"get_dataframe","text":"<pre><code>get_dataframe() -&gt; DataFrame\n</code></pre> <p>Returns the processed DataFrame.</p> Source code in <code>src/ts_shape/utils/base.py</code> <pre><code>def get_dataframe(self) -&gt; pd.DataFrame:\n    \"\"\"Returns the processed DataFrame.\"\"\"\n    return self.dataframe\n</code></pre>"},{"location":"reference/ts_shape/events/production/__init__/#ts_shape.events.production.FlowConstraintEvents.starved_events","title":"starved_events","text":"<pre><code>starved_events(*, roles: Dict[str, str], tolerance: str = '200ms', min_duration: str = '0s') -&gt; DataFrame\n</code></pre> <p>Starved: downstream_run=True while upstream_run=False.</p> <p>roles = {'upstream_run': uuid, 'downstream_run': uuid}</p> Source code in <code>src/ts_shape/events/production/flow_constraints.py</code> <pre><code>def starved_events(\n    self,\n    *,\n    roles: Dict[str, str],\n    tolerance: str = \"200ms\",\n    min_duration: str = \"0s\",\n) -&gt; pd.DataFrame:\n    \"\"\"Starved: downstream_run=True while upstream_run=False.\n\n    roles = {'upstream_run': uuid, 'downstream_run': uuid}\n    \"\"\"\n    up = self._align_bool(roles[\"upstream_run\"])  # time, state\n    dn = self._align_bool(roles[\"downstream_run\"])  # time, state\n    if up.empty or dn.empty:\n        return pd.DataFrame(\n            columns=[\"start\", \"end\", \"uuid\", \"source_uuid\", \"is_delta\", \"type\"]\n        )\n    tol = pd.to_timedelta(tolerance)\n    merged = pd.merge_asof(dn, up, on=self.time_column, suffixes=(\"_dn\", \"_up\"), tolerance=tol, direction=\"nearest\")\n    cond = merged[\"state_dn\"] &amp; (~merged[\"state_up\"].fillna(False))\n    gid = (cond.ne(cond.shift())).cumsum()\n    min_td = pd.to_timedelta(min_duration)\n    rows: List[Dict[str, Any]] = []\n    for _, seg in merged.groupby(gid):\n        m = cond.loc[seg.index]\n        if not m.any():\n            continue\n        start = seg[self.time_column].iloc[0]\n        end = seg[self.time_column].iloc[-1]\n        if (end - start) &lt; min_td:\n            continue\n        rows.append(\n            {\n                \"start\": start,\n                \"end\": end,\n                \"uuid\": self.event_uuid,\n                \"source_uuid\": roles[\"downstream_run\"],\n                \"is_delta\": True,\n                \"type\": \"starved\",\n            }\n        )\n    return pd.DataFrame(rows)\n</code></pre>"},{"location":"reference/ts_shape/events/production/__init__/#ts_shape.events.production.LineThroughputEvents","title":"LineThroughputEvents","text":"<pre><code>LineThroughputEvents(dataframe: DataFrame, *, event_uuid: str = 'prod:throughput', time_column: str = 'systime')\n</code></pre> <p>               Bases: <code>Base</code></p> <p>Production: Line Throughput</p> <p>Methods: - count_parts: Part counts per fixed window from a monotonically increasing counter. - takt_adherence: Cycle time violations against a takt time from step/boolean triggers.</p> <p>Methods:</p> <ul> <li> <code>count_parts</code>             \u2013              <p>Compute parts per window for a counter uuid.</p> </li> <li> <code>get_dataframe</code>             \u2013              <p>Returns the processed DataFrame.</p> </li> <li> <code>takt_adherence</code>             \u2013              <p>Flag cycles whose durations exceed the takt_time.</p> </li> </ul> Source code in <code>src/ts_shape/events/production/line_throughput.py</code> <pre><code>def __init__(\n    self,\n    dataframe: pd.DataFrame,\n    *,\n    event_uuid: str = \"prod:throughput\",\n    time_column: str = \"systime\",\n) -&gt; None:\n    super().__init__(dataframe, column_name=time_column)\n    self.event_uuid = event_uuid\n    self.time_column = time_column\n</code></pre>"},{"location":"reference/ts_shape/events/production/__init__/#ts_shape.events.production.LineThroughputEvents.count_parts","title":"count_parts","text":"<pre><code>count_parts(counter_uuid: str, *, value_column: str = 'value_integer', window: str = '1m') -&gt; DataFrame\n</code></pre> <p>Compute parts per window for a counter uuid.</p> <p>Returns columns: window_start, uuid, source_uuid, is_delta, count</p> Source code in <code>src/ts_shape/events/production/line_throughput.py</code> <pre><code>def count_parts(\n    self,\n    counter_uuid: str,\n    *,\n    value_column: str = \"value_integer\",\n    window: str = \"1m\",\n) -&gt; pd.DataFrame:\n    \"\"\"Compute parts per window for a counter uuid.\n\n    Returns columns: window_start, uuid, source_uuid, is_delta, count\n    \"\"\"\n    c = (\n        self.dataframe[self.dataframe[\"uuid\"] == counter_uuid]\n        .copy()\n        .sort_values(self.time_column)\n    )\n    if c.empty:\n        return pd.DataFrame(\n            columns=[\"window_start\", \"uuid\", \"source_uuid\", \"is_delta\", \"count\"]\n        )\n    c[self.time_column] = pd.to_datetime(c[self.time_column])\n    c = c.set_index(self.time_column)\n    # take diff of last values within each window\n    grp = c[value_column].resample(window)\n    counts = grp.max().fillna(method=\"ffill\").diff().fillna(0).clip(lower=0)\n    out = counts.to_frame(\"count\").reset_index().rename(columns={self.time_column: \"window_start\"})\n    out[\"uuid\"] = self.event_uuid\n    out[\"source_uuid\"] = counter_uuid\n    out[\"is_delta\"] = True\n    return out\n</code></pre>"},{"location":"reference/ts_shape/events/production/__init__/#ts_shape.events.production.LineThroughputEvents.get_dataframe","title":"get_dataframe","text":"<pre><code>get_dataframe() -&gt; DataFrame\n</code></pre> <p>Returns the processed DataFrame.</p> Source code in <code>src/ts_shape/utils/base.py</code> <pre><code>def get_dataframe(self) -&gt; pd.DataFrame:\n    \"\"\"Returns the processed DataFrame.\"\"\"\n    return self.dataframe\n</code></pre>"},{"location":"reference/ts_shape/events/production/__init__/#ts_shape.events.production.LineThroughputEvents.takt_adherence","title":"takt_adherence","text":"<pre><code>takt_adherence(cycle_uuid: str, *, value_column: str = 'value_bool', takt_time: str = '60s', min_violation: str = '0s') -&gt; DataFrame\n</code></pre> <p>Flag cycles whose durations exceed the takt_time.</p> <p>For boolean triggers: detect True rising edges as cycle boundaries. For integer steps: detect increments as cycle boundaries.</p> <p>Returns: systime (at boundary), uuid, source_uuid, is_delta, cycle_time_seconds, violation</p> Source code in <code>src/ts_shape/events/production/line_throughput.py</code> <pre><code>def takt_adherence(\n    self,\n    cycle_uuid: str,\n    *,\n    value_column: str = \"value_bool\",\n    takt_time: str = \"60s\",\n    min_violation: str = \"0s\",\n) -&gt; pd.DataFrame:\n    \"\"\"Flag cycles whose durations exceed the takt_time.\n\n    For boolean triggers: detect True rising edges as cycle boundaries.\n    For integer steps: detect increments as cycle boundaries.\n\n    Returns: systime (at boundary), uuid, source_uuid, is_delta, cycle_time_seconds, violation\n    \"\"\"\n    s = (\n        self.dataframe[self.dataframe[\"uuid\"] == cycle_uuid]\n        .copy()\n        .sort_values(self.time_column)\n    )\n    if s.empty:\n        return pd.DataFrame(\n            columns=[\n                \"systime\",\n                \"uuid\",\n                \"source_uuid\",\n                \"is_delta\",\n                \"cycle_time_seconds\",\n                \"violation\",\n            ]\n        )\n    s[self.time_column] = pd.to_datetime(s[self.time_column])\n    if value_column == \"value_bool\":\n        s[\"prev\"] = s[value_column].shift(fill_value=False)\n        edges = s[(~s[\"prev\"]) &amp; (s[value_column].fillna(False))]\n        times = edges[self.time_column].reset_index(drop=True)\n    else:\n        s[\"prev\"] = s[value_column].shift(1)\n        edges = s[s[value_column].fillna(0) != s[\"prev\"].fillna(0)]\n        times = edges[self.time_column].reset_index(drop=True)\n    if len(times) &lt; 2:\n        return pd.DataFrame(\n            columns=[\n                \"systime\",\n                \"uuid\",\n                \"source_uuid\",\n                \"is_delta\",\n                \"cycle_time_seconds\",\n                \"violation\",\n            ]\n        )\n    cycle_times = (times.diff().dt.total_seconds()).iloc[1:].reset_index(drop=True)\n    min_td = pd.to_timedelta(min_violation).total_seconds()\n    target = pd.to_timedelta(takt_time).total_seconds()\n    viol = (cycle_times - target) &gt;= min_td\n    out = pd.DataFrame(\n        {\n            \"systime\": times.iloc[1:].reset_index(drop=True),\n            \"uuid\": self.event_uuid,\n            \"source_uuid\": cycle_uuid,\n            \"is_delta\": True,\n            \"cycle_time_seconds\": cycle_times,\n            \"violation\": viol,\n        }\n    )\n    return out\n</code></pre>"},{"location":"reference/ts_shape/events/production/__init__/#ts_shape.events.production.MachineStateEvents","title":"MachineStateEvents","text":"<pre><code>MachineStateEvents(dataframe: DataFrame, run_state_uuid: str, *, event_uuid: str = 'prod:run_idle', value_column: str = 'value_bool', time_column: str = 'systime')\n</code></pre> <p>               Bases: <code>Base</code></p> <p>Production: Machine State</p> <p>Detect run/idle transitions and intervals from a boolean state signal.</p> <ul> <li>MachineStateEvents: Run/idle state intervals and transitions.</li> <li>detect_run_idle: Intervalize run/idle states with optional min duration filter.</li> <li>transition_events: Point events on state changes (idle-&gt;run, run-&gt;idle).</li> </ul> <p>Methods:</p> <ul> <li> <code>detect_run_idle</code>             \u2013              <p>Return intervals labeled as 'run' or 'idle'.</p> </li> <li> <code>get_dataframe</code>             \u2013              <p>Returns the processed DataFrame.</p> </li> <li> <code>transition_events</code>             \u2013              <p>Return point events at state transitions.</p> </li> </ul> Source code in <code>src/ts_shape/events/production/machine_state.py</code> <pre><code>def __init__(\n    self,\n    dataframe: pd.DataFrame,\n    run_state_uuid: str,\n    *,\n    event_uuid: str = \"prod:run_idle\",\n    value_column: str = \"value_bool\",\n    time_column: str = \"systime\",\n) -&gt; None:\n    super().__init__(dataframe, column_name=time_column)\n    self.run_state_uuid = run_state_uuid\n    self.event_uuid = event_uuid\n    self.value_column = value_column\n    self.time_column = time_column\n    self.series = (\n        self.dataframe[self.dataframe[\"uuid\"] == self.run_state_uuid]\n        .copy()\n        .sort_values(self.time_column)\n    )\n    self.series[self.time_column] = pd.to_datetime(self.series[self.time_column])\n</code></pre>"},{"location":"reference/ts_shape/events/production/__init__/#ts_shape.events.production.MachineStateEvents.detect_run_idle","title":"detect_run_idle","text":"<pre><code>detect_run_idle(min_duration: str = '0s') -&gt; DataFrame\n</code></pre> <p>Return intervals labeled as 'run' or 'idle'.</p> <ul> <li>min_duration: discard intervals shorter than this duration. Columns: start, end, uuid, source_uuid, is_delta, state</li> </ul> Source code in <code>src/ts_shape/events/production/machine_state.py</code> <pre><code>def detect_run_idle(self, min_duration: str = \"0s\") -&gt; pd.DataFrame:\n    \"\"\"Return intervals labeled as 'run' or 'idle'.\n\n    - min_duration: discard intervals shorter than this duration.\n    Columns: start, end, uuid, source_uuid, is_delta, state\n    \"\"\"\n    if self.series.empty:\n        return pd.DataFrame(\n            columns=[\"start\", \"end\", \"uuid\", \"source_uuid\", \"is_delta\", \"state\"]\n        )\n    s = self.series[[self.time_column, self.value_column]].copy()\n    s[\"state\"] = s[self.value_column].fillna(False).astype(bool)\n    state_change = (s[\"state\"] != s[\"state\"].shift()).cumsum()\n    min_td = pd.to_timedelta(min_duration)\n    rows: List[Dict[str, Any]] = []\n    for _, seg in s.groupby(state_change):\n        state = bool(seg[\"state\"].iloc[0])\n        start = seg[self.time_column].iloc[0]\n        end = seg[self.time_column].iloc[-1]\n        if (end - start) &lt; min_td:\n            continue\n        rows.append(\n            {\n                \"start\": start,\n                \"end\": end,\n                \"uuid\": self.event_uuid,\n                \"source_uuid\": self.run_state_uuid,\n                \"is_delta\": True,\n                \"state\": \"run\" if state else \"idle\",\n            }\n        )\n    return pd.DataFrame(rows)\n</code></pre>"},{"location":"reference/ts_shape/events/production/__init__/#ts_shape.events.production.MachineStateEvents.get_dataframe","title":"get_dataframe","text":"<pre><code>get_dataframe() -&gt; DataFrame\n</code></pre> <p>Returns the processed DataFrame.</p> Source code in <code>src/ts_shape/utils/base.py</code> <pre><code>def get_dataframe(self) -&gt; pd.DataFrame:\n    \"\"\"Returns the processed DataFrame.\"\"\"\n    return self.dataframe\n</code></pre>"},{"location":"reference/ts_shape/events/production/__init__/#ts_shape.events.production.MachineStateEvents.transition_events","title":"transition_events","text":"<pre><code>transition_events() -&gt; DataFrame\n</code></pre> <p>Return point events at state transitions.</p> <p>Columns: systime, uuid, source_uuid, is_delta, transition ('idle_to_run'|'run_to_idle')</p> Source code in <code>src/ts_shape/events/production/machine_state.py</code> <pre><code>def transition_events(self) -&gt; pd.DataFrame:\n    \"\"\"Return point events at state transitions.\n\n    Columns: systime, uuid, source_uuid, is_delta, transition ('idle_to_run'|'run_to_idle')\n    \"\"\"\n    if self.series.empty:\n        return pd.DataFrame(\n            columns=[\"systime\", \"uuid\", \"source_uuid\", \"is_delta\", \"transition\"]\n        )\n    s = self.series[[self.time_column, self.value_column]].copy()\n    s[\"state\"] = s[self.value_column].fillna(False).astype(bool)\n    s[\"prev\"] = s[\"state\"].shift()\n    changes = s[s[\"state\"] != s[\"prev\"]].dropna(subset=[\"prev\"])  # ignore first row\n    if changes.empty:\n        return pd.DataFrame(\n            columns=[\"systime\", \"uuid\", \"source_uuid\", \"is_delta\", \"transition\"]\n        )\n    changes = changes.rename(columns={self.time_column: \"systime\"})\n    changes[\"transition\"] = changes.apply(\n        lambda r: \"idle_to_run\" if (r[\"prev\"] is False and r[\"state\"] is True) else \"run_to_idle\",\n        axis=1,\n    )\n    return pd.DataFrame(\n        {\n            \"systime\": changes[\"systime\"],\n            \"uuid\": self.event_uuid,\n            \"source_uuid\": self.run_state_uuid,\n            \"is_delta\": True,\n            \"transition\": changes[\"transition\"],\n        }\n    )\n</code></pre>"},{"location":"reference/ts_shape/events/production/changeover/","title":"changeover","text":""},{"location":"reference/ts_shape/events/production/changeover/#ts_shape.events.production.changeover","title":"ts_shape.events.production.changeover","text":"<p>Classes:</p> <ul> <li> <code>ChangeoverEvents</code>           \u2013            <p>Production: Changeover</p> </li> </ul>"},{"location":"reference/ts_shape/events/production/changeover/#ts_shape.events.production.changeover.ChangeoverEvents","title":"ChangeoverEvents","text":"<pre><code>ChangeoverEvents(dataframe: DataFrame, *, event_uuid: str = 'prod:changeover', time_column: str = 'systime')\n</code></pre> <p>               Bases: <code>Base</code></p> <p>Production: Changeover</p> <p>Detect product/recipe changes and compute changeover windows without requiring a dedicated 'first good' signal.</p> <p>Methods: - detect_changeover: point events when product/recipe changes. - changeover_window: derive an end time via fixed window or 'stable_band' metrics.</p> <p>Methods:</p> <ul> <li> <code>changeover_window</code>             \u2013              <p>Compute changeover windows per product change.</p> </li> <li> <code>detect_changeover</code>             \u2013              <p>Emit point events when the product/recipe changes value.</p> </li> <li> <code>get_dataframe</code>             \u2013              <p>Returns the processed DataFrame.</p> </li> </ul> Source code in <code>src/ts_shape/events/production/changeover.py</code> <pre><code>def __init__(\n    self,\n    dataframe: pd.DataFrame,\n    *,\n    event_uuid: str = \"prod:changeover\",\n    time_column: str = \"systime\",\n) -&gt; None:\n    super().__init__(dataframe, column_name=time_column)\n    self.event_uuid = event_uuid\n    self.time_column = time_column\n</code></pre>"},{"location":"reference/ts_shape/events/production/changeover/#ts_shape.events.production.changeover.ChangeoverEvents.changeover_window","title":"changeover_window","text":"<pre><code>changeover_window(product_uuid: str, *, value_column: str = 'value_string', start_time: Optional[Timestamp] = None, until: str = 'fixed_window', config: Optional[Dict[str, Any]] = None, fallback: Optional[Dict[str, Any]] = None) -&gt; DataFrame\n</code></pre> <p>Compute changeover windows per product change.</p> until <ul> <li>fixed_window: end = start + config['duration'] (e.g., '10m')</li> <li>stable_band: end when all metrics stabilize within band for hold:       config = {         'metrics': [           {'uuid': 'm1', 'value_column': 'value_double', 'band': 0.2, 'hold': '2m'},           ...         ]       }</li> </ul> <p>fallback: {'default_duration': '10m', 'completed': False}</p> Source code in <code>src/ts_shape/events/production/changeover.py</code> <pre><code>def changeover_window(\n    self,\n    product_uuid: str,\n    *,\n    value_column: str = \"value_string\",\n    start_time: Optional[pd.Timestamp] = None,\n    until: str = \"fixed_window\",\n    config: Optional[Dict[str, Any]] = None,\n    fallback: Optional[Dict[str, Any]] = None,\n) -&gt; pd.DataFrame:\n    \"\"\"Compute changeover windows per product change.\n\n    until:\n      - fixed_window: end = start + config['duration'] (e.g., '10m')\n      - stable_band: end when all metrics stabilize within band for hold:\n            config = {\n              'metrics': [\n                {'uuid': 'm1', 'value_column': 'value_double', 'band': 0.2, 'hold': '2m'},\n                ...\n              ]\n            }\n    fallback: {'default_duration': '10m', 'completed': False}\n    \"\"\"\n    config = config or {}\n    fallback = fallback or {\"default_duration\": \"10m\", \"completed\": False}\n\n    changes = self.detect_changeover(product_uuid, value_column=value_column, min_hold=config.get(\"min_hold\", \"0s\"))\n    if start_time is not None:\n        changes = changes[changes[\"systime\"] &gt;= pd.to_datetime(start_time)]\n    if changes.empty:\n        return pd.DataFrame(\n            columns=[\"start\", \"end\", \"uuid\", \"source_uuid\", \"is_delta\", \"method\", \"completed\"]\n        )\n\n    rows: List[Dict[str, Any]] = []\n    for _, r in changes.iterrows():\n        t0 = pd.to_datetime(r[\"systime\"])\n        if until == \"fixed_window\":\n            duration = pd.to_timedelta(config.get(\"duration\", \"10m\"))\n            end = t0 + duration\n            rows.append(\n                {\n                    \"start\": t0,\n                    \"end\": end,\n                    \"uuid\": self.event_uuid,\n                    \"source_uuid\": product_uuid,\n                    \"is_delta\": True,\n                    \"method\": \"fixed_window\",\n                    \"completed\": True,\n                }\n            )\n            continue\n\n        if until == \"stable_band\":\n            metric_defs = config.get(\"metrics\", [])\n            metric_ends: List[pd.Timestamp] = []\n            for mdef in metric_defs:\n                uid = mdef[\"uuid\"]\n                vcol = mdef.get(\"value_column\", \"value_double\")\n                band = float(mdef.get(\"band\", 0.0))\n                hold_td = pd.to_timedelta(mdef.get(\"hold\", \"0s\"))\n                s = (\n                    self.dataframe[self.dataframe[\"uuid\"] == uid]\n                    .copy()\n                    .sort_values(self.time_column)\n                )\n                s[self.time_column] = pd.to_datetime(s[self.time_column])\n                s = s[s[self.time_column] &gt;= t0]\n                if s.empty:\n                    continue\n                # Rolling median reference and band mask\n                # Use expanding median to be robust soon after change\n                ref = s[vcol].expanding(min_periods=3).median()\n                inside = (s[vcol] - ref).abs() &lt;= band\n                if not inside.any():\n                    continue\n                gid = (inside.ne(inside.shift())).cumsum()\n                end_found: Optional[pd.Timestamp] = None\n                for _, seg in s.groupby(gid):\n                    seg_inside = inside.loc[seg.index]\n                    if not seg_inside.iloc[0]:\n                        continue\n                    start_seg = seg[self.time_column].iloc[0]\n                    end_seg = seg[self.time_column].iloc[-1]\n                    if (end_seg - start_seg) &gt;= hold_td:\n                        end_found = start_seg\n                        break\n                if end_found is not None:\n                    metric_ends.append(end_found)\n            if metric_defs and len(metric_ends) == len(metric_defs):\n                end = max(metric_ends)\n                rows.append(\n                    {\n                        \"start\": t0,\n                        \"end\": end,\n                        \"uuid\": self.event_uuid,\n                        \"source_uuid\": product_uuid,\n                        \"is_delta\": True,\n                        \"method\": \"stable_band\",\n                        \"completed\": True,\n                    }\n                )\n                continue\n\n        # fallback\n        end = t0 + pd.to_timedelta(fallback.get(\"default_duration\", \"10m\"))\n        rows.append(\n            {\n                \"start\": t0,\n                \"end\": end,\n                \"uuid\": self.event_uuid,\n                \"source_uuid\": product_uuid,\n                \"is_delta\": True,\n                \"method\": until,\n                \"completed\": bool(fallback.get(\"completed\", False)),\n            }\n        )\n\n    return pd.DataFrame(rows)\n</code></pre>"},{"location":"reference/ts_shape/events/production/changeover/#ts_shape.events.production.changeover.ChangeoverEvents.detect_changeover","title":"detect_changeover","text":"<pre><code>detect_changeover(product_uuid: str, *, value_column: str = 'value_string', min_hold: str = '0s') -&gt; DataFrame\n</code></pre> <p>Emit point events when the product/recipe changes value.</p> <p>Uses a hold check: the new product must persist for at least min_hold until the next change.</p> Source code in <code>src/ts_shape/events/production/changeover.py</code> <pre><code>def detect_changeover(\n    self,\n    product_uuid: str,\n    *,\n    value_column: str = \"value_string\",\n    min_hold: str = \"0s\",\n) -&gt; pd.DataFrame:\n    \"\"\"Emit point events when the product/recipe changes value.\n\n    Uses a hold check: the new product must persist for at least min_hold\n    until the next change.\n    \"\"\"\n    p = (\n        self.dataframe[self.dataframe[\"uuid\"] == product_uuid]\n        .copy()\n        .sort_values(self.time_column)\n    )\n    if p.empty:\n        return pd.DataFrame(\n            columns=[\"systime\", \"uuid\", \"source_uuid\", \"is_delta\", \"new_value\"]\n        )\n    p[self.time_column] = pd.to_datetime(p[self.time_column])\n    series = p[value_column]\n    changed = series.ne(series.shift())\n    change_times = p.loc[changed, self.time_column]\n    min_td = pd.to_timedelta(min_hold)\n    next_change = change_times.shift(-1)\n    ok = (next_change - change_times &gt;= min_td) | next_change.isna()\n    change_times = change_times[ok]\n    out = p[p[self.time_column].isin(change_times)][\n        [self.time_column, value_column]\n    ].rename(columns={self.time_column: \"systime\", value_column: \"new_value\"})\n    out[\"uuid\"] = self.event_uuid\n    out[\"source_uuid\"] = product_uuid\n    out[\"is_delta\"] = True\n    return out[[\"systime\", \"uuid\", \"source_uuid\", \"is_delta\", \"new_value\"]]\n</code></pre>"},{"location":"reference/ts_shape/events/production/changeover/#ts_shape.events.production.changeover.ChangeoverEvents.get_dataframe","title":"get_dataframe","text":"<pre><code>get_dataframe() -&gt; DataFrame\n</code></pre> <p>Returns the processed DataFrame.</p> Source code in <code>src/ts_shape/utils/base.py</code> <pre><code>def get_dataframe(self) -&gt; pd.DataFrame:\n    \"\"\"Returns the processed DataFrame.\"\"\"\n    return self.dataframe\n</code></pre>"},{"location":"reference/ts_shape/events/production/downtime/","title":"downtime","text":""},{"location":"reference/ts_shape/events/production/downtime/#ts_shape.events.production.downtime","title":"ts_shape.events.production.downtime","text":""},{"location":"reference/ts_shape/events/production/flow_constraints/","title":"flow_constraints","text":""},{"location":"reference/ts_shape/events/production/flow_constraints/#ts_shape.events.production.flow_constraints","title":"ts_shape.events.production.flow_constraints","text":"<p>Classes:</p> <ul> <li> <code>FlowConstraintEvents</code>           \u2013            <p>Production: Flow Constraints</p> </li> </ul>"},{"location":"reference/ts_shape/events/production/flow_constraints/#ts_shape.events.production.flow_constraints.FlowConstraintEvents","title":"FlowConstraintEvents","text":"<pre><code>FlowConstraintEvents(dataframe: DataFrame, *, time_column: str = 'systime', event_uuid: str = 'prod:flow')\n</code></pre> <p>               Bases: <code>Base</code></p> <p>Production: Flow Constraints</p> <ul> <li>blocked_events: upstream running while downstream not consuming.</li> <li>starved_events: downstream idle due to lack of upstream supply.</li> </ul> <p>Methods:</p> <ul> <li> <code>blocked_events</code>             \u2013              <p>Blocked: upstream_run=True while downstream_run=False.</p> </li> <li> <code>get_dataframe</code>             \u2013              <p>Returns the processed DataFrame.</p> </li> <li> <code>starved_events</code>             \u2013              <p>Starved: downstream_run=True while upstream_run=False.</p> </li> </ul> Source code in <code>src/ts_shape/events/production/flow_constraints.py</code> <pre><code>def __init__(\n    self,\n    dataframe: pd.DataFrame,\n    *,\n    time_column: str = \"systime\",\n    event_uuid: str = \"prod:flow\",\n) -&gt; None:\n    super().__init__(dataframe, column_name=time_column)\n    self.time_column = time_column\n    self.event_uuid = event_uuid\n</code></pre>"},{"location":"reference/ts_shape/events/production/flow_constraints/#ts_shape.events.production.flow_constraints.FlowConstraintEvents.blocked_events","title":"blocked_events","text":"<pre><code>blocked_events(*, roles: Dict[str, str], tolerance: str = '200ms', min_duration: str = '0s') -&gt; DataFrame\n</code></pre> <p>Blocked: upstream_run=True while downstream_run=False.</p> <p>roles = {'upstream_run': uuid, 'downstream_run': uuid}</p> Source code in <code>src/ts_shape/events/production/flow_constraints.py</code> <pre><code>def blocked_events(\n    self,\n    *,\n    roles: Dict[str, str],\n    tolerance: str = \"200ms\",\n    min_duration: str = \"0s\",\n) -&gt; pd.DataFrame:\n    \"\"\"Blocked: upstream_run=True while downstream_run=False.\n\n    roles = {'upstream_run': uuid, 'downstream_run': uuid}\n    \"\"\"\n    up = self._align_bool(roles[\"upstream_run\"])  # time, state\n    dn = self._align_bool(roles[\"downstream_run\"])  # time, state\n    if up.empty or dn.empty:\n        return pd.DataFrame(\n            columns=[\"start\", \"end\", \"uuid\", \"source_uuid\", \"is_delta\", \"type\"]\n        )\n    tol = pd.to_timedelta(tolerance)\n    merged = pd.merge_asof(up, dn, on=self.time_column, suffixes=(\"_up\", \"_dn\"), tolerance=tol, direction=\"nearest\")\n    cond = merged[\"state_up\"] &amp; (~merged[\"state_dn\"].fillna(False))\n    gid = (cond.ne(cond.shift())).cumsum()\n    min_td = pd.to_timedelta(min_duration)\n    rows: List[Dict[str, Any]] = []\n    for _, seg in merged.groupby(gid):\n        m = cond.loc[seg.index]\n        if not m.any():\n            continue\n        start = seg[self.time_column].iloc[0]\n        end = seg[self.time_column].iloc[-1]\n        if (end - start) &lt; min_td:\n            continue\n        rows.append(\n            {\n                \"start\": start,\n                \"end\": end,\n                \"uuid\": self.event_uuid,\n                \"source_uuid\": roles[\"upstream_run\"],\n                \"is_delta\": True,\n                \"type\": \"blocked\",\n            }\n        )\n    return pd.DataFrame(rows)\n</code></pre>"},{"location":"reference/ts_shape/events/production/flow_constraints/#ts_shape.events.production.flow_constraints.FlowConstraintEvents.get_dataframe","title":"get_dataframe","text":"<pre><code>get_dataframe() -&gt; DataFrame\n</code></pre> <p>Returns the processed DataFrame.</p> Source code in <code>src/ts_shape/utils/base.py</code> <pre><code>def get_dataframe(self) -&gt; pd.DataFrame:\n    \"\"\"Returns the processed DataFrame.\"\"\"\n    return self.dataframe\n</code></pre>"},{"location":"reference/ts_shape/events/production/flow_constraints/#ts_shape.events.production.flow_constraints.FlowConstraintEvents.starved_events","title":"starved_events","text":"<pre><code>starved_events(*, roles: Dict[str, str], tolerance: str = '200ms', min_duration: str = '0s') -&gt; DataFrame\n</code></pre> <p>Starved: downstream_run=True while upstream_run=False.</p> <p>roles = {'upstream_run': uuid, 'downstream_run': uuid}</p> Source code in <code>src/ts_shape/events/production/flow_constraints.py</code> <pre><code>def starved_events(\n    self,\n    *,\n    roles: Dict[str, str],\n    tolerance: str = \"200ms\",\n    min_duration: str = \"0s\",\n) -&gt; pd.DataFrame:\n    \"\"\"Starved: downstream_run=True while upstream_run=False.\n\n    roles = {'upstream_run': uuid, 'downstream_run': uuid}\n    \"\"\"\n    up = self._align_bool(roles[\"upstream_run\"])  # time, state\n    dn = self._align_bool(roles[\"downstream_run\"])  # time, state\n    if up.empty or dn.empty:\n        return pd.DataFrame(\n            columns=[\"start\", \"end\", \"uuid\", \"source_uuid\", \"is_delta\", \"type\"]\n        )\n    tol = pd.to_timedelta(tolerance)\n    merged = pd.merge_asof(dn, up, on=self.time_column, suffixes=(\"_dn\", \"_up\"), tolerance=tol, direction=\"nearest\")\n    cond = merged[\"state_dn\"] &amp; (~merged[\"state_up\"].fillna(False))\n    gid = (cond.ne(cond.shift())).cumsum()\n    min_td = pd.to_timedelta(min_duration)\n    rows: List[Dict[str, Any]] = []\n    for _, seg in merged.groupby(gid):\n        m = cond.loc[seg.index]\n        if not m.any():\n            continue\n        start = seg[self.time_column].iloc[0]\n        end = seg[self.time_column].iloc[-1]\n        if (end - start) &lt; min_td:\n            continue\n        rows.append(\n            {\n                \"start\": start,\n                \"end\": end,\n                \"uuid\": self.event_uuid,\n                \"source_uuid\": roles[\"downstream_run\"],\n                \"is_delta\": True,\n                \"type\": \"starved\",\n            }\n        )\n    return pd.DataFrame(rows)\n</code></pre>"},{"location":"reference/ts_shape/events/production/line_throughput/","title":"line_throughput","text":""},{"location":"reference/ts_shape/events/production/line_throughput/#ts_shape.events.production.line_throughput","title":"ts_shape.events.production.line_throughput","text":"<p>Classes:</p> <ul> <li> <code>LineThroughputEvents</code>           \u2013            <p>Production: Line Throughput</p> </li> </ul>"},{"location":"reference/ts_shape/events/production/line_throughput/#ts_shape.events.production.line_throughput.LineThroughputEvents","title":"LineThroughputEvents","text":"<pre><code>LineThroughputEvents(dataframe: DataFrame, *, event_uuid: str = 'prod:throughput', time_column: str = 'systime')\n</code></pre> <p>               Bases: <code>Base</code></p> <p>Production: Line Throughput</p> <p>Methods: - count_parts: Part counts per fixed window from a monotonically increasing counter. - takt_adherence: Cycle time violations against a takt time from step/boolean triggers.</p> <p>Methods:</p> <ul> <li> <code>count_parts</code>             \u2013              <p>Compute parts per window for a counter uuid.</p> </li> <li> <code>get_dataframe</code>             \u2013              <p>Returns the processed DataFrame.</p> </li> <li> <code>takt_adherence</code>             \u2013              <p>Flag cycles whose durations exceed the takt_time.</p> </li> </ul> Source code in <code>src/ts_shape/events/production/line_throughput.py</code> <pre><code>def __init__(\n    self,\n    dataframe: pd.DataFrame,\n    *,\n    event_uuid: str = \"prod:throughput\",\n    time_column: str = \"systime\",\n) -&gt; None:\n    super().__init__(dataframe, column_name=time_column)\n    self.event_uuid = event_uuid\n    self.time_column = time_column\n</code></pre>"},{"location":"reference/ts_shape/events/production/line_throughput/#ts_shape.events.production.line_throughput.LineThroughputEvents.count_parts","title":"count_parts","text":"<pre><code>count_parts(counter_uuid: str, *, value_column: str = 'value_integer', window: str = '1m') -&gt; DataFrame\n</code></pre> <p>Compute parts per window for a counter uuid.</p> <p>Returns columns: window_start, uuid, source_uuid, is_delta, count</p> Source code in <code>src/ts_shape/events/production/line_throughput.py</code> <pre><code>def count_parts(\n    self,\n    counter_uuid: str,\n    *,\n    value_column: str = \"value_integer\",\n    window: str = \"1m\",\n) -&gt; pd.DataFrame:\n    \"\"\"Compute parts per window for a counter uuid.\n\n    Returns columns: window_start, uuid, source_uuid, is_delta, count\n    \"\"\"\n    c = (\n        self.dataframe[self.dataframe[\"uuid\"] == counter_uuid]\n        .copy()\n        .sort_values(self.time_column)\n    )\n    if c.empty:\n        return pd.DataFrame(\n            columns=[\"window_start\", \"uuid\", \"source_uuid\", \"is_delta\", \"count\"]\n        )\n    c[self.time_column] = pd.to_datetime(c[self.time_column])\n    c = c.set_index(self.time_column)\n    # take diff of last values within each window\n    grp = c[value_column].resample(window)\n    counts = grp.max().fillna(method=\"ffill\").diff().fillna(0).clip(lower=0)\n    out = counts.to_frame(\"count\").reset_index().rename(columns={self.time_column: \"window_start\"})\n    out[\"uuid\"] = self.event_uuid\n    out[\"source_uuid\"] = counter_uuid\n    out[\"is_delta\"] = True\n    return out\n</code></pre>"},{"location":"reference/ts_shape/events/production/line_throughput/#ts_shape.events.production.line_throughput.LineThroughputEvents.get_dataframe","title":"get_dataframe","text":"<pre><code>get_dataframe() -&gt; DataFrame\n</code></pre> <p>Returns the processed DataFrame.</p> Source code in <code>src/ts_shape/utils/base.py</code> <pre><code>def get_dataframe(self) -&gt; pd.DataFrame:\n    \"\"\"Returns the processed DataFrame.\"\"\"\n    return self.dataframe\n</code></pre>"},{"location":"reference/ts_shape/events/production/line_throughput/#ts_shape.events.production.line_throughput.LineThroughputEvents.takt_adherence","title":"takt_adherence","text":"<pre><code>takt_adherence(cycle_uuid: str, *, value_column: str = 'value_bool', takt_time: str = '60s', min_violation: str = '0s') -&gt; DataFrame\n</code></pre> <p>Flag cycles whose durations exceed the takt_time.</p> <p>For boolean triggers: detect True rising edges as cycle boundaries. For integer steps: detect increments as cycle boundaries.</p> <p>Returns: systime (at boundary), uuid, source_uuid, is_delta, cycle_time_seconds, violation</p> Source code in <code>src/ts_shape/events/production/line_throughput.py</code> <pre><code>def takt_adherence(\n    self,\n    cycle_uuid: str,\n    *,\n    value_column: str = \"value_bool\",\n    takt_time: str = \"60s\",\n    min_violation: str = \"0s\",\n) -&gt; pd.DataFrame:\n    \"\"\"Flag cycles whose durations exceed the takt_time.\n\n    For boolean triggers: detect True rising edges as cycle boundaries.\n    For integer steps: detect increments as cycle boundaries.\n\n    Returns: systime (at boundary), uuid, source_uuid, is_delta, cycle_time_seconds, violation\n    \"\"\"\n    s = (\n        self.dataframe[self.dataframe[\"uuid\"] == cycle_uuid]\n        .copy()\n        .sort_values(self.time_column)\n    )\n    if s.empty:\n        return pd.DataFrame(\n            columns=[\n                \"systime\",\n                \"uuid\",\n                \"source_uuid\",\n                \"is_delta\",\n                \"cycle_time_seconds\",\n                \"violation\",\n            ]\n        )\n    s[self.time_column] = pd.to_datetime(s[self.time_column])\n    if value_column == \"value_bool\":\n        s[\"prev\"] = s[value_column].shift(fill_value=False)\n        edges = s[(~s[\"prev\"]) &amp; (s[value_column].fillna(False))]\n        times = edges[self.time_column].reset_index(drop=True)\n    else:\n        s[\"prev\"] = s[value_column].shift(1)\n        edges = s[s[value_column].fillna(0) != s[\"prev\"].fillna(0)]\n        times = edges[self.time_column].reset_index(drop=True)\n    if len(times) &lt; 2:\n        return pd.DataFrame(\n            columns=[\n                \"systime\",\n                \"uuid\",\n                \"source_uuid\",\n                \"is_delta\",\n                \"cycle_time_seconds\",\n                \"violation\",\n            ]\n        )\n    cycle_times = (times.diff().dt.total_seconds()).iloc[1:].reset_index(drop=True)\n    min_td = pd.to_timedelta(min_violation).total_seconds()\n    target = pd.to_timedelta(takt_time).total_seconds()\n    viol = (cycle_times - target) &gt;= min_td\n    out = pd.DataFrame(\n        {\n            \"systime\": times.iloc[1:].reset_index(drop=True),\n            \"uuid\": self.event_uuid,\n            \"source_uuid\": cycle_uuid,\n            \"is_delta\": True,\n            \"cycle_time_seconds\": cycle_times,\n            \"violation\": viol,\n        }\n    )\n    return out\n</code></pre>"},{"location":"reference/ts_shape/events/production/machine_state/","title":"machine_state","text":""},{"location":"reference/ts_shape/events/production/machine_state/#ts_shape.events.production.machine_state","title":"ts_shape.events.production.machine_state","text":"<p>Classes:</p> <ul> <li> <code>MachineStateEvents</code>           \u2013            <p>Production: Machine State</p> </li> </ul>"},{"location":"reference/ts_shape/events/production/machine_state/#ts_shape.events.production.machine_state.MachineStateEvents","title":"MachineStateEvents","text":"<pre><code>MachineStateEvents(dataframe: DataFrame, run_state_uuid: str, *, event_uuid: str = 'prod:run_idle', value_column: str = 'value_bool', time_column: str = 'systime')\n</code></pre> <p>               Bases: <code>Base</code></p> <p>Production: Machine State</p> <p>Detect run/idle transitions and intervals from a boolean state signal.</p> <ul> <li>MachineStateEvents: Run/idle state intervals and transitions.</li> <li>detect_run_idle: Intervalize run/idle states with optional min duration filter.</li> <li>transition_events: Point events on state changes (idle-&gt;run, run-&gt;idle).</li> </ul> <p>Methods:</p> <ul> <li> <code>detect_run_idle</code>             \u2013              <p>Return intervals labeled as 'run' or 'idle'.</p> </li> <li> <code>get_dataframe</code>             \u2013              <p>Returns the processed DataFrame.</p> </li> <li> <code>transition_events</code>             \u2013              <p>Return point events at state transitions.</p> </li> </ul> Source code in <code>src/ts_shape/events/production/machine_state.py</code> <pre><code>def __init__(\n    self,\n    dataframe: pd.DataFrame,\n    run_state_uuid: str,\n    *,\n    event_uuid: str = \"prod:run_idle\",\n    value_column: str = \"value_bool\",\n    time_column: str = \"systime\",\n) -&gt; None:\n    super().__init__(dataframe, column_name=time_column)\n    self.run_state_uuid = run_state_uuid\n    self.event_uuid = event_uuid\n    self.value_column = value_column\n    self.time_column = time_column\n    self.series = (\n        self.dataframe[self.dataframe[\"uuid\"] == self.run_state_uuid]\n        .copy()\n        .sort_values(self.time_column)\n    )\n    self.series[self.time_column] = pd.to_datetime(self.series[self.time_column])\n</code></pre>"},{"location":"reference/ts_shape/events/production/machine_state/#ts_shape.events.production.machine_state.MachineStateEvents.detect_run_idle","title":"detect_run_idle","text":"<pre><code>detect_run_idle(min_duration: str = '0s') -&gt; DataFrame\n</code></pre> <p>Return intervals labeled as 'run' or 'idle'.</p> <ul> <li>min_duration: discard intervals shorter than this duration. Columns: start, end, uuid, source_uuid, is_delta, state</li> </ul> Source code in <code>src/ts_shape/events/production/machine_state.py</code> <pre><code>def detect_run_idle(self, min_duration: str = \"0s\") -&gt; pd.DataFrame:\n    \"\"\"Return intervals labeled as 'run' or 'idle'.\n\n    - min_duration: discard intervals shorter than this duration.\n    Columns: start, end, uuid, source_uuid, is_delta, state\n    \"\"\"\n    if self.series.empty:\n        return pd.DataFrame(\n            columns=[\"start\", \"end\", \"uuid\", \"source_uuid\", \"is_delta\", \"state\"]\n        )\n    s = self.series[[self.time_column, self.value_column]].copy()\n    s[\"state\"] = s[self.value_column].fillna(False).astype(bool)\n    state_change = (s[\"state\"] != s[\"state\"].shift()).cumsum()\n    min_td = pd.to_timedelta(min_duration)\n    rows: List[Dict[str, Any]] = []\n    for _, seg in s.groupby(state_change):\n        state = bool(seg[\"state\"].iloc[0])\n        start = seg[self.time_column].iloc[0]\n        end = seg[self.time_column].iloc[-1]\n        if (end - start) &lt; min_td:\n            continue\n        rows.append(\n            {\n                \"start\": start,\n                \"end\": end,\n                \"uuid\": self.event_uuid,\n                \"source_uuid\": self.run_state_uuid,\n                \"is_delta\": True,\n                \"state\": \"run\" if state else \"idle\",\n            }\n        )\n    return pd.DataFrame(rows)\n</code></pre>"},{"location":"reference/ts_shape/events/production/machine_state/#ts_shape.events.production.machine_state.MachineStateEvents.get_dataframe","title":"get_dataframe","text":"<pre><code>get_dataframe() -&gt; DataFrame\n</code></pre> <p>Returns the processed DataFrame.</p> Source code in <code>src/ts_shape/utils/base.py</code> <pre><code>def get_dataframe(self) -&gt; pd.DataFrame:\n    \"\"\"Returns the processed DataFrame.\"\"\"\n    return self.dataframe\n</code></pre>"},{"location":"reference/ts_shape/events/production/machine_state/#ts_shape.events.production.machine_state.MachineStateEvents.transition_events","title":"transition_events","text":"<pre><code>transition_events() -&gt; DataFrame\n</code></pre> <p>Return point events at state transitions.</p> <p>Columns: systime, uuid, source_uuid, is_delta, transition ('idle_to_run'|'run_to_idle')</p> Source code in <code>src/ts_shape/events/production/machine_state.py</code> <pre><code>def transition_events(self) -&gt; pd.DataFrame:\n    \"\"\"Return point events at state transitions.\n\n    Columns: systime, uuid, source_uuid, is_delta, transition ('idle_to_run'|'run_to_idle')\n    \"\"\"\n    if self.series.empty:\n        return pd.DataFrame(\n            columns=[\"systime\", \"uuid\", \"source_uuid\", \"is_delta\", \"transition\"]\n        )\n    s = self.series[[self.time_column, self.value_column]].copy()\n    s[\"state\"] = s[self.value_column].fillna(False).astype(bool)\n    s[\"prev\"] = s[\"state\"].shift()\n    changes = s[s[\"state\"] != s[\"prev\"]].dropna(subset=[\"prev\"])  # ignore first row\n    if changes.empty:\n        return pd.DataFrame(\n            columns=[\"systime\", \"uuid\", \"source_uuid\", \"is_delta\", \"transition\"]\n        )\n    changes = changes.rename(columns={self.time_column: \"systime\"})\n    changes[\"transition\"] = changes.apply(\n        lambda r: \"idle_to_run\" if (r[\"prev\"] is False and r[\"state\"] is True) else \"run_to_idle\",\n        axis=1,\n    )\n    return pd.DataFrame(\n        {\n            \"systime\": changes[\"systime\"],\n            \"uuid\": self.event_uuid,\n            \"source_uuid\": self.run_state_uuid,\n            \"is_delta\": True,\n            \"transition\": changes[\"transition\"],\n        }\n    )\n</code></pre>"},{"location":"reference/ts_shape/events/quality/__init__/","title":"init","text":""},{"location":"reference/ts_shape/events/quality/__init__/#ts_shape.events.quality","title":"ts_shape.events.quality","text":"<p>Quality Events</p> <p>Detectors for quality-related events: outliers, statistical process control, and tolerance deviations over time series.</p> <ul> <li>OutlierDetectionEvents: Detect and group outlier events in a time series.</li> <li>detect_outliers_zscore: Detect outliers using Z-score thresholding and group nearby points.</li> <li> <p>detect_outliers_iqr: Detect outliers using IQR bounds and group nearby points.</p> </li> <li> <p>StatisticalProcessControlRuleBased: Apply Western Electric rules to actual values   using tolerance context to flag control-limit violations.</p> </li> <li>calculate_control_limits: Compute mean and \u00b11/\u00b12/\u00b13 standard-deviation bands from tolerance rows.</li> <li>process: Apply selected rules and emit event rows for violations.</li> <li>rule_1: One point beyond the 3-sigma control limits.</li> <li>rule_2: Nine consecutive points on one side of the mean.</li> <li>rule_3: Six consecutive points steadily increasing or decreasing.</li> <li>rule_4: Fourteen consecutive points alternating up and down.</li> <li>rule_5: Two of three consecutive points near the control limit (between 2 and 3 sigma).</li> <li>rule_6: Four of five consecutive points near the control limit (between 1 and 2 sigma).</li> <li>rule_7: Fifteen consecutive points within 1 sigma of the mean.</li> <li> <p>rule_8: Eight consecutive points on both sides of the mean within 1 sigma.</p> </li> <li> <p>ToleranceDeviationEvents: Flag intervals where actual values cross/compare against   tolerance settings and group them into start/end events.</p> </li> <li>process_and_group_data_with_events: Build grouped deviation events with event UUIDs.</li> </ul> <p>Modules:</p> <ul> <li> <code>outlier_detection</code>           \u2013            </li> <li> <code>statistical_process_control</code>           \u2013            </li> <li> <code>tolerance_deviation</code>           \u2013            </li> </ul>"},{"location":"reference/ts_shape/events/quality/outlier_detection/","title":"outlier_detection","text":""},{"location":"reference/ts_shape/events/quality/outlier_detection/#ts_shape.events.quality.outlier_detection","title":"ts_shape.events.quality.outlier_detection","text":"<p>Classes:</p> <ul> <li> <code>OutlierDetectionEvents</code>           \u2013            <p>Processes time series data to detect outliers based on specified statistical methods.</p> </li> </ul>"},{"location":"reference/ts_shape/events/quality/outlier_detection/#ts_shape.events.quality.outlier_detection.OutlierDetectionEvents","title":"OutlierDetectionEvents","text":"<pre><code>OutlierDetectionEvents(dataframe: DataFrame, value_column: str, event_uuid: str = 'outlier_event', time_threshold: str = '5min')\n</code></pre> <p>               Bases: <code>Base</code></p> <p>Processes time series data to detect outliers based on specified statistical methods.</p> <p>Parameters:</p> <p>Methods:</p> <ul> <li> <code>detect_outliers_iqr</code>             \u2013              <p>Detects outliers using the IQR method.</p> </li> <li> <code>detect_outliers_zscore</code>             \u2013              <p>Detects outliers using the Z-score method.</p> </li> <li> <code>get_dataframe</code>             \u2013              <p>Returns the processed DataFrame.</p> </li> </ul> Source code in <code>src/ts_shape/events/quality/outlier_detection.py</code> <pre><code>def __init__(self, dataframe: pd.DataFrame, value_column: str, event_uuid: str = 'outlier_event', \n             time_threshold: str = '5min') -&gt; None:\n    \"\"\"\n    Initializes the OutlierDetectionEvents with specific attributes for outlier detection.\n\n    Args:\n        dataframe (pd.DataFrame): The input time series DataFrame.\n        value_column (str): The name of the column containing the values for outlier detection.\n        event_uuid (str): A UUID or identifier for detected outlier events.\n        time_threshold (str): The time threshold to group close events together.\n    \"\"\"\n    super().__init__(dataframe)\n    self.value_column = value_column\n    self.event_uuid = event_uuid\n    self.time_threshold = time_threshold\n</code></pre>"},{"location":"reference/ts_shape/events/quality/outlier_detection/#ts_shape.events.quality.outlier_detection.OutlierDetectionEvents(dataframe)","title":"<code>dataframe</code>","text":"(<code>DataFrame</code>)           \u2013            <p>The input time series DataFrame.</p>"},{"location":"reference/ts_shape/events/quality/outlier_detection/#ts_shape.events.quality.outlier_detection.OutlierDetectionEvents(value_column)","title":"<code>value_column</code>","text":"(<code>str</code>)           \u2013            <p>The name of the column containing the values for outlier detection.</p>"},{"location":"reference/ts_shape/events/quality/outlier_detection/#ts_shape.events.quality.outlier_detection.OutlierDetectionEvents(event_uuid)","title":"<code>event_uuid</code>","text":"(<code>str</code>, default:                   <code>'outlier_event'</code> )           \u2013            <p>A UUID or identifier for detected outlier events.</p>"},{"location":"reference/ts_shape/events/quality/outlier_detection/#ts_shape.events.quality.outlier_detection.OutlierDetectionEvents(time_threshold)","title":"<code>time_threshold</code>","text":"(<code>str</code>, default:                   <code>'5min'</code> )           \u2013            <p>The time threshold to group close events together.</p>"},{"location":"reference/ts_shape/events/quality/outlier_detection/#ts_shape.events.quality.outlier_detection.OutlierDetectionEvents.detect_outliers_iqr","title":"detect_outliers_iqr","text":"<pre><code>detect_outliers_iqr(threshold: tuple = (1.5, 1.5)) -&gt; DataFrame\n</code></pre> <p>Detects outliers using the IQR method.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>pd.DataFrame: A DataFrame of detected outliers and grouped events.</p> </li> </ul> Source code in <code>src/ts_shape/events/quality/outlier_detection.py</code> <pre><code>def detect_outliers_iqr(self, threshold: tuple = (1.5, 1.5)) -&gt; pd.DataFrame:\n    \"\"\"\n    Detects outliers using the IQR method.\n\n    Args:\n        threshold (tuple): The multipliers for the IQR range for detecting outliers (lower, upper).\n\n    Returns:\n        pd.DataFrame: A DataFrame of detected outliers and grouped events.\n    \"\"\"\n    df = self.dataframe.copy()\n\n    # Convert 'systime' to datetime and sort the DataFrame by 'systime' in descending order\n    df['systime'] = pd.to_datetime(df['systime'])\n    df = df.sort_values(by='systime', ascending=False)\n\n    # Detect outliers using the IQR method\n    Q1 = df[self.value_column].quantile(0.25)\n    Q3 = df[self.value_column].quantile(0.75)\n    IQR = Q3 - Q1\n    lower_bound = Q1 - threshold[0] * IQR\n    upper_bound = Q3 + threshold[1] * IQR\n    df['outlier'] = (df[self.value_column] &lt; lower_bound) | (df[self.value_column] &gt; upper_bound)\n\n    # Filter to keep only outliers\n    outliers_df = df.loc[df['outlier']].copy()\n\n    # Group and return the outliers\n    return self._group_outliers(outliers_df)\n</code></pre>"},{"location":"reference/ts_shape/events/quality/outlier_detection/#ts_shape.events.quality.outlier_detection.OutlierDetectionEvents.detect_outliers_iqr(threshold)","title":"<code>threshold</code>","text":"(<code>tuple</code>, default:                   <code>(1.5, 1.5)</code> )           \u2013            <p>The multipliers for the IQR range for detecting outliers (lower, upper).</p>"},{"location":"reference/ts_shape/events/quality/outlier_detection/#ts_shape.events.quality.outlier_detection.OutlierDetectionEvents.detect_outliers_zscore","title":"detect_outliers_zscore","text":"<pre><code>detect_outliers_zscore(threshold: float = 3.0) -&gt; DataFrame\n</code></pre> <p>Detects outliers using the Z-score method.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>pd.DataFrame: A DataFrame of detected outliers and grouped events.</p> </li> </ul> Source code in <code>src/ts_shape/events/quality/outlier_detection.py</code> <pre><code>def detect_outliers_zscore(self, threshold: float = 3.0) -&gt; pd.DataFrame:\n    \"\"\"\n    Detects outliers using the Z-score method.\n\n    Args:\n        threshold (float): The Z-score threshold for detecting outliers.\n\n    Returns:\n        pd.DataFrame: A DataFrame of detected outliers and grouped events.\n    \"\"\"\n    df = self.dataframe.copy()\n\n    # Convert 'systime' to datetime and sort the DataFrame by 'systime' in descending order\n    df['systime'] = pd.to_datetime(df['systime'])\n    df = df.sort_values(by='systime', ascending=False)\n\n    # Detect outliers using the Z-score method\n    df['outlier'] = np.abs(zscore(df[self.value_column])) &gt; threshold\n\n    # Filter to keep only outliers\n    outliers_df = df.loc[df['outlier']].copy()\n\n    # Group and return the outliers\n    return self._group_outliers(outliers_df)\n</code></pre>"},{"location":"reference/ts_shape/events/quality/outlier_detection/#ts_shape.events.quality.outlier_detection.OutlierDetectionEvents.detect_outliers_zscore(threshold)","title":"<code>threshold</code>","text":"(<code>float</code>, default:                   <code>3.0</code> )           \u2013            <p>The Z-score threshold for detecting outliers.</p>"},{"location":"reference/ts_shape/events/quality/outlier_detection/#ts_shape.events.quality.outlier_detection.OutlierDetectionEvents.get_dataframe","title":"get_dataframe","text":"<pre><code>get_dataframe() -&gt; DataFrame\n</code></pre> <p>Returns the processed DataFrame.</p> Source code in <code>src/ts_shape/utils/base.py</code> <pre><code>def get_dataframe(self) -&gt; pd.DataFrame:\n    \"\"\"Returns the processed DataFrame.\"\"\"\n    return self.dataframe\n</code></pre>"},{"location":"reference/ts_shape/events/quality/statistical_process_control/","title":"statistical_process_control","text":""},{"location":"reference/ts_shape/events/quality/statistical_process_control/#ts_shape.events.quality.statistical_process_control","title":"ts_shape.events.quality.statistical_process_control","text":"<p>Classes:</p> <ul> <li> <code>StatisticalProcessControlRuleBased</code>           \u2013            <p>Inherits from Base and applies SPC rules (Western Electric Rules) to a DataFrame for event detection.</p> </li> </ul>"},{"location":"reference/ts_shape/events/quality/statistical_process_control/#ts_shape.events.quality.statistical_process_control.StatisticalProcessControlRuleBased","title":"StatisticalProcessControlRuleBased","text":"<pre><code>StatisticalProcessControlRuleBased(dataframe: DataFrame, value_column: str, tolerance_uuid: str, actual_uuid: str, event_uuid: str)\n</code></pre> <p>               Bases: <code>Base</code></p> <p>Inherits from Base and applies SPC rules (Western Electric Rules) to a DataFrame for event detection. Processes data based on control limit UUIDs, actual value UUIDs, and generates events with an event UUID.</p> <p>Parameters:</p> <p>Methods:</p> <ul> <li> <code>calculate_control_limits</code>             \u2013              <p>Calculate the control limits (mean \u00b1 1\u03c3, 2\u03c3, 3\u03c3) for the tolerance values.</p> </li> <li> <code>get_dataframe</code>             \u2013              <p>Returns the processed DataFrame.</p> </li> <li> <code>process</code>             \u2013              <p>Applies the selected SPC rules and generates a DataFrame of events where any rules are violated.</p> </li> <li> <code>rule_1</code>             \u2013              <p>Rule 1: One point beyond the 3\u03c3 control limits.</p> </li> <li> <code>rule_2</code>             \u2013              <p>Rule 2: Nine consecutive points on one side of the mean.</p> </li> <li> <code>rule_3</code>             \u2013              <p>Rule 3: Six consecutive points steadily increasing or decreasing.</p> </li> <li> <code>rule_4</code>             \u2013              <p>Rule 4: Fourteen consecutive points alternating up and down.</p> </li> <li> <code>rule_5</code>             \u2013              <p>Rule 5: Two out of three consecutive points near the control limit (beyond 2\u03c3 but within 3\u03c3).</p> </li> <li> <code>rule_6</code>             \u2013              <p>Rule 6: Four out of five consecutive points near the control limit (beyond 1\u03c3 but within 2\u03c3).</p> </li> <li> <code>rule_7</code>             \u2013              <p>Rule 7: Fifteen consecutive points within 1\u03c3 of the centerline.</p> </li> <li> <code>rule_8</code>             \u2013              <p>Rule 8: Eight consecutive points on both sides of the mean within 1\u03c3.</p> </li> </ul> Source code in <code>src/ts_shape/events/quality/statistical_process_control.py</code> <pre><code>def __init__(self, dataframe: pd.DataFrame, value_column: str, tolerance_uuid: str, actual_uuid: str, event_uuid: str) -&gt; None:\n    \"\"\"\n    Initializes the SPCMonitor with UUIDs for tolerance, actual, and event values.\n    Inherits the sorted dataframe from the Base class.\n\n    Args:\n        dataframe (pd.DataFrame): The input DataFrame containing the data to be processed.\n        value_column (str): The column containing the values to monitor.\n        tolerance_uuid (str): UUID identifier for rows that set tolerance values.\n        actual_uuid (str): UUID identifier for rows containing actual values.\n        event_uuid (str): UUID to assign to generated events.\n    \"\"\"\n    super().__init__(dataframe)  # Initialize the Base class\n    self.value_column: str = value_column\n    self.tolerance_uuid: str = tolerance_uuid\n    self.actual_uuid: str = actual_uuid\n    self.event_uuid: str = event_uuid\n</code></pre>"},{"location":"reference/ts_shape/events/quality/statistical_process_control/#ts_shape.events.quality.statistical_process_control.StatisticalProcessControlRuleBased(dataframe)","title":"<code>dataframe</code>","text":"(<code>DataFrame</code>)           \u2013            <p>The input DataFrame containing the data to be processed.</p>"},{"location":"reference/ts_shape/events/quality/statistical_process_control/#ts_shape.events.quality.statistical_process_control.StatisticalProcessControlRuleBased(value_column)","title":"<code>value_column</code>","text":"(<code>str</code>)           \u2013            <p>The column containing the values to monitor.</p>"},{"location":"reference/ts_shape/events/quality/statistical_process_control/#ts_shape.events.quality.statistical_process_control.StatisticalProcessControlRuleBased(tolerance_uuid)","title":"<code>tolerance_uuid</code>","text":"(<code>str</code>)           \u2013            <p>UUID identifier for rows that set tolerance values.</p>"},{"location":"reference/ts_shape/events/quality/statistical_process_control/#ts_shape.events.quality.statistical_process_control.StatisticalProcessControlRuleBased(actual_uuid)","title":"<code>actual_uuid</code>","text":"(<code>str</code>)           \u2013            <p>UUID identifier for rows containing actual values.</p>"},{"location":"reference/ts_shape/events/quality/statistical_process_control/#ts_shape.events.quality.statistical_process_control.StatisticalProcessControlRuleBased(event_uuid)","title":"<code>event_uuid</code>","text":"(<code>str</code>)           \u2013            <p>UUID to assign to generated events.</p>"},{"location":"reference/ts_shape/events/quality/statistical_process_control/#ts_shape.events.quality.statistical_process_control.StatisticalProcessControlRuleBased.calculate_control_limits","title":"calculate_control_limits","text":"<pre><code>calculate_control_limits() -&gt; DataFrame\n</code></pre> <p>Calculate the control limits (mean \u00b1 1\u03c3, 2\u03c3, 3\u03c3) for the tolerance values.</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>pd.DataFrame: DataFrame with control limits for each tolerance group.</p> </li> </ul> Source code in <code>src/ts_shape/events/quality/statistical_process_control.py</code> <pre><code>def calculate_control_limits(self) -&gt; pd.DataFrame:\n    \"\"\"\n    Calculate the control limits (mean \u00b1 1\u03c3, 2\u03c3, 3\u03c3) for the tolerance values.\n\n    Returns:\n        pd.DataFrame: DataFrame with control limits for each tolerance group.\n    \"\"\"\n    df = self.dataframe[self.dataframe['uuid'] == self.tolerance_uuid]\n    mean = df[self.value_column].mean()\n    sigma = df[self.value_column].std()\n\n    control_limits = {\n        'mean': mean,\n        '1sigma_upper': mean + sigma,\n        '1sigma_lower': mean - sigma,\n        '2sigma_upper': mean + 2 * sigma,\n        '2sigma_lower': mean - 2 * sigma,\n        '3sigma_upper': mean + 3 * sigma,\n        '3sigma_lower': mean - 3 * sigma,\n    }\n\n    return pd.DataFrame([control_limits])\n</code></pre>"},{"location":"reference/ts_shape/events/quality/statistical_process_control/#ts_shape.events.quality.statistical_process_control.StatisticalProcessControlRuleBased.get_dataframe","title":"get_dataframe","text":"<pre><code>get_dataframe() -&gt; DataFrame\n</code></pre> <p>Returns the processed DataFrame.</p> Source code in <code>src/ts_shape/utils/base.py</code> <pre><code>def get_dataframe(self) -&gt; pd.DataFrame:\n    \"\"\"Returns the processed DataFrame.\"\"\"\n    return self.dataframe\n</code></pre>"},{"location":"reference/ts_shape/events/quality/statistical_process_control/#ts_shape.events.quality.statistical_process_control.StatisticalProcessControlRuleBased.process","title":"process","text":"<pre><code>process(selected_rules: Optional[List[str]] = None) -&gt; DataFrame\n</code></pre> <p>Applies the selected SPC rules and generates a DataFrame of events where any rules are violated.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>pd.DataFrame: DataFrame with rule violations and detected events.</p> </li> </ul> Source code in <code>src/ts_shape/events/quality/statistical_process_control.py</code> <pre><code>def process(self, selected_rules: Optional[List[str]] = None) -&gt; pd.DataFrame:\n    \"\"\"\n    Applies the selected SPC rules and generates a DataFrame of events where any rules are violated.\n\n    Args:\n        selected_rules (Optional[List[str]]): List of rule names (e.g., ['rule_1', 'rule_3']) to apply.\n\n    Returns:\n        pd.DataFrame: DataFrame with rule violations and detected events.\n    \"\"\"\n    df = self.dataframe[self.dataframe['uuid'] == self.actual_uuid].copy()\n    df['systime'] = pd.to_datetime(df['systime'])\n    df = df.sort_values(by='systime')\n\n    limits = self.calculate_control_limits()\n\n    # Dictionary of rule functions\n    rules = {\n        'rule_1': lambda df: self.rule_1(df, limits),\n        'rule_2': lambda df: self.rule_2(df),\n        'rule_3': lambda df: self.rule_3(df),\n        'rule_4': lambda df: self.rule_4(df),\n        'rule_5': lambda df: self.rule_5(df, limits),\n        'rule_6': lambda df: self.rule_6(df, limits),\n        'rule_7': lambda df: self.rule_7(df, limits),\n        'rule_8': lambda df: self.rule_8(df, limits)\n    }\n\n    # If no specific rules are provided, use all rules\n    if selected_rules is None:\n        selected_rules = list(rules.keys())\n\n    # Apply selected rules and concatenate results\n    events = pd.concat([rules[rule](df) for rule in selected_rules if rule in rules]).drop_duplicates()\n\n    # Add the event UUID to the detected events\n    events['uuid'] = self.event_uuid\n\n    return events[['systime', self.value_column, 'uuid']].drop_duplicates()\n</code></pre>"},{"location":"reference/ts_shape/events/quality/statistical_process_control/#ts_shape.events.quality.statistical_process_control.StatisticalProcessControlRuleBased.process(selected_rules)","title":"<code>selected_rules</code>","text":"(<code>Optional[List[str]]</code>, default:                   <code>None</code> )           \u2013            <p>List of rule names (e.g., ['rule_1', 'rule_3']) to apply.</p>"},{"location":"reference/ts_shape/events/quality/statistical_process_control/#ts_shape.events.quality.statistical_process_control.StatisticalProcessControlRuleBased.rule_1","title":"rule_1","text":"<pre><code>rule_1(df: DataFrame, limits: DataFrame) -&gt; DataFrame\n</code></pre> <p>Rule 1: One point beyond the 3\u03c3 control limits.</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>pd.DataFrame: Filtered DataFrame with rule violations.</p> </li> </ul> Source code in <code>src/ts_shape/events/quality/statistical_process_control.py</code> <pre><code>def rule_1(self, df: pd.DataFrame, limits: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Rule 1: One point beyond the 3\u03c3 control limits.\n\n    Returns:\n        pd.DataFrame: Filtered DataFrame with rule violations.\n    \"\"\"\n    df['rule_1'] = (df[self.value_column] &gt; limits['3sigma_upper'].values[0]) | (df[self.value_column] &lt; limits['3sigma_lower'].values[0])\n    return df[df['rule_1']]\n</code></pre>"},{"location":"reference/ts_shape/events/quality/statistical_process_control/#ts_shape.events.quality.statistical_process_control.StatisticalProcessControlRuleBased.rule_2","title":"rule_2","text":"<pre><code>rule_2(df: DataFrame) -&gt; DataFrame\n</code></pre> <p>Rule 2: Nine consecutive points on one side of the mean.</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>pd.DataFrame: Filtered DataFrame with rule violations.</p> </li> </ul> Source code in <code>src/ts_shape/events/quality/statistical_process_control.py</code> <pre><code>def rule_2(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Rule 2: Nine consecutive points on one side of the mean.\n\n    Returns:\n        pd.DataFrame: Filtered DataFrame with rule violations.\n    \"\"\"\n    mean = df[self.value_column].mean()\n    df['above_mean'] = df[self.value_column] &gt; mean\n    df['below_mean'] = df[self.value_column] &lt; mean\n    df['rule_2'] = (df['above_mean'].rolling(window=9).sum() == 9) | (df['below_mean'].rolling(window=9).sum() == 9)\n    return df[df['rule_2']]\n</code></pre>"},{"location":"reference/ts_shape/events/quality/statistical_process_control/#ts_shape.events.quality.statistical_process_control.StatisticalProcessControlRuleBased.rule_3","title":"rule_3","text":"<pre><code>rule_3(df: DataFrame) -&gt; DataFrame\n</code></pre> <p>Rule 3: Six consecutive points steadily increasing or decreasing.</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>pd.DataFrame: Filtered DataFrame with rule violations.</p> </li> </ul> Source code in <code>src/ts_shape/events/quality/statistical_process_control.py</code> <pre><code>def rule_3(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Rule 3: Six consecutive points steadily increasing or decreasing.\n\n    Returns:\n        pd.DataFrame: Filtered DataFrame with rule violations.\n    \"\"\"\n    df['increasing'] = df[self.value_column].diff().gt(0)\n    df['decreasing'] = df[self.value_column].diff().lt(0)\n    df['rule_3'] = (df['increasing'].rolling(window=6).sum() == 6) | (df['decreasing'].rolling(window=6).sum() == 6)\n    return df[df['rule_3']]\n</code></pre>"},{"location":"reference/ts_shape/events/quality/statistical_process_control/#ts_shape.events.quality.statistical_process_control.StatisticalProcessControlRuleBased.rule_4","title":"rule_4","text":"<pre><code>rule_4(df: DataFrame) -&gt; DataFrame\n</code></pre> <p>Rule 4: Fourteen consecutive points alternating up and down.</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>pd.DataFrame: Filtered DataFrame with rule violations.</p> </li> </ul> Source code in <code>src/ts_shape/events/quality/statistical_process_control.py</code> <pre><code>def rule_4(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Rule 4: Fourteen consecutive points alternating up and down.\n\n    Returns:\n        pd.DataFrame: Filtered DataFrame with rule violations.\n    \"\"\"\n    df['alternating'] = df[self.value_column].diff().apply(np.sign)\n    df['rule_4'] = df['alternating'].rolling(window=14).apply(lambda x: (x != x.shift()).sum() == 13, raw=True)\n    return df[df['rule_4']]\n</code></pre>"},{"location":"reference/ts_shape/events/quality/statistical_process_control/#ts_shape.events.quality.statistical_process_control.StatisticalProcessControlRuleBased.rule_5","title":"rule_5","text":"<pre><code>rule_5(df: DataFrame, limits: DataFrame) -&gt; DataFrame\n</code></pre> <p>Rule 5: Two out of three consecutive points near the control limit (beyond 2\u03c3 but within 3\u03c3).</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>pd.DataFrame: Filtered DataFrame with rule violations.</p> </li> </ul> Source code in <code>src/ts_shape/events/quality/statistical_process_control.py</code> <pre><code>def rule_5(self, df: pd.DataFrame, limits: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Rule 5: Two out of three consecutive points near the control limit (beyond 2\u03c3 but within 3\u03c3).\n\n    Returns:\n        pd.DataFrame: Filtered DataFrame with rule violations.\n    \"\"\"\n    df['rule_5'] = df[self.value_column].apply(\n        lambda x: 1 if ((x &gt; limits['2sigma_upper'].values[0] and x &lt; limits['3sigma_upper'].values[0]) or \n                        (x &lt; limits['2sigma_lower'].values[0] and x &gt; limits['3sigma_lower'].values[0])) else 0\n    )\n    df['rule_5'] = df['rule_5'].rolling(window=3).sum() &gt;= 2\n    return df[df['rule_5']]\n</code></pre>"},{"location":"reference/ts_shape/events/quality/statistical_process_control/#ts_shape.events.quality.statistical_process_control.StatisticalProcessControlRuleBased.rule_6","title":"rule_6","text":"<pre><code>rule_6(df: DataFrame, limits: DataFrame) -&gt; DataFrame\n</code></pre> <p>Rule 6: Four out of five consecutive points near the control limit (beyond 1\u03c3 but within 2\u03c3).</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>pd.DataFrame: Filtered DataFrame with rule violations.</p> </li> </ul> Source code in <code>src/ts_shape/events/quality/statistical_process_control.py</code> <pre><code>def rule_6(self, df: pd.DataFrame, limits: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Rule 6: Four out of five consecutive points near the control limit (beyond 1\u03c3 but within 2\u03c3).\n\n    Returns:\n        pd.DataFrame: Filtered DataFrame with rule violations.\n    \"\"\"\n    df['rule_6'] = df[self.value_column].apply(\n        lambda x: 1 if ((x &gt; limits['1sigma_upper'].values[0] and x &lt; limits['2sigma_upper'].values[0]) or \n                        (x &lt; limits['1sigma_lower'].values[0] and x &gt; limits['2sigma_lower'].values[0])) else 0\n    )\n    df['rule_6'] = df['rule_6'].rolling(window=5).sum() &gt;= 4\n    return df[df['rule_6']]\n</code></pre>"},{"location":"reference/ts_shape/events/quality/statistical_process_control/#ts_shape.events.quality.statistical_process_control.StatisticalProcessControlRuleBased.rule_7","title":"rule_7","text":"<pre><code>rule_7(df: DataFrame, limits: DataFrame) -&gt; DataFrame\n</code></pre> <p>Rule 7: Fifteen consecutive points within 1\u03c3 of the centerline.</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>pd.DataFrame: Filtered DataFrame with rule violations.</p> </li> </ul> Source code in <code>src/ts_shape/events/quality/statistical_process_control.py</code> <pre><code>def rule_7(self, df: pd.DataFrame, limits: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Rule 7: Fifteen consecutive points within 1\u03c3 of the centerline.\n\n    Returns:\n        pd.DataFrame: Filtered DataFrame with rule violations.\n    \"\"\"\n    df['rule_7'] = df[self.value_column].apply(\n        lambda x: 1 if (x &lt; limits['1sigma_upper'].values[0] and x &gt; limits['1sigma_lower'].values[0]) else 0\n    )\n    df['rule_7'] = df['rule_7'].rolling(window=15).sum() == 15\n    return df[df['rule_7']]\n</code></pre>"},{"location":"reference/ts_shape/events/quality/statistical_process_control/#ts_shape.events.quality.statistical_process_control.StatisticalProcessControlRuleBased.rule_8","title":"rule_8","text":"<pre><code>rule_8(df: DataFrame, limits: DataFrame) -&gt; DataFrame\n</code></pre> <p>Rule 8: Eight consecutive points on both sides of the mean within 1\u03c3.</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>pd.DataFrame: Filtered DataFrame with rule violations.</p> </li> </ul> Source code in <code>src/ts_shape/events/quality/statistical_process_control.py</code> <pre><code>def rule_8(self, df: pd.DataFrame, limits: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Rule 8: Eight consecutive points on both sides of the mean within 1\u03c3.\n\n    Returns:\n        pd.DataFrame: Filtered DataFrame with rule violations.\n    \"\"\"\n    df['rule_8'] = df[self.value_column].apply(\n        lambda x: 1 if (x &lt; limits['1sigma_upper'].values[0] and x &gt; limits['1sigma_lower'].values[0]) else 0\n    )\n    df['rule_8'] = df['rule_8'].rolling(window=8).sum() == 8\n    return df[df['rule_8']]\n</code></pre>"},{"location":"reference/ts_shape/events/quality/tolerance_deviation/","title":"tolerance_deviation","text":""},{"location":"reference/ts_shape/events/quality/tolerance_deviation/#ts_shape.events.quality.tolerance_deviation","title":"ts_shape.events.quality.tolerance_deviation","text":"<p>Classes:</p> <ul> <li> <code>ToleranceDeviationEvents</code>           \u2013            <p>Inherits from Base and processes DataFrame data for specific events, comparing tolerance and actual values.</p> </li> </ul>"},{"location":"reference/ts_shape/events/quality/tolerance_deviation/#ts_shape.events.quality.tolerance_deviation.ToleranceDeviationEvents","title":"ToleranceDeviationEvents","text":"<pre><code>ToleranceDeviationEvents(dataframe: DataFrame, tolerance_column: str, actual_column: str, tolerance_uuid: str, actual_uuid: str, event_uuid: str, compare_func: Callable[[Series, Series], Series] = ge, time_threshold: str = '5min')\n</code></pre> <p>               Bases: <code>Base</code></p> <p>Inherits from Base and processes DataFrame data for specific events, comparing tolerance and actual values.</p> <p>Methods:</p> <ul> <li> <code>get_dataframe</code>             \u2013              <p>Returns the processed DataFrame.</p> </li> <li> <code>process_and_group_data_with_events</code>             \u2013              <p>Processes DataFrame to apply tolerance checks, group events by time, and generate an events DataFrame.</p> </li> </ul> Source code in <code>src/ts_shape/events/quality/tolerance_deviation.py</code> <pre><code>def __init__(self, dataframe: pd.DataFrame, tolerance_column: str, actual_column: str, \n             tolerance_uuid: str, actual_uuid: str, event_uuid: str, \n             compare_func: Callable[[pd.Series, pd.Series], pd.Series] = operator.ge, \n             time_threshold: str = '5min') -&gt; None:\n    \"\"\"\n    Initializes the ToleranceDeviationEvents with specific event attributes.\n    Inherits the sorted dataframe from the Base class.\n    \"\"\"\n    super().__init__(dataframe)  # Inherit and initialize Base class\n\n    self.tolerance_column: str = tolerance_column\n    self.actual_column: str = actual_column\n    self.tolerance_uuid: str = tolerance_uuid\n    self.actual_uuid: str = actual_uuid\n    self.event_uuid: str = event_uuid\n    self.compare_func: Callable[[pd.Series, pd.Series], pd.Series] = compare_func\n    self.time_threshold: str = time_threshold\n</code></pre>"},{"location":"reference/ts_shape/events/quality/tolerance_deviation/#ts_shape.events.quality.tolerance_deviation.ToleranceDeviationEvents.get_dataframe","title":"get_dataframe","text":"<pre><code>get_dataframe() -&gt; DataFrame\n</code></pre> <p>Returns the processed DataFrame.</p> Source code in <code>src/ts_shape/utils/base.py</code> <pre><code>def get_dataframe(self) -&gt; pd.DataFrame:\n    \"\"\"Returns the processed DataFrame.\"\"\"\n    return self.dataframe\n</code></pre>"},{"location":"reference/ts_shape/events/quality/tolerance_deviation/#ts_shape.events.quality.tolerance_deviation.ToleranceDeviationEvents.process_and_group_data_with_events","title":"process_and_group_data_with_events","text":"<pre><code>process_and_group_data_with_events() -&gt; DataFrame\n</code></pre> <p>Processes DataFrame to apply tolerance checks, group events by time, and generate an events DataFrame.</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>pd.DataFrame: A DataFrame of processed and grouped event data.</p> </li> </ul> Source code in <code>src/ts_shape/events/quality/tolerance_deviation.py</code> <pre><code>def process_and_group_data_with_events(self) -&gt; pd.DataFrame:\n    \"\"\"\n    Processes DataFrame to apply tolerance checks, group events by time, and generate an events DataFrame.\n\n    Returns:\n        pd.DataFrame: A DataFrame of processed and grouped event data.\n    \"\"\"\n    df = self.dataframe  # Inherited from Base class\n\n    # Convert 'systime' to datetime and sort the DataFrame by 'systime' in descending order\n    df['systime'] = pd.to_datetime(df['systime'])\n    df = df.sort_values(by='systime', ascending=False)\n\n    # Create a column for lagged tolerance values\n    df['tolerance_value'] = df.apply(\n        lambda row: row[self.tolerance_column] if (row['uuid'] == self.tolerance_uuid and row['is_delta']) else pd.NA, axis=1\n    )\n\n    # Forward fill the tolerance values to propagate the last observed tolerance value\n    df['tolerance_value'] = df['tolerance_value'].ffill()\n\n    # Remove tolerance setting rows from the dataset\n    df = df[df['uuid'] != self.tolerance_uuid]\n\n    # Ensure there are no NA values in the tolerance_value column before comparison\n    df = df.dropna(subset=['tolerance_value'])\n\n    # Apply comparison function to compare actual values with tolerance values\n    df = df[self.compare_func(df[self.actual_column], df['tolerance_value'])]\n    df['value_bool'] = True  # Assign True in the value_bool column for kept rows\n\n    # Grouping events that are close to each other in terms of time\n    df['group_id'] = (df['systime'].diff().abs() &gt; pd.to_timedelta(self.time_threshold)).cumsum()\n\n    # Filter for specific UUID and prepare events DataFrame\n    filtered_df = df[df['uuid'] == self.actual_uuid]\n    events_data = []\n\n    for group_id in filtered_df['group_id'].unique():\n        group_data = filtered_df[filtered_df['group_id'] == group_id]\n        if group_data.shape[0] &gt; 1:  # Ensure there's more than one row to work with\n            first_row = group_data.nsmallest(1, 'systime')\n            last_row = group_data.nlargest(1, 'systime')\n            combined_rows = pd.concat([first_row, last_row])\n            events_data.append(combined_rows)\n\n    # Convert list of DataFrame slices to a single DataFrame\n    if events_data:\n        events_df = pd.concat(events_data)\n        events_df['uuid'] = self.event_uuid\n    else:\n        events_df = pd.DataFrame(columns=filtered_df.columns)  # Create empty DataFrame if no data\n\n    events_df = events_df.drop(['tolerance_value', 'group_id'], axis=1)\n    events_df[self.actual_column] = np.nan\n    events_df['is_delta'] = True\n\n    return events_df\n</code></pre>"},{"location":"reference/ts_shape/events/supplychain/__init__/","title":"supplychain","text":""},{"location":"reference/ts_shape/events/supplychain/__init__/#ts_shape.events.supplychain","title":"ts_shape.events.supplychain","text":"<p>Supply Chain Events</p> <p>Detectors for supply chain\u2013related events and anomalies over shaped timeseries.</p> <p>Classes: - None yet: Placeholder module for future supply chain event detectors.</p>"},{"location":"reference/ts_shape/features/__init__/","title":"init","text":""},{"location":"reference/ts_shape/features/__init__/#ts_shape.features","title":"ts_shape.features","text":"<p>Features</p> <p>Feature extraction and summarization utilities for shaped timeseries.</p> <ul> <li>NumericStatistics: Compute descriptive statistics for numeric columns.</li> <li>column_mean: Mean of a column.</li> <li>column_median: Median of a column.</li> <li>column_std: Standard deviation of a column.</li> <li>column_variance: Variance of a column.</li> <li>column_min: Minimum value.</li> <li>column_max: Maximum value.</li> <li>column_sum: Sum of values.</li> <li>column_kurtosis: Kurtosis of values.</li> <li>column_skewness: Skewness of values.</li> <li>column_quantile: Quantile of a column.</li> <li>column_iqr: Interquartile range.</li> <li>column_range: Range (max - min).</li> <li>column_mad: Mean absolute deviation.</li> <li>coefficient_of_variation: Standard deviation divided by mean (guarded).</li> <li>standard_error_mean: Standard error of the mean.</li> <li>describe: Pandas describe wrapper.</li> <li>summary_as_dict: Comprehensive numeric summary as dict.</li> <li> <p>summary_as_dataframe: Comprehensive numeric summary as DataFrame.</p> </li> <li> <p>StringStatistics: String-based statistics for categorical/text columns.</p> </li> <li>count_unique: Number of unique strings.</li> <li>most_frequent: Most frequent string.</li> <li>count_most_frequent: Count of the most frequent string.</li> <li>count_null: Number of nulls.</li> <li>average_string_length: Average length of non-null strings.</li> <li>longest_string: Longest string.</li> <li>shortest_string: Shortest string.</li> <li>string_length_summary: Summary of lengths.</li> <li>most_common_n_strings: Top-N most frequent strings.</li> <li>contains_substring_count: Count of strings containing a substring.</li> <li>starts_with_count: Count of strings starting with a prefix.</li> <li>ends_with_count: Count of strings ending with a suffix.</li> <li>uppercase_percentage: Percentage of uppercase strings.</li> <li>lowercase_percentage: Percentage of lowercase strings.</li> <li>contains_digit_count: Count of strings containing digits.</li> <li>summary_as_dict: Comprehensive string summary as dict.</li> <li> <p>summary_as_dataframe: Comprehensive string summary as DataFrame.</p> </li> <li> <p>BooleanStatistics: Boolean column statistics.</p> </li> <li>count_true: Count of True values.</li> <li>count_false: Count of False values.</li> <li>count_null: Count of nulls.</li> <li>count_not_null: Count of non-nulls.</li> <li>true_percentage: Percentage True.</li> <li>false_percentage: Percentage False.</li> <li>mode: Most common boolean value.</li> <li>is_balanced: Whether distribution is 50/50.</li> <li>summary_as_dict: Summary as dict.</li> <li> <p>summary_as_dataframe: Summary as DataFrame.</p> </li> <li> <p>TimestampStatistics: Timestamp distributions and ranges.</p> </li> <li>count_null: Count of null timestamps.</li> <li>count_not_null: Count of non-null timestamps.</li> <li>earliest_timestamp: Earliest timestamp.</li> <li>latest_timestamp: Latest timestamp.</li> <li>timestamp_range: Time range (latest - earliest).</li> <li>most_frequent_timestamp: Most frequent timestamp.</li> <li>count_most_frequent_timestamp: Count of the modal timestamp.</li> <li>year_distribution: Distribution by year.</li> <li>month_distribution: Distribution by month.</li> <li>weekday_distribution: Distribution by weekday.</li> <li>hour_distribution: Distribution by hour.</li> <li>most_frequent_day: Most frequent weekday.</li> <li>most_frequent_hour: Most frequent hour.</li> <li>average_time_gap: Average gap between consecutive timestamps.</li> <li>median_timestamp: Median timestamp.</li> <li>standard_deviation_timestamps: Standard deviation of consecutive differences.</li> <li>timestamp_quartiles: 25th/50th/75th percentiles.</li> <li> <p>days_with_most_activity: Top-N active days.</p> </li> <li> <p>TimeGroupedStatistics: Time-windowed aggregations for numeric series.</p> </li> <li>calculate_statistic: Single aggregation per window (mean/sum/min/max/diff/range).</li> <li>calculate_statistics: Multiple aggregations merged.</li> <li> <p>calculate_custom_func: Apply a custom aggregation per window.</p> </li> <li> <p>CycleExtractor: Build cycles from state/step/value changes.</p> </li> <li>process_persistent_cycle: True stretches define cycles.</li> <li>process_trigger_cycle: True-to-False transition defines a cycle end.</li> <li>process_separate_start_end_cycle: Separate starts and ends signals.</li> <li>process_step_sequence: Start/end steps in integer values.</li> <li>process_state_change_cycle: Sequential rows define boundaries.</li> <li> <p>process_value_change_cycle: Any value change defines a boundary.</p> </li> <li> <p>CycleDataProcessor: Split/merge/group by cycle windows.</p> </li> <li>split_by_cycle: Split values by cycle ranges.</li> <li>merge_dataframes_by_cycle: Annotate values with cycle UUIDs.</li> <li>group_by_cycle_uuid: Group values by cycle key.</li> <li>split_dataframes_by_group: Further split by column groupings.</li> </ul> <p>Modules:</p> <ul> <li> <code>cycles</code>           \u2013            <p>Cycles</p> </li> <li> <code>stats</code>           \u2013            <p>Statistics</p> </li> <li> <code>time_stats</code>           \u2013            <p>Time-Grouped Statistics</p> </li> </ul>"},{"location":"reference/ts_shape/features/cycles/__init__/","title":"init","text":""},{"location":"reference/ts_shape/features/cycles/__init__/#ts_shape.features.cycles","title":"ts_shape.features.cycles","text":"<p>Cycles</p> <p>Utilities to detect and process cycles in timeseries.</p> <ul> <li>CycleExtractor: Build cycles from state/step/value changes.</li> <li>process_persistent_cycle: True stretches define cycles.</li> <li>process_trigger_cycle: True-to-False transition defines a cycle end.</li> <li>process_separate_start_end_cycle: Separate starts and ends signals.</li> <li>process_step_sequence: Start/end steps in integer values.</li> <li>process_state_change_cycle: Sequential rows define boundaries.</li> <li> <p>process_value_change_cycle: Any value change defines a boundary.</p> </li> <li> <p>CycleDataProcessor: Split/merge/group by cycle windows.</p> </li> <li>split_by_cycle: Split values by cycle ranges.</li> <li>merge_dataframes_by_cycle: Annotate values with cycle UUIDs.</li> <li>group_by_cycle_uuid: Group values by cycle key.</li> <li>split_dataframes_by_group: Further split by column groupings.</li> </ul> <p>Modules:</p> <ul> <li> <code>cycle_processor</code>           \u2013            </li> <li> <code>cycles_extractor</code>           \u2013            </li> </ul>"},{"location":"reference/ts_shape/features/cycles/cycle_processor/","title":"cycle_processor","text":""},{"location":"reference/ts_shape/features/cycles/cycle_processor/#ts_shape.features.cycles.cycle_processor","title":"ts_shape.features.cycles.cycle_processor","text":"<p>Classes:</p> <ul> <li> <code>CycleDataProcessor</code>           \u2013            <p>A class to process cycle-based data and values. It allows for splitting, merging, and grouping DataFrames </p> </li> </ul>"},{"location":"reference/ts_shape/features/cycles/cycle_processor/#ts_shape.features.cycles.cycle_processor.CycleDataProcessor","title":"CycleDataProcessor","text":"<pre><code>CycleDataProcessor(cycles_df: DataFrame, values_df: DataFrame, cycle_uuid_col: str = 'cycle_uuid', systime_col: str = 'systime')\n</code></pre> <p>               Bases: <code>Base</code></p> <p>A class to process cycle-based data and values. It allows for splitting, merging, and grouping DataFrames  based on cycles, as well as handling grouping and transformations by cycle UUIDs.</p> <p>Parameters:</p> <p>Methods:</p> <ul> <li> <code>get_dataframe</code>             \u2013              <p>Returns the processed DataFrame.</p> </li> <li> <code>group_by_cycle_uuid</code>             \u2013              <p>Group the DataFrame by the cycle_uuid column, resulting in a list of DataFrames, each containing data for one cycle.</p> </li> <li> <code>merge_dataframes_by_cycle</code>             \u2013              <p>Merges the values DataFrame with the cycles DataFrame based on the cycle time intervals. </p> </li> <li> <code>split_by_cycle</code>             \u2013              <p>Splits the values DataFrame by cycles defined in the cycles DataFrame. </p> </li> <li> <code>split_dataframes_by_group</code>             \u2013              <p>Splits a list of DataFrames by groups based on a specified column. </p> </li> </ul> Source code in <code>src/ts_shape/features/cycles/cycle_processor.py</code> <pre><code>def __init__(self, cycles_df: pd.DataFrame, values_df: pd.DataFrame, cycle_uuid_col: str = \"cycle_uuid\", systime_col: str = \"systime\"):\n    \"\"\"\n    Initializes the CycleDataProcessor with cycles and values DataFrames.\n\n    Args:\n        cycles_df: DataFrame containing columns 'cycle_start', 'cycle_end', and 'cycle_uuid'.\n        values_df: DataFrame containing the values and timestamps in the 'systime' column.\n        cycle_uuid_col: Name of the column representing cycle UUIDs.\n        systime_col: Name of the column representing the timestamps for the values.\n    \"\"\"\n    super().__init__(values_df)  # Call the parent constructor\n    self.values_df = values_df.copy()  # Initialize self.values_df explicitly\n    self.cycles_df = cycles_df.copy()\n    self.cycle_uuid_col = cycle_uuid_col\n    self.systime_col = systime_col\n\n    # Ensure proper datetime format\n    self.cycles_df['cycle_start'] = pd.to_datetime(self.cycles_df['cycle_start'])\n    self.cycles_df['cycle_end'] = pd.to_datetime(self.cycles_df['cycle_end'])\n    self.values_df[systime_col] = pd.to_datetime(self.values_df[systime_col])\n\n    logging.info(\"CycleDataProcessor initialized with cycles and values DataFrames.\")\n</code></pre>"},{"location":"reference/ts_shape/features/cycles/cycle_processor/#ts_shape.features.cycles.cycle_processor.CycleDataProcessor(cycles_df)","title":"<code>cycles_df</code>","text":"(<code>DataFrame</code>)           \u2013            <p>DataFrame containing columns 'cycle_start', 'cycle_end', and 'cycle_uuid'.</p>"},{"location":"reference/ts_shape/features/cycles/cycle_processor/#ts_shape.features.cycles.cycle_processor.CycleDataProcessor(values_df)","title":"<code>values_df</code>","text":"(<code>DataFrame</code>)           \u2013            <p>DataFrame containing the values and timestamps in the 'systime' column.</p>"},{"location":"reference/ts_shape/features/cycles/cycle_processor/#ts_shape.features.cycles.cycle_processor.CycleDataProcessor(cycle_uuid_col)","title":"<code>cycle_uuid_col</code>","text":"(<code>str</code>, default:                   <code>'cycle_uuid'</code> )           \u2013            <p>Name of the column representing cycle UUIDs.</p>"},{"location":"reference/ts_shape/features/cycles/cycle_processor/#ts_shape.features.cycles.cycle_processor.CycleDataProcessor(systime_col)","title":"<code>systime_col</code>","text":"(<code>str</code>, default:                   <code>'systime'</code> )           \u2013            <p>Name of the column representing the timestamps for the values.</p>"},{"location":"reference/ts_shape/features/cycles/cycle_processor/#ts_shape.features.cycles.cycle_processor.CycleDataProcessor.get_dataframe","title":"get_dataframe","text":"<pre><code>get_dataframe() -&gt; DataFrame\n</code></pre> <p>Returns the processed DataFrame.</p> Source code in <code>src/ts_shape/utils/base.py</code> <pre><code>def get_dataframe(self) -&gt; pd.DataFrame:\n    \"\"\"Returns the processed DataFrame.\"\"\"\n    return self.dataframe\n</code></pre>"},{"location":"reference/ts_shape/features/cycles/cycle_processor/#ts_shape.features.cycles.cycle_processor.CycleDataProcessor.group_by_cycle_uuid","title":"group_by_cycle_uuid","text":"<pre><code>group_by_cycle_uuid(data: Optional[DataFrame] = None) -&gt; List[DataFrame]\n</code></pre> <p>Group the DataFrame by the cycle_uuid column, resulting in a list of DataFrames, each containing data for one cycle.</p> <p>Parameters:</p> Return <p>List of DataFrames, each containing data for a unique cycle_uuid.</p> Source code in <code>src/ts_shape/features/cycles/cycle_processor.py</code> <pre><code>def group_by_cycle_uuid(self, data: Optional[pd.DataFrame] = None) -&gt; List[pd.DataFrame]:\n    \"\"\"\n    Group the DataFrame by the cycle_uuid column, resulting in a list of DataFrames, each containing data for one cycle.\n\n    Args:\n        data: DataFrame containing the data to be grouped by cycle_uuid. If None, uses the internal values_df.\n\n    Return:\n        List of DataFrames, each containing data for a unique cycle_uuid.\n    \"\"\"\n    if data is None:\n        data = self.values_df\n\n    grouped_dataframes = [group for _, group in data.groupby(self.cycle_uuid_col)]\n    logging.info(f\"Grouped data into {len(grouped_dataframes)} cycle UUID groups.\")\n    return grouped_dataframes\n</code></pre>"},{"location":"reference/ts_shape/features/cycles/cycle_processor/#ts_shape.features.cycles.cycle_processor.CycleDataProcessor.group_by_cycle_uuid(data)","title":"<code>data</code>","text":"(<code>Optional[DataFrame]</code>, default:                   <code>None</code> )           \u2013            <p>DataFrame containing the data to be grouped by cycle_uuid. If None, uses the internal values_df.</p>"},{"location":"reference/ts_shape/features/cycles/cycle_processor/#ts_shape.features.cycles.cycle_processor.CycleDataProcessor.merge_dataframes_by_cycle","title":"merge_dataframes_by_cycle","text":"<pre><code>merge_dataframes_by_cycle() -&gt; DataFrame\n</code></pre> <p>Merges the values DataFrame with the cycles DataFrame based on the cycle time intervals.  Appends the 'cycle_uuid' to the values DataFrame.</p> Return <p>DataFrame with an added 'cycle_uuid' column.</p> Source code in <code>src/ts_shape/features/cycles/cycle_processor.py</code> <pre><code>def merge_dataframes_by_cycle(self) -&gt; pd.DataFrame:\n    \"\"\"\n    Merges the values DataFrame with the cycles DataFrame based on the cycle time intervals. \n    Appends the 'cycle_uuid' to the values DataFrame.\n\n    Return:\n        DataFrame with an added 'cycle_uuid' column.\n    \"\"\"\n    # Merge based on systime falling within cycle_start and cycle_end\n    self.values_df[self.cycle_uuid_col] = None\n\n    for _, row in self.cycles_df.iterrows():\n        mask = (self.values_df[self.systime_col] &gt;= row['cycle_start']) &amp; (self.values_df[self.systime_col] &lt;= row['cycle_end'])\n        self.values_df.loc[mask, self.cycle_uuid_col] = row[self.cycle_uuid_col]\n\n    merged_df = self.values_df.dropna(subset=[self.cycle_uuid_col])\n    logging.info(f\"Merged DataFrame contains {len(merged_df)} records.\")\n    return merged_df\n</code></pre>"},{"location":"reference/ts_shape/features/cycles/cycle_processor/#ts_shape.features.cycles.cycle_processor.CycleDataProcessor.split_by_cycle","title":"split_by_cycle","text":"<pre><code>split_by_cycle() -&gt; Dict[str, DataFrame]\n</code></pre> <p>Splits the values DataFrame by cycles defined in the cycles DataFrame.  Each cycle is defined by a start and end time, and the corresponding values are filtered accordingly.</p> Return <p>Dictionary where keys are cycle_uuids and values are DataFrames with the corresponding cycle data.</p> Source code in <code>src/ts_shape/features/cycles/cycle_processor.py</code> <pre><code>def split_by_cycle(self) -&gt; Dict[str, pd.DataFrame]:\n    \"\"\"\n    Splits the values DataFrame by cycles defined in the cycles DataFrame. \n    Each cycle is defined by a start and end time, and the corresponding values are filtered accordingly.\n\n    Return:\n        Dictionary where keys are cycle_uuids and values are DataFrames with the corresponding cycle data.\n    \"\"\"\n    result = {}\n    for _, row in self.cycles_df.iterrows():\n        mask = (self.values_df[self.systime_col] &gt;= row['cycle_start']) &amp; (self.values_df[self.systime_col] &lt;= row['cycle_end'])\n        result[row[self.cycle_uuid_col]] = self.values_df[mask].copy()\n\n    logging.info(f\"Split {len(result)} cycles.\")\n    return result\n</code></pre>"},{"location":"reference/ts_shape/features/cycles/cycle_processor/#ts_shape.features.cycles.cycle_processor.CycleDataProcessor.split_dataframes_by_group","title":"split_dataframes_by_group","text":"<pre><code>split_dataframes_by_group(dfs: List[DataFrame], column: str) -&gt; List[DataFrame]\n</code></pre> <p>Splits a list of DataFrames by groups based on a specified column.  This function performs a groupby operation on each DataFrame in the list and then flattens the result.</p> <p>Parameters:</p> Return <p>List of DataFrames, each corresponding to a group in the original DataFrames.</p> Source code in <code>src/ts_shape/features/cycles/cycle_processor.py</code> <pre><code>def split_dataframes_by_group(self, dfs: List[pd.DataFrame], column: str) -&gt; List[pd.DataFrame]:\n    \"\"\"\n    Splits a list of DataFrames by groups based on a specified column. \n    This function performs a groupby operation on each DataFrame in the list and then flattens the result.\n\n    Args:\n        dfs: List of DataFrames to be split.\n        column: Column name to group by.\n\n    Return:\n        List of DataFrames, each corresponding to a group in the original DataFrames.\n    \"\"\"\n    split_dfs = []\n    for df in dfs:\n        groups = df.groupby(column)\n        for _, group in groups:\n            split_dfs.append(group)\n\n    logging.info(f\"Split data into {len(split_dfs)} groups based on column '{column}'.\")\n    return split_dfs\n</code></pre>"},{"location":"reference/ts_shape/features/cycles/cycle_processor/#ts_shape.features.cycles.cycle_processor.CycleDataProcessor.split_dataframes_by_group(dfs)","title":"<code>dfs</code>","text":"(<code>List[DataFrame]</code>)           \u2013            <p>List of DataFrames to be split.</p>"},{"location":"reference/ts_shape/features/cycles/cycle_processor/#ts_shape.features.cycles.cycle_processor.CycleDataProcessor.split_dataframes_by_group(column)","title":"<code>column</code>","text":"(<code>str</code>)           \u2013            <p>Column name to group by.</p>"},{"location":"reference/ts_shape/features/cycles/cycles_extractor/","title":"cycles_extractor","text":""},{"location":"reference/ts_shape/features/cycles/cycles_extractor/#ts_shape.features.cycles.cycles_extractor","title":"ts_shape.features.cycles.cycles_extractor","text":"<p>Classes:</p> <ul> <li> <code>CycleExtractor</code>           \u2013            <p>Class for processing cycles based on different criteria.</p> </li> </ul>"},{"location":"reference/ts_shape/features/cycles/cycles_extractor/#ts_shape.features.cycles.cycles_extractor.CycleExtractor","title":"CycleExtractor","text":"<pre><code>CycleExtractor(dataframe: DataFrame, start_uuid: str, end_uuid: Optional[str] = None)\n</code></pre> <p>               Bases: <code>Base</code></p> <p>Class for processing cycles based on different criteria.</p> <p>Methods:</p> <ul> <li> <code>get_dataframe</code>             \u2013              <p>Returns the processed DataFrame.</p> </li> <li> <code>process_persistent_cycle</code>             \u2013              <p>Processes cycles where the value of the variable stays true during the cycle.</p> </li> <li> <code>process_separate_start_end_cycle</code>             \u2013              <p>Processes cycles where different variables indicate cycle start and end.</p> </li> <li> <code>process_state_change_cycle</code>             \u2013              <p>Processes cycles where the start of a new cycle is the end of the previous cycle.</p> </li> <li> <code>process_step_sequence</code>             \u2013              <p>Processes cycles based on a step sequence, where specific integer values denote cycle start and end.</p> </li> <li> <code>process_trigger_cycle</code>             \u2013              <p>Processes cycles where the value of the variable goes from true to false during the cycle.</p> </li> <li> <code>process_value_change_cycle</code>             \u2013              <p>Processes cycles where a change in the value indicates a new cycle.</p> </li> </ul> Source code in <code>src/ts_shape/features/cycles/cycles_extractor.py</code> <pre><code>def __init__(self, dataframe: pd.DataFrame, start_uuid: str, end_uuid: Optional[str] = None):\n    \"\"\"Initializes the class with the data and the UUIDs for cycle start and end.\"\"\"\n    super().__init__(dataframe)\n\n    # Validate input types\n    if not isinstance(dataframe, pd.DataFrame):\n        raise ValueError(\"dataframe must be a pandas DataFrame\")\n    if not isinstance(start_uuid, str):\n        raise ValueError(\"start_uuid must be a string\")\n\n    self.df = dataframe  # Use the provided DataFrame directly\n    self.start_uuid = start_uuid\n    self.end_uuid = end_uuid if end_uuid else start_uuid\n    logging.info(f\"CycleExtractor initialized with start_uuid: {self.start_uuid} and end_uuid: {self.end_uuid}\")\n</code></pre>"},{"location":"reference/ts_shape/features/cycles/cycles_extractor/#ts_shape.features.cycles.cycles_extractor.CycleExtractor.get_dataframe","title":"get_dataframe","text":"<pre><code>get_dataframe() -&gt; DataFrame\n</code></pre> <p>Returns the processed DataFrame.</p> Source code in <code>src/ts_shape/utils/base.py</code> <pre><code>def get_dataframe(self) -&gt; pd.DataFrame:\n    \"\"\"Returns the processed DataFrame.\"\"\"\n    return self.dataframe\n</code></pre>"},{"location":"reference/ts_shape/features/cycles/cycles_extractor/#ts_shape.features.cycles.cycles_extractor.CycleExtractor.process_persistent_cycle","title":"process_persistent_cycle","text":"<pre><code>process_persistent_cycle() -&gt; DataFrame\n</code></pre> <p>Processes cycles where the value of the variable stays true during the cycle.</p> Source code in <code>src/ts_shape/features/cycles/cycles_extractor.py</code> <pre><code>def process_persistent_cycle(self) -&gt; pd.DataFrame:\n    \"\"\"Processes cycles where the value of the variable stays true during the cycle.\"\"\"\n    # Assuming dataframe is pre-filtered\n    cycle_starts = self.df[self.df['value_bool'] == True]\n    cycle_ends = self.df[self.df['value_bool'] == False]\n\n    return self._generate_cycle_dataframe(cycle_starts, cycle_ends)\n</code></pre>"},{"location":"reference/ts_shape/features/cycles/cycles_extractor/#ts_shape.features.cycles.cycles_extractor.CycleExtractor.process_separate_start_end_cycle","title":"process_separate_start_end_cycle","text":"<pre><code>process_separate_start_end_cycle() -&gt; DataFrame\n</code></pre> <p>Processes cycles where different variables indicate cycle start and end.</p> Source code in <code>src/ts_shape/features/cycles/cycles_extractor.py</code> <pre><code>def process_separate_start_end_cycle(self) -&gt; pd.DataFrame:\n    \"\"\"Processes cycles where different variables indicate cycle start and end.\"\"\"\n    # Assuming dataframe is pre-filtered for both start_uuid and end_uuid\n    cycle_starts = self.df[self.df['value_bool'] == True]\n    cycle_ends = self.df[self.df['value_bool'] == True]\n\n    return self._generate_cycle_dataframe(cycle_starts, cycle_ends)\n</code></pre>"},{"location":"reference/ts_shape/features/cycles/cycles_extractor/#ts_shape.features.cycles.cycles_extractor.CycleExtractor.process_state_change_cycle","title":"process_state_change_cycle","text":"<pre><code>process_state_change_cycle() -&gt; DataFrame\n</code></pre> <p>Processes cycles where the start of a new cycle is the end of the previous cycle.</p> Source code in <code>src/ts_shape/features/cycles/cycles_extractor.py</code> <pre><code>def process_state_change_cycle(self) -&gt; pd.DataFrame:\n    \"\"\"Processes cycles where the start of a new cycle is the end of the previous cycle.\"\"\"\n    # Assuming dataframe is pre-filtered\n    cycle_starts = self.df.copy()\n    cycle_ends = self.df.shift(-1)\n\n    return self._generate_cycle_dataframe(cycle_starts, cycle_ends)\n</code></pre>"},{"location":"reference/ts_shape/features/cycles/cycles_extractor/#ts_shape.features.cycles.cycles_extractor.CycleExtractor.process_step_sequence","title":"process_step_sequence","text":"<pre><code>process_step_sequence(start_step: int, end_step: int) -&gt; DataFrame\n</code></pre> <p>Processes cycles based on a step sequence, where specific integer values denote cycle start and end.</p> Source code in <code>src/ts_shape/features/cycles/cycles_extractor.py</code> <pre><code>def process_step_sequence(self, start_step: int, end_step: int) -&gt; pd.DataFrame:\n    \"\"\"Processes cycles based on a step sequence, where specific integer values denote cycle start and end.\"\"\"\n    # Assuming dataframe is pre-filtered\n    cycle_starts = self.df[self.df['value_integer'] == start_step]\n    cycle_ends = self.df[self.df['value_integer'] == end_step]\n\n    return self._generate_cycle_dataframe(cycle_starts, cycle_ends)\n</code></pre>"},{"location":"reference/ts_shape/features/cycles/cycles_extractor/#ts_shape.features.cycles.cycles_extractor.CycleExtractor.process_trigger_cycle","title":"process_trigger_cycle","text":"<pre><code>process_trigger_cycle() -&gt; DataFrame\n</code></pre> <p>Processes cycles where the value of the variable goes from true to false during the cycle.</p> Source code in <code>src/ts_shape/features/cycles/cycles_extractor.py</code> <pre><code>def process_trigger_cycle(self) -&gt; pd.DataFrame:\n    \"\"\"Processes cycles where the value of the variable goes from true to false during the cycle.\"\"\"\n    # Assuming dataframe is pre-filtered\n    cycle_starts = self.df[self.df['value_bool'] == True]\n    cycle_ends = self.df[self.df['value_bool'] == False].shift(-1)\n\n    return self._generate_cycle_dataframe(cycle_starts, cycle_ends)\n</code></pre>"},{"location":"reference/ts_shape/features/cycles/cycles_extractor/#ts_shape.features.cycles.cycles_extractor.CycleExtractor.process_value_change_cycle","title":"process_value_change_cycle","text":"<pre><code>process_value_change_cycle() -&gt; DataFrame\n</code></pre> <p>Processes cycles where a change in the value indicates a new cycle.</p> Source code in <code>src/ts_shape/features/cycles/cycles_extractor.py</code> <pre><code>def process_value_change_cycle(self) -&gt; pd.DataFrame:\n    \"\"\"Processes cycles where a change in the value indicates a new cycle.\"\"\"\n    # Assuming dataframe is pre-filtered\n\n    # Fill NaN or None values with appropriate defaults for diff() to work\n    self.df['value_double'] = self.df['value_double'].fillna(0)  # Assuming numeric column\n    self.df['value_bool'] = self.df['value_bool'].fillna(False)  # Assuming boolean column\n    self.df['value_string'] = self.df['value_string'].fillna('')  # Assuming string column\n    self.df['value_integer'] = self.df['value_integer'].fillna(0)  # Assuming integer column\n\n    # Detect changes across the relevant columns using diff()\n    self.df['value_change'] = (\n        (self.df['value_double'].diff().ne(0)) |\n        (self.df['value_bool'].diff().ne(0)) |\n        (self.df['value_string'].shift().ne(self.df['value_string'])) |\n        (self.df['value_integer'].diff().ne(0))\n    )\n\n    # Define cycle starts and ends based on changes\n    cycle_starts = self.df[self.df['value_change'] == True]\n    cycle_ends = self.df[self.df['value_change'] == True].shift(-1)\n\n    return self._generate_cycle_dataframe(cycle_starts, cycle_ends)\n</code></pre>"},{"location":"reference/ts_shape/features/stats/__init__/","title":"init","text":""},{"location":"reference/ts_shape/features/stats/__init__/#ts_shape.features.stats","title":"ts_shape.features.stats","text":"<p>Statistics</p> <p>Descriptive statistics for numeric, string, boolean, and timestamp columns.</p> <ul> <li>NumericStatistics: Descriptive metrics for numeric columns.</li> <li>column_mean: Mean of a column.</li> <li>column_median: Median of a column.</li> <li>column_std: Standard deviation of a column.</li> <li>column_variance: Variance of a column.</li> <li>column_min: Minimum value.</li> <li>column_max: Maximum value.</li> <li>column_sum: Sum of values.</li> <li>column_kurtosis: Kurtosis of values.</li> <li>column_skewness: Skewness of values.</li> <li>column_quantile: Quantile of a column.</li> <li>column_iqr: Interquartile range.</li> <li>column_range: Range (max - min).</li> <li>column_mad: Mean absolute deviation.</li> <li>coefficient_of_variation: Standard deviation divided by mean (guarded).</li> <li>standard_error_mean: Standard error of the mean.</li> <li>describe: Pandas describe wrapper.</li> <li>summary_as_dict: Comprehensive numeric summary as dict.</li> <li> <p>summary_as_dataframe: Comprehensive numeric summary as DataFrame.</p> </li> <li> <p>StringStatistics: Metrics for string/categorical columns.</p> </li> <li>count_unique: Number of unique strings.</li> <li>most_frequent: Most frequent string.</li> <li>count_most_frequent: Count of the most frequent string.</li> <li>count_null: Number of nulls.</li> <li>average_string_length: Average length of non-null strings.</li> <li>longest_string: Longest string.</li> <li>shortest_string: Shortest string.</li> <li>string_length_summary: Summary of lengths.</li> <li>most_common_n_strings: Top-N most frequent strings.</li> <li>contains_substring_count: Count of strings containing a substring.</li> <li>starts_with_count: Count of strings starting with a prefix.</li> <li>ends_with_count: Count of strings ending with a suffix.</li> <li>uppercase_percentage: Percentage of uppercase strings.</li> <li>lowercase_percentage: Percentage of lowercase strings.</li> <li>contains_digit_count: Count of strings containing digits.</li> <li>summary_as_dict: Comprehensive string summary as dict.</li> <li> <p>summary_as_dataframe: Comprehensive string summary as DataFrame.</p> </li> <li> <p>BooleanStatistics: Metrics for boolean columns.</p> </li> <li>count_true: Count of True values.</li> <li>count_false: Count of False values.</li> <li>count_null: Count of nulls.</li> <li>count_not_null: Count of non-nulls.</li> <li>true_percentage: Percentage True.</li> <li>false_percentage: Percentage False.</li> <li>mode: Most common boolean value.</li> <li>is_balanced: Whether distribution is 50/50.</li> <li>summary_as_dict: Summary as dict.</li> <li> <p>summary_as_dataframe: Summary as DataFrame.</p> </li> <li> <p>TimestampStatistics: Metrics for timestamp columns.</p> </li> <li>count_null: Count of null timestamps.</li> <li>count_not_null: Count of non-null timestamps.</li> <li>earliest_timestamp: Earliest timestamp.</li> <li>latest_timestamp: Latest timestamp.</li> <li>timestamp_range: Time range (latest - earliest).</li> <li>most_frequent_timestamp: Most frequent timestamp.</li> <li>count_most_frequent_timestamp: Count of the modal timestamp.</li> <li>year_distribution: Distribution by year.</li> <li>month_distribution: Distribution by month.</li> <li>weekday_distribution: Distribution by weekday.</li> <li>hour_distribution: Distribution by hour.</li> <li>most_frequent_day: Most frequent weekday.</li> <li>most_frequent_hour: Most frequent hour.</li> <li>average_time_gap: Average gap between consecutive timestamps.</li> <li>median_timestamp: Median timestamp.</li> <li>standard_deviation_timestamps: Standard deviation of consecutive differences.</li> <li>timestamp_quartiles: 25th/50th/75th percentiles.</li> <li>days_with_most_activity: Top-N active days.</li> </ul> <p>Modules:</p> <ul> <li> <code>boolean_stats</code>           \u2013            </li> <li> <code>feature_table</code>           \u2013            </li> <li> <code>numeric_stats</code>           \u2013            </li> <li> <code>string_stats</code>           \u2013            </li> <li> <code>timestamp_stats</code>           \u2013            </li> </ul>"},{"location":"reference/ts_shape/features/stats/boolean_stats/","title":"boolean_stats","text":""},{"location":"reference/ts_shape/features/stats/boolean_stats/#ts_shape.features.stats.boolean_stats","title":"ts_shape.features.stats.boolean_stats","text":"<p>Classes:</p> <ul> <li> <code>BooleanStatistics</code>           \u2013            <p>Provides class methods to calculate statistics on a boolean column in a pandas DataFrame.</p> </li> </ul>"},{"location":"reference/ts_shape/features/stats/boolean_stats/#ts_shape.features.stats.boolean_stats.BooleanStatistics","title":"BooleanStatistics","text":"<pre><code>BooleanStatistics(dataframe: DataFrame, column_name: str = 'systime')\n</code></pre> <p>               Bases: <code>Base</code></p> <p>Provides class methods to calculate statistics on a boolean column in a pandas DataFrame.</p> <p>Parameters:</p> <p>Methods:</p> <ul> <li> <code>count_false</code>             \u2013              <p>Returns the count of False values in the boolean column.</p> </li> <li> <code>count_not_null</code>             \u2013              <p>Returns the count of non-null (True or False) values in the boolean column.</p> </li> <li> <code>count_null</code>             \u2013              <p>Returns the count of null (NaN) values in the boolean column.</p> </li> <li> <code>count_true</code>             \u2013              <p>Returns the count of True values in the boolean column.</p> </li> <li> <code>false_percentage</code>             \u2013              <p>Returns the percentage of False values in the boolean column.</p> </li> <li> <code>get_dataframe</code>             \u2013              <p>Returns the processed DataFrame.</p> </li> <li> <code>is_balanced</code>             \u2013              <p>Indicates if the distribution is balanced (50% True and False) in the specified boolean column.</p> </li> <li> <code>mode</code>             \u2013              <p>Returns the mode (most common value) of the specified boolean column.</p> </li> <li> <code>summary_as_dataframe</code>             \u2013              <p>Returns a summary of boolean statistics for the specified column as a DataFrame.</p> </li> <li> <code>summary_as_dict</code>             \u2013              <p>Returns a summary of boolean statistics for the specified column as a dictionary.</p> </li> <li> <code>true_percentage</code>             \u2013              <p>Returns the percentage of True values in the boolean column.</p> </li> </ul> Source code in <code>src/ts_shape/utils/base.py</code> <pre><code>def __init__(self, dataframe: pd.DataFrame, column_name: str = 'systime') -&gt; None:\n    \"\"\"\n    Initializes the Base with a DataFrame, detects time columns, converts them to datetime,\n    and sorts the DataFrame by the specified column (or the detected time column if applicable).\n\n    Args:\n        dataframe (pd.DataFrame): The DataFrame to be processed.\n        column_name (str): The column to sort by. Default is 'systime'. If the column is not found or is not a time column, the class will attempt to detect other time columns.\n    \"\"\"\n    self.dataframe = dataframe.copy()\n\n    # Attempt to convert the specified column_name to datetime if it exists\n    if column_name in self.dataframe.columns:\n        self.dataframe[column_name] = pd.to_datetime(self.dataframe[column_name], errors='coerce')\n    else:\n        # If the column_name is not in the DataFrame, fallback to automatic time detection\n        time_columns = [col for col in self.dataframe.columns if 'time' in col.lower() or 'date' in col.lower()]\n\n        # Convert all detected time columns to datetime, if any\n        for col in time_columns:\n            self.dataframe[col] = pd.to_datetime(self.dataframe[col], errors='coerce')\n\n        # If any time columns are detected, sort by the first one; otherwise, do nothing\n        if time_columns:\n            column_name = time_columns[0]\n\n    # Sort by the datetime column (either specified or detected)\n    if column_name in self.dataframe.columns:\n        self.dataframe = self.dataframe.sort_values(by=column_name)\n</code></pre>"},{"location":"reference/ts_shape/features/stats/boolean_stats/#ts_shape.features.stats.boolean_stats.BooleanStatistics(dataframe)","title":"<code>dataframe</code>","text":"(<code>DataFrame</code>)           \u2013            <p>The DataFrame to be processed.</p>"},{"location":"reference/ts_shape/features/stats/boolean_stats/#ts_shape.features.stats.boolean_stats.BooleanStatistics(column_name)","title":"<code>column_name</code>","text":"(<code>str</code>, default:                   <code>'systime'</code> )           \u2013            <p>The column to sort by. Default is 'systime'. If the column is not found or is not a time column, the class will attempt to detect other time columns.</p>"},{"location":"reference/ts_shape/features/stats/boolean_stats/#ts_shape.features.stats.boolean_stats.BooleanStatistics.count_false","title":"count_false  <code>classmethod</code>","text":"<pre><code>count_false(dataframe: DataFrame, column_name: str = 'value_bool') -&gt; int\n</code></pre> <p>Returns the count of False values in the boolean column.</p> Source code in <code>src/ts_shape/features/stats/boolean_stats.py</code> <pre><code>@classmethod\ndef count_false(cls, dataframe: pd.DataFrame, column_name: str = 'value_bool') -&gt; int:\n    \"\"\"Returns the count of False values in the boolean column.\"\"\"\n    return (dataframe[column_name] == False).sum()\n</code></pre>"},{"location":"reference/ts_shape/features/stats/boolean_stats/#ts_shape.features.stats.boolean_stats.BooleanStatistics.count_not_null","title":"count_not_null  <code>classmethod</code>","text":"<pre><code>count_not_null(dataframe: DataFrame, column_name: str = 'value_bool') -&gt; int\n</code></pre> <p>Returns the count of non-null (True or False) values in the boolean column.</p> Source code in <code>src/ts_shape/features/stats/boolean_stats.py</code> <pre><code>@classmethod\ndef count_not_null(cls, dataframe: pd.DataFrame, column_name: str = 'value_bool') -&gt; int:\n    \"\"\"Returns the count of non-null (True or False) values in the boolean column.\"\"\"\n    return dataframe[column_name].notna().sum()\n</code></pre>"},{"location":"reference/ts_shape/features/stats/boolean_stats/#ts_shape.features.stats.boolean_stats.BooleanStatistics.count_null","title":"count_null  <code>classmethod</code>","text":"<pre><code>count_null(dataframe: DataFrame, column_name: str = 'value_bool') -&gt; int\n</code></pre> <p>Returns the count of null (NaN) values in the boolean column.</p> Source code in <code>src/ts_shape/features/stats/boolean_stats.py</code> <pre><code>@classmethod\ndef count_null(cls, dataframe: pd.DataFrame, column_name: str = 'value_bool') -&gt; int:\n    \"\"\"Returns the count of null (NaN) values in the boolean column.\"\"\"\n    return dataframe[column_name].isna().sum()\n</code></pre>"},{"location":"reference/ts_shape/features/stats/boolean_stats/#ts_shape.features.stats.boolean_stats.BooleanStatistics.count_true","title":"count_true  <code>classmethod</code>","text":"<pre><code>count_true(dataframe: DataFrame, column_name: str = 'value_bool') -&gt; int\n</code></pre> <p>Returns the count of True values in the boolean column.</p> Source code in <code>src/ts_shape/features/stats/boolean_stats.py</code> <pre><code>@classmethod\ndef count_true(cls, dataframe: pd.DataFrame, column_name: str = 'value_bool') -&gt; int:\n    \"\"\"Returns the count of True values in the boolean column.\"\"\"\n    return dataframe[column_name].sum()\n</code></pre>"},{"location":"reference/ts_shape/features/stats/boolean_stats/#ts_shape.features.stats.boolean_stats.BooleanStatistics.false_percentage","title":"false_percentage  <code>classmethod</code>","text":"<pre><code>false_percentage(dataframe: DataFrame, column_name: str = 'value_bool') -&gt; float\n</code></pre> <p>Returns the percentage of False values in the boolean column.</p> Source code in <code>src/ts_shape/features/stats/boolean_stats.py</code> <pre><code>@classmethod\ndef false_percentage(cls, dataframe: pd.DataFrame, column_name: str = 'value_bool') -&gt; float:\n    \"\"\"Returns the percentage of False values in the boolean column.\"\"\"\n    false_count = cls.count_false(dataframe, column_name)\n    total_count = cls.count_not_null(dataframe, column_name)\n    return (false_count / total_count) * 100 if total_count &gt; 0 else 0.0\n</code></pre>"},{"location":"reference/ts_shape/features/stats/boolean_stats/#ts_shape.features.stats.boolean_stats.BooleanStatistics.get_dataframe","title":"get_dataframe","text":"<pre><code>get_dataframe() -&gt; DataFrame\n</code></pre> <p>Returns the processed DataFrame.</p> Source code in <code>src/ts_shape/utils/base.py</code> <pre><code>def get_dataframe(self) -&gt; pd.DataFrame:\n    \"\"\"Returns the processed DataFrame.\"\"\"\n    return self.dataframe\n</code></pre>"},{"location":"reference/ts_shape/features/stats/boolean_stats/#ts_shape.features.stats.boolean_stats.BooleanStatistics.is_balanced","title":"is_balanced  <code>classmethod</code>","text":"<pre><code>is_balanced(dataframe: DataFrame, column_name: str) -&gt; bool\n</code></pre> <p>Indicates if the distribution is balanced (50% True and False) in the specified boolean column.</p> Source code in <code>src/ts_shape/features/stats/boolean_stats.py</code> <pre><code>@classmethod\ndef is_balanced(cls, dataframe: pd.DataFrame, column_name: str) -&gt; bool:\n    \"\"\"Indicates if the distribution is balanced (50% True and False) in the specified boolean column.\"\"\"\n    true_percentage = dataframe[column_name].mean()\n    return true_percentage == 0.5\n</code></pre>"},{"location":"reference/ts_shape/features/stats/boolean_stats/#ts_shape.features.stats.boolean_stats.BooleanStatistics.mode","title":"mode  <code>classmethod</code>","text":"<pre><code>mode(dataframe: DataFrame, column_name: str) -&gt; bool\n</code></pre> <p>Returns the mode (most common value) of the specified boolean column.</p> Source code in <code>src/ts_shape/features/stats/boolean_stats.py</code> <pre><code>@classmethod\ndef mode(cls, dataframe: pd.DataFrame, column_name: str) -&gt; bool:\n    \"\"\"Returns the mode (most common value) of the specified boolean column.\"\"\"\n    return dataframe[column_name].mode()[0]\n</code></pre>"},{"location":"reference/ts_shape/features/stats/boolean_stats/#ts_shape.features.stats.boolean_stats.BooleanStatistics.summary_as_dataframe","title":"summary_as_dataframe  <code>classmethod</code>","text":"<pre><code>summary_as_dataframe(dataframe: DataFrame, column_name: str) -&gt; DataFrame\n</code></pre> <p>Returns a summary of boolean statistics for the specified column as a DataFrame.</p> Source code in <code>src/ts_shape/features/stats/boolean_stats.py</code> <pre><code>@classmethod\ndef summary_as_dataframe(cls, dataframe: pd.DataFrame, column_name: str) -&gt; pd.DataFrame:\n    \"\"\"Returns a summary of boolean statistics for the specified column as a DataFrame.\"\"\"\n    summary_data = cls.summary_as_dict(dataframe, column_name)\n    return pd.DataFrame([summary_data])\n</code></pre>"},{"location":"reference/ts_shape/features/stats/boolean_stats/#ts_shape.features.stats.boolean_stats.BooleanStatistics.summary_as_dict","title":"summary_as_dict  <code>classmethod</code>","text":"<pre><code>summary_as_dict(dataframe: DataFrame, column_name: str) -&gt; Dict[str, Union[int, float, bool]]\n</code></pre> <p>Returns a summary of boolean statistics for the specified column as a dictionary.</p> Source code in <code>src/ts_shape/features/stats/boolean_stats.py</code> <pre><code>@classmethod\ndef summary_as_dict(cls, dataframe: pd.DataFrame, column_name: str) -&gt; Dict[str, Union[int, float, bool]]:\n    \"\"\"Returns a summary of boolean statistics for the specified column as a dictionary.\"\"\"\n    return {\n        'true_count': cls.count_true(dataframe, column_name),\n        'false_count': cls.count_false(dataframe, column_name),\n        'true_percentage': cls.true_percentage(dataframe, column_name),\n        'false_percentage': cls.false_percentage(dataframe, column_name),\n        'mode': cls.mode(dataframe, column_name),\n        'is_balanced': cls.is_balanced(dataframe, column_name)\n    }\n</code></pre>"},{"location":"reference/ts_shape/features/stats/boolean_stats/#ts_shape.features.stats.boolean_stats.BooleanStatistics.true_percentage","title":"true_percentage  <code>classmethod</code>","text":"<pre><code>true_percentage(dataframe: DataFrame, column_name: str = 'value_bool') -&gt; float\n</code></pre> <p>Returns the percentage of True values in the boolean column.</p> Source code in <code>src/ts_shape/features/stats/boolean_stats.py</code> <pre><code>@classmethod\ndef true_percentage(cls, dataframe: pd.DataFrame, column_name: str = 'value_bool') -&gt; float:\n    \"\"\"Returns the percentage of True values in the boolean column.\"\"\"\n    true_count = cls.count_true(dataframe, column_name)\n    total_count = cls.count_not_null(dataframe, column_name)\n    return (true_count / total_count) * 100 if total_count &gt; 0 else 0.0\n</code></pre>"},{"location":"reference/ts_shape/features/stats/feature_table/","title":"feature_table","text":""},{"location":"reference/ts_shape/features/stats/feature_table/#ts_shape.features.stats.feature_table","title":"ts_shape.features.stats.feature_table","text":"<p>Classes:</p> <ul> <li> <code>DescriptiveFeatures</code>           \u2013            <p>A class used to compute descriptive statistics for a DataFrame, grouped by UUID.</p> </li> </ul>"},{"location":"reference/ts_shape/features/stats/feature_table/#ts_shape.features.stats.feature_table.DescriptiveFeatures","title":"DescriptiveFeatures","text":"<pre><code>DescriptiveFeatures(dataframe: DataFrame)\n</code></pre> <p>A class used to compute descriptive statistics for a DataFrame, grouped by UUID.</p>"},{"location":"reference/ts_shape/features/stats/feature_table/#ts_shape.features.stats.feature_table.DescriptiveFeatures--attributes","title":"Attributes","text":"<p>data : pandas.DataFrame     DataFrame containing the data</p>"},{"location":"reference/ts_shape/features/stats/feature_table/#ts_shape.features.stats.feature_table.DescriptiveFeatures--methods","title":"Methods","text":"<p>compute():     Compute and return descriptive statistics for each UUID in the DataFrame.</p> <p>dataframe : pandas.DataFrame     DataFrame containing the data</p> <p>Methods:</p> <ul> <li> <code>compute</code>             \u2013              <p>Compute and return descriptive statistics for each UUID in the DataFrame.</p> </li> <li> <code>compute_per_group</code>             \u2013              <p>Compute and return statistics for each column in the DataFrame group.</p> </li> <li> <code>overall_stats</code>             \u2013              <p>Compute and return overall statistics for the DataFrame group.</p> </li> </ul> Source code in <code>src/ts_shape/features/stats/feature_table.py</code> <pre><code>def __init__(self, dataframe: pd.DataFrame):\n    \"\"\"\n    Parameters\n    ----------\n    dataframe : pandas.DataFrame\n        DataFrame containing the data\n    \"\"\"\n    self.data = dataframe\n</code></pre>"},{"location":"reference/ts_shape/features/stats/feature_table/#ts_shape.features.stats.feature_table.DescriptiveFeatures.compute","title":"compute","text":"<pre><code>compute(output_format: str = 'dict') -&gt; Union[DataFrame, Dict[str, Dict[str, Dict[str, Union[int, float, str, bool]]]]]\n</code></pre> <p>Compute and return descriptive statistics for each UUID in the DataFrame.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>Union[DataFrame, Dict[str, Dict[str, Dict[str, Union[int, float, str, bool]]]]]</code>           \u2013            <p>Union[DataFrame, dict]: A DataFrame or a nested dictionary with the UUID as the key and specific statistics related to that UUID's data type.</p> </li> </ul> Source code in <code>src/ts_shape/features/stats/feature_table.py</code> <pre><code>def compute(self, output_format: str = 'dict') -&gt; Union[pd.DataFrame, Dict[str, Dict[str, Dict[str, Union[int, float, str, bool]]]]]:\n    \"\"\"Compute and return descriptive statistics for each UUID in the DataFrame.\n\n    Args:\n        output_format (str, optional): The desired output format ('dict' or 'dataframe'). Defaults to 'dict'.\n\n    Returns:\n        Union[DataFrame, dict]: A DataFrame or a nested dictionary with the UUID as the key and specific statistics related to that UUID's data type.\n    \"\"\"\n    if output_format == 'dataframe':\n        rows_list = []\n\n        # Iterate through each group of UUID\n        for uuid, group in self.data.groupby('uuid'):\n            stats_per_group = self.compute_per_group(group)\n\n            # Iterate through the nested stats and create flat columns\n            row_dict = {}\n            for section, stats in stats_per_group.items():\n                if isinstance(stats, dict):\n                    for key, value in stats.items():\n                        if isinstance(value, dict):\n                            for sub_key, sub_value in value.items():\n                                column_name = f'{uuid}::{section}::{key}::{sub_key}'\n                                row_dict[column_name] = sub_value\n                        else:\n                            column_name = f'{uuid}::{section}::{key}'\n                            row_dict[column_name] = value\n                else:\n                    column_name = f'{uuid}::{section}'\n                    row_dict[column_name] = stats\n\n            rows_list.append(row_dict)\n\n        return pd.DataFrame(rows_list)\n\n    elif output_format == 'dict':\n        return self.data.groupby('uuid').apply(self.compute_per_group).to_dict()\n\n    else:\n        raise ValueError(\"Invalid output format. Choose either 'dict' or 'dataframe'.\")\n</code></pre>"},{"location":"reference/ts_shape/features/stats/feature_table/#ts_shape.features.stats.feature_table.DescriptiveFeatures.compute(output_format)","title":"<code>output_format</code>","text":"(<code>str</code>, default:                   <code>'dict'</code> )           \u2013            <p>The desired output format ('dict' or 'dataframe'). Defaults to 'dict'.</p>"},{"location":"reference/ts_shape/features/stats/feature_table/#ts_shape.features.stats.feature_table.DescriptiveFeatures.compute_per_group","title":"compute_per_group","text":"<pre><code>compute_per_group(group: DataFrame) -&gt; Dict[str, Dict[str, Union[int, float, str, bool]]]\n</code></pre> <p>Compute and return statistics for each column in the DataFrame group.</p> <p>Returns:</p> <ul> <li> <code>dict</code> (              <code>Dict[str, Dict[str, Union[int, float, str, bool]]]</code> )          \u2013            <p>A dictionary with overall statistics, and string, numeric, and boolean statistics per column.</p> </li> </ul> Source code in <code>src/ts_shape/features/stats/feature_table.py</code> <pre><code>def compute_per_group(self, group: pd.DataFrame) -&gt; Dict[str, Dict[str, Union[int, float, str, bool]]]:\n    \"\"\"Compute and return statistics for each column in the DataFrame group.\n\n    Returns:\n        dict: A dictionary with overall statistics, and string, numeric, and boolean statistics per column.\n    \"\"\"\n    results = {\n        'overall': self.overall_stats(group)\n    }\n    for col in group.columns:\n        if col == 'uuid':\n            continue\n        elif is_bool_dtype(group[col]):\n            # Use BooleanStatistics for boolean columns\n            results[col] = {'boolean_stats': BooleanStatistics.summary_as_dict(group, col)}\n        elif is_numeric_dtype(group[col]):\n            # Use NumericStatistics for numeric columns\n            results[col] = {'numeric_stats': NumericStatistics.summary_as_dict(group, col)}\n        elif is_object_dtype(group[col]):\n            # Use StringStatistics for string columns\n            results[col] = {'string_stats': StringStatistics.summary_as_dict(group, col)}\n\n    return results\n</code></pre>"},{"location":"reference/ts_shape/features/stats/feature_table/#ts_shape.features.stats.feature_table.DescriptiveFeatures.overall_stats","title":"overall_stats","text":"<pre><code>overall_stats(group: DataFrame) -&gt; Dict[str, Union[int, float]]\n</code></pre> <p>Compute and return overall statistics for the DataFrame group.</p> <ul> <li>total_rows: Total number of rows in the group.</li> <li>total_time: Total time difference from max and min of 'systime' column.</li> <li>is_delta_sum: Sum of the 'is_delta' column.</li> <li>is_delta_avg: Mean of the 'is_delta' column.</li> <li>is_delta_std: Standard deviation of the 'is_delta' column.</li> </ul> <p>Returns:</p> <ul> <li> <code>dict</code> (              <code>Dict[str, Union[int, float]]</code> )          \u2013            <p>A dictionary with overall statistics.</p> </li> </ul> Source code in <code>src/ts_shape/features/stats/feature_table.py</code> <pre><code>def overall_stats(self, group: pd.DataFrame) -&gt; Dict[str, Union[int, float]]:\n    \"\"\"Compute and return overall statistics for the DataFrame group.\n\n    - **total_rows**: Total number of rows in the group.\n    - **total_time**: Total time difference from max and min of 'systime' column.\n    - **is_delta_sum**: Sum of the 'is_delta' column.\n    - **is_delta_avg**: Mean of the 'is_delta' column.\n    - **is_delta_std**: Standard deviation of the 'is_delta' column.\n\n    Returns:\n        dict: A dictionary with overall statistics.\n    \"\"\"\n    statistics = {\n        'total_rows': len(group),\n        'total_time': group['systime'].max() - group['systime'].min(),\n        'is_delta_sum': group['is_delta'].sum(),\n        'is_delta_avg': group['is_delta'].mean(),\n        'is_delta_std': group['is_delta'].std(),\n    }\n    return statistics\n</code></pre>"},{"location":"reference/ts_shape/features/stats/numeric_stats/","title":"numeric_stats","text":""},{"location":"reference/ts_shape/features/stats/numeric_stats/#ts_shape.features.stats.numeric_stats","title":"ts_shape.features.stats.numeric_stats","text":"<p>Classes:</p> <ul> <li> <code>NumericStatistics</code>           \u2013            <p>Provides class methods to calculate statistics on numeric columns in a pandas DataFrame.</p> </li> </ul>"},{"location":"reference/ts_shape/features/stats/numeric_stats/#ts_shape.features.stats.numeric_stats.NumericStatistics","title":"NumericStatistics","text":"<pre><code>NumericStatistics(dataframe: DataFrame, column_name: str = 'systime')\n</code></pre> <p>               Bases: <code>Base</code></p> <p>Provides class methods to calculate statistics on numeric columns in a pandas DataFrame.</p> <p>Parameters:</p> <p>Methods:</p> <ul> <li> <code>coefficient_of_variation</code>             \u2013              <p>Calculate the coefficient of variation of the column.</p> </li> <li> <code>column_iqr</code>             \u2013              <p>Calculate the interquartile range of the column.</p> </li> <li> <code>column_kurtosis</code>             \u2013              <p>Calculate the kurtosis of a specified column.</p> </li> <li> <code>column_mad</code>             \u2013              <p>Calculate the mean absolute deviation of the column.</p> </li> <li> <code>column_max</code>             \u2013              <p>Calculate the maximum value of a specified column.</p> </li> <li> <code>column_mean</code>             \u2013              <p>Calculate the mean of a specified column.</p> </li> <li> <code>column_median</code>             \u2013              <p>Calculate the median of a specified column.</p> </li> <li> <code>column_min</code>             \u2013              <p>Calculate the minimum value of a specified column.</p> </li> <li> <code>column_quantile</code>             \u2013              <p>Calculate a specific quantile of the column.</p> </li> <li> <code>column_range</code>             \u2013              <p>Calculate the range of the column.</p> </li> <li> <code>column_skewness</code>             \u2013              <p>Calculate the skewness of a specified column.</p> </li> <li> <code>column_std</code>             \u2013              <p>Calculate the standard deviation of a specified column.</p> </li> <li> <code>column_sum</code>             \u2013              <p>Calculate the sum of a specified column.</p> </li> <li> <code>column_variance</code>             \u2013              <p>Calculate the variance of a specified column.</p> </li> <li> <code>describe</code>             \u2013              <p>Provide a statistical summary for numeric columns in the DataFrame.</p> </li> <li> <code>get_dataframe</code>             \u2013              <p>Returns the processed DataFrame.</p> </li> <li> <code>standard_error_mean</code>             \u2013              <p>Calculate the standard error of the mean for the column.</p> </li> <li> <code>summary_as_dataframe</code>             \u2013              <p>Returns a DataFrame with comprehensive numeric statistics for the specified column.</p> </li> <li> <code>summary_as_dict</code>             \u2013              <p>Returns a dictionary with comprehensive numeric statistics for the specified column.</p> </li> </ul> Source code in <code>src/ts_shape/utils/base.py</code> <pre><code>def __init__(self, dataframe: pd.DataFrame, column_name: str = 'systime') -&gt; None:\n    \"\"\"\n    Initializes the Base with a DataFrame, detects time columns, converts them to datetime,\n    and sorts the DataFrame by the specified column (or the detected time column if applicable).\n\n    Args:\n        dataframe (pd.DataFrame): The DataFrame to be processed.\n        column_name (str): The column to sort by. Default is 'systime'. If the column is not found or is not a time column, the class will attempt to detect other time columns.\n    \"\"\"\n    self.dataframe = dataframe.copy()\n\n    # Attempt to convert the specified column_name to datetime if it exists\n    if column_name in self.dataframe.columns:\n        self.dataframe[column_name] = pd.to_datetime(self.dataframe[column_name], errors='coerce')\n    else:\n        # If the column_name is not in the DataFrame, fallback to automatic time detection\n        time_columns = [col for col in self.dataframe.columns if 'time' in col.lower() or 'date' in col.lower()]\n\n        # Convert all detected time columns to datetime, if any\n        for col in time_columns:\n            self.dataframe[col] = pd.to_datetime(self.dataframe[col], errors='coerce')\n\n        # If any time columns are detected, sort by the first one; otherwise, do nothing\n        if time_columns:\n            column_name = time_columns[0]\n\n    # Sort by the datetime column (either specified or detected)\n    if column_name in self.dataframe.columns:\n        self.dataframe = self.dataframe.sort_values(by=column_name)\n</code></pre>"},{"location":"reference/ts_shape/features/stats/numeric_stats/#ts_shape.features.stats.numeric_stats.NumericStatistics(dataframe)","title":"<code>dataframe</code>","text":"(<code>DataFrame</code>)           \u2013            <p>The DataFrame to be processed.</p>"},{"location":"reference/ts_shape/features/stats/numeric_stats/#ts_shape.features.stats.numeric_stats.NumericStatistics(column_name)","title":"<code>column_name</code>","text":"(<code>str</code>, default:                   <code>'systime'</code> )           \u2013            <p>The column to sort by. Default is 'systime'. If the column is not found or is not a time column, the class will attempt to detect other time columns.</p>"},{"location":"reference/ts_shape/features/stats/numeric_stats/#ts_shape.features.stats.numeric_stats.NumericStatistics.coefficient_of_variation","title":"coefficient_of_variation  <code>classmethod</code>","text":"<pre><code>coefficient_of_variation(dataframe: DataFrame, column_name: str) -&gt; float\n</code></pre> <p>Calculate the coefficient of variation of the column.</p> Source code in <code>src/ts_shape/features/stats/numeric_stats.py</code> <pre><code>@classmethod\ndef coefficient_of_variation(cls, dataframe: pd.DataFrame, column_name: str) -&gt; float:\n    \"\"\"Calculate the coefficient of variation of the column.\"\"\"\n    mean = cls.column_mean(dataframe, column_name)\n    return cls.column_std(dataframe, column_name) / mean if mean != 0 else None\n</code></pre>"},{"location":"reference/ts_shape/features/stats/numeric_stats/#ts_shape.features.stats.numeric_stats.NumericStatistics.column_iqr","title":"column_iqr  <code>classmethod</code>","text":"<pre><code>column_iqr(dataframe: DataFrame, column_name: str) -&gt; float\n</code></pre> <p>Calculate the interquartile range of the column.</p> Source code in <code>src/ts_shape/features/stats/numeric_stats.py</code> <pre><code>@classmethod\ndef column_iqr(cls, dataframe: pd.DataFrame, column_name: str) -&gt; float:\n    \"\"\"Calculate the interquartile range of the column.\"\"\"\n    return stats.iqr(dataframe[column_name])\n</code></pre>"},{"location":"reference/ts_shape/features/stats/numeric_stats/#ts_shape.features.stats.numeric_stats.NumericStatistics.column_kurtosis","title":"column_kurtosis  <code>classmethod</code>","text":"<pre><code>column_kurtosis(dataframe: DataFrame, column_name: str) -&gt; float\n</code></pre> <p>Calculate the kurtosis of a specified column.</p> Source code in <code>src/ts_shape/features/stats/numeric_stats.py</code> <pre><code>@classmethod\ndef column_kurtosis(cls, dataframe: pd.DataFrame, column_name: str) -&gt; float:\n    \"\"\"Calculate the kurtosis of a specified column.\"\"\"\n    return dataframe[column_name].kurt()\n</code></pre>"},{"location":"reference/ts_shape/features/stats/numeric_stats/#ts_shape.features.stats.numeric_stats.NumericStatistics.column_mad","title":"column_mad  <code>classmethod</code>","text":"<pre><code>column_mad(dataframe: DataFrame, column_name: str) -&gt; float\n</code></pre> <p>Calculate the mean absolute deviation of the column.</p> Source code in <code>src/ts_shape/features/stats/numeric_stats.py</code> <pre><code>@classmethod\ndef column_mad(cls, dataframe: pd.DataFrame, column_name: str) -&gt; float:\n    \"\"\"Calculate the mean absolute deviation of the column.\"\"\"\n    return dataframe[column_name].mad()\n</code></pre>"},{"location":"reference/ts_shape/features/stats/numeric_stats/#ts_shape.features.stats.numeric_stats.NumericStatistics.column_max","title":"column_max  <code>classmethod</code>","text":"<pre><code>column_max(dataframe: DataFrame, column_name: str) -&gt; float\n</code></pre> <p>Calculate the maximum value of a specified column.</p> Source code in <code>src/ts_shape/features/stats/numeric_stats.py</code> <pre><code>@classmethod\ndef column_max(cls, dataframe: pd.DataFrame, column_name: str) -&gt; float:\n    \"\"\"Calculate the maximum value of a specified column.\"\"\"\n    return dataframe[column_name].max()\n</code></pre>"},{"location":"reference/ts_shape/features/stats/numeric_stats/#ts_shape.features.stats.numeric_stats.NumericStatistics.column_mean","title":"column_mean  <code>classmethod</code>","text":"<pre><code>column_mean(dataframe: DataFrame, column_name: str) -&gt; float\n</code></pre> <p>Calculate the mean of a specified column.</p> Source code in <code>src/ts_shape/features/stats/numeric_stats.py</code> <pre><code>@classmethod\ndef column_mean(cls, dataframe: pd.DataFrame, column_name: str) -&gt; float:\n    \"\"\"Calculate the mean of a specified column.\"\"\"\n    return dataframe[column_name].mean()\n</code></pre>"},{"location":"reference/ts_shape/features/stats/numeric_stats/#ts_shape.features.stats.numeric_stats.NumericStatistics.column_median","title":"column_median  <code>classmethod</code>","text":"<pre><code>column_median(dataframe: DataFrame, column_name: str) -&gt; float\n</code></pre> <p>Calculate the median of a specified column.</p> Source code in <code>src/ts_shape/features/stats/numeric_stats.py</code> <pre><code>@classmethod\ndef column_median(cls, dataframe: pd.DataFrame, column_name: str) -&gt; float:\n    \"\"\"Calculate the median of a specified column.\"\"\"\n    return dataframe[column_name].median()\n</code></pre>"},{"location":"reference/ts_shape/features/stats/numeric_stats/#ts_shape.features.stats.numeric_stats.NumericStatistics.column_min","title":"column_min  <code>classmethod</code>","text":"<pre><code>column_min(dataframe: DataFrame, column_name: str) -&gt; float\n</code></pre> <p>Calculate the minimum value of a specified column.</p> Source code in <code>src/ts_shape/features/stats/numeric_stats.py</code> <pre><code>@classmethod\ndef column_min(cls, dataframe: pd.DataFrame, column_name: str) -&gt; float:\n    \"\"\"Calculate the minimum value of a specified column.\"\"\"\n    return dataframe[column_name].min()\n</code></pre>"},{"location":"reference/ts_shape/features/stats/numeric_stats/#ts_shape.features.stats.numeric_stats.NumericStatistics.column_quantile","title":"column_quantile  <code>classmethod</code>","text":"<pre><code>column_quantile(dataframe: DataFrame, column_name: str, quantile: float) -&gt; float\n</code></pre> <p>Calculate a specific quantile of the column.</p> Source code in <code>src/ts_shape/features/stats/numeric_stats.py</code> <pre><code>@classmethod\ndef column_quantile(cls, dataframe: pd.DataFrame, column_name: str, quantile: float) -&gt; float:\n    \"\"\"Calculate a specific quantile of the column.\"\"\"\n    return dataframe[column_name].quantile(quantile)\n</code></pre>"},{"location":"reference/ts_shape/features/stats/numeric_stats/#ts_shape.features.stats.numeric_stats.NumericStatistics.column_range","title":"column_range  <code>classmethod</code>","text":"<pre><code>column_range(dataframe: DataFrame, column_name: str) -&gt; float\n</code></pre> <p>Calculate the range of the column.</p> Source code in <code>src/ts_shape/features/stats/numeric_stats.py</code> <pre><code>@classmethod\ndef column_range(cls, dataframe: pd.DataFrame, column_name: str) -&gt; float:\n    \"\"\"Calculate the range of the column.\"\"\"\n    return cls.column_max(dataframe, column_name) - cls.column_min(dataframe, column_name)\n</code></pre>"},{"location":"reference/ts_shape/features/stats/numeric_stats/#ts_shape.features.stats.numeric_stats.NumericStatistics.column_skewness","title":"column_skewness  <code>classmethod</code>","text":"<pre><code>column_skewness(dataframe: DataFrame, column_name: str) -&gt; float\n</code></pre> <p>Calculate the skewness of a specified column.</p> Source code in <code>src/ts_shape/features/stats/numeric_stats.py</code> <pre><code>@classmethod\ndef column_skewness(cls, dataframe: pd.DataFrame, column_name: str) -&gt; float:\n    \"\"\"Calculate the skewness of a specified column.\"\"\"\n    return dataframe[column_name].skew()\n</code></pre>"},{"location":"reference/ts_shape/features/stats/numeric_stats/#ts_shape.features.stats.numeric_stats.NumericStatistics.column_std","title":"column_std  <code>classmethod</code>","text":"<pre><code>column_std(dataframe: DataFrame, column_name: str) -&gt; float\n</code></pre> <p>Calculate the standard deviation of a specified column.</p> Source code in <code>src/ts_shape/features/stats/numeric_stats.py</code> <pre><code>@classmethod\ndef column_std(cls, dataframe: pd.DataFrame, column_name: str) -&gt; float:\n    \"\"\"Calculate the standard deviation of a specified column.\"\"\"\n    return dataframe[column_name].std()\n</code></pre>"},{"location":"reference/ts_shape/features/stats/numeric_stats/#ts_shape.features.stats.numeric_stats.NumericStatistics.column_sum","title":"column_sum  <code>classmethod</code>","text":"<pre><code>column_sum(dataframe: DataFrame, column_name: str) -&gt; float\n</code></pre> <p>Calculate the sum of a specified column.</p> Source code in <code>src/ts_shape/features/stats/numeric_stats.py</code> <pre><code>@classmethod\ndef column_sum(cls, dataframe: pd.DataFrame, column_name: str) -&gt; float:\n    \"\"\"Calculate the sum of a specified column.\"\"\"\n    return dataframe[column_name].sum()\n</code></pre>"},{"location":"reference/ts_shape/features/stats/numeric_stats/#ts_shape.features.stats.numeric_stats.NumericStatistics.column_variance","title":"column_variance  <code>classmethod</code>","text":"<pre><code>column_variance(dataframe: DataFrame, column_name: str) -&gt; float\n</code></pre> <p>Calculate the variance of a specified column.</p> Source code in <code>src/ts_shape/features/stats/numeric_stats.py</code> <pre><code>@classmethod\ndef column_variance(cls, dataframe: pd.DataFrame, column_name: str) -&gt; float:\n    \"\"\"Calculate the variance of a specified column.\"\"\"\n    return dataframe[column_name].var()\n</code></pre>"},{"location":"reference/ts_shape/features/stats/numeric_stats/#ts_shape.features.stats.numeric_stats.NumericStatistics.describe","title":"describe  <code>classmethod</code>","text":"<pre><code>describe(dataframe: DataFrame) -&gt; DataFrame\n</code></pre> <p>Provide a statistical summary for numeric columns in the DataFrame.</p> Source code in <code>src/ts_shape/features/stats/numeric_stats.py</code> <pre><code>@classmethod\ndef describe(cls, dataframe: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Provide a statistical summary for numeric columns in the DataFrame.\"\"\"\n    return dataframe.describe()\n</code></pre>"},{"location":"reference/ts_shape/features/stats/numeric_stats/#ts_shape.features.stats.numeric_stats.NumericStatistics.get_dataframe","title":"get_dataframe","text":"<pre><code>get_dataframe() -&gt; DataFrame\n</code></pre> <p>Returns the processed DataFrame.</p> Source code in <code>src/ts_shape/utils/base.py</code> <pre><code>def get_dataframe(self) -&gt; pd.DataFrame:\n    \"\"\"Returns the processed DataFrame.\"\"\"\n    return self.dataframe\n</code></pre>"},{"location":"reference/ts_shape/features/stats/numeric_stats/#ts_shape.features.stats.numeric_stats.NumericStatistics.standard_error_mean","title":"standard_error_mean  <code>classmethod</code>","text":"<pre><code>standard_error_mean(dataframe: DataFrame, column_name: str) -&gt; float\n</code></pre> <p>Calculate the standard error of the mean for the column.</p> Source code in <code>src/ts_shape/features/stats/numeric_stats.py</code> <pre><code>@classmethod\ndef standard_error_mean(cls, dataframe: pd.DataFrame, column_name: str) -&gt; float:\n    \"\"\"Calculate the standard error of the mean for the column.\"\"\"\n    return dataframe[column_name].sem()\n</code></pre>"},{"location":"reference/ts_shape/features/stats/numeric_stats/#ts_shape.features.stats.numeric_stats.NumericStatistics.summary_as_dataframe","title":"summary_as_dataframe  <code>classmethod</code>","text":"<pre><code>summary_as_dataframe(dataframe: DataFrame, column_name: str) -&gt; DataFrame\n</code></pre> <p>Returns a DataFrame with comprehensive numeric statistics for the specified column.</p> Source code in <code>src/ts_shape/features/stats/numeric_stats.py</code> <pre><code>@classmethod\ndef summary_as_dataframe(cls, dataframe: pd.DataFrame, column_name: str) -&gt; pd.DataFrame:\n    \"\"\"Returns a DataFrame with comprehensive numeric statistics for the specified column.\"\"\"\n    summary_data = cls.summary_as_dict(dataframe, column_name)\n    return pd.DataFrame([summary_data])\n</code></pre>"},{"location":"reference/ts_shape/features/stats/numeric_stats/#ts_shape.features.stats.numeric_stats.NumericStatistics.summary_as_dict","title":"summary_as_dict  <code>classmethod</code>","text":"<pre><code>summary_as_dict(dataframe: DataFrame, column_name: str) -&gt; Dict[str, Union[float, int]]\n</code></pre> <p>Returns a dictionary with comprehensive numeric statistics for the specified column.</p> Source code in <code>src/ts_shape/features/stats/numeric_stats.py</code> <pre><code>@classmethod\ndef summary_as_dict(cls, dataframe: pd.DataFrame, column_name: str) -&gt; Dict[str, Union[float, int]]:\n    \"\"\"Returns a dictionary with comprehensive numeric statistics for the specified column.\"\"\"\n    series = dataframe[column_name]\n    return {\n        'min': cls.column_min(dataframe, column_name),\n        'max': cls.column_max(dataframe, column_name),\n        'mean': cls.column_mean(dataframe, column_name),\n        'median': cls.column_median(dataframe, column_name),\n        'std': cls.column_std(dataframe, column_name),\n        'var': cls.column_variance(dataframe, column_name),\n        'sum': cls.column_sum(dataframe, column_name),\n        'kurtosis': cls.column_kurtosis(dataframe, column_name),\n        'skewness': cls.column_skewness(dataframe, column_name),\n        'q1': cls.column_quantile(dataframe, column_name, 0.25),\n        'q3': cls.column_quantile(dataframe, column_name, 0.75),\n        'iqr': cls.column_iqr(dataframe, column_name),\n        'range': cls.column_range(dataframe, column_name),\n        'mad': cls.column_mad(dataframe, column_name),\n        'coeff_var': cls.coefficient_of_variation(dataframe, column_name),\n        'sem': cls.standard_error_mean(dataframe, column_name),\n        'mode': cls.column_mode(dataframe, column_name),\n        'percentile_90': cls.column_quantile(dataframe, column_name, 0.90),\n        'percentile_10': cls.column_quantile(dataframe, column_name, 0.10),\n    }\n</code></pre>"},{"location":"reference/ts_shape/features/stats/string_stats/","title":"string_stats","text":""},{"location":"reference/ts_shape/features/stats/string_stats/#ts_shape.features.stats.string_stats","title":"ts_shape.features.stats.string_stats","text":"<p>Classes:</p> <ul> <li> <code>StringStatistics</code>           \u2013            <p>Provides class methods to calculate statistics on string columns in a pandas DataFrame.</p> </li> </ul>"},{"location":"reference/ts_shape/features/stats/string_stats/#ts_shape.features.stats.string_stats.StringStatistics","title":"StringStatistics","text":"<pre><code>StringStatistics(dataframe: DataFrame, column_name: str = 'systime')\n</code></pre> <p>               Bases: <code>Base</code></p> <p>Provides class methods to calculate statistics on string columns in a pandas DataFrame.</p> <p>Parameters:</p> <p>Methods:</p> <ul> <li> <code>average_string_length</code>             \u2013              <p>Returns the average length of strings in the column, excluding null values.</p> </li> <li> <code>contains_digit_count</code>             \u2013              <p>Counts how many strings contain digits.</p> </li> <li> <code>contains_substring_count</code>             \u2013              <p>Counts how many strings contain the specified substring.</p> </li> <li> <code>count_most_frequent</code>             \u2013              <p>Returns the count of the most frequent string in the column.</p> </li> <li> <code>count_null</code>             \u2013              <p>Returns the number of null (NaN) values in the column.</p> </li> <li> <code>count_unique</code>             \u2013              <p>Returns the number of unique strings in the column.</p> </li> <li> <code>ends_with_count</code>             \u2013              <p>Counts how many strings end with the specified suffix.</p> </li> <li> <code>get_dataframe</code>             \u2013              <p>Returns the processed DataFrame.</p> </li> <li> <code>longest_string</code>             \u2013              <p>Returns the longest string in the column.</p> </li> <li> <code>lowercase_percentage</code>             \u2013              <p>Returns the percentage of strings that are fully lowercase.</p> </li> <li> <code>most_common_n_strings</code>             \u2013              <p>Returns the top N most frequent strings in the column.</p> </li> <li> <code>most_frequent</code>             \u2013              <p>Returns the most frequent string in the column.</p> </li> <li> <code>shortest_string</code>             \u2013              <p>Returns the shortest string in the column.</p> </li> <li> <code>starts_with_count</code>             \u2013              <p>Counts how many strings start with the specified prefix.</p> </li> <li> <code>string_length_summary</code>             \u2013              <p>Returns a summary of string lengths, including min, max, and average lengths.</p> </li> <li> <code>summary_as_dataframe</code>             \u2013              <p>Returns a DataFrame with comprehensive string statistics for the specified column.</p> </li> <li> <code>summary_as_dict</code>             \u2013              <p>Returns a dictionary with comprehensive string statistics for the specified column.</p> </li> <li> <code>uppercase_percentage</code>             \u2013              <p>Returns the percentage of strings that are fully uppercase.</p> </li> </ul> Source code in <code>src/ts_shape/utils/base.py</code> <pre><code>def __init__(self, dataframe: pd.DataFrame, column_name: str = 'systime') -&gt; None:\n    \"\"\"\n    Initializes the Base with a DataFrame, detects time columns, converts them to datetime,\n    and sorts the DataFrame by the specified column (or the detected time column if applicable).\n\n    Args:\n        dataframe (pd.DataFrame): The DataFrame to be processed.\n        column_name (str): The column to sort by. Default is 'systime'. If the column is not found or is not a time column, the class will attempt to detect other time columns.\n    \"\"\"\n    self.dataframe = dataframe.copy()\n\n    # Attempt to convert the specified column_name to datetime if it exists\n    if column_name in self.dataframe.columns:\n        self.dataframe[column_name] = pd.to_datetime(self.dataframe[column_name], errors='coerce')\n    else:\n        # If the column_name is not in the DataFrame, fallback to automatic time detection\n        time_columns = [col for col in self.dataframe.columns if 'time' in col.lower() or 'date' in col.lower()]\n\n        # Convert all detected time columns to datetime, if any\n        for col in time_columns:\n            self.dataframe[col] = pd.to_datetime(self.dataframe[col], errors='coerce')\n\n        # If any time columns are detected, sort by the first one; otherwise, do nothing\n        if time_columns:\n            column_name = time_columns[0]\n\n    # Sort by the datetime column (either specified or detected)\n    if column_name in self.dataframe.columns:\n        self.dataframe = self.dataframe.sort_values(by=column_name)\n</code></pre>"},{"location":"reference/ts_shape/features/stats/string_stats/#ts_shape.features.stats.string_stats.StringStatistics(dataframe)","title":"<code>dataframe</code>","text":"(<code>DataFrame</code>)           \u2013            <p>The DataFrame to be processed.</p>"},{"location":"reference/ts_shape/features/stats/string_stats/#ts_shape.features.stats.string_stats.StringStatistics(column_name)","title":"<code>column_name</code>","text":"(<code>str</code>, default:                   <code>'systime'</code> )           \u2013            <p>The column to sort by. Default is 'systime'. If the column is not found or is not a time column, the class will attempt to detect other time columns.</p>"},{"location":"reference/ts_shape/features/stats/string_stats/#ts_shape.features.stats.string_stats.StringStatistics.average_string_length","title":"average_string_length  <code>classmethod</code>","text":"<pre><code>average_string_length(dataframe: DataFrame, column_name: str = 'value_string') -&gt; float\n</code></pre> <p>Returns the average length of strings in the column, excluding null values.</p> Source code in <code>src/ts_shape/features/stats/string_stats.py</code> <pre><code>@classmethod\ndef average_string_length(cls, dataframe: pd.DataFrame, column_name: str = 'value_string') -&gt; float:\n    \"\"\"Returns the average length of strings in the column, excluding null values.\"\"\"\n    return dataframe[column_name].dropna().str.len().mean()\n</code></pre>"},{"location":"reference/ts_shape/features/stats/string_stats/#ts_shape.features.stats.string_stats.StringStatistics.contains_digit_count","title":"contains_digit_count  <code>classmethod</code>","text":"<pre><code>contains_digit_count(dataframe: DataFrame, column_name: str = 'value_string') -&gt; int\n</code></pre> <p>Counts how many strings contain digits.</p> Source code in <code>src/ts_shape/features/stats/string_stats.py</code> <pre><code>@classmethod\ndef contains_digit_count(cls, dataframe: pd.DataFrame, column_name: str = 'value_string') -&gt; int:\n    \"\"\"Counts how many strings contain digits.\"\"\"\n    return dataframe[column_name].dropna().str.contains(r'\\d').sum()\n</code></pre>"},{"location":"reference/ts_shape/features/stats/string_stats/#ts_shape.features.stats.string_stats.StringStatistics.contains_substring_count","title":"contains_substring_count  <code>classmethod</code>","text":"<pre><code>contains_substring_count(dataframe: DataFrame, substring: str, column_name: str = 'value_string') -&gt; int\n</code></pre> <p>Counts how many strings contain the specified substring.</p> Source code in <code>src/ts_shape/features/stats/string_stats.py</code> <pre><code>@classmethod\ndef contains_substring_count(cls, dataframe: pd.DataFrame, substring: str, column_name: str = 'value_string') -&gt; int:\n    \"\"\"Counts how many strings contain the specified substring.\"\"\"\n    return dataframe[column_name].dropna().str.contains(substring).sum()\n</code></pre>"},{"location":"reference/ts_shape/features/stats/string_stats/#ts_shape.features.stats.string_stats.StringStatistics.count_most_frequent","title":"count_most_frequent  <code>classmethod</code>","text":"<pre><code>count_most_frequent(dataframe: DataFrame, column_name: str = 'value_string') -&gt; int\n</code></pre> <p>Returns the count of the most frequent string in the column.</p> Source code in <code>src/ts_shape/features/stats/string_stats.py</code> <pre><code>@classmethod\ndef count_most_frequent(cls, dataframe: pd.DataFrame, column_name: str = 'value_string') -&gt; int:\n    \"\"\"Returns the count of the most frequent string in the column.\"\"\"\n    most_frequent_value = cls.most_frequent(dataframe, column_name)\n    return dataframe[column_name].value_counts().loc[most_frequent_value]\n</code></pre>"},{"location":"reference/ts_shape/features/stats/string_stats/#ts_shape.features.stats.string_stats.StringStatistics.count_null","title":"count_null  <code>classmethod</code>","text":"<pre><code>count_null(dataframe: DataFrame, column_name: str = 'value_string') -&gt; int\n</code></pre> <p>Returns the number of null (NaN) values in the column.</p> Source code in <code>src/ts_shape/features/stats/string_stats.py</code> <pre><code>@classmethod\ndef count_null(cls, dataframe: pd.DataFrame, column_name: str = 'value_string') -&gt; int:\n    \"\"\"Returns the number of null (NaN) values in the column.\"\"\"\n    return dataframe[column_name].isna().sum()\n</code></pre>"},{"location":"reference/ts_shape/features/stats/string_stats/#ts_shape.features.stats.string_stats.StringStatistics.count_unique","title":"count_unique  <code>classmethod</code>","text":"<pre><code>count_unique(dataframe: DataFrame, column_name: str = 'value_string') -&gt; int\n</code></pre> <p>Returns the number of unique strings in the column.</p> Source code in <code>src/ts_shape/features/stats/string_stats.py</code> <pre><code>@classmethod\ndef count_unique(cls, dataframe: pd.DataFrame, column_name: str = 'value_string') -&gt; int:\n    \"\"\"Returns the number of unique strings in the column.\"\"\"\n    return dataframe[column_name].nunique()\n</code></pre>"},{"location":"reference/ts_shape/features/stats/string_stats/#ts_shape.features.stats.string_stats.StringStatistics.ends_with_count","title":"ends_with_count  <code>classmethod</code>","text":"<pre><code>ends_with_count(dataframe: DataFrame, suffix: str, column_name: str = 'value_string') -&gt; int\n</code></pre> <p>Counts how many strings end with the specified suffix.</p> Source code in <code>src/ts_shape/features/stats/string_stats.py</code> <pre><code>@classmethod\ndef ends_with_count(cls, dataframe: pd.DataFrame, suffix: str, column_name: str = 'value_string') -&gt; int:\n    \"\"\"Counts how many strings end with the specified suffix.\"\"\"\n    return dataframe[column_name].dropna().str.endswith(suffix).sum()\n</code></pre>"},{"location":"reference/ts_shape/features/stats/string_stats/#ts_shape.features.stats.string_stats.StringStatistics.get_dataframe","title":"get_dataframe","text":"<pre><code>get_dataframe() -&gt; DataFrame\n</code></pre> <p>Returns the processed DataFrame.</p> Source code in <code>src/ts_shape/utils/base.py</code> <pre><code>def get_dataframe(self) -&gt; pd.DataFrame:\n    \"\"\"Returns the processed DataFrame.\"\"\"\n    return self.dataframe\n</code></pre>"},{"location":"reference/ts_shape/features/stats/string_stats/#ts_shape.features.stats.string_stats.StringStatistics.longest_string","title":"longest_string  <code>classmethod</code>","text":"<pre><code>longest_string(dataframe: DataFrame, column_name: str = 'value_string') -&gt; str\n</code></pre> <p>Returns the longest string in the column.</p> Source code in <code>src/ts_shape/features/stats/string_stats.py</code> <pre><code>@classmethod\ndef longest_string(cls, dataframe: pd.DataFrame, column_name: str = 'value_string') -&gt; str:\n    \"\"\"Returns the longest string in the column.\"\"\"\n    return dataframe[column_name].dropna().loc[dataframe[column_name].dropna().str.len().idxmax()]\n</code></pre>"},{"location":"reference/ts_shape/features/stats/string_stats/#ts_shape.features.stats.string_stats.StringStatistics.lowercase_percentage","title":"lowercase_percentage  <code>classmethod</code>","text":"<pre><code>lowercase_percentage(dataframe: DataFrame, column_name: str = 'value_string') -&gt; float\n</code></pre> <p>Returns the percentage of strings that are fully lowercase.</p> Source code in <code>src/ts_shape/features/stats/string_stats.py</code> <pre><code>@classmethod\ndef lowercase_percentage(cls, dataframe: pd.DataFrame, column_name: str = 'value_string') -&gt; float:\n    \"\"\"Returns the percentage of strings that are fully lowercase.\"\"\"\n    total_non_null = dataframe[column_name].notna().sum()\n    if total_non_null == 0:\n        return 0.0\n    lowercase_count = dataframe[column_name].dropna().str.islower().sum()\n    return (lowercase_count / total_non_null) * 100\n</code></pre>"},{"location":"reference/ts_shape/features/stats/string_stats/#ts_shape.features.stats.string_stats.StringStatistics.most_common_n_strings","title":"most_common_n_strings  <code>classmethod</code>","text":"<pre><code>most_common_n_strings(dataframe: DataFrame, n: int, column_name: str = 'value_string') -&gt; Series\n</code></pre> <p>Returns the top N most frequent strings in the column.</p> Source code in <code>src/ts_shape/features/stats/string_stats.py</code> <pre><code>@classmethod\ndef most_common_n_strings(cls, dataframe: pd.DataFrame, n: int, column_name: str = 'value_string') -&gt; pd.Series:\n    \"\"\"Returns the top N most frequent strings in the column.\"\"\"\n    return dataframe[column_name].value_counts().head(n)\n</code></pre>"},{"location":"reference/ts_shape/features/stats/string_stats/#ts_shape.features.stats.string_stats.StringStatistics.most_frequent","title":"most_frequent  <code>classmethod</code>","text":"<pre><code>most_frequent(dataframe: DataFrame, column_name: str = 'value_string') -&gt; str\n</code></pre> <p>Returns the most frequent string in the column.</p> Source code in <code>src/ts_shape/features/stats/string_stats.py</code> <pre><code>@classmethod\ndef most_frequent(cls, dataframe: pd.DataFrame, column_name: str = 'value_string') -&gt; str:\n    \"\"\"Returns the most frequent string in the column.\"\"\"\n    return dataframe[column_name].mode().iloc[0]\n</code></pre>"},{"location":"reference/ts_shape/features/stats/string_stats/#ts_shape.features.stats.string_stats.StringStatistics.shortest_string","title":"shortest_string  <code>classmethod</code>","text":"<pre><code>shortest_string(dataframe: DataFrame, column_name: str = 'value_string') -&gt; str\n</code></pre> <p>Returns the shortest string in the column.</p> Source code in <code>src/ts_shape/features/stats/string_stats.py</code> <pre><code>@classmethod\ndef shortest_string(cls, dataframe: pd.DataFrame, column_name: str = 'value_string') -&gt; str:\n    \"\"\"Returns the shortest string in the column.\"\"\"\n    return dataframe[column_name].dropna().loc[dataframe[column_name].dropna().str.len().idxmin()]\n</code></pre>"},{"location":"reference/ts_shape/features/stats/string_stats/#ts_shape.features.stats.string_stats.StringStatistics.starts_with_count","title":"starts_with_count  <code>classmethod</code>","text":"<pre><code>starts_with_count(dataframe: DataFrame, prefix: str, column_name: str = 'value_string') -&gt; int\n</code></pre> <p>Counts how many strings start with the specified prefix.</p> Source code in <code>src/ts_shape/features/stats/string_stats.py</code> <pre><code>@classmethod\ndef starts_with_count(cls, dataframe: pd.DataFrame, prefix: str, column_name: str = 'value_string') -&gt; int:\n    \"\"\"Counts how many strings start with the specified prefix.\"\"\"\n    return dataframe[column_name].dropna().str.startswith(prefix).sum()\n</code></pre>"},{"location":"reference/ts_shape/features/stats/string_stats/#ts_shape.features.stats.string_stats.StringStatistics.string_length_summary","title":"string_length_summary  <code>classmethod</code>","text":"<pre><code>string_length_summary(dataframe: DataFrame, column_name: str = 'value_string') -&gt; DataFrame\n</code></pre> <p>Returns a summary of string lengths, including min, max, and average lengths.</p> Source code in <code>src/ts_shape/features/stats/string_stats.py</code> <pre><code>@classmethod\ndef string_length_summary(cls, dataframe: pd.DataFrame, column_name: str = 'value_string') -&gt; pd.DataFrame:\n    \"\"\"Returns a summary of string lengths, including min, max, and average lengths.\"\"\"\n    lengths = dataframe[column_name].dropna().str.len()\n    return pd.DataFrame({\n        'Min Length': [lengths.min()],\n        'Max Length': [lengths.max()],\n        'Average Length': [lengths.mean()]\n    })\n</code></pre>"},{"location":"reference/ts_shape/features/stats/string_stats/#ts_shape.features.stats.string_stats.StringStatistics.summary_as_dataframe","title":"summary_as_dataframe  <code>classmethod</code>","text":"<pre><code>summary_as_dataframe(dataframe: DataFrame, column_name: str) -&gt; DataFrame\n</code></pre> <p>Returns a DataFrame with comprehensive string statistics for the specified column.</p> Source code in <code>src/ts_shape/features/stats/string_stats.py</code> <pre><code>@classmethod\ndef summary_as_dataframe(cls, dataframe: pd.DataFrame, column_name: str) -&gt; pd.DataFrame:\n    \"\"\"Returns a DataFrame with comprehensive string statistics for the specified column.\"\"\"\n    summary_data = cls.summary_as_dict(dataframe, column_name)\n    return pd.DataFrame([summary_data])\n</code></pre>"},{"location":"reference/ts_shape/features/stats/string_stats/#ts_shape.features.stats.string_stats.StringStatistics.summary_as_dict","title":"summary_as_dict  <code>classmethod</code>","text":"<pre><code>summary_as_dict(dataframe: DataFrame, column_name: str) -&gt; Dict[str, Union[int, str, float]]\n</code></pre> <p>Returns a dictionary with comprehensive string statistics for the specified column.</p> Source code in <code>src/ts_shape/features/stats/string_stats.py</code> <pre><code>@classmethod\ndef summary_as_dict(cls, dataframe: pd.DataFrame, column_name: str) -&gt; Dict[str, Union[int, str, float]]:\n    \"\"\"Returns a dictionary with comprehensive string statistics for the specified column.\"\"\"\n    most_frequent = cls.most_frequent(dataframe, column_name)\n    value_counts = dataframe[column_name].value_counts()\n\n    return {\n        'unique_values': cls.count_unique(dataframe, column_name),\n        'most_frequent': most_frequent,\n        'count_most_frequent': cls.count_most_frequent(dataframe, column_name),\n        'count_null': cls.count_null(dataframe, column_name),\n        'average_string_length': cls.average_string_length(dataframe, column_name),\n        'longest_string': cls.longest_string(dataframe, column_name),\n        'shortest_string': cls.shortest_string(dataframe, column_name),\n        'uppercase_percentage': cls.uppercase_percentage(dataframe, column_name),\n        'lowercase_percentage': cls.lowercase_percentage(dataframe, column_name),\n        'contains_digit_count': cls.contains_digit_count(dataframe, column_name),\n        'least_common': value_counts.idxmin() if not value_counts.empty else None,\n        'frequency_least_common': value_counts.min() if not value_counts.empty else 0\n    }\n</code></pre>"},{"location":"reference/ts_shape/features/stats/string_stats/#ts_shape.features.stats.string_stats.StringStatistics.uppercase_percentage","title":"uppercase_percentage  <code>classmethod</code>","text":"<pre><code>uppercase_percentage(dataframe: DataFrame, column_name: str = 'value_string') -&gt; float\n</code></pre> <p>Returns the percentage of strings that are fully uppercase.</p> Source code in <code>src/ts_shape/features/stats/string_stats.py</code> <pre><code>@classmethod\ndef uppercase_percentage(cls, dataframe: pd.DataFrame, column_name: str = 'value_string') -&gt; float:\n    \"\"\"Returns the percentage of strings that are fully uppercase.\"\"\"\n    total_non_null = dataframe[column_name].notna().sum()\n    if total_non_null == 0:\n        return 0.0\n    uppercase_count = dataframe[column_name].dropna().str.isupper().sum()\n    return (uppercase_count / total_non_null) * 100\n</code></pre>"},{"location":"reference/ts_shape/features/stats/timestamp_stats/","title":"timestamp_stats","text":""},{"location":"reference/ts_shape/features/stats/timestamp_stats/#ts_shape.features.stats.timestamp_stats","title":"ts_shape.features.stats.timestamp_stats","text":"<p>Classes:</p> <ul> <li> <code>TimestampStatistics</code>           \u2013            <p>Provides class methods to calculate statistics on timestamp columns in a pandas DataFrame.</p> </li> </ul>"},{"location":"reference/ts_shape/features/stats/timestamp_stats/#ts_shape.features.stats.timestamp_stats.TimestampStatistics","title":"TimestampStatistics","text":"<pre><code>TimestampStatistics(dataframe: DataFrame, column_name: str = 'systime')\n</code></pre> <p>               Bases: <code>Base</code></p> <p>Provides class methods to calculate statistics on timestamp columns in a pandas DataFrame. The default column for calculations is 'systime'.</p> <p>Parameters:</p> <p>Methods:</p> <ul> <li> <code>average_time_gap</code>             \u2013              <p>Returns the average time gap between consecutive timestamps.</p> </li> <li> <code>count_most_frequent_timestamp</code>             \u2013              <p>Returns the count of the most frequent timestamp in the column.</p> </li> <li> <code>count_not_null</code>             \u2013              <p>Returns the number of non-null (valid) timestamps in the column.</p> </li> <li> <code>count_null</code>             \u2013              <p>Returns the number of null (NaN) values in the timestamp column.</p> </li> <li> <code>days_with_most_activity</code>             \u2013              <p>Returns the top N days with the most timestamp activity.</p> </li> <li> <code>earliest_timestamp</code>             \u2013              <p>Returns the earliest timestamp in the column.</p> </li> <li> <code>get_dataframe</code>             \u2013              <p>Returns the processed DataFrame.</p> </li> <li> <code>hour_distribution</code>             \u2013              <p>Returns the distribution of timestamps per hour of the day.</p> </li> <li> <code>latest_timestamp</code>             \u2013              <p>Returns the latest timestamp in the column.</p> </li> <li> <code>median_timestamp</code>             \u2013              <p>Returns the median timestamp in the column.</p> </li> <li> <code>month_distribution</code>             \u2013              <p>Returns the distribution of timestamps per month.</p> </li> <li> <code>most_frequent_day</code>             \u2013              <p>Returns the most frequent day of the week (0=Monday, 6=Sunday).</p> </li> <li> <code>most_frequent_hour</code>             \u2013              <p>Returns the most frequent hour of the day (0-23).</p> </li> <li> <code>most_frequent_timestamp</code>             \u2013              <p>Returns the most frequent timestamp in the column.</p> </li> <li> <code>standard_deviation_timestamps</code>             \u2013              <p>Returns the standard deviation of the time differences between consecutive timestamps.</p> </li> <li> <code>timestamp_quartiles</code>             \u2013              <p>Returns the 25th, 50th (median), and 75th percentiles of the timestamps.</p> </li> <li> <code>timestamp_range</code>             \u2013              <p>Returns the time range (difference) between the earliest and latest timestamps.</p> </li> <li> <code>weekday_distribution</code>             \u2013              <p>Returns the distribution of timestamps per weekday.</p> </li> <li> <code>year_distribution</code>             \u2013              <p>Returns the distribution of timestamps per year.</p> </li> </ul> Source code in <code>src/ts_shape/utils/base.py</code> <pre><code>def __init__(self, dataframe: pd.DataFrame, column_name: str = 'systime') -&gt; None:\n    \"\"\"\n    Initializes the Base with a DataFrame, detects time columns, converts them to datetime,\n    and sorts the DataFrame by the specified column (or the detected time column if applicable).\n\n    Args:\n        dataframe (pd.DataFrame): The DataFrame to be processed.\n        column_name (str): The column to sort by. Default is 'systime'. If the column is not found or is not a time column, the class will attempt to detect other time columns.\n    \"\"\"\n    self.dataframe = dataframe.copy()\n\n    # Attempt to convert the specified column_name to datetime if it exists\n    if column_name in self.dataframe.columns:\n        self.dataframe[column_name] = pd.to_datetime(self.dataframe[column_name], errors='coerce')\n    else:\n        # If the column_name is not in the DataFrame, fallback to automatic time detection\n        time_columns = [col for col in self.dataframe.columns if 'time' in col.lower() or 'date' in col.lower()]\n\n        # Convert all detected time columns to datetime, if any\n        for col in time_columns:\n            self.dataframe[col] = pd.to_datetime(self.dataframe[col], errors='coerce')\n\n        # If any time columns are detected, sort by the first one; otherwise, do nothing\n        if time_columns:\n            column_name = time_columns[0]\n\n    # Sort by the datetime column (either specified or detected)\n    if column_name in self.dataframe.columns:\n        self.dataframe = self.dataframe.sort_values(by=column_name)\n</code></pre>"},{"location":"reference/ts_shape/features/stats/timestamp_stats/#ts_shape.features.stats.timestamp_stats.TimestampStatistics(dataframe)","title":"<code>dataframe</code>","text":"(<code>DataFrame</code>)           \u2013            <p>The DataFrame to be processed.</p>"},{"location":"reference/ts_shape/features/stats/timestamp_stats/#ts_shape.features.stats.timestamp_stats.TimestampStatistics(column_name)","title":"<code>column_name</code>","text":"(<code>str</code>, default:                   <code>'systime'</code> )           \u2013            <p>The column to sort by. Default is 'systime'. If the column is not found or is not a time column, the class will attempt to detect other time columns.</p>"},{"location":"reference/ts_shape/features/stats/timestamp_stats/#ts_shape.features.stats.timestamp_stats.TimestampStatistics.average_time_gap","title":"average_time_gap  <code>classmethod</code>","text":"<pre><code>average_time_gap(dataframe: DataFrame, column_name: str = 'systime') -&gt; Timedelta\n</code></pre> <p>Returns the average time gap between consecutive timestamps.</p> Source code in <code>src/ts_shape/features/stats/timestamp_stats.py</code> <pre><code>@classmethod\ndef average_time_gap(cls, dataframe: pd.DataFrame, column_name: str = 'systime') -&gt; pd.Timedelta:\n    \"\"\"Returns the average time gap between consecutive timestamps.\"\"\"\n    sorted_times = dataframe[column_name].dropna().sort_values()\n    time_deltas = sorted_times.diff().dropna()\n    return time_deltas.mean()\n</code></pre>"},{"location":"reference/ts_shape/features/stats/timestamp_stats/#ts_shape.features.stats.timestamp_stats.TimestampStatistics.count_most_frequent_timestamp","title":"count_most_frequent_timestamp  <code>classmethod</code>","text":"<pre><code>count_most_frequent_timestamp(dataframe: DataFrame, column_name: str = 'systime') -&gt; int\n</code></pre> <p>Returns the count of the most frequent timestamp in the column.</p> Source code in <code>src/ts_shape/features/stats/timestamp_stats.py</code> <pre><code>@classmethod\ndef count_most_frequent_timestamp(cls, dataframe: pd.DataFrame, column_name: str = 'systime') -&gt; int:\n    \"\"\"Returns the count of the most frequent timestamp in the column.\"\"\"\n    most_frequent_value = cls.most_frequent_timestamp(dataframe, column_name)\n    return dataframe[column_name].value_counts().loc[most_frequent_value]\n</code></pre>"},{"location":"reference/ts_shape/features/stats/timestamp_stats/#ts_shape.features.stats.timestamp_stats.TimestampStatistics.count_not_null","title":"count_not_null  <code>classmethod</code>","text":"<pre><code>count_not_null(dataframe: DataFrame, column_name: str = 'systime') -&gt; int\n</code></pre> <p>Returns the number of non-null (valid) timestamps in the column.</p> Source code in <code>src/ts_shape/features/stats/timestamp_stats.py</code> <pre><code>@classmethod\ndef count_not_null(cls, dataframe: pd.DataFrame, column_name: str = 'systime') -&gt; int:\n    \"\"\"Returns the number of non-null (valid) timestamps in the column.\"\"\"\n    return dataframe[column_name].notna().sum()\n</code></pre>"},{"location":"reference/ts_shape/features/stats/timestamp_stats/#ts_shape.features.stats.timestamp_stats.TimestampStatistics.count_null","title":"count_null  <code>classmethod</code>","text":"<pre><code>count_null(dataframe: DataFrame, column_name: str = 'systime') -&gt; int\n</code></pre> <p>Returns the number of null (NaN) values in the timestamp column.</p> Source code in <code>src/ts_shape/features/stats/timestamp_stats.py</code> <pre><code>@classmethod\ndef count_null(cls, dataframe: pd.DataFrame, column_name: str = 'systime') -&gt; int:\n    \"\"\"Returns the number of null (NaN) values in the timestamp column.\"\"\"\n    return dataframe[column_name].isna().sum()\n</code></pre>"},{"location":"reference/ts_shape/features/stats/timestamp_stats/#ts_shape.features.stats.timestamp_stats.TimestampStatistics.days_with_most_activity","title":"days_with_most_activity  <code>classmethod</code>","text":"<pre><code>days_with_most_activity(dataframe: DataFrame, column_name: str = 'systime', n: int = 3) -&gt; Series\n</code></pre> <p>Returns the top N days with the most timestamp activity.</p> Source code in <code>src/ts_shape/features/stats/timestamp_stats.py</code> <pre><code>@classmethod\ndef days_with_most_activity(cls, dataframe: pd.DataFrame, column_name: str = 'systime', n: int = 3) -&gt; pd.Series:\n    \"\"\"Returns the top N days with the most timestamp activity.\"\"\"\n    return dataframe[column_name].dt.date.value_counts().head(n)\n</code></pre>"},{"location":"reference/ts_shape/features/stats/timestamp_stats/#ts_shape.features.stats.timestamp_stats.TimestampStatistics.earliest_timestamp","title":"earliest_timestamp  <code>classmethod</code>","text":"<pre><code>earliest_timestamp(dataframe: DataFrame, column_name: str = 'systime')\n</code></pre> <p>Returns the earliest timestamp in the column.</p> Source code in <code>src/ts_shape/features/stats/timestamp_stats.py</code> <pre><code>@classmethod\ndef earliest_timestamp(cls, dataframe: pd.DataFrame, column_name: str = 'systime'):\n    \"\"\"Returns the earliest timestamp in the column.\"\"\"\n    return dataframe[column_name].min()\n</code></pre>"},{"location":"reference/ts_shape/features/stats/timestamp_stats/#ts_shape.features.stats.timestamp_stats.TimestampStatistics.get_dataframe","title":"get_dataframe","text":"<pre><code>get_dataframe() -&gt; DataFrame\n</code></pre> <p>Returns the processed DataFrame.</p> Source code in <code>src/ts_shape/utils/base.py</code> <pre><code>def get_dataframe(self) -&gt; pd.DataFrame:\n    \"\"\"Returns the processed DataFrame.\"\"\"\n    return self.dataframe\n</code></pre>"},{"location":"reference/ts_shape/features/stats/timestamp_stats/#ts_shape.features.stats.timestamp_stats.TimestampStatistics.hour_distribution","title":"hour_distribution  <code>classmethod</code>","text":"<pre><code>hour_distribution(dataframe: DataFrame, column_name: str = 'systime') -&gt; Series\n</code></pre> <p>Returns the distribution of timestamps per hour of the day.</p> Source code in <code>src/ts_shape/features/stats/timestamp_stats.py</code> <pre><code>@classmethod\ndef hour_distribution(cls, dataframe: pd.DataFrame, column_name: str = 'systime') -&gt; pd.Series:\n    \"\"\"Returns the distribution of timestamps per hour of the day.\"\"\"\n    return dataframe[column_name].dt.hour.value_counts()\n</code></pre>"},{"location":"reference/ts_shape/features/stats/timestamp_stats/#ts_shape.features.stats.timestamp_stats.TimestampStatistics.latest_timestamp","title":"latest_timestamp  <code>classmethod</code>","text":"<pre><code>latest_timestamp(dataframe: DataFrame, column_name: str = 'systime')\n</code></pre> <p>Returns the latest timestamp in the column.</p> Source code in <code>src/ts_shape/features/stats/timestamp_stats.py</code> <pre><code>@classmethod\ndef latest_timestamp(cls, dataframe: pd.DataFrame, column_name: str = 'systime'):\n    \"\"\"Returns the latest timestamp in the column.\"\"\"\n    return dataframe[column_name].max()\n</code></pre>"},{"location":"reference/ts_shape/features/stats/timestamp_stats/#ts_shape.features.stats.timestamp_stats.TimestampStatistics.median_timestamp","title":"median_timestamp  <code>classmethod</code>","text":"<pre><code>median_timestamp(dataframe: DataFrame, column_name: str = 'systime')\n</code></pre> <p>Returns the median timestamp in the column.</p> Source code in <code>src/ts_shape/features/stats/timestamp_stats.py</code> <pre><code>@classmethod\ndef median_timestamp(cls, dataframe: pd.DataFrame, column_name: str = 'systime'):\n    \"\"\"Returns the median timestamp in the column.\"\"\"\n    return dataframe[column_name].median()\n</code></pre>"},{"location":"reference/ts_shape/features/stats/timestamp_stats/#ts_shape.features.stats.timestamp_stats.TimestampStatistics.month_distribution","title":"month_distribution  <code>classmethod</code>","text":"<pre><code>month_distribution(dataframe: DataFrame, column_name: str = 'systime') -&gt; Series\n</code></pre> <p>Returns the distribution of timestamps per month.</p> Source code in <code>src/ts_shape/features/stats/timestamp_stats.py</code> <pre><code>@classmethod\ndef month_distribution(cls, dataframe: pd.DataFrame, column_name: str = 'systime') -&gt; pd.Series:\n    \"\"\"Returns the distribution of timestamps per month.\"\"\"\n    return dataframe[column_name].dt.month.value_counts()\n</code></pre>"},{"location":"reference/ts_shape/features/stats/timestamp_stats/#ts_shape.features.stats.timestamp_stats.TimestampStatistics.most_frequent_day","title":"most_frequent_day  <code>classmethod</code>","text":"<pre><code>most_frequent_day(dataframe: DataFrame, column_name: str = 'systime') -&gt; int\n</code></pre> <p>Returns the most frequent day of the week (0=Monday, 6=Sunday).</p> Source code in <code>src/ts_shape/features/stats/timestamp_stats.py</code> <pre><code>@classmethod\ndef most_frequent_day(cls, dataframe: pd.DataFrame, column_name: str = 'systime') -&gt; int:\n    \"\"\"Returns the most frequent day of the week (0=Monday, 6=Sunday).\"\"\"\n    return dataframe[column_name].dt.weekday.mode().iloc[0]\n</code></pre>"},{"location":"reference/ts_shape/features/stats/timestamp_stats/#ts_shape.features.stats.timestamp_stats.TimestampStatistics.most_frequent_hour","title":"most_frequent_hour  <code>classmethod</code>","text":"<pre><code>most_frequent_hour(dataframe: DataFrame, column_name: str = 'systime') -&gt; int\n</code></pre> <p>Returns the most frequent hour of the day (0-23).</p> Source code in <code>src/ts_shape/features/stats/timestamp_stats.py</code> <pre><code>@classmethod\ndef most_frequent_hour(cls, dataframe: pd.DataFrame, column_name: str = 'systime') -&gt; int:\n    \"\"\"Returns the most frequent hour of the day (0-23).\"\"\"\n    return dataframe[column_name].dt.hour.mode().iloc[0]\n</code></pre>"},{"location":"reference/ts_shape/features/stats/timestamp_stats/#ts_shape.features.stats.timestamp_stats.TimestampStatistics.most_frequent_timestamp","title":"most_frequent_timestamp  <code>classmethod</code>","text":"<pre><code>most_frequent_timestamp(dataframe: DataFrame, column_name: str = 'systime')\n</code></pre> <p>Returns the most frequent timestamp in the column.</p> Source code in <code>src/ts_shape/features/stats/timestamp_stats.py</code> <pre><code>@classmethod\ndef most_frequent_timestamp(cls, dataframe: pd.DataFrame, column_name: str = 'systime'):\n    \"\"\"Returns the most frequent timestamp in the column.\"\"\"\n    return dataframe[column_name].mode().iloc[0]\n</code></pre>"},{"location":"reference/ts_shape/features/stats/timestamp_stats/#ts_shape.features.stats.timestamp_stats.TimestampStatistics.standard_deviation_timestamps","title":"standard_deviation_timestamps  <code>classmethod</code>","text":"<pre><code>standard_deviation_timestamps(dataframe: DataFrame, column_name: str = 'systime') -&gt; Timedelta\n</code></pre> <p>Returns the standard deviation of the time differences between consecutive timestamps.</p> Source code in <code>src/ts_shape/features/stats/timestamp_stats.py</code> <pre><code>@classmethod\ndef standard_deviation_timestamps(cls, dataframe: pd.DataFrame, column_name: str = 'systime') -&gt; pd.Timedelta:\n    \"\"\"Returns the standard deviation of the time differences between consecutive timestamps.\"\"\"\n    sorted_times = dataframe[column_name].dropna().sort_values()\n    time_deltas = sorted_times.diff().dropna()\n    return time_deltas.std()\n</code></pre>"},{"location":"reference/ts_shape/features/stats/timestamp_stats/#ts_shape.features.stats.timestamp_stats.TimestampStatistics.timestamp_quartiles","title":"timestamp_quartiles  <code>classmethod</code>","text":"<pre><code>timestamp_quartiles(dataframe: DataFrame, column_name: str = 'systime') -&gt; Series\n</code></pre> <p>Returns the 25th, 50th (median), and 75th percentiles of the timestamps.</p> Source code in <code>src/ts_shape/features/stats/timestamp_stats.py</code> <pre><code>@classmethod\ndef timestamp_quartiles(cls, dataframe: pd.DataFrame, column_name: str = 'systime') -&gt; pd.Series:\n    \"\"\"Returns the 25th, 50th (median), and 75th percentiles of the timestamps.\"\"\"\n    return dataframe[column_name].quantile([0.25, 0.5, 0.75])\n</code></pre>"},{"location":"reference/ts_shape/features/stats/timestamp_stats/#ts_shape.features.stats.timestamp_stats.TimestampStatistics.timestamp_range","title":"timestamp_range  <code>classmethod</code>","text":"<pre><code>timestamp_range(dataframe: DataFrame, column_name: str = 'systime')\n</code></pre> <p>Returns the time range (difference) between the earliest and latest timestamps.</p> Source code in <code>src/ts_shape/features/stats/timestamp_stats.py</code> <pre><code>@classmethod\ndef timestamp_range(cls, dataframe: pd.DataFrame, column_name: str = 'systime'):\n    \"\"\"Returns the time range (difference) between the earliest and latest timestamps.\"\"\"\n    return cls.latest_timestamp(dataframe, column_name) - cls.earliest_timestamp(dataframe, column_name)\n</code></pre>"},{"location":"reference/ts_shape/features/stats/timestamp_stats/#ts_shape.features.stats.timestamp_stats.TimestampStatistics.weekday_distribution","title":"weekday_distribution  <code>classmethod</code>","text":"<pre><code>weekday_distribution(dataframe: DataFrame, column_name: str = 'systime') -&gt; Series\n</code></pre> <p>Returns the distribution of timestamps per weekday.</p> Source code in <code>src/ts_shape/features/stats/timestamp_stats.py</code> <pre><code>@classmethod\ndef weekday_distribution(cls, dataframe: pd.DataFrame, column_name: str = 'systime') -&gt; pd.Series:\n    \"\"\"Returns the distribution of timestamps per weekday.\"\"\"\n    return dataframe[column_name].dt.weekday.value_counts()\n</code></pre>"},{"location":"reference/ts_shape/features/stats/timestamp_stats/#ts_shape.features.stats.timestamp_stats.TimestampStatistics.year_distribution","title":"year_distribution  <code>classmethod</code>","text":"<pre><code>year_distribution(dataframe: DataFrame, column_name: str = 'systime') -&gt; Series\n</code></pre> <p>Returns the distribution of timestamps per year.</p> Source code in <code>src/ts_shape/features/stats/timestamp_stats.py</code> <pre><code>@classmethod\ndef year_distribution(cls, dataframe: pd.DataFrame, column_name: str = 'systime') -&gt; pd.Series:\n    \"\"\"Returns the distribution of timestamps per year.\"\"\"\n    return dataframe[column_name].dt.year.value_counts()\n</code></pre>"},{"location":"reference/ts_shape/features/time_stats/__init__/","title":"init","text":""},{"location":"reference/ts_shape/features/time_stats/__init__/#ts_shape.features.time_stats","title":"ts_shape.features.time_stats","text":"<p>Time-Grouped Statistics</p> <p>Aggregations over fixed time windows (e.g., hourly/daily) for numeric values.</p> <ul> <li>TimeGroupedStatistics: Time-windowed aggregations for numeric series.</li> <li>calculate_statistic: Single aggregation per window (mean/sum/min/max/diff/range).</li> <li>calculate_statistics: Multiple aggregations merged.</li> <li>calculate_custom_func: Apply a custom aggregation per window.</li> </ul> <p>Modules:</p> <ul> <li> <code>time_stats_numeric</code>           \u2013            </li> </ul>"},{"location":"reference/ts_shape/features/time_stats/time_stats_numeric/","title":"time_stats_numeric","text":""},{"location":"reference/ts_shape/features/time_stats/time_stats_numeric/#ts_shape.features.time_stats.time_stats_numeric","title":"ts_shape.features.time_stats.time_stats_numeric","text":"<p>Classes:</p> <ul> <li> <code>TimeGroupedStatistics</code>           \u2013            <p>A class for calculating time-grouped statistics on numeric data, with class methods to apply various statistical functions.</p> </li> </ul>"},{"location":"reference/ts_shape/features/time_stats/time_stats_numeric/#ts_shape.features.time_stats.time_stats_numeric.TimeGroupedStatistics","title":"TimeGroupedStatistics","text":"<pre><code>TimeGroupedStatistics(dataframe: DataFrame, column_name: str = 'systime')\n</code></pre> <p>               Bases: <code>Base</code></p> <p>A class for calculating time-grouped statistics on numeric data, with class methods to apply various statistical functions.</p> <p>Parameters:</p> <p>Methods:</p> <ul> <li> <code>calculate_custom_func</code>             \u2013              <p>Apply a custom aggregation function on the value column over the grouped time intervals.</p> </li> <li> <code>calculate_statistic</code>             \u2013              <p>Calculate a specified statistic on the value column over the grouped time intervals.</p> </li> <li> <code>calculate_statistics</code>             \u2013              <p>Calculate multiple specified statistics on the value column over the grouped time intervals.</p> </li> <li> <code>get_dataframe</code>             \u2013              <p>Returns the processed DataFrame.</p> </li> </ul> Source code in <code>src/ts_shape/utils/base.py</code> <pre><code>def __init__(self, dataframe: pd.DataFrame, column_name: str = 'systime') -&gt; None:\n    \"\"\"\n    Initializes the Base with a DataFrame, detects time columns, converts them to datetime,\n    and sorts the DataFrame by the specified column (or the detected time column if applicable).\n\n    Args:\n        dataframe (pd.DataFrame): The DataFrame to be processed.\n        column_name (str): The column to sort by. Default is 'systime'. If the column is not found or is not a time column, the class will attempt to detect other time columns.\n    \"\"\"\n    self.dataframe = dataframe.copy()\n\n    # Attempt to convert the specified column_name to datetime if it exists\n    if column_name in self.dataframe.columns:\n        self.dataframe[column_name] = pd.to_datetime(self.dataframe[column_name], errors='coerce')\n    else:\n        # If the column_name is not in the DataFrame, fallback to automatic time detection\n        time_columns = [col for col in self.dataframe.columns if 'time' in col.lower() or 'date' in col.lower()]\n\n        # Convert all detected time columns to datetime, if any\n        for col in time_columns:\n            self.dataframe[col] = pd.to_datetime(self.dataframe[col], errors='coerce')\n\n        # If any time columns are detected, sort by the first one; otherwise, do nothing\n        if time_columns:\n            column_name = time_columns[0]\n\n    # Sort by the datetime column (either specified or detected)\n    if column_name in self.dataframe.columns:\n        self.dataframe = self.dataframe.sort_values(by=column_name)\n</code></pre>"},{"location":"reference/ts_shape/features/time_stats/time_stats_numeric/#ts_shape.features.time_stats.time_stats_numeric.TimeGroupedStatistics(dataframe)","title":"<code>dataframe</code>","text":"(<code>DataFrame</code>)           \u2013            <p>The DataFrame to be processed.</p>"},{"location":"reference/ts_shape/features/time_stats/time_stats_numeric/#ts_shape.features.time_stats.time_stats_numeric.TimeGroupedStatistics(column_name)","title":"<code>column_name</code>","text":"(<code>str</code>, default:                   <code>'systime'</code> )           \u2013            <p>The column to sort by. Default is 'systime'. If the column is not found or is not a time column, the class will attempt to detect other time columns.</p>"},{"location":"reference/ts_shape/features/time_stats/time_stats_numeric/#ts_shape.features.time_stats.time_stats_numeric.TimeGroupedStatistics.calculate_custom_func","title":"calculate_custom_func  <code>classmethod</code>","text":"<pre><code>calculate_custom_func(dataframe: DataFrame, time_column: str, value_column: str, freq: str, func) -&gt; DataFrame\n</code></pre> <p>Apply a custom aggregation function on the value column over the grouped time intervals.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>pd.DataFrame: A DataFrame with the custom calculated statistics.</p> </li> </ul> Source code in <code>src/ts_shape/features/time_stats/time_stats_numeric.py</code> <pre><code>@classmethod\ndef calculate_custom_func(cls, dataframe: pd.DataFrame, time_column: str, value_column: str, freq: str, func) -&gt; pd.DataFrame:\n    \"\"\"\n    Apply a custom aggregation function on the value column over the grouped time intervals.\n\n    Args:\n        dataframe (pd.DataFrame): The DataFrame containing the data.\n        time_column (str): The name of the time column to group and sort by.\n        value_column (str): The name of the numeric column to calculate statistics on.\n        freq (str): Frequency string for time grouping (e.g., 'H' for hourly, 'D' for daily).\n        func (callable): Custom function to apply to each group.\n\n    Returns:\n        pd.DataFrame: A DataFrame with the custom calculated statistics.\n    \"\"\"\n    grouped_df = dataframe.set_index(time_column).resample(freq)\n    result = grouped_df[value_column].apply(func).to_frame('custom')\n    return result\n</code></pre>"},{"location":"reference/ts_shape/features/time_stats/time_stats_numeric/#ts_shape.features.time_stats.time_stats_numeric.TimeGroupedStatistics.calculate_custom_func(dataframe)","title":"<code>dataframe</code>","text":"(<code>DataFrame</code>)           \u2013            <p>The DataFrame containing the data.</p>"},{"location":"reference/ts_shape/features/time_stats/time_stats_numeric/#ts_shape.features.time_stats.time_stats_numeric.TimeGroupedStatistics.calculate_custom_func(time_column)","title":"<code>time_column</code>","text":"(<code>str</code>)           \u2013            <p>The name of the time column to group and sort by.</p>"},{"location":"reference/ts_shape/features/time_stats/time_stats_numeric/#ts_shape.features.time_stats.time_stats_numeric.TimeGroupedStatistics.calculate_custom_func(value_column)","title":"<code>value_column</code>","text":"(<code>str</code>)           \u2013            <p>The name of the numeric column to calculate statistics on.</p>"},{"location":"reference/ts_shape/features/time_stats/time_stats_numeric/#ts_shape.features.time_stats.time_stats_numeric.TimeGroupedStatistics.calculate_custom_func(freq)","title":"<code>freq</code>","text":"(<code>str</code>)           \u2013            <p>Frequency string for time grouping (e.g., 'H' for hourly, 'D' for daily).</p>"},{"location":"reference/ts_shape/features/time_stats/time_stats_numeric/#ts_shape.features.time_stats.time_stats_numeric.TimeGroupedStatistics.calculate_custom_func(func)","title":"<code>func</code>","text":"(<code>callable</code>)           \u2013            <p>Custom function to apply to each group.</p>"},{"location":"reference/ts_shape/features/time_stats/time_stats_numeric/#ts_shape.features.time_stats.time_stats_numeric.TimeGroupedStatistics.calculate_statistic","title":"calculate_statistic  <code>classmethod</code>","text":"<pre><code>calculate_statistic(dataframe: DataFrame, time_column: str, value_column: str, freq: str, stat_method: str) -&gt; DataFrame\n</code></pre> <p>Calculate a specified statistic on the value column over the grouped time intervals.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>pd.DataFrame: A DataFrame with the time intervals and the calculated statistics.</p> </li> </ul> Source code in <code>src/ts_shape/features/time_stats/time_stats_numeric.py</code> <pre><code>@classmethod\ndef calculate_statistic(cls, dataframe: pd.DataFrame, time_column: str, value_column: str, freq: str, stat_method: str) -&gt; pd.DataFrame:\n    \"\"\"\n    Calculate a specified statistic on the value column over the grouped time intervals.\n\n    Args:\n        dataframe (pd.DataFrame): The DataFrame containing the data.\n        time_column (str): The name of the time column to group and sort by.\n        value_column (str): The name of the numeric column to calculate statistics on.\n        freq (str): Frequency string for time grouping (e.g., 'H' for hourly, 'D' for daily).\n        stat_method (str): The statistical method to apply ('mean', 'sum', 'min', 'max', 'diff', 'range').\n\n    Returns:\n        pd.DataFrame: A DataFrame with the time intervals and the calculated statistics.\n    \"\"\"\n    # Set the DataFrame index to the time column and resample to the specified frequency\n    grouped_df = dataframe.set_index(time_column).resample(freq)\n\n    # Select the calculation method\n    if stat_method == 'mean':\n        result = grouped_df[value_column].mean().to_frame('mean')\n    elif stat_method == 'sum':\n        result = grouped_df[value_column].sum().to_frame('sum')\n    elif stat_method == 'min':\n        result = grouped_df[value_column].min().to_frame('min')\n    elif stat_method == 'max':\n        result = grouped_df[value_column].max().to_frame('max')\n    elif stat_method == 'diff':\n        # Improved diff: last value - first value within each interval\n        result = (grouped_df[value_column].last() - grouped_df[value_column].first()).to_frame('difference')\n    elif stat_method == 'range':\n        # Range: max value - min value within each interval\n        result = (grouped_df[value_column].max() - grouped_df[value_column].min()).to_frame('range')\n    else:\n        raise ValueError(\"Invalid stat_method. Choose from 'mean', 'sum', 'min', 'max', 'diff', 'range'.\")\n\n    return result\n</code></pre>"},{"location":"reference/ts_shape/features/time_stats/time_stats_numeric/#ts_shape.features.time_stats.time_stats_numeric.TimeGroupedStatistics.calculate_statistic(dataframe)","title":"<code>dataframe</code>","text":"(<code>DataFrame</code>)           \u2013            <p>The DataFrame containing the data.</p>"},{"location":"reference/ts_shape/features/time_stats/time_stats_numeric/#ts_shape.features.time_stats.time_stats_numeric.TimeGroupedStatistics.calculate_statistic(time_column)","title":"<code>time_column</code>","text":"(<code>str</code>)           \u2013            <p>The name of the time column to group and sort by.</p>"},{"location":"reference/ts_shape/features/time_stats/time_stats_numeric/#ts_shape.features.time_stats.time_stats_numeric.TimeGroupedStatistics.calculate_statistic(value_column)","title":"<code>value_column</code>","text":"(<code>str</code>)           \u2013            <p>The name of the numeric column to calculate statistics on.</p>"},{"location":"reference/ts_shape/features/time_stats/time_stats_numeric/#ts_shape.features.time_stats.time_stats_numeric.TimeGroupedStatistics.calculate_statistic(freq)","title":"<code>freq</code>","text":"(<code>str</code>)           \u2013            <p>Frequency string for time grouping (e.g., 'H' for hourly, 'D' for daily).</p>"},{"location":"reference/ts_shape/features/time_stats/time_stats_numeric/#ts_shape.features.time_stats.time_stats_numeric.TimeGroupedStatistics.calculate_statistic(stat_method)","title":"<code>stat_method</code>","text":"(<code>str</code>)           \u2013            <p>The statistical method to apply ('mean', 'sum', 'min', 'max', 'diff', 'range').</p>"},{"location":"reference/ts_shape/features/time_stats/time_stats_numeric/#ts_shape.features.time_stats.time_stats_numeric.TimeGroupedStatistics.calculate_statistics","title":"calculate_statistics  <code>classmethod</code>","text":"<pre><code>calculate_statistics(dataframe: DataFrame, time_column: str, value_column: str, freq: str, stat_methods: list) -&gt; DataFrame\n</code></pre> <p>Calculate multiple specified statistics on the value column over the grouped time intervals.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>pd.DataFrame: A DataFrame with the time intervals and the calculated statistics for each method.</p> </li> </ul> Source code in <code>src/ts_shape/features/time_stats/time_stats_numeric.py</code> <pre><code>@classmethod\ndef calculate_statistics(cls, dataframe: pd.DataFrame, time_column: str, value_column: str, freq: str, stat_methods: list) -&gt; pd.DataFrame:\n    \"\"\"\n    Calculate multiple specified statistics on the value column over the grouped time intervals.\n\n    Args:\n        dataframe (pd.DataFrame): The DataFrame containing the data.\n        time_column (str): The name of the time column to group and sort by.\n        value_column (str): The name of the numeric column to calculate statistics on.\n        freq (str): Frequency string for time grouping (e.g., 'H' for hourly, 'D' for daily).\n        stat_methods (list): A list of statistical methods to apply (e.g., ['mean', 'sum', 'diff', 'range']).\n\n    Returns:\n        pd.DataFrame: A DataFrame with the time intervals and the calculated statistics for each method.\n    \"\"\"\n    # Initialize an empty DataFrame for combining results\n    result_df = pd.DataFrame()\n\n    # Calculate each requested statistic and join to the result DataFrame\n    for method in stat_methods:\n        stat_df = cls.calculate_statistic(dataframe, time_column, value_column, freq, method)\n        result_df = result_df.join(stat_df, how='outer')\n\n    return result_df\n</code></pre>"},{"location":"reference/ts_shape/features/time_stats/time_stats_numeric/#ts_shape.features.time_stats.time_stats_numeric.TimeGroupedStatistics.calculate_statistics(dataframe)","title":"<code>dataframe</code>","text":"(<code>DataFrame</code>)           \u2013            <p>The DataFrame containing the data.</p>"},{"location":"reference/ts_shape/features/time_stats/time_stats_numeric/#ts_shape.features.time_stats.time_stats_numeric.TimeGroupedStatistics.calculate_statistics(time_column)","title":"<code>time_column</code>","text":"(<code>str</code>)           \u2013            <p>The name of the time column to group and sort by.</p>"},{"location":"reference/ts_shape/features/time_stats/time_stats_numeric/#ts_shape.features.time_stats.time_stats_numeric.TimeGroupedStatistics.calculate_statistics(value_column)","title":"<code>value_column</code>","text":"(<code>str</code>)           \u2013            <p>The name of the numeric column to calculate statistics on.</p>"},{"location":"reference/ts_shape/features/time_stats/time_stats_numeric/#ts_shape.features.time_stats.time_stats_numeric.TimeGroupedStatistics.calculate_statistics(freq)","title":"<code>freq</code>","text":"(<code>str</code>)           \u2013            <p>Frequency string for time grouping (e.g., 'H' for hourly, 'D' for daily).</p>"},{"location":"reference/ts_shape/features/time_stats/time_stats_numeric/#ts_shape.features.time_stats.time_stats_numeric.TimeGroupedStatistics.calculate_statistics(stat_methods)","title":"<code>stat_methods</code>","text":"(<code>list</code>)           \u2013            <p>A list of statistical methods to apply (e.g., ['mean', 'sum', 'diff', 'range']).</p>"},{"location":"reference/ts_shape/features/time_stats/time_stats_numeric/#ts_shape.features.time_stats.time_stats_numeric.TimeGroupedStatistics.get_dataframe","title":"get_dataframe","text":"<pre><code>get_dataframe() -&gt; DataFrame\n</code></pre> <p>Returns the processed DataFrame.</p> Source code in <code>src/ts_shape/utils/base.py</code> <pre><code>def get_dataframe(self) -&gt; pd.DataFrame:\n    \"\"\"Returns the processed DataFrame.\"\"\"\n    return self.dataframe\n</code></pre>"},{"location":"reference/ts_shape/loader/__init__/","title":"init","text":""},{"location":"reference/ts_shape/loader/__init__/#ts_shape.loader","title":"ts_shape.loader","text":"<p>Loaders</p> <p>Load timeseries and metadata from various backends and combine them.</p> <ul> <li>DataIntegratorHybrid: Combine timeseries and metadata from DataFrames or source objects.</li> <li> <p>combine_data: Merge sources on a join key with optional UUID filter.</p> </li> <li> <p>ParquetLoader: Read parquet files from local folder structures.</p> </li> <li>load_all_files: Load all parquet under a base path.</li> <li>load_by_time_range: Load files within YYYY/MM/DD/HH path range.</li> <li>load_by_uuid_list: Load files matching UUIDs in filenames.</li> <li> <p>load_files_by_time_range_and_uuids: Combine time range and UUID filters.</p> </li> <li> <p>S3ProxyDataAccess: Retrieve parquet via an S3-compatible proxy.</p> </li> <li>fetch_data_as_parquet: Save parquet files to a local folder structure.</li> <li> <p>fetch_data_as_dataframe: Return a combined DataFrame.</p> </li> <li> <p>AzureBlobParquetLoader: Load parquet from Azure Blob Storage.</p> </li> <li>load_all_files: Load all parquet under an optional prefix.</li> <li>load_by_time_range: Load hourly folders between start and end.</li> <li>load_files_by_time_range_and_uuids: Load per-hour per-UUID parquet files.</li> <li> <p>list_structure: List folders and files under a prefix.</p> </li> <li> <p>TimescaleDBDataAccess: Stream timeseries from TimescaleDB.</p> </li> <li>fetch_data_as_parquet: Partition-by-hour and write parquet.</li> <li> <p>fetch_data_as_dataframe: Return a combined DataFrame.</p> </li> <li> <p>MetadataJsonLoader: Ingest JSON metadata and flatten config.</p> </li> <li>from_file: Create from file.</li> <li>from_str: Create from string.</li> <li>to_df: Return DataFrame view.</li> <li>head: Preview top rows.</li> <li>get_by_uuid: Access row by UUID.</li> <li>get_by_label: Access row by label.</li> <li>join_with: Join with other DataFrames.</li> <li>filter_by_uuid: Filter by UUID set.</li> <li>filter_by_label: Filter by label set.</li> <li>list_uuids: List UUIDs.</li> <li> <p>list_labels: List non-null labels.</p> </li> <li> <p>DatapointAPI: Retrieve datapoint metadata from a REST API.</p> </li> <li>get_all_uuids: UUIDs per device.</li> <li>get_all_metadata: Metadata per device.</li> <li> <p>display_dataframe: Print DataFrames for devices.</p> </li> <li> <p>DatapointDB: Retrieve datapoint metadata from PostgreSQL.</p> </li> <li>get_all_uuids: UUIDs per device.</li> <li>get_all_metadata: Metadata per device.</li> <li>display_dataframe: Print DataFrames for devices.</li> </ul> <p>Modules:</p> <ul> <li> <code>combine</code>           \u2013            <p>Combine</p> </li> <li> <code>context</code>           \u2013            <p>Context Loaders</p> </li> <li> <code>metadata</code>           \u2013            <p>Metadata Loaders</p> </li> <li> <code>timeseries</code>           \u2013            <p>Timeseries Loaders</p> </li> </ul>"},{"location":"reference/ts_shape/loader/combine/__init__/","title":"init","text":""},{"location":"reference/ts_shape/loader/combine/__init__/#ts_shape.loader.combine","title":"ts_shape.loader.combine","text":"<p>Combine</p> <p>Utilities to combine timeseries and metadata from multiple sources.</p> <ul> <li>DataIntegratorHybrid: Combine timeseries and metadata from DataFrames or source objects.</li> <li>combine_data: Merge sources on a join key with optional UUID filter.</li> </ul> <p>Modules:</p> <ul> <li> <code>integrator</code>           \u2013            </li> </ul>"},{"location":"reference/ts_shape/loader/combine/integrator/","title":"integrator","text":""},{"location":"reference/ts_shape/loader/combine/integrator/#ts_shape.loader.combine.integrator","title":"ts_shape.loader.combine.integrator","text":"<p>Classes:</p> <ul> <li> <code>DataIntegratorHybrid</code>           \u2013            <p>A flexible utility class to integrate data from various sources, including:</p> </li> </ul>"},{"location":"reference/ts_shape/loader/combine/integrator/#ts_shape.loader.combine.integrator.DataIntegratorHybrid","title":"DataIntegratorHybrid","text":"<p>A flexible utility class to integrate data from various sources, including: - API instances (e.g., DatapointAPI) - Direct raw data (e.g., UUID list, metadata, timeseries DataFrame) - Hybrid approaches (combination of instances and raw data)</p> <p>Methods:</p> <ul> <li> <code>combine_data</code>             \u2013              <p>Combine timeseries and metadata from various sources.</p> </li> </ul>"},{"location":"reference/ts_shape/loader/combine/integrator/#ts_shape.loader.combine.integrator.DataIntegratorHybrid.combine_data","title":"combine_data  <code>classmethod</code>","text":"<pre><code>combine_data(timeseries_sources: Optional[List[Union[DataFrame, object]]] = None, metadata_sources: Optional[List[Union[DataFrame, object]]] = None, uuids: Optional[List[str]] = None, join_key: str = 'uuid', merge_how: str = 'left') -&gt; DataFrame\n</code></pre> <p>Combine timeseries and metadata from various sources.</p> <p>:param timeseries_sources: List of timeseries sources (DataFrame or instances with <code>fetch_data_as_dataframe</code>). :param metadata_sources: List of metadata sources (DataFrame or instances with <code>fetch_metadata</code>). :param uuids: Optional list of UUIDs to filter the combined data. :param join_key: Key column to use for merging, default is \"uuid\". :param merge_how: Merge strategy ('left', 'inner', etc.), default is \"left\". :return: A combined DataFrame.</p> Source code in <code>src/ts_shape/loader/combine/integrator.py</code> <pre><code>@classmethod\ndef combine_data(\n    cls,\n    timeseries_sources: Optional[List[Union[pd.DataFrame, object]]] = None,\n    metadata_sources: Optional[List[Union[pd.DataFrame, object]]] = None,\n    uuids: Optional[List[str]] = None,\n    join_key: str = \"uuid\",\n    merge_how: str = \"left\",\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Combine timeseries and metadata from various sources.\n\n    :param timeseries_sources: List of timeseries sources (DataFrame or instances with `fetch_data_as_dataframe`).\n    :param metadata_sources: List of metadata sources (DataFrame or instances with `fetch_metadata`).\n    :param uuids: Optional list of UUIDs to filter the combined data.\n    :param join_key: Key column to use for merging, default is \"uuid\".\n    :param merge_how: Merge strategy ('left', 'inner', etc.), default is \"left\".\n    :return: A combined DataFrame.\n    \"\"\"\n    # Retrieve and combine timeseries data\n    timeseries_data = cls._combine_timeseries(timeseries_sources, join_key)\n\n    if timeseries_data.empty:\n        print(\"No timeseries data found.\")\n        return pd.DataFrame()\n\n    # Retrieve and combine metadata\n    metadata = cls._combine_metadata(metadata_sources, join_key)\n\n    if metadata.empty:\n        print(\"No metadata found.\")\n        return timeseries_data\n\n    missing_timeseries_key = join_key not in timeseries_data.columns\n    missing_metadata_key = join_key not in metadata.columns\n\n    if missing_timeseries_key or missing_metadata_key:\n        missing_parts = []\n        if missing_timeseries_key:\n            missing_parts.append(\"timeseries data\")\n        if missing_metadata_key:\n            missing_parts.append(\"metadata\")\n        print(\n            f\"Cannot merge because join key '{join_key}' is missing in \"\n            f\"{', '.join(missing_parts)}.\"\n        )\n        return timeseries_data\n\n    # Merge timeseries data with metadata\n    combined_data = pd.merge(timeseries_data, metadata, on=join_key, how=merge_how)\n\n    # Optionally filter the combined data by UUIDs\n    if uuids:\n        combined_data = combined_data[combined_data[join_key].isin(uuids)]\n\n    return combined_data\n</code></pre>"},{"location":"reference/ts_shape/loader/context/__init__/","title":"context","text":""},{"location":"reference/ts_shape/loader/context/__init__/#ts_shape.loader.context","title":"ts_shape.loader.context","text":"<p>Context Loaders</p> <p>Helpers for context loading under the loader namespace.</p> <p>Classes: - None yet: Placeholder module for future context loader utilities.</p>"},{"location":"reference/ts_shape/loader/metadata/__init__/","title":"init","text":""},{"location":"reference/ts_shape/loader/metadata/__init__/#ts_shape.loader.metadata","title":"ts_shape.loader.metadata","text":"<p>Metadata Loaders</p> <p>Load and normalize datapoint metadata from JSON, REST APIs, or databases.</p> <ul> <li>MetadataJsonLoader: Normalize metadata JSONs into a UUID-indexed DataFrame.</li> <li>from_file: Create from a JSON file.</li> <li>from_str: Create from a JSON string.</li> <li>to_df: Return DataFrame view.</li> <li>head: Preview top rows.</li> <li>get_by_uuid: Access row by UUID.</li> <li>get_by_label: Access row by label.</li> <li>join_with: Join with other DataFrames.</li> <li>filter_by_uuid: Filter by UUID set.</li> <li>filter_by_label: Filter by label set.</li> <li>list_uuids: List UUIDs.</li> <li> <p>list_labels: List non-null labels.</p> </li> <li> <p>DatapointAPI: Retrieve datapoint metadata from a REST API.</p> </li> <li>get_all_uuids: UUIDs per device.</li> <li>get_all_metadata: Metadata per device.</li> <li> <p>display_dataframe: Print DataFrames for devices.</p> </li> <li> <p>DatapointDB: Retrieve datapoint metadata from PostgreSQL.</p> </li> <li>get_all_uuids: UUIDs per device.</li> <li>get_all_metadata: Metadata per device.</li> <li>display_dataframe: Print DataFrames for devices.</li> </ul> <p>Modules:</p> <ul> <li> <code>metadata_api_loader</code>           \u2013            </li> <li> <code>metadata_db_loader</code>           \u2013            </li> <li> <code>metadata_json_loader</code>           \u2013            </li> </ul>"},{"location":"reference/ts_shape/loader/metadata/metadata_api_loader/","title":"metadata_api_loader","text":""},{"location":"reference/ts_shape/loader/metadata/metadata_api_loader/#ts_shape.loader.metadata.metadata_api_loader","title":"ts_shape.loader.metadata.metadata_api_loader","text":"<p>Classes:</p> <ul> <li> <code>DatapointAPI</code>           \u2013            <p>Class for accessing datapoints for multiple devices via an API.</p> </li> </ul>"},{"location":"reference/ts_shape/loader/metadata/metadata_api_loader/#ts_shape.loader.metadata.metadata_api_loader.DatapointAPI","title":"DatapointAPI","text":"<pre><code>DatapointAPI(device_names: List[str], base_url: str, api_token: str, output_path: str = 'data', required_uuid_list: List[str] = None, filter_enabled: bool = True)\n</code></pre> <p>Class for accessing datapoints for multiple devices via an API.</p> <p>:param device_names: List of device names to retrieve metadata for. :param base_url: Base URL of the API. :param api_token: API token for authentication. :param output_path: Directory to save the data points JSON files. :param required_uuid_list: Mixed list of UUIDs to filter the metadata across devices (optional). :param filter_enabled: Whether to filter metadata by \"enabled == True\" (default is True).</p> <p>Methods:</p> <ul> <li> <code>display_dataframe</code>             \u2013              <p>Print the metadata DataFrame for a specific device or all devices.</p> </li> <li> <code>get_all_metadata</code>             \u2013              <p>Return a dictionary of metadata for each device.</p> </li> <li> <code>get_all_uuids</code>             \u2013              <p>Return a dictionary of UUIDs for each device.</p> </li> </ul> Source code in <code>src/ts_shape/loader/metadata/metadata_api_loader.py</code> <pre><code>def __init__(self, device_names: List[str], base_url: str, api_token: str, output_path: str = \"data\", required_uuid_list: List[str] = None, filter_enabled: bool = True):\n    \"\"\"\n    Initialize the DatapointAPI class.\n\n    :param device_names: List of device names to retrieve metadata for.\n    :param base_url: Base URL of the API.\n    :param api_token: API token for authentication.\n    :param output_path: Directory to save the data points JSON files.\n    :param required_uuid_list: Mixed list of UUIDs to filter the metadata across devices (optional).\n    :param filter_enabled: Whether to filter metadata by \"enabled == True\" (default is True).\n    \"\"\"\n    self.device_names = device_names\n    self.base_url = base_url\n    self.api_token = api_token\n    self.output_path = output_path\n    self.required_uuid_list = required_uuid_list or []  # Defaults to an empty list if None\n    self.filter_enabled = filter_enabled\n    self.device_metadata: Dict[str, pd.DataFrame] = {}  # Store metadata for each device\n    self.device_uuids: Dict[str, List[str]] = {}  # Store UUIDs for each device\n    self._api_access()\n</code></pre>"},{"location":"reference/ts_shape/loader/metadata/metadata_api_loader/#ts_shape.loader.metadata.metadata_api_loader.DatapointAPI.display_dataframe","title":"display_dataframe","text":"<pre><code>display_dataframe(device_name: str = None) -&gt; None\n</code></pre> <p>Print the metadata DataFrame for a specific device or all devices.</p> <p>:param device_name: Name of the device to display metadata for (optional).                     If None, displays metadata for all devices.</p> Source code in <code>src/ts_shape/loader/metadata/metadata_api_loader.py</code> <pre><code>def display_dataframe(self, device_name: str = None) -&gt; None:\n    \"\"\"\n    Print the metadata DataFrame for a specific device or all devices.\n\n    :param device_name: Name of the device to display metadata for (optional).\n                        If None, displays metadata for all devices.\n    \"\"\"\n    if device_name:\n        # Display metadata for a specific device\n        if device_name in self.device_metadata:\n            print(f\"Metadata for device: {device_name}\")\n            print(self.device_metadata[device_name])\n        else:\n            print(f\"No metadata found for device: {device_name}\")\n    else:\n        # Display metadata for all devices\n        for device, metadata in self.device_metadata.items():\n            print(f\"\\nMetadata for device: {device}\")\n            print(metadata)\n</code></pre>"},{"location":"reference/ts_shape/loader/metadata/metadata_api_loader/#ts_shape.loader.metadata.metadata_api_loader.DatapointAPI.get_all_metadata","title":"get_all_metadata","text":"<pre><code>get_all_metadata() -&gt; Dict[str, List[Dict[str, str]]]\n</code></pre> <p>Return a dictionary of metadata for each device.</p> Source code in <code>src/ts_shape/loader/metadata/metadata_api_loader.py</code> <pre><code>def get_all_metadata(self) -&gt; Dict[str, List[Dict[str, str]]]:\n    \"\"\"Return a dictionary of metadata for each device.\"\"\"\n    return {device: metadata.to_dict(orient=\"records\") for device, metadata in self.device_metadata.items()}\n</code></pre>"},{"location":"reference/ts_shape/loader/metadata/metadata_api_loader/#ts_shape.loader.metadata.metadata_api_loader.DatapointAPI.get_all_uuids","title":"get_all_uuids","text":"<pre><code>get_all_uuids() -&gt; Dict[str, List[str]]\n</code></pre> <p>Return a dictionary of UUIDs for each device.</p> Source code in <code>src/ts_shape/loader/metadata/metadata_api_loader.py</code> <pre><code>def get_all_uuids(self) -&gt; Dict[str, List[str]]:\n    \"\"\"Return a dictionary of UUIDs for each device.\"\"\"\n    return self.device_uuids\n</code></pre>"},{"location":"reference/ts_shape/loader/metadata/metadata_db_loader/","title":"metadata_db_loader","text":""},{"location":"reference/ts_shape/loader/metadata/metadata_db_loader/#ts_shape.loader.metadata.metadata_db_loader","title":"ts_shape.loader.metadata.metadata_db_loader","text":"<p>Classes:</p> <ul> <li> <code>DatapointDB</code>           \u2013            <p>Class for accessing datapoints via a database.</p> </li> </ul>"},{"location":"reference/ts_shape/loader/metadata/metadata_db_loader/#ts_shape.loader.metadata.metadata_db_loader.DatapointDB","title":"DatapointDB","text":"<pre><code>DatapointDB(device_names: List[str], db_user: str, db_pass: str, db_host: str, output_path: str = 'data', required_uuid_list: List[str] = None, filter_enabled: bool = True)\n</code></pre> <p>Class for accessing datapoints via a database.</p> <p>:param device_names: List of device names to retrieve metadata for. :param db_user: Database user. :param db_pass: Database password. :param db_host: Database host. :param output_path: Directory to save JSON files. :param required_uuid_list: List of UUIDs to filter the metadata (optional). :param filter_enabled: Whether to filter metadata by \"enabled == True\" and \"archived == False\" (default is True).</p> <p>Methods:</p> <ul> <li> <code>display_dataframe</code>             \u2013              <p>Display metadata as a DataFrame for a specific device or all devices.</p> </li> <li> <code>get_all_metadata</code>             \u2013              <p>Return a dictionary of metadata for each device.</p> </li> <li> <code>get_all_uuids</code>             \u2013              <p>Return a dictionary of UUIDs for each device.</p> </li> </ul> Source code in <code>src/ts_shape/loader/metadata/metadata_db_loader.py</code> <pre><code>def __init__(self, device_names: List[str], db_user: str, db_pass: str, db_host: str, output_path: str = \"data\", required_uuid_list: List[str] = None, filter_enabled: bool = True):\n    \"\"\"\n    Initialize the DatapointDB class.\n\n    :param device_names: List of device names to retrieve metadata for.\n    :param db_user: Database user.\n    :param db_pass: Database password.\n    :param db_host: Database host.\n    :param output_path: Directory to save JSON files.\n    :param required_uuid_list: List of UUIDs to filter the metadata (optional).\n    :param filter_enabled: Whether to filter metadata by \"enabled == True\" and \"archived == False\" (default is True).\n    \"\"\"\n    self.device_names = device_names\n    self.db_user = db_user\n    self.db_pass = db_pass\n    self.db_host = db_host\n    self.output_path = output_path\n    self.required_uuid_list = required_uuid_list or []\n    self.filter_enabled = filter_enabled\n    self.device_metadata: Dict[str, pd.DataFrame] = {}  # Store metadata for each device\n    self.device_uuids: Dict[str, List[str]] = {}  # Store UUIDs for each device\n    self._db_access()\n</code></pre>"},{"location":"reference/ts_shape/loader/metadata/metadata_db_loader/#ts_shape.loader.metadata.metadata_db_loader.DatapointDB.display_dataframe","title":"display_dataframe","text":"<pre><code>display_dataframe(device_name: str = None, aggregate: bool = False) -&gt; None\n</code></pre> <p>Display metadata as a DataFrame for a specific device or all devices.</p> <p>:param device_name: Name of the device to display metadata for (optional). :param aggregate: If True, combine metadata from all devices into a single DataFrame.</p> Source code in <code>src/ts_shape/loader/metadata/metadata_db_loader.py</code> <pre><code>def display_dataframe(self, device_name: str = None, aggregate: bool = False) -&gt; None:\n    \"\"\"\n    Display metadata as a DataFrame for a specific device or all devices.\n\n    :param device_name: Name of the device to display metadata for (optional).\n    :param aggregate: If True, combine metadata from all devices into a single DataFrame.\n    \"\"\"\n    if aggregate:\n        combined_df = pd.concat(self.device_metadata.values(), keys=self.device_metadata.keys())\n        print(\"Aggregated metadata for all devices:\")\n        print(combined_df)\n    elif device_name:\n        if device_name in self.device_metadata:\n            print(f\"Metadata for device: {device_name}\")\n            print(self.device_metadata[device_name])\n        else:\n            print(f\"No metadata found for device: {device_name}\")\n    else:\n        for device, metadata in self.device_metadata.items():\n            print(f\"\\nMetadata for device: {device}\")\n            print(metadata)\n</code></pre>"},{"location":"reference/ts_shape/loader/metadata/metadata_db_loader/#ts_shape.loader.metadata.metadata_db_loader.DatapointDB.get_all_metadata","title":"get_all_metadata","text":"<pre><code>get_all_metadata() -&gt; Dict[str, List[Dict[str, str]]]\n</code></pre> <p>Return a dictionary of metadata for each device.</p> Source code in <code>src/ts_shape/loader/metadata/metadata_db_loader.py</code> <pre><code>def get_all_metadata(self) -&gt; Dict[str, List[Dict[str, str]]]:\n    \"\"\"Return a dictionary of metadata for each device.\"\"\"\n    return {device: metadata.to_dict(orient=\"records\") for device, metadata in self.device_metadata.items()}\n</code></pre>"},{"location":"reference/ts_shape/loader/metadata/metadata_db_loader/#ts_shape.loader.metadata.metadata_db_loader.DatapointDB.get_all_uuids","title":"get_all_uuids","text":"<pre><code>get_all_uuids() -&gt; Dict[str, List[str]]\n</code></pre> <p>Return a dictionary of UUIDs for each device.</p> Source code in <code>src/ts_shape/loader/metadata/metadata_db_loader.py</code> <pre><code>def get_all_uuids(self) -&gt; Dict[str, List[str]]:\n    \"\"\"Return a dictionary of UUIDs for each device.\"\"\"\n    return self.device_uuids\n</code></pre>"},{"location":"reference/ts_shape/loader/metadata/metadata_json_loader/","title":"metadata_json_loader","text":""},{"location":"reference/ts_shape/loader/metadata/metadata_json_loader/#ts_shape.loader.metadata.metadata_json_loader","title":"ts_shape.loader.metadata.metadata_json_loader","text":"<p>Classes:</p> <ul> <li> <code>MetadataJsonLoader</code>           \u2013            <p>Load metadata JSON of shape:</p> </li> </ul>"},{"location":"reference/ts_shape/loader/metadata/metadata_json_loader/#ts_shape.loader.metadata.metadata_json_loader.MetadataJsonLoader","title":"MetadataJsonLoader","text":"<pre><code>MetadataJsonLoader(json_data: Any, *, strict: bool = True)\n</code></pre> Load metadata JSON of shape <p>{   \"uuid\":   {\"0\": \"...\", \"1\": \"...\", ...},   \"label\":  {\"0\": \"...\", \"1\": \"...\", ...},   \"config\": {\"0\": {...},  \"1\": {...},  ...} }</p> <p>into a pandas DataFrame with flattened config columns.</p> <p>Parameters:</p> <p>Methods:</p> <ul> <li> <code>filter_by_label</code>             \u2013              <p>Filter rows by a set or sequence of labels.</p> </li> <li> <code>filter_by_uuid</code>             \u2013              <p>Filter rows by a set or sequence of UUIDs.</p> </li> <li> <code>from_file</code>             \u2013              <p>Create a loader from a JSON file on disk.</p> </li> <li> <code>from_str</code>             \u2013              <p>Create a loader from a JSON string.</p> </li> <li> <code>get_by_label</code>             \u2013              <p>Retrieve the first row matching a label as a dictionary.</p> </li> <li> <code>get_by_uuid</code>             \u2013              <p>Retrieve a row by UUID as a dictionary.</p> </li> <li> <code>head</code>             \u2013              <p>Convenience wrapper for DataFrame.head.</p> </li> <li> <code>join_with</code>             \u2013              <p>Join the metadata DataFrame with another DataFrame on the 'uuid' index.</p> </li> <li> <code>list_labels</code>             \u2013              <p>Return all non-null labels.</p> </li> <li> <code>list_uuids</code>             \u2013              <p>Return a list of UUIDs present in the metadata index.</p> </li> <li> <code>to_df</code>             \u2013              <p>Return the underlying DataFrame.</p> </li> </ul> Source code in <code>src/ts_shape/loader/metadata/metadata_json_loader.py</code> <pre><code>def __init__(self, json_data: Any, *, strict: bool = True):\n    \"\"\"\n    Create a loader from JSON-like data.\n\n    Args:\n        json_data: Supported shapes:\n          - dict of columns with index-maps: {\"uuid\": {\"0\": ...}, \"label\": {...}, \"config\": {...}}\n          - dict of columns with lists: {\"uuid\": [...], \"label\": [...], \"config\": [...]}\n          - list of records: [{\"uuid\": ..., \"label\": ..., \"config\": {...}}, ...]\n        strict: If True, enforce presence of required keys and unique UUIDs.\n    \"\"\"\n    self.json_data = json_data\n    self.strict = strict\n    self.df = self._to_dataframe()\n</code></pre>"},{"location":"reference/ts_shape/loader/metadata/metadata_json_loader/#ts_shape.loader.metadata.metadata_json_loader.MetadataJsonLoader(json_data)","title":"<code>json_data</code>","text":"(<code>Any</code>)           \u2013            <p>Supported shapes: - dict of columns with index-maps: {\"uuid\": {\"0\": ...}, \"label\": {...}, \"config\": {...}} - dict of columns with lists: {\"uuid\": [...], \"label\": [...], \"config\": [...]} - list of records: [{\"uuid\": ..., \"label\": ..., \"config\": {...}}, ...]</p>"},{"location":"reference/ts_shape/loader/metadata/metadata_json_loader/#ts_shape.loader.metadata.metadata_json_loader.MetadataJsonLoader(strict)","title":"<code>strict</code>","text":"(<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, enforce presence of required keys and unique UUIDs.</p>"},{"location":"reference/ts_shape/loader/metadata/metadata_json_loader/#ts_shape.loader.metadata.metadata_json_loader.MetadataJsonLoader.filter_by_label","title":"filter_by_label","text":"<pre><code>filter_by_label(labels: Iterable[str]) -&gt; DataFrame\n</code></pre> <p>Filter rows by a set or sequence of labels.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>Filtered DataFrame.</p> </li> </ul> Source code in <code>src/ts_shape/loader/metadata/metadata_json_loader.py</code> <pre><code>def filter_by_label(self, labels: Iterable[str]) -&gt; pd.DataFrame:\n    \"\"\"\n    Filter rows by a set or sequence of labels.\n\n    Args:\n        labels: Iterable of label strings to retain.\n\n    Returns:\n        Filtered DataFrame.\n    \"\"\"\n    labels_set = set(labels)\n    return self.df[self.df[\"label\"].isin(labels_set)]\n</code></pre>"},{"location":"reference/ts_shape/loader/metadata/metadata_json_loader/#ts_shape.loader.metadata.metadata_json_loader.MetadataJsonLoader.filter_by_label(labels)","title":"<code>labels</code>","text":"(<code>Iterable[str]</code>)           \u2013            <p>Iterable of label strings to retain.</p>"},{"location":"reference/ts_shape/loader/metadata/metadata_json_loader/#ts_shape.loader.metadata.metadata_json_loader.MetadataJsonLoader.filter_by_uuid","title":"filter_by_uuid","text":"<pre><code>filter_by_uuid(uuids: Iterable[str]) -&gt; DataFrame\n</code></pre> <p>Filter rows by a set or sequence of UUIDs.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>Filtered DataFrame.</p> </li> </ul> Source code in <code>src/ts_shape/loader/metadata/metadata_json_loader.py</code> <pre><code>def filter_by_uuid(self, uuids: Iterable[str]) -&gt; pd.DataFrame:\n    \"\"\"\n    Filter rows by a set or sequence of UUIDs.\n\n    Args:\n        uuids: Iterable of UUID strings to retain.\n\n    Returns:\n        Filtered DataFrame.\n    \"\"\"\n    uuids_set = set(uuids)\n    return self.df[self.df.index.isin(uuids_set)]\n</code></pre>"},{"location":"reference/ts_shape/loader/metadata/metadata_json_loader/#ts_shape.loader.metadata.metadata_json_loader.MetadataJsonLoader.filter_by_uuid(uuids)","title":"<code>uuids</code>","text":"(<code>Iterable[str]</code>)           \u2013            <p>Iterable of UUID strings to retain.</p>"},{"location":"reference/ts_shape/loader/metadata/metadata_json_loader/#ts_shape.loader.metadata.metadata_json_loader.MetadataJsonLoader.from_file","title":"from_file  <code>classmethod</code>","text":"<pre><code>from_file(filepath: str, *, strict: bool = True) -&gt; MetadataJsonLoader\n</code></pre> <p>Create a loader from a JSON file on disk.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>MetadataJsonLoader</code>           \u2013            <p>MetadataJsonLoader instance.</p> </li> </ul> Source code in <code>src/ts_shape/loader/metadata/metadata_json_loader.py</code> <pre><code>@classmethod\ndef from_file(cls, filepath: str, *, strict: bool = True) -&gt; \"MetadataJsonLoader\":\n    \"\"\"\n    Create a loader from a JSON file on disk.\n\n    Args:\n        filepath: Path to the JSON file.\n        strict: Validation behavior; when True, enforces required fields and unique UUIDs.\n\n    Returns:\n        MetadataJsonLoader instance.\n    \"\"\"\n    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n        data = json.load(f)\n    return cls(data, strict=strict)\n</code></pre>"},{"location":"reference/ts_shape/loader/metadata/metadata_json_loader/#ts_shape.loader.metadata.metadata_json_loader.MetadataJsonLoader.from_file(filepath)","title":"<code>filepath</code>","text":"(<code>str</code>)           \u2013            <p>Path to the JSON file.</p>"},{"location":"reference/ts_shape/loader/metadata/metadata_json_loader/#ts_shape.loader.metadata.metadata_json_loader.MetadataJsonLoader.from_file(strict)","title":"<code>strict</code>","text":"(<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Validation behavior; when True, enforces required fields and unique UUIDs.</p>"},{"location":"reference/ts_shape/loader/metadata/metadata_json_loader/#ts_shape.loader.metadata.metadata_json_loader.MetadataJsonLoader.from_str","title":"from_str  <code>classmethod</code>","text":"<pre><code>from_str(json_str: str, *, strict: bool = True) -&gt; MetadataJsonLoader\n</code></pre> <p>Create a loader from a JSON string.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>MetadataJsonLoader</code>           \u2013            <p>MetadataJsonLoader instance.</p> </li> </ul> Source code in <code>src/ts_shape/loader/metadata/metadata_json_loader.py</code> <pre><code>@classmethod\ndef from_str(cls, json_str: str, *, strict: bool = True) -&gt; \"MetadataJsonLoader\":\n    \"\"\"\n    Create a loader from a JSON string.\n\n    Args:\n        json_str: Raw JSON content as a string.\n        strict: Validation behavior; when True, enforces required fields and unique UUIDs.\n\n    Returns:\n        MetadataJsonLoader instance.\n    \"\"\"\n    return cls(json.loads(json_str), strict=strict)\n</code></pre>"},{"location":"reference/ts_shape/loader/metadata/metadata_json_loader/#ts_shape.loader.metadata.metadata_json_loader.MetadataJsonLoader.from_str(json_str)","title":"<code>json_str</code>","text":"(<code>str</code>)           \u2013            <p>Raw JSON content as a string.</p>"},{"location":"reference/ts_shape/loader/metadata/metadata_json_loader/#ts_shape.loader.metadata.metadata_json_loader.MetadataJsonLoader.from_str(strict)","title":"<code>strict</code>","text":"(<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Validation behavior; when True, enforces required fields and unique UUIDs.</p>"},{"location":"reference/ts_shape/loader/metadata/metadata_json_loader/#ts_shape.loader.metadata.metadata_json_loader.MetadataJsonLoader.get_by_label","title":"get_by_label","text":"<pre><code>get_by_label(label: str) -&gt; Optional[Dict[str, Any]]\n</code></pre> <p>Retrieve the first row matching a label as a dictionary.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>Optional[Dict[str, Any]]</code>           \u2013            <p>Row as a dict, or None if not found.</p> </li> </ul> Source code in <code>src/ts_shape/loader/metadata/metadata_json_loader.py</code> <pre><code>def get_by_label(self, label: str) -&gt; Optional[Dict[str, Any]]:\n    \"\"\"\n    Retrieve the first row matching a label as a dictionary.\n\n    Args:\n        label: Label value to search for.\n\n    Returns:\n        Row as a dict, or None if not found.\n    \"\"\"\n    row = self.df[self.df[\"label\"] == label]\n    return None if row.empty else row.iloc[0].to_dict()\n</code></pre>"},{"location":"reference/ts_shape/loader/metadata/metadata_json_loader/#ts_shape.loader.metadata.metadata_json_loader.MetadataJsonLoader.get_by_label(label)","title":"<code>label</code>","text":"(<code>str</code>)           \u2013            <p>Label value to search for.</p>"},{"location":"reference/ts_shape/loader/metadata/metadata_json_loader/#ts_shape.loader.metadata.metadata_json_loader.MetadataJsonLoader.get_by_uuid","title":"get_by_uuid","text":"<pre><code>get_by_uuid(uuid: str) -&gt; Optional[Dict[str, Any]]\n</code></pre> <p>Retrieve a row by UUID as a dictionary.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>Optional[Dict[str, Any]]</code>           \u2013            <p>Row as a dict, or None if not present.</p> </li> </ul> Source code in <code>src/ts_shape/loader/metadata/metadata_json_loader.py</code> <pre><code>def get_by_uuid(self, uuid: str) -&gt; Optional[Dict[str, Any]]:\n    \"\"\"\n    Retrieve a row by UUID as a dictionary.\n\n    Args:\n        uuid: UUID key (index) to look up.\n\n    Returns:\n        Row as a dict, or None if not present.\n    \"\"\"\n    if uuid not in self.df.index:\n        return None\n    return self.df.loc[uuid].to_dict()\n</code></pre>"},{"location":"reference/ts_shape/loader/metadata/metadata_json_loader/#ts_shape.loader.metadata.metadata_json_loader.MetadataJsonLoader.get_by_uuid(uuid)","title":"<code>uuid</code>","text":"(<code>str</code>)           \u2013            <p>UUID key (index) to look up.</p>"},{"location":"reference/ts_shape/loader/metadata/metadata_json_loader/#ts_shape.loader.metadata.metadata_json_loader.MetadataJsonLoader.head","title":"head","text":"<pre><code>head(n: int = 5) -&gt; DataFrame\n</code></pre> <p>Convenience wrapper for DataFrame.head.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>Top n rows of the metadata DataFrame.</p> </li> </ul> Source code in <code>src/ts_shape/loader/metadata/metadata_json_loader.py</code> <pre><code>def head(self, n: int = 5) -&gt; pd.DataFrame:\n    \"\"\"\n    Convenience wrapper for DataFrame.head.\n\n    Args:\n        n: Number of rows to return.\n\n    Returns:\n        Top n rows of the metadata DataFrame.\n    \"\"\"\n    return self.df.head(n)\n</code></pre>"},{"location":"reference/ts_shape/loader/metadata/metadata_json_loader/#ts_shape.loader.metadata.metadata_json_loader.MetadataJsonLoader.head(n)","title":"<code>n</code>","text":"(<code>int</code>, default:                   <code>5</code> )           \u2013            <p>Number of rows to return.</p>"},{"location":"reference/ts_shape/loader/metadata/metadata_json_loader/#ts_shape.loader.metadata.metadata_json_loader.MetadataJsonLoader.join_with","title":"join_with","text":"<pre><code>join_with(other_df: DataFrame, how: str = 'inner') -&gt; DataFrame\n</code></pre> <p>Join the metadata DataFrame with another DataFrame on the 'uuid' index.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>Joined pandas DataFrame.</p> </li> </ul> Source code in <code>src/ts_shape/loader/metadata/metadata_json_loader.py</code> <pre><code>def join_with(self, other_df: pd.DataFrame, how: str = \"inner\") -&gt; pd.DataFrame:\n    \"\"\"\n    Join the metadata DataFrame with another DataFrame on the 'uuid' index.\n\n    Args:\n        other_df: DataFrame to join with (must be indexed compatibly).\n        how: Join strategy (e.g., 'inner', 'left', 'outer').\n\n    Returns:\n        Joined pandas DataFrame.\n    \"\"\"\n    return self.df.join(other_df, how=how)\n</code></pre>"},{"location":"reference/ts_shape/loader/metadata/metadata_json_loader/#ts_shape.loader.metadata.metadata_json_loader.MetadataJsonLoader.join_with(other_df)","title":"<code>other_df</code>","text":"(<code>DataFrame</code>)           \u2013            <p>DataFrame to join with (must be indexed compatibly).</p>"},{"location":"reference/ts_shape/loader/metadata/metadata_json_loader/#ts_shape.loader.metadata.metadata_json_loader.MetadataJsonLoader.join_with(how)","title":"<code>how</code>","text":"(<code>str</code>, default:                   <code>'inner'</code> )           \u2013            <p>Join strategy (e.g., 'inner', 'left', 'outer').</p>"},{"location":"reference/ts_shape/loader/metadata/metadata_json_loader/#ts_shape.loader.metadata.metadata_json_loader.MetadataJsonLoader.list_labels","title":"list_labels","text":"<pre><code>list_labels() -&gt; List[str]\n</code></pre> <p>Return all non-null labels.</p> <p>Returns:</p> <ul> <li> <code>List[str]</code>           \u2013            <p>List of label strings.</p> </li> </ul> Source code in <code>src/ts_shape/loader/metadata/metadata_json_loader.py</code> <pre><code>def list_labels(self) -&gt; List[str]:\n    \"\"\"\n    Return all non-null labels.\n\n    Returns:\n        List of label strings.\n    \"\"\"\n    return list(self.df[\"label\"].dropna().astype(str))\n</code></pre>"},{"location":"reference/ts_shape/loader/metadata/metadata_json_loader/#ts_shape.loader.metadata.metadata_json_loader.MetadataJsonLoader.list_uuids","title":"list_uuids","text":"<pre><code>list_uuids() -&gt; List[str]\n</code></pre> <p>Return a list of UUIDs present in the metadata index.</p> <p>Returns:</p> <ul> <li> <code>List[str]</code>           \u2013            <p>List of UUID strings.</p> </li> </ul> Source code in <code>src/ts_shape/loader/metadata/metadata_json_loader.py</code> <pre><code>def list_uuids(self) -&gt; List[str]:\n    \"\"\"\n    Return a list of UUIDs present in the metadata index.\n\n    Returns:\n        List of UUID strings.\n    \"\"\"\n    return list(self.df.index.astype(str))\n</code></pre>"},{"location":"reference/ts_shape/loader/metadata/metadata_json_loader/#ts_shape.loader.metadata.metadata_json_loader.MetadataJsonLoader.to_df","title":"to_df","text":"<pre><code>to_df(copy: bool = True) -&gt; DataFrame\n</code></pre> <p>Return the underlying DataFrame.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>pandas DataFrame indexed by 'uuid'.</p> </li> </ul> Source code in <code>src/ts_shape/loader/metadata/metadata_json_loader.py</code> <pre><code>def to_df(self, copy: bool = True) -&gt; pd.DataFrame:\n    \"\"\"\n    Return the underlying DataFrame.\n\n    Args:\n        copy: When True, returns a copy; otherwise returns a view/reference.\n\n    Returns:\n        pandas DataFrame indexed by 'uuid'.\n    \"\"\"\n    return self.df.copy() if copy else self.df\n</code></pre>"},{"location":"reference/ts_shape/loader/metadata/metadata_json_loader/#ts_shape.loader.metadata.metadata_json_loader.MetadataJsonLoader.to_df(copy)","title":"<code>copy</code>","text":"(<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>When True, returns a copy; otherwise returns a view/reference.</p>"},{"location":"reference/ts_shape/loader/timeseries/__init__/","title":"init","text":""},{"location":"reference/ts_shape/loader/timeseries/__init__/#ts_shape.loader.timeseries","title":"ts_shape.loader.timeseries","text":"<p>Timeseries Loaders</p> <p>Load timeseries data from parquet folders, S3-compatible stores, Azure Blob, and TimescaleDB.</p> <ul> <li>ParquetLoader: Read parquet files from local folder structures.</li> <li>load_all_files: Load all parquet under a base path.</li> <li>load_by_time_range: Load files within YYYY/MM/DD/HH path range.</li> <li>load_by_uuid_list: Load files matching UUIDs in filenames.</li> <li> <p>load_files_by_time_range_and_uuids: Combine time range and UUID filters.</p> </li> <li> <p>S3ProxyDataAccess: Retrieve parquet via an S3-compatible proxy.</p> </li> <li>fetch_data_as_parquet: Save parquet files to a local folder structure.</li> <li> <p>fetch_data_as_dataframe: Return a combined DataFrame.</p> </li> <li> <p>AzureBlobParquetLoader: Load parquet from Azure Blob Storage.</p> </li> <li>load_all_files: Load all parquet under an optional prefix.</li> <li>load_by_time_range: Load hourly folders between start and end.</li> <li>stream_by_time_range: Yield (blob, DataFrame) incrementally.</li> <li>load_files_by_time_range_and_uuids: Load per-hour per-UUID parquet files.</li> <li>stream_files_by_time_range_and_uuids: Yield per-UUID frames incrementally.</li> <li> <p>list_structure: List folders and files under a prefix.</p> </li> <li> <p>AzureBlobFlexibleFileLoader: Load arbitrary file types from Azure Blob Storage.</p> </li> <li>list_files_by_time_range: List matching files (by extension) under hourly folders.</li> <li>iter_file_names_by_time_range: Generator of names without downloading.</li> <li>fetch_files_by_time_range: Download matching files as raw bytes or parsed objects.</li> <li>stream_files_by_time_range: Stream (blob, bytes/parsed) incrementally.</li> <li>fetch_files_by_time_range_and_basenames: Download by explicit basenames.</li> <li>stream_files_by_time_range_and_basenames: Stream by explicit basenames.</li> <li> <p>register_parser/unregister_parser: Plug-in parser functions per file extension.</p> </li> <li> <p>TimescaleDBDataAccess: Stream timeseries from TimescaleDB.</p> </li> <li>fetch_data_as_parquet: Partition-by-hour and write parquet.</li> <li>fetch_data_as_dataframe: Return a combined DataFrame.</li> </ul> <p>Modules:</p> <ul> <li> <code>azure_blob_loader</code>           \u2013            </li> <li> <code>parquet_loader</code>           \u2013            </li> <li> <code>s3proxy_parquet_loader</code>           \u2013            </li> <li> <code>timescale_loader</code>           \u2013            </li> </ul>"},{"location":"reference/ts_shape/loader/timeseries/azure_blob_loader/","title":"azure_blob_loader","text":""},{"location":"reference/ts_shape/loader/timeseries/azure_blob_loader/#ts_shape.loader.timeseries.azure_blob_loader","title":"ts_shape.loader.timeseries.azure_blob_loader","text":"<p>Classes:</p> <ul> <li> <code>AzureBlobFlexibleFileLoader</code>           \u2013            <p>Load arbitrary file types from Azure Blob Storage under time-structured folders.</p> </li> <li> <code>AzureBlobParquetLoader</code>           \u2013            <p>Load parquet files from an Azure Blob Storage container filtered by a list of UUIDs.</p> </li> </ul>"},{"location":"reference/ts_shape/loader/timeseries/azure_blob_loader/#ts_shape.loader.timeseries.azure_blob_loader.AzureBlobFlexibleFileLoader","title":"AzureBlobFlexibleFileLoader","text":"<pre><code>AzureBlobFlexibleFileLoader(container_name: str, *, connection_string: Optional[str] = None, account_url: Optional[str] = None, credential: Optional[object] = None, prefix: str = '', max_workers: int = 8, hour_pattern: str = '{Y}/{m}/{d}/{H}/')\n</code></pre> <p>Load arbitrary file types from Azure Blob Storage under time-structured folders.</p> <p>Designed for containers with paths like: prefix/YYYY/MM/DD/HH//file.ext This class lists by per-hour prefix and can filter by extensions and/or basenames, then downloads files concurrently as raw bytes. <p>Methods:</p> <ul> <li> <code>fetch_files_by_time_range</code>             \u2013              <p>Download files that match extensions within [start, end] hour prefixes.</p> </li> <li> <code>fetch_files_by_time_range_and_basenames</code>             \u2013              <p>Download files whose basename (final path segment) is in <code>basenames</code>,</p> </li> <li> <code>iter_file_names_by_time_range</code>             \u2013              <p>Yield blob names under each hourly prefix within [start, end].</p> </li> <li> <code>list_files_by_time_range</code>             \u2013              <p>List blob names under each hourly prefix within [start, end].</p> </li> <li> <code>stream_files_by_time_range</code>             \u2013              <p>Stream matching files as (blob_name, bytes-or-parsed) within [start, end].</p> </li> <li> <code>stream_files_by_time_range_and_basenames</code>             \u2013              <p>Stream files whose basename is in <code>basenames</code> within [start, end].</p> </li> </ul> Source code in <code>src/ts_shape/loader/timeseries/azure_blob_loader.py</code> <pre><code>def __init__(\n    self,\n    container_name: str,\n    *,\n    connection_string: Optional[str] = None,\n    account_url: Optional[str] = None,\n    credential: Optional[object] = None,\n    prefix: str = \"\",\n    max_workers: int = 8,\n    hour_pattern: str = \"{Y}/{m}/{d}/{H}/\",\n) -&gt; None:\n    try:\n        from azure.storage.blob import ContainerClient  # type: ignore\n    except Exception as exc:  # pragma: no cover - import guard\n        raise ImportError(\n            \"azure-storage-blob is required for AzureBlobFlexibleFileLoader. \"\n            \"Install with `pip install azure-storage-blob`.\"\n        ) from exc\n\n    # Prefer AAD credential path if account_url provided or credential is given\n    if account_url or (credential is not None and not connection_string):\n        if not account_url:\n            raise ValueError(\"account_url must be provided when using AAD credential auth\")\n        if credential is None:\n            raise ValueError(\"credential must be provided when using AAD credential auth\")\n        self.container_client = ContainerClient(account_url=account_url, container_name=container_name, credential=credential)\n    else:\n        if not connection_string:\n            raise ValueError(\"Either connection_string or (account_url + credential) must be provided\")\n        self.container_client = ContainerClient.from_connection_string(\n            conn_str=connection_string, container_name=container_name\n        )\n    self.prefix = prefix\n    self.max_workers = max_workers if max_workers &gt; 0 else 1\n    # Pattern for hour-level subpath; tokens: {Y} {m} {d} {H}\n    self.hour_pattern = hour_pattern\n    # Initialize parsers lazily once per process\n    if not AzureBlobFlexibleFileLoader._parsers_initialized:\n        self._enable_builtin_parsers()\n        AzureBlobFlexibleFileLoader._parsers_initialized = True\n</code></pre>"},{"location":"reference/ts_shape/loader/timeseries/azure_blob_loader/#ts_shape.loader.timeseries.azure_blob_loader.AzureBlobFlexibleFileLoader.fetch_files_by_time_range","title":"fetch_files_by_time_range","text":"<pre><code>fetch_files_by_time_range(start_timestamp: str | Timestamp, end_timestamp: str | Timestamp, *, extensions: Optional[Iterable[str]] = None, parse: bool = False) -&gt; Dict[str, Any]\n</code></pre> <p>Download files that match extensions within [start, end] hour prefixes. Returns a dict mapping blob_name -&gt; parsed object (if parse=True and a parser exists), otherwise raw bytes.</p> Source code in <code>src/ts_shape/loader/timeseries/azure_blob_loader.py</code> <pre><code>def fetch_files_by_time_range(\n    self,\n    start_timestamp: str | pd.Timestamp,\n    end_timestamp: str | pd.Timestamp,\n    *,\n    extensions: Optional[Iterable[str]] = None,\n    parse: bool = False,\n) -&gt; Dict[str, Any]:\n    \"\"\"\n    Download files that match extensions within [start, end] hour prefixes.\n    Returns a dict mapping blob_name -&gt; parsed object (if parse=True and a parser exists),\n    otherwise raw bytes.\n    \"\"\"\n    blob_names = self.list_files_by_time_range(start_timestamp, end_timestamp, extensions=extensions)\n    if not blob_names:\n        return {}\n    results: Dict[str, Any] = {}\n    with ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n        future_to_name = {executor.submit(self._download_bytes, n): n for n in blob_names}\n        for fut in as_completed(future_to_name):\n            name = future_to_name[fut]\n            content = fut.result()\n            if content is not None:\n                results[name] = self._parse_bytes(name, content) if parse else content\n    return results\n</code></pre>"},{"location":"reference/ts_shape/loader/timeseries/azure_blob_loader/#ts_shape.loader.timeseries.azure_blob_loader.AzureBlobFlexibleFileLoader.fetch_files_by_time_range_and_basenames","title":"fetch_files_by_time_range_and_basenames","text":"<pre><code>fetch_files_by_time_range_and_basenames(start_timestamp: str | Timestamp, end_timestamp: str | Timestamp, basenames: Iterable[str], *, extensions: Optional[Iterable[str]] = None, parse: bool = False) -&gt; Dict[str, Any]\n</code></pre> <p>Download files whose basename (final path segment) is in <code>basenames</code>, optionally filtered by extensions, within [start, end] hour prefixes. Returns blob_name -&gt; parsed object (if parse=True and a parser exists), otherwise raw bytes.</p> Source code in <code>src/ts_shape/loader/timeseries/azure_blob_loader.py</code> <pre><code>def fetch_files_by_time_range_and_basenames(\n    self,\n    start_timestamp: str | pd.Timestamp,\n    end_timestamp: str | pd.Timestamp,\n    basenames: Iterable[str],\n    *,\n    extensions: Optional[Iterable[str]] = None,\n    parse: bool = False,\n) -&gt; Dict[str, Any]:\n    \"\"\"\n    Download files whose basename (final path segment) is in `basenames`,\n    optionally filtered by extensions, within [start, end] hour prefixes.\n    Returns blob_name -&gt; parsed object (if parse=True and a parser exists), otherwise raw bytes.\n    \"\"\"\n    base_set = {str(b).strip() for b in basenames if str(b).strip()}\n    allowed_exts = self._normalize_exts(extensions)\n    candidates: List[str] = []\n    for pfx in (self._hour_prefix(ts) for ts in self._hourly_slots(start_timestamp, end_timestamp)):\n        blob_iter = self.container_client.list_blobs(name_starts_with=pfx)\n        for b in blob_iter:  # type: ignore[attr-defined]\n            name = str(b.name)\n            base = name.rsplit('/', 1)[-1]\n            if base not in base_set:\n                continue\n            if allowed_exts is not None and not any(name.lower().endswith(ext) for ext in allowed_exts):\n                continue\n            candidates.append(name)\n\n    if not candidates:\n        return {}\n\n    results: Dict[str, Any] = {}\n    with ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n        future_to_name = {executor.submit(self._download_bytes, n): n for n in candidates}\n        for fut in as_completed(future_to_name):\n            name = future_to_name[fut]\n            content = fut.result()\n            if content is not None:\n                results[name] = self._parse_bytes(name, content) if parse else content\n    return results\n</code></pre>"},{"location":"reference/ts_shape/loader/timeseries/azure_blob_loader/#ts_shape.loader.timeseries.azure_blob_loader.AzureBlobFlexibleFileLoader.iter_file_names_by_time_range","title":"iter_file_names_by_time_range","text":"<pre><code>iter_file_names_by_time_range(start_timestamp: str | Timestamp, end_timestamp: str | Timestamp, *, extensions: Optional[Iterable[str]] = None) -&gt; Iterator[str]\n</code></pre> <p>Yield blob names under each hourly prefix within [start, end]. Uses server-side prefix listing and client-side extension filtering.</p> Source code in <code>src/ts_shape/loader/timeseries/azure_blob_loader.py</code> <pre><code>def iter_file_names_by_time_range(\n    self,\n    start_timestamp: str | pd.Timestamp,\n    end_timestamp: str | pd.Timestamp,\n    *,\n    extensions: Optional[Iterable[str]] = None,\n) -&gt; Iterator[str]:\n    \"\"\"\n    Yield blob names under each hourly prefix within [start, end].\n    Uses server-side prefix listing and client-side extension filtering.\n    \"\"\"\n    allowed_exts = self._normalize_exts(extensions)\n    for pfx in (self._hour_prefix(ts) for ts in self._hourly_slots(start_timestamp, end_timestamp)):\n        blob_iter = self.container_client.list_blobs(name_starts_with=pfx)\n        for b in blob_iter:  # type: ignore[attr-defined]\n            name = str(b.name)\n            if allowed_exts is not None:\n                if not any(name.lower().endswith(ext) for ext in allowed_exts):\n                    continue\n            yield name\n</code></pre>"},{"location":"reference/ts_shape/loader/timeseries/azure_blob_loader/#ts_shape.loader.timeseries.azure_blob_loader.AzureBlobFlexibleFileLoader.list_files_by_time_range","title":"list_files_by_time_range","text":"<pre><code>list_files_by_time_range(start_timestamp: str | Timestamp, end_timestamp: str | Timestamp, *, extensions: Optional[Iterable[str]] = None, limit: Optional[int] = None) -&gt; List[str]\n</code></pre> <p>List blob names under each hourly prefix within [start, end].</p> <p>Parameters:</p> Source code in <code>src/ts_shape/loader/timeseries/azure_blob_loader.py</code> <pre><code>def list_files_by_time_range(\n    self,\n    start_timestamp: str | pd.Timestamp,\n    end_timestamp: str | pd.Timestamp,\n    *,\n    extensions: Optional[Iterable[str]] = None,\n    limit: Optional[int] = None,\n) -&gt; List[str]:\n    \"\"\"\n    List blob names under each hourly prefix within [start, end].\n\n    Args:\n        extensions: Optional set/list of file extensions (e.g., {\"json\", \".bmp\"}). Case-insensitive.\n        limit: Optional cap on number of files collected.\n    \"\"\"\n    allowed_exts = self._normalize_exts(extensions)\n    names: List[str] = []\n    collected = 0\n    for pfx in (self._hour_prefix(ts) for ts in self._hourly_slots(start_timestamp, end_timestamp)):\n        blob_iter = self.container_client.list_blobs(name_starts_with=pfx)\n        for b in blob_iter:  # type: ignore[attr-defined]\n            name = str(b.name)\n            if allowed_exts is not None:\n                lower_name = name.lower()\n                if not any(lower_name.endswith(ext) for ext in allowed_exts):\n                    continue\n            names.append(name)\n            collected += 1\n            if limit is not None and collected &gt;= limit:\n                return names\n    return names\n</code></pre>"},{"location":"reference/ts_shape/loader/timeseries/azure_blob_loader/#ts_shape.loader.timeseries.azure_blob_loader.AzureBlobFlexibleFileLoader.list_files_by_time_range(extensions)","title":"<code>extensions</code>","text":"(<code>Optional[Iterable[str]]</code>, default:                   <code>None</code> )           \u2013            <p>Optional set/list of file extensions (e.g., {\"json\", \".bmp\"}). Case-insensitive.</p>"},{"location":"reference/ts_shape/loader/timeseries/azure_blob_loader/#ts_shape.loader.timeseries.azure_blob_loader.AzureBlobFlexibleFileLoader.list_files_by_time_range(limit)","title":"<code>limit</code>","text":"(<code>Optional[int]</code>, default:                   <code>None</code> )           \u2013            <p>Optional cap on number of files collected.</p>"},{"location":"reference/ts_shape/loader/timeseries/azure_blob_loader/#ts_shape.loader.timeseries.azure_blob_loader.AzureBlobFlexibleFileLoader.stream_files_by_time_range","title":"stream_files_by_time_range","text":"<pre><code>stream_files_by_time_range(start_timestamp: str | Timestamp, end_timestamp: str | Timestamp, *, extensions: Optional[Iterable[str]] = None, parse: bool = False) -&gt; Iterator[Tuple[str, Any]]\n</code></pre> <p>Stream matching files as (blob_name, bytes-or-parsed) within [start, end]. Maintains up to <code>max_workers</code> concurrent downloads while yielding incrementally.</p> Source code in <code>src/ts_shape/loader/timeseries/azure_blob_loader.py</code> <pre><code>def stream_files_by_time_range(\n    self,\n    start_timestamp: str | pd.Timestamp,\n    end_timestamp: str | pd.Timestamp,\n    *,\n    extensions: Optional[Iterable[str]] = None,\n    parse: bool = False,\n) -&gt; Iterator[Tuple[str, Any]]:\n    \"\"\"\n    Stream matching files as (blob_name, bytes-or-parsed) within [start, end].\n    Maintains up to `max_workers` concurrent downloads while yielding incrementally.\n    \"\"\"\n    names_iter = self.iter_file_names_by_time_range(start_timestamp, end_timestamp, extensions=extensions)\n\n    with ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n        future_to_name: Dict[Any, str] = {}\n        # initial fill\n        try:\n            while len(future_to_name) &lt; self.max_workers:\n                n = next(names_iter)\n                future_to_name[executor.submit(self._download_bytes, n)] = n\n        except StopIteration:\n            pass\n\n        while future_to_name:\n            # Drain\n            for fut in as_completed(list(future_to_name.keys())):\n                name = future_to_name.pop(fut)\n                try:\n                    content = fut.result()\n                except Exception:\n                    content = None\n                if content is not None:\n                    yield (name, self._parse_bytes(name, content) if parse else content)\n\n            # Refill\n            try:\n                while len(future_to_name) &lt; self.max_workers:\n                    n = next(names_iter)\n                    future_to_name[executor.submit(self._download_bytes, n)] = n\n            except StopIteration:\n                pass\n</code></pre>"},{"location":"reference/ts_shape/loader/timeseries/azure_blob_loader/#ts_shape.loader.timeseries.azure_blob_loader.AzureBlobFlexibleFileLoader.stream_files_by_time_range_and_basenames","title":"stream_files_by_time_range_and_basenames","text":"<pre><code>stream_files_by_time_range_and_basenames(start_timestamp: str | Timestamp, end_timestamp: str | Timestamp, basenames: Iterable[str], *, extensions: Optional[Iterable[str]] = None, parse: bool = False) -&gt; Iterator[Tuple[str, Any]]\n</code></pre> <p>Stream files whose basename is in <code>basenames</code> within [start, end]. Yields (blob_name, bytes-or-parsed) incrementally with bounded concurrency.</p> Source code in <code>src/ts_shape/loader/timeseries/azure_blob_loader.py</code> <pre><code>def stream_files_by_time_range_and_basenames(\n    self,\n    start_timestamp: str | pd.Timestamp,\n    end_timestamp: str | pd.Timestamp,\n    basenames: Iterable[str],\n    *,\n    extensions: Optional[Iterable[str]] = None,\n    parse: bool = False,\n) -&gt; Iterator[Tuple[str, Any]]:\n    \"\"\"\n    Stream files whose basename is in `basenames` within [start, end].\n    Yields (blob_name, bytes-or-parsed) incrementally with bounded concurrency.\n    \"\"\"\n    base_set = {str(b).strip() for b in basenames if str(b).strip()}\n    allowed_exts = self._normalize_exts(extensions)\n\n    def _names_iter() -&gt; Iterator[str]:\n        for pfx in (self._hour_prefix(ts) for ts in self._hourly_slots(start_timestamp, end_timestamp)):\n            blob_iter = self.container_client.list_blobs(name_starts_with=pfx)\n            for b in blob_iter:  # type: ignore[attr-defined]\n                name = str(b.name)\n                base = name.rsplit('/', 1)[-1]\n                if base not in base_set:\n                    continue\n                if allowed_exts is not None and not any(name.lower().endswith(ext) for ext in allowed_exts):\n                    continue\n                yield name\n\n    names_iter = _names_iter()\n    with ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n        future_to_name: Dict[Any, str] = {}\n        # initial fill\n        try:\n            while len(future_to_name) &lt; self.max_workers:\n                n = next(names_iter)\n                future_to_name[executor.submit(self._download_bytes, n)] = n\n        except StopIteration:\n            pass\n\n        while future_to_name:\n            for fut in as_completed(list(future_to_name.keys())):\n                name = future_to_name.pop(fut)\n                try:\n                    content = fut.result()\n                except Exception:\n                    content = None\n                if content is not None:\n                    yield (name, self._parse_bytes(name, content) if parse else content)\n\n            try:\n                while len(future_to_name) &lt; self.max_workers:\n                    n = next(names_iter)\n                    future_to_name[executor.submit(self._download_bytes, n)] = n\n            except StopIteration:\n                pass\n</code></pre>"},{"location":"reference/ts_shape/loader/timeseries/azure_blob_loader/#ts_shape.loader.timeseries.azure_blob_loader.AzureBlobParquetLoader","title":"AzureBlobParquetLoader","text":"<pre><code>AzureBlobParquetLoader(container_name: str, *, connection_string: Optional[str] = None, account_url: Optional[str] = None, credential: Optional[object] = None, prefix: str = '', max_workers: int = 8, hour_pattern: str = '{Y}/{m}/{d}/{H}/')\n</code></pre> <p>Load parquet files from an Azure Blob Storage container filtered by a list of UUIDs.</p> <p>Optimized for speed by: - Using server-side prefix filtering when provided - Streaming blob listings and filtering client-side by UUID containment - Downloading and parsing parquet files concurrently</p> <p>Parameters:</p> <p>Methods:</p> <ul> <li> <code>from_account_name</code>             \u2013              <p>Construct a loader using AAD credentials with an account name.</p> </li> <li> <code>list_structure</code>             \u2013              <p>List folder prefixes (hours) and blob names under the configured <code>prefix</code>.</p> </li> <li> <code>load_all_files</code>             \u2013              <p>Load all parquet blobs in the container (optionally under <code>prefix</code>).</p> </li> <li> <code>load_by_time_range</code>             \u2013              <p>Load all parquet blobs under hourly folders within [start, end].</p> </li> <li> <code>load_files_by_time_range_and_uuids</code>             \u2013              <p>Load parquet blobs for given UUIDs within [start, end] hours.</p> </li> <li> <code>stream_by_time_range</code>             \u2013              <p>Stream parquet DataFrames under hourly folders within [start, end].</p> </li> <li> <code>stream_files_by_time_range_and_uuids</code>             \u2013              <p>Stream parquet DataFrames for given UUIDs within [start, end] hours.</p> </li> </ul> Source code in <code>src/ts_shape/loader/timeseries/azure_blob_loader.py</code> <pre><code>def __init__(\n    self,\n    container_name: str,\n    *,\n    connection_string: Optional[str] = None,\n    account_url: Optional[str] = None,\n    credential: Optional[object] = None,\n    prefix: str = \"\",\n    max_workers: int = 8,\n    hour_pattern: str = \"{Y}/{m}/{d}/{H}/\",\n) -&gt; None:\n    \"\"\"\n    Initialize the loader with Azure connection details.\n\n    Args:\n        connection_string: Azure Storage connection string.\n        container_name: Target container name.\n        prefix: Optional path prefix to narrow listing (e.g. \"year/month/\").\n        max_workers: Max concurrent downloads/reads.\n    \"\"\"\n    try:\n        from azure.storage.blob import ContainerClient  # type: ignore\n    except Exception as exc:  # pragma: no cover - import guard\n        raise ImportError(\n            \"azure-storage-blob is required for AzureBlobParquetLoader. \"\n            \"Install with `pip install azure-storage-blob`.\"\n        ) from exc\n\n    self._ContainerClient = ContainerClient\n    # Prefer AAD credential path if account_url provided or credential is given\n    if account_url or (credential is not None and not connection_string):\n        if not account_url:\n            raise ValueError(\"account_url must be provided when using AAD credential auth\")\n        if credential is None:\n            raise ValueError(\"credential must be provided when using AAD credential auth\")\n        self.container_client = ContainerClient(account_url=account_url, container_name=container_name, credential=credential)\n    else:\n        if not connection_string:\n            raise ValueError(\"Either connection_string or (account_url + credential) must be provided\")\n        self.container_client = ContainerClient.from_connection_string(\n            conn_str=connection_string, container_name=container_name\n        )\n    self.prefix = prefix\n    self.max_workers = max_workers if max_workers &gt; 0 else 1\n    # Pattern for hour-level subpath; tokens: {Y} {m} {d} {H}\n    # Default matches many data lake layouts: YYYY/MM/DD/HH/\n    self.hour_pattern = hour_pattern\n</code></pre>"},{"location":"reference/ts_shape/loader/timeseries/azure_blob_loader/#ts_shape.loader.timeseries.azure_blob_loader.AzureBlobParquetLoader(connection_string)","title":"<code>connection_string</code>","text":"(<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Azure Storage connection string.</p>"},{"location":"reference/ts_shape/loader/timeseries/azure_blob_loader/#ts_shape.loader.timeseries.azure_blob_loader.AzureBlobParquetLoader(container_name)","title":"<code>container_name</code>","text":"(<code>str</code>)           \u2013            <p>Target container name.</p>"},{"location":"reference/ts_shape/loader/timeseries/azure_blob_loader/#ts_shape.loader.timeseries.azure_blob_loader.AzureBlobParquetLoader(prefix)","title":"<code>prefix</code>","text":"(<code>str</code>, default:                   <code>''</code> )           \u2013            <p>Optional path prefix to narrow listing (e.g. \"year/month/\").</p>"},{"location":"reference/ts_shape/loader/timeseries/azure_blob_loader/#ts_shape.loader.timeseries.azure_blob_loader.AzureBlobParquetLoader(max_workers)","title":"<code>max_workers</code>","text":"(<code>int</code>, default:                   <code>8</code> )           \u2013            <p>Max concurrent downloads/reads.</p>"},{"location":"reference/ts_shape/loader/timeseries/azure_blob_loader/#ts_shape.loader.timeseries.azure_blob_loader.AzureBlobParquetLoader.from_account_name","title":"from_account_name  <code>classmethod</code>","text":"<pre><code>from_account_name(account_name: str, container_name: str, *, credential: Optional[object] = None, endpoint_suffix: str = 'blob.core.windows.net', prefix: str = '', max_workers: int = 8) -&gt; AzureBlobParquetLoader\n</code></pre> <p>Construct a loader using AAD credentials with an account name.</p> <p>Parameters:</p> Source code in <code>src/ts_shape/loader/timeseries/azure_blob_loader.py</code> <pre><code>@classmethod\ndef from_account_name(\n    cls,\n    account_name: str,\n    container_name: str,\n    *,\n    credential: Optional[object] = None,\n    endpoint_suffix: str = \"blob.core.windows.net\",\n    prefix: str = \"\",\n    max_workers: int = 8,\n) -&gt; \"AzureBlobParquetLoader\":\n    \"\"\"\n    Construct a loader using AAD credentials with an account name.\n\n    Args:\n        account_name: Storage account name.\n        container_name: Target container.\n        credential: Optional Azure credential (DefaultAzureCredential if None).\n        endpoint_suffix: DNS suffix for the blob endpoint (e.g., for sovereign clouds).\n        prefix: Optional listing prefix (e.g., \"parquet/\").\n        max_workers: Concurrency for downloads.\n    \"\"\"\n    account_url = f\"https://{account_name}.{endpoint_suffix}\"\n    if credential is None:\n        raise ValueError(\"credential must be provided when using AAD credential auth\")\n    return cls(\n        container_name=container_name,\n        account_url=account_url,\n        credential=credential,\n        prefix=prefix,\n        max_workers=max_workers,\n    )\n</code></pre>"},{"location":"reference/ts_shape/loader/timeseries/azure_blob_loader/#ts_shape.loader.timeseries.azure_blob_loader.AzureBlobParquetLoader.from_account_name(account_name)","title":"<code>account_name</code>","text":"(<code>str</code>)           \u2013            <p>Storage account name.</p>"},{"location":"reference/ts_shape/loader/timeseries/azure_blob_loader/#ts_shape.loader.timeseries.azure_blob_loader.AzureBlobParquetLoader.from_account_name(container_name)","title":"<code>container_name</code>","text":"(<code>str</code>)           \u2013            <p>Target container.</p>"},{"location":"reference/ts_shape/loader/timeseries/azure_blob_loader/#ts_shape.loader.timeseries.azure_blob_loader.AzureBlobParquetLoader.from_account_name(credential)","title":"<code>credential</code>","text":"(<code>Optional[object]</code>, default:                   <code>None</code> )           \u2013            <p>Optional Azure credential (DefaultAzureCredential if None).</p>"},{"location":"reference/ts_shape/loader/timeseries/azure_blob_loader/#ts_shape.loader.timeseries.azure_blob_loader.AzureBlobParquetLoader.from_account_name(endpoint_suffix)","title":"<code>endpoint_suffix</code>","text":"(<code>str</code>, default:                   <code>'blob.core.windows.net'</code> )           \u2013            <p>DNS suffix for the blob endpoint (e.g., for sovereign clouds).</p>"},{"location":"reference/ts_shape/loader/timeseries/azure_blob_loader/#ts_shape.loader.timeseries.azure_blob_loader.AzureBlobParquetLoader.from_account_name(prefix)","title":"<code>prefix</code>","text":"(<code>str</code>, default:                   <code>''</code> )           \u2013            <p>Optional listing prefix (e.g., \"parquet/\").</p>"},{"location":"reference/ts_shape/loader/timeseries/azure_blob_loader/#ts_shape.loader.timeseries.azure_blob_loader.AzureBlobParquetLoader.from_account_name(max_workers)","title":"<code>max_workers</code>","text":"(<code>int</code>, default:                   <code>8</code> )           \u2013            <p>Concurrency for downloads.</p>"},{"location":"reference/ts_shape/loader/timeseries/azure_blob_loader/#ts_shape.loader.timeseries.azure_blob_loader.AzureBlobParquetLoader.list_structure","title":"list_structure","text":"<pre><code>list_structure(parquet_only: bool = True, limit: Optional[int] = None) -&gt; Dict[str, List[str]]\n</code></pre> <p>List folder prefixes (hours) and blob names under the configured <code>prefix</code>.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>Dict[str, List[str]]</code>           \u2013            <p>A dict with:</p> </li> <li> <code>Dict[str, List[str]]</code>           \u2013            <ul> <li>folders: Sorted unique hour-level prefixes like 'parquet/YYYY/MM/DD/HH/'</li> </ul> </li> <li> <code>Dict[str, List[str]]</code>           \u2013            <ul> <li>files: Sorted blob names (full paths) matching the filter</li> </ul> </li> </ul> Source code in <code>src/ts_shape/loader/timeseries/azure_blob_loader.py</code> <pre><code>def list_structure(self, parquet_only: bool = True, limit: Optional[int] = None) -&gt; Dict[str, List[str]]:\n    \"\"\"\n    List folder prefixes (hours) and blob names under the configured `prefix`.\n\n    Args:\n        parquet_only: If True, only include blobs ending with .parquet.\n        limit: Optional cap on number of files collected for quick inspection.\n\n    Returns:\n        A dict with:\n        - folders: Sorted unique hour-level prefixes like 'parquet/YYYY/MM/DD/HH/'\n        - files: Sorted blob names (full paths) matching the filter\n    \"\"\"\n    folders: Set[str] = set()\n    files: List[str] = []\n    collected = 0\n\n    blob_iter = self.container_client.list_blobs(name_starts_with=self.prefix or None)\n    for b in blob_iter:\n        name = str(b.name)\n        if parquet_only and not name.endswith(\".parquet\"):\n            continue\n        files.append(name)\n        # Derive hour-level folder prefix\n        if \"/\" in name:\n            folders.add(name.rsplit(\"/\", 1)[0].rstrip(\"/\") + \"/\")\n        collected += 1\n        if limit is not None and collected &gt;= limit:\n            break\n\n    return {\n        \"folders\": sorted(folders),\n        \"files\": sorted(files),\n    }\n</code></pre>"},{"location":"reference/ts_shape/loader/timeseries/azure_blob_loader/#ts_shape.loader.timeseries.azure_blob_loader.AzureBlobParquetLoader.list_structure(parquet_only)","title":"<code>parquet_only</code>","text":"(<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, only include blobs ending with .parquet.</p>"},{"location":"reference/ts_shape/loader/timeseries/azure_blob_loader/#ts_shape.loader.timeseries.azure_blob_loader.AzureBlobParquetLoader.list_structure(limit)","title":"<code>limit</code>","text":"(<code>Optional[int]</code>, default:                   <code>None</code> )           \u2013            <p>Optional cap on number of files collected for quick inspection.</p>"},{"location":"reference/ts_shape/loader/timeseries/azure_blob_loader/#ts_shape.loader.timeseries.azure_blob_loader.AzureBlobParquetLoader.load_all_files","title":"load_all_files","text":"<pre><code>load_all_files() -&gt; DataFrame\n</code></pre> <p>Load all parquet blobs in the container (optionally under <code>prefix</code>).</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>A concatenated DataFrame of all parquet blobs. Returns an empty DataFrame</p> </li> <li> <code>DataFrame</code>           \u2013            <p>if none are found.</p> </li> </ul> Source code in <code>src/ts_shape/loader/timeseries/azure_blob_loader.py</code> <pre><code>def load_all_files(self) -&gt; pd.DataFrame:\n    \"\"\"\n    Load all parquet blobs in the container (optionally under `prefix`).\n\n    Returns:\n        A concatenated DataFrame of all parquet blobs. Returns an empty DataFrame\n        if none are found.\n    \"\"\"\n    # List all parquet blob names using optional prefix for server-side filtering\n    blob_iter = self.container_client.list_blobs(name_starts_with=self.prefix or None)\n    blob_names = [b.name for b in blob_iter if str(b.name).endswith(\".parquet\")]  # type: ignore[attr-defined]\n    if not blob_names:\n        return pd.DataFrame()\n\n    frames: List[pd.DataFrame] = []\n    with ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n        future_to_name = {executor.submit(self._download_parquet, name): name for name in blob_names}\n        for future in as_completed(future_to_name):\n            df = future.result()\n            if df is not None and not df.empty:\n                frames.append(df)\n\n    return pd.concat(frames, ignore_index=True) if frames else pd.DataFrame()\n</code></pre>"},{"location":"reference/ts_shape/loader/timeseries/azure_blob_loader/#ts_shape.loader.timeseries.azure_blob_loader.AzureBlobParquetLoader.load_by_time_range","title":"load_by_time_range","text":"<pre><code>load_by_time_range(start_timestamp: str | Timestamp, end_timestamp: str | Timestamp) -&gt; DataFrame\n</code></pre> <p>Load all parquet blobs under hourly folders within [start, end].</p> <p>Assumes container structure: prefix/year/month/day/hour/{file}.parquet Listing is constrained per-hour for speed.</p> Source code in <code>src/ts_shape/loader/timeseries/azure_blob_loader.py</code> <pre><code>def load_by_time_range(self, start_timestamp: str | pd.Timestamp, end_timestamp: str | pd.Timestamp) -&gt; pd.DataFrame:\n    \"\"\"\n    Load all parquet blobs under hourly folders within [start, end].\n\n    Assumes container structure: prefix/year/month/day/hour/{file}.parquet\n    Listing is constrained per-hour for speed.\n    \"\"\"\n    hour_prefixes = [self._hour_prefix(ts) for ts in self._hourly_slots(start_timestamp, end_timestamp)]\n    blob_names: List[str] = []\n    for pfx in hour_prefixes:\n        blob_iter = self.container_client.list_blobs(name_starts_with=pfx)\n        blob_names.extend([b.name for b in blob_iter if str(b.name).endswith(\".parquet\")])  # type: ignore[attr-defined]\n\n    if not blob_names:\n        return pd.DataFrame()\n\n    frames: List[pd.DataFrame] = []\n    with ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n        future_to_name = {executor.submit(self._download_parquet, name): name for name in blob_names}\n        for future in as_completed(future_to_name):\n            df = future.result()\n            if df is not None and not df.empty:\n                frames.append(df)\n\n    return pd.concat(frames, ignore_index=True) if frames else pd.DataFrame()\n</code></pre>"},{"location":"reference/ts_shape/loader/timeseries/azure_blob_loader/#ts_shape.loader.timeseries.azure_blob_loader.AzureBlobParquetLoader.load_files_by_time_range_and_uuids","title":"load_files_by_time_range_and_uuids","text":"<pre><code>load_files_by_time_range_and_uuids(start_timestamp: str | Timestamp, end_timestamp: str | Timestamp, uuid_list: List[str]) -&gt; DataFrame\n</code></pre> <p>Load parquet blobs for given UUIDs within [start, end] hours.</p> <p>Strategy: 1) Construct direct blob paths assuming pattern prefix/YYYY/MM/DD/HH/{uuid}.parquet    (fast path, no listing). 2) For robustness, also list each hour prefix and include any blob whose basename    equals one of the requested UUID variants (handles case differences and extra    subfolders below the hour level).</p> Source code in <code>src/ts_shape/loader/timeseries/azure_blob_loader.py</code> <pre><code>def load_files_by_time_range_and_uuids(\n    self,\n    start_timestamp: str | pd.Timestamp,\n    end_timestamp: str | pd.Timestamp,\n    uuid_list: List[str],\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Load parquet blobs for given UUIDs within [start, end] hours.\n\n    Strategy:\n    1) Construct direct blob paths assuming pattern prefix/YYYY/MM/DD/HH/{uuid}.parquet\n       (fast path, no listing).\n    2) For robustness, also list each hour prefix and include any blob whose basename\n       equals one of the requested UUID variants (handles case differences and extra\n       subfolders below the hour level).\n    \"\"\"\n    if not uuid_list:\n        return pd.DataFrame()\n\n    # Sanitize and deduplicate UUIDs while preserving order\n    def _clean_uuid(u: object) -&gt; str:\n        s = str(u).strip().strip(\"{}\").strip()\n        return s\n\n    raw = [_clean_uuid(u) for u in uuid_list]\n    # Include lowercase variants to be tolerant of case differences in filenames\n    variants_ordered: List[str] = []\n    seen: Set[str] = set()\n    for u in raw:\n        for v in (u, u.lower()):\n            if v and v not in seen:\n                seen.add(v)\n                variants_ordered.append(v)\n\n    hour_prefixes = [self._hour_prefix(ts) for ts in self._hourly_slots(start_timestamp, end_timestamp)]\n\n    # 1) Fast path: build direct blob names\n    direct_names = [f\"{pfx}{u}.parquet\" for pfx in hour_prefixes for u in variants_ordered]\n\n    # 2) Robust path: list each hour prefix and filter by basename match\n    basenames = {f\"{u}.parquet\" for u in variants_ordered}\n    listed_names: List[str] = []\n    try:\n        for pfx in hour_prefixes:\n            blob_iter = self.container_client.list_blobs(name_starts_with=pfx)\n            for b in blob_iter:  # type: ignore[attr-defined]\n                name = str(b.name)\n                if not name.endswith(\".parquet\"):\n                    continue\n                base = name.rsplit(\"/\", 1)[-1]\n                if base in basenames:\n                    listed_names.append(name)\n    except Exception:\n        # If listing fails for any reason, continue with direct names only\n        pass\n\n    # Merge and preserve order, avoid duplicates\n    all_blob_names = list(dict.fromkeys([*direct_names, *listed_names]))\n\n    if not all_blob_names:\n        return pd.DataFrame()\n\n    frames: List[pd.DataFrame] = []\n    with ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n        future_to_name = {executor.submit(self._download_parquet, name): name for name in all_blob_names}\n        for future in as_completed(future_to_name):\n            df = future.result()\n            if df is not None and not df.empty:\n                frames.append(df)\n\n    return pd.concat(frames, ignore_index=True) if frames else pd.DataFrame()\n</code></pre>"},{"location":"reference/ts_shape/loader/timeseries/azure_blob_loader/#ts_shape.loader.timeseries.azure_blob_loader.AzureBlobParquetLoader.stream_by_time_range","title":"stream_by_time_range","text":"<pre><code>stream_by_time_range(start_timestamp: str | Timestamp, end_timestamp: str | Timestamp) -&gt; Iterator[Tuple[str, DataFrame]]\n</code></pre> <p>Stream parquet DataFrames under hourly folders within [start, end].</p> <p>Yields (blob_name, DataFrame) one by one to avoid holding everything in memory.</p> Source code in <code>src/ts_shape/loader/timeseries/azure_blob_loader.py</code> <pre><code>def stream_by_time_range(self, start_timestamp: str | pd.Timestamp, end_timestamp: str | pd.Timestamp) -&gt; Iterator[Tuple[str, pd.DataFrame]]:\n    \"\"\"\n    Stream parquet DataFrames under hourly folders within [start, end].\n\n    Yields (blob_name, DataFrame) one by one to avoid holding everything in memory.\n    \"\"\"\n    hour_prefixes = [self._hour_prefix(ts) for ts in self._hourly_slots(start_timestamp, end_timestamp)]\n\n    def _names_iter() -&gt; Iterator[str]:\n        for pfx in hour_prefixes:\n            blob_iter = self.container_client.list_blobs(name_starts_with=pfx)\n            for b in blob_iter:  # type: ignore[attr-defined]\n                name = str(b.name)\n                if name.endswith(\".parquet\"):\n                    yield name\n\n    names_iter = _names_iter()\n    with ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n        futures: Dict[Any, str] = {}\n        # initial fill\n        try:\n            while len(futures) &lt; self.max_workers:\n                n = next(names_iter)\n                futures[executor.submit(self._download_parquet, n)] = n\n        except StopIteration:\n            pass\n\n        while futures:\n            # Drain current batch\n            for fut in as_completed(list(futures.keys())):\n                name = futures.pop(fut)\n                try:\n                    df = fut.result()\n                except Exception:\n                    df = None\n                if df is not None and not df.empty:\n                    yield (name, df)\n\n            # Refill\n            try:\n                while len(futures) &lt; self.max_workers:\n                    n = next(names_iter)\n                    futures[executor.submit(self._download_parquet, n)] = n\n            except StopIteration:\n                pass\n</code></pre>"},{"location":"reference/ts_shape/loader/timeseries/azure_blob_loader/#ts_shape.loader.timeseries.azure_blob_loader.AzureBlobParquetLoader.stream_files_by_time_range_and_uuids","title":"stream_files_by_time_range_and_uuids","text":"<pre><code>stream_files_by_time_range_and_uuids(start_timestamp: str | Timestamp, end_timestamp: str | Timestamp, uuid_list: List[str]) -&gt; Iterator[Tuple[str, DataFrame]]\n</code></pre> <p>Stream parquet DataFrames for given UUIDs within [start, end] hours.</p> <p>Yields (blob_name, DataFrame) as they arrive. Uses direct names plus per-hour listing fallback.</p> Source code in <code>src/ts_shape/loader/timeseries/azure_blob_loader.py</code> <pre><code>def stream_files_by_time_range_and_uuids(\n    self,\n    start_timestamp: str | pd.Timestamp,\n    end_timestamp: str | pd.Timestamp,\n    uuid_list: List[str],\n) -&gt; Iterator[Tuple[str, pd.DataFrame]]:\n    \"\"\"\n    Stream parquet DataFrames for given UUIDs within [start, end] hours.\n\n    Yields (blob_name, DataFrame) as they arrive. Uses direct names plus per-hour listing fallback.\n    \"\"\"\n    if not uuid_list:\n        return iter(())\n\n    def _clean_uuid(u: object) -&gt; str:\n        return str(u).strip().strip(\"{}\").strip()\n\n    raw = [_clean_uuid(u) for u in uuid_list]\n    variants_ordered: List[str] = []\n    seen: Set[str] = set()\n    for u in raw:\n        for v in (u, u.lower()):\n            if v and v not in seen:\n                seen.add(v)\n                variants_ordered.append(v)\n\n    hour_prefixes = [self._hour_prefix(ts) for ts in self._hourly_slots(start_timestamp, end_timestamp)]\n    direct_names = [f\"{pfx}{u}.parquet\" for pfx in hour_prefixes for u in variants_ordered]\n\n    basenames = {f\"{u}.parquet\" for u in variants_ordered}\n\n    def _names_iter() -&gt; Iterator[str]:\n        # yield direct first\n        yielded: Set[str] = set()\n        for n in direct_names:\n            yielded.add(n)\n            yield n\n        # then list per-hour\n        for pfx in hour_prefixes:\n            blob_iter = self.container_client.list_blobs(name_starts_with=pfx)\n            for b in blob_iter:  # type: ignore[attr-defined]\n                name = str(b.name)\n                if not name.endswith(\".parquet\"):\n                    continue\n                base = name.rsplit(\"/\", 1)[-1]\n                if base in basenames and name not in yielded:\n                    yielded.add(name)\n                    yield name\n\n    names_iter = _names_iter()\n    with ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n        futures: Dict[Any, str] = {}\n        try:\n            while len(futures) &lt; self.max_workers:\n                n = next(names_iter)\n                futures[executor.submit(self._download_parquet, n)] = n\n        except StopIteration:\n            pass\n\n        while futures:\n            for fut in as_completed(list(futures.keys())):\n                name = futures.pop(fut)\n                try:\n                    df = fut.result()\n                except Exception:\n                    df = None\n                if df is not None and not df.empty:\n                    yield (name, df)\n\n            try:\n                while len(futures) &lt; self.max_workers:\n                    n = next(names_iter)\n                    futures[executor.submit(self._download_parquet, n)] = n\n            except StopIteration:\n                pass\n</code></pre>"},{"location":"reference/ts_shape/loader/timeseries/energy_api_loader/","title":"energy_api_loader","text":""},{"location":"reference/ts_shape/loader/timeseries/energy_api_loader/#ts_shape.loader.timeseries.energy_api_loader","title":"ts_shape.loader.timeseries.energy_api_loader","text":""},{"location":"reference/ts_shape/loader/timeseries/parquet_loader/","title":"parquet_loader","text":""},{"location":"reference/ts_shape/loader/timeseries/parquet_loader/#ts_shape.loader.timeseries.parquet_loader","title":"ts_shape.loader.timeseries.parquet_loader","text":"<p>Classes:</p> <ul> <li> <code>ParquetLoader</code>           \u2013            <p>This class provides class methods to load parquet files from a specified directory structure.</p> </li> </ul>"},{"location":"reference/ts_shape/loader/timeseries/parquet_loader/#ts_shape.loader.timeseries.parquet_loader.ParquetLoader","title":"ParquetLoader","text":"<pre><code>ParquetLoader(base_path: str)\n</code></pre> <p>This class provides class methods to load parquet files from a specified directory structure.</p> <p>Parameters:</p> <p>Methods:</p> <ul> <li> <code>load_all_files</code>             \u2013              <p>Loads all parquet files in the specified base directory into a single pandas DataFrame.</p> </li> <li> <code>load_by_time_range</code>             \u2013              <p>Loads parquet files that fall within a specified time range based on the directory structure.</p> </li> <li> <code>load_by_uuid_list</code>             \u2013              <p>Loads parquet files that match any UUID in the specified list.</p> </li> <li> <code>load_files_by_time_range_and_uuids</code>             \u2013              <p>Loads parquet files that fall within a specified time range and match any UUID in the list.</p> </li> </ul> Source code in <code>src/ts_shape/loader/timeseries/parquet_loader.py</code> <pre><code>def __init__(self, base_path: str):\n    \"\"\"\n    Initialize the ParquetLoader with the base directory path.\n\n    Args:\n        base_path (str): The base directory where parquet files are stored.\n    \"\"\"\n    self.base_path = Path(base_path)\n</code></pre>"},{"location":"reference/ts_shape/loader/timeseries/parquet_loader/#ts_shape.loader.timeseries.parquet_loader.ParquetLoader(base_path)","title":"<code>base_path</code>","text":"(<code>str</code>)           \u2013            <p>The base directory where parquet files are stored.</p>"},{"location":"reference/ts_shape/loader/timeseries/parquet_loader/#ts_shape.loader.timeseries.parquet_loader.ParquetLoader.load_all_files","title":"load_all_files  <code>classmethod</code>","text":"<pre><code>load_all_files(base_path: str) -&gt; DataFrame\n</code></pre> <p>Loads all parquet files in the specified base directory into a single pandas DataFrame.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>pd.DataFrame: A DataFrame containing all the data from the parquet files.</p> </li> </ul> Source code in <code>src/ts_shape/loader/timeseries/parquet_loader.py</code> <pre><code>@classmethod\ndef load_all_files(cls, base_path: str) -&gt; pd.DataFrame:\n    \"\"\"\n    Loads all parquet files in the specified base directory into a single pandas DataFrame.\n\n    Args:\n        base_path (str): The base directory where parquet files are stored.\n\n    Returns:\n        pd.DataFrame: A DataFrame containing all the data from the parquet files.\n    \"\"\"\n    # Convert base path to a Path object\n    base_path = Path(base_path)\n    # Get all parquet files in the directory\n    parquet_files = cls._get_parquet_files(base_path)\n    # Load all files into pandas DataFrames\n    dataframes = [pd.read_parquet(file) for file in parquet_files]\n\n    # Concatenate all DataFrames into a single DataFrame\n    return pd.concat(dataframes, ignore_index=True)\n</code></pre>"},{"location":"reference/ts_shape/loader/timeseries/parquet_loader/#ts_shape.loader.timeseries.parquet_loader.ParquetLoader.load_all_files(base_path)","title":"<code>base_path</code>","text":"(<code>str</code>)           \u2013            <p>The base directory where parquet files are stored.</p>"},{"location":"reference/ts_shape/loader/timeseries/parquet_loader/#ts_shape.loader.timeseries.parquet_loader.ParquetLoader.load_by_time_range","title":"load_by_time_range  <code>classmethod</code>","text":"<pre><code>load_by_time_range(base_path: str, start_time: Timestamp, end_time: Timestamp) -&gt; DataFrame\n</code></pre> <p>Loads parquet files that fall within a specified time range based on the directory structure.</p> <p>The directory structure is expected to be in the format YYYY/MM/DD/HH.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>pd.DataFrame: A DataFrame containing the data from the parquet files within the time range.</p> </li> </ul> Source code in <code>src/ts_shape/loader/timeseries/parquet_loader.py</code> <pre><code>@classmethod\ndef load_by_time_range(cls, base_path: str, start_time: pd.Timestamp, end_time: pd.Timestamp) -&gt; pd.DataFrame:\n    \"\"\"\n    Loads parquet files that fall within a specified time range based on the directory structure.\n\n    The directory structure is expected to be in the format YYYY/MM/DD/HH.\n\n    Args:\n        base_path (str): The base directory where parquet files are stored.\n        start_time (pd.Timestamp): The start timestamp.\n        end_time (pd.Timestamp): The end timestamp.\n\n    Returns:\n        pd.DataFrame: A DataFrame containing the data from the parquet files within the time range.\n    \"\"\"\n    # Convert base path to a Path object\n    base_path = Path(base_path)\n    # Get all parquet files in the directory\n    parquet_files = cls._get_parquet_files(base_path)\n    valid_files = []\n\n    for file in parquet_files:\n        try:\n            # Extract the timestamp from the file's relative path\n            folder_parts = file.relative_to(base_path).parts[:4]  # Extract YYYY/MM/DD/HH parts\n            folder_time_str = \"/\".join(folder_parts)\n            file_time = pd.to_datetime(folder_time_str, format=\"%Y/%m/%d/%H\")\n\n            # Check if the file's timestamp falls within the specified time range\n            if start_time &lt;= file_time &lt;= end_time:\n                valid_files.append(file)\n        except ValueError:\n            # Skip files that do not follow the expected folder structure\n            continue\n\n    # Load all valid files into pandas DataFrames\n    dataframes = [pd.read_parquet(file) for file in valid_files]\n    return pd.concat(dataframes, ignore_index=True)\n</code></pre>"},{"location":"reference/ts_shape/loader/timeseries/parquet_loader/#ts_shape.loader.timeseries.parquet_loader.ParquetLoader.load_by_time_range(base_path)","title":"<code>base_path</code>","text":"(<code>str</code>)           \u2013            <p>The base directory where parquet files are stored.</p>"},{"location":"reference/ts_shape/loader/timeseries/parquet_loader/#ts_shape.loader.timeseries.parquet_loader.ParquetLoader.load_by_time_range(start_time)","title":"<code>start_time</code>","text":"(<code>Timestamp</code>)           \u2013            <p>The start timestamp.</p>"},{"location":"reference/ts_shape/loader/timeseries/parquet_loader/#ts_shape.loader.timeseries.parquet_loader.ParquetLoader.load_by_time_range(end_time)","title":"<code>end_time</code>","text":"(<code>Timestamp</code>)           \u2013            <p>The end timestamp.</p>"},{"location":"reference/ts_shape/loader/timeseries/parquet_loader/#ts_shape.loader.timeseries.parquet_loader.ParquetLoader.load_by_uuid_list","title":"load_by_uuid_list  <code>classmethod</code>","text":"<pre><code>load_by_uuid_list(base_path: str, uuid_list: list) -&gt; DataFrame\n</code></pre> <p>Loads parquet files that match any UUID in the specified list.</p> <p>The UUIDs are expected to be part of the file names.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>pd.DataFrame: A DataFrame containing the data from the parquet files with matching UUIDs.</p> </li> </ul> Source code in <code>src/ts_shape/loader/timeseries/parquet_loader.py</code> <pre><code>@classmethod\ndef load_by_uuid_list(cls, base_path: str, uuid_list: list) -&gt; pd.DataFrame:\n    \"\"\"\n    Loads parquet files that match any UUID in the specified list.\n\n    The UUIDs are expected to be part of the file names.\n\n    Args:\n        base_path (str): The base directory where parquet files are stored.\n        uuid_list (list): A list of UUIDs to filter the files.\n\n    Returns:\n        pd.DataFrame: A DataFrame containing the data from the parquet files with matching UUIDs.\n    \"\"\"\n    # Convert base path to a Path object\n    base_path = Path(base_path)\n    # Get all parquet files in the directory\n    parquet_files = cls._get_parquet_files(base_path)\n    valid_files = []\n\n    for file in parquet_files:\n        # Extract the file name without extension\n        file_name = file.stem\n        # Check if the file name contains any of the UUIDs in the list\n        for uuid in uuid_list:\n            if uuid in file_name:\n                valid_files.append(file)\n                break  # Stop checking other UUIDs for this file\n\n    # Load all valid files into pandas DataFrames\n    dataframes = [pd.read_parquet(file) for file in valid_files]\n    return pd.concat(dataframes, ignore_index=True)\n</code></pre>"},{"location":"reference/ts_shape/loader/timeseries/parquet_loader/#ts_shape.loader.timeseries.parquet_loader.ParquetLoader.load_by_uuid_list(base_path)","title":"<code>base_path</code>","text":"(<code>str</code>)           \u2013            <p>The base directory where parquet files are stored.</p>"},{"location":"reference/ts_shape/loader/timeseries/parquet_loader/#ts_shape.loader.timeseries.parquet_loader.ParquetLoader.load_by_uuid_list(uuid_list)","title":"<code>uuid_list</code>","text":"(<code>list</code>)           \u2013            <p>A list of UUIDs to filter the files.</p>"},{"location":"reference/ts_shape/loader/timeseries/parquet_loader/#ts_shape.loader.timeseries.parquet_loader.ParquetLoader.load_files_by_time_range_and_uuids","title":"load_files_by_time_range_and_uuids  <code>classmethod</code>","text":"<pre><code>load_files_by_time_range_and_uuids(base_path: str, start_time: Timestamp, end_time: Timestamp, uuid_list: list) -&gt; DataFrame\n</code></pre> <p>Loads parquet files that fall within a specified time range and match any UUID in the list.</p> <p>The directory structure is expected to be in the format YYYY/MM/DD/HH, and UUIDs are part of the file names.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>pd.DataFrame: A DataFrame containing the data from the parquet files that meet both criteria.</p> </li> </ul> Source code in <code>src/ts_shape/loader/timeseries/parquet_loader.py</code> <pre><code>@classmethod\ndef load_files_by_time_range_and_uuids(cls, base_path: str, start_time: pd.Timestamp, end_time: pd.Timestamp, uuid_list: list) -&gt; pd.DataFrame:\n    \"\"\"\n    Loads parquet files that fall within a specified time range and match any UUID in the list.\n\n    The directory structure is expected to be in the format YYYY/MM/DD/HH, and UUIDs are part of the file names.\n\n    Args:\n        base_path (str): The base directory where parquet files are stored.\n        start_time (pd.Timestamp): The start timestamp.\n        end_time (pd.Timestamp): The end timestamp.\n        uuid_list (list): A list of UUIDs to filter the files.\n\n    Returns:\n        pd.DataFrame: A DataFrame containing the data from the parquet files that meet both criteria.\n    \"\"\"\n    # Convert base path to a Path object\n    base_path = Path(base_path)\n    # Get all parquet files in the directory\n    parquet_files = cls._get_parquet_files(base_path)\n    valid_files = []\n\n    for file in parquet_files:\n        try:\n            # Extract the timestamp from the file's relative path\n            folder_parts = file.relative_to(base_path).parts[:4]  # Extract YYYY/MM/DD/HH parts\n            folder_time_str = \"/\".join(folder_parts)\n            file_time = pd.to_datetime(folder_time_str, format=\"%Y/%m/%d/%H\")\n\n            # Check if the file's timestamp falls within the specified time range\n            if start_time &lt;= file_time &lt;= end_time:\n                # Extract the file name without extension\n                file_name = file.stem\n                # Check if the file name contains any of the UUIDs in the list\n                for uuid in uuid_list:\n                    if uuid in file_name:\n                        valid_files.append(file)\n                        break  # Stop checking other UUIDs for this file\n        except ValueError:\n            # Skip files that do not follow the expected folder structure\n            continue\n\n    # Load all valid files into pandas DataFrames\n    dataframes = [pd.read_parquet(file) for file in valid_files]\n    return pd.concat(dataframes, ignore_index=True)\n</code></pre>"},{"location":"reference/ts_shape/loader/timeseries/parquet_loader/#ts_shape.loader.timeseries.parquet_loader.ParquetLoader.load_files_by_time_range_and_uuids(base_path)","title":"<code>base_path</code>","text":"(<code>str</code>)           \u2013            <p>The base directory where parquet files are stored.</p>"},{"location":"reference/ts_shape/loader/timeseries/parquet_loader/#ts_shape.loader.timeseries.parquet_loader.ParquetLoader.load_files_by_time_range_and_uuids(start_time)","title":"<code>start_time</code>","text":"(<code>Timestamp</code>)           \u2013            <p>The start timestamp.</p>"},{"location":"reference/ts_shape/loader/timeseries/parquet_loader/#ts_shape.loader.timeseries.parquet_loader.ParquetLoader.load_files_by_time_range_and_uuids(end_time)","title":"<code>end_time</code>","text":"(<code>Timestamp</code>)           \u2013            <p>The end timestamp.</p>"},{"location":"reference/ts_shape/loader/timeseries/parquet_loader/#ts_shape.loader.timeseries.parquet_loader.ParquetLoader.load_files_by_time_range_and_uuids(uuid_list)","title":"<code>uuid_list</code>","text":"(<code>list</code>)           \u2013            <p>A list of UUIDs to filter the files.</p>"},{"location":"reference/ts_shape/loader/timeseries/s3proxy_parquet_loader/","title":"s3proxy_parquet_loader","text":""},{"location":"reference/ts_shape/loader/timeseries/s3proxy_parquet_loader/#ts_shape.loader.timeseries.s3proxy_parquet_loader","title":"ts_shape.loader.timeseries.s3proxy_parquet_loader","text":"<p>Classes:</p> <ul> <li> <code>S3ProxyDataAccess</code>           \u2013            <p>A class to access timeseries data via an S3 proxy. This class retrieves </p> </li> </ul>"},{"location":"reference/ts_shape/loader/timeseries/s3proxy_parquet_loader/#ts_shape.loader.timeseries.s3proxy_parquet_loader.S3ProxyDataAccess","title":"S3ProxyDataAccess","text":"<pre><code>S3ProxyDataAccess(start_timestamp: str, end_timestamp: str, uuids: List[str], s3_config: Dict[str, str])\n</code></pre> <p>A class to access timeseries data via an S3 proxy. This class retrieves  data for specified UUIDs within a defined time range, with the option to  output data as Parquet files or as a single combined DataFrame.</p> <p>:param end_timestamp: End timestamp in \"Year-Month-Day Hour:Minute:Second\" format. :param uuids: List of UUIDs to retrieve data for. :param s3_config: Configuration dictionary for S3 connection.</p> <p>Methods:</p> <ul> <li> <code>fetch_data_as_dataframe</code>             \u2013              <p>Retrieves timeseries data from S3 and returns it as a single DataFrame.</p> </li> <li> <code>fetch_data_as_parquet</code>             \u2013              <p>Retrieves timeseries data from S3 and saves it as Parquet files.</p> </li> </ul> Source code in <code>src/ts_shape/loader/timeseries/s3proxy_parquet_loader.py</code> <pre><code>def __init__(self, start_timestamp: str, end_timestamp: str, uuids: List[str], s3_config: Dict[str, str]):\n    \"\"\"\n    Initialize the S3ProxyDataAccess object.\n    :param start_timestamp: Start timestamp in \"Year-Month-Day Hour:Minute:Second\" format.\n    :param end_timestamp: End timestamp in \"Year-Month-Day Hour:Minute:Second\" format.\n    :param uuids: List of UUIDs to retrieve data for.\n    :param s3_config: Configuration dictionary for S3 connection.\n    \"\"\"\n    self.start_timestamp = start_timestamp\n    self.end_timestamp = end_timestamp\n    self.uuids = uuids\n    self.s3_config = s3_config\n\n    # Establish connection to S3 using provided configuration\n    self.s3 = s3fs.S3FileSystem(\n        endpoint_url=s3_config[\"endpoint_url\"],\n        key=s3_config[\"key\"],\n        secret=s3_config[\"secret\"],\n        use_ssl=s3_config[\"use_ssl\"],\n        version_aware=s3_config[\"version_aware\"]\n    )\n    self.s3_path_base = s3_config[\"s3_path_base\"]\n</code></pre>"},{"location":"reference/ts_shape/loader/timeseries/s3proxy_parquet_loader/#ts_shape.loader.timeseries.s3proxy_parquet_loader.S3ProxyDataAccess.fetch_data_as_dataframe","title":"fetch_data_as_dataframe","text":"<pre><code>fetch_data_as_dataframe() -&gt; DataFrame\n</code></pre> <p>Retrieves timeseries data from S3 and returns it as a single DataFrame. :return: A combined DataFrame with data for all specified UUIDs and time slots.</p> Source code in <code>src/ts_shape/loader/timeseries/s3proxy_parquet_loader.py</code> <pre><code>def fetch_data_as_dataframe(self) -&gt; pd.DataFrame:\n    \"\"\"\n    Retrieves timeseries data from S3 and returns it as a single DataFrame.\n    :return: A combined DataFrame with data for all specified UUIDs and time slots.\n    \"\"\"\n    data_frames = [self._fetch_parquet(uuid, timeslot_dir) \n                   for timeslot_dir in self._generate_timeslot_paths()\n                   for uuid in set(self.uuids)]\n    return pd.concat([df for df in data_frames if df is not None], ignore_index=True) if data_frames else pd.DataFrame()\n</code></pre>"},{"location":"reference/ts_shape/loader/timeseries/s3proxy_parquet_loader/#ts_shape.loader.timeseries.s3proxy_parquet_loader.S3ProxyDataAccess.fetch_data_as_parquet","title":"fetch_data_as_parquet","text":"<pre><code>fetch_data_as_parquet(output_dir: str)\n</code></pre> <p>Retrieves timeseries data from S3 and saves it as Parquet files. Each file is saved in a directory structure of UUID/year/month/day/hour. :param output_dir: Base directory to save the Parquet files.</p> Source code in <code>src/ts_shape/loader/timeseries/s3proxy_parquet_loader.py</code> <pre><code>def fetch_data_as_parquet(self, output_dir: str):\n    \"\"\"\n    Retrieves timeseries data from S3 and saves it as Parquet files.\n    Each file is saved in a directory structure of UUID/year/month/day/hour.\n    :param output_dir: Base directory to save the Parquet files.\n    \"\"\"\n    for timeslot_dir in self._generate_timeslot_paths():\n        for uuid in set(self.uuids):\n            df = self._fetch_parquet(uuid, timeslot_dir)\n            if df is not None:\n                output_path = Path(output_dir, timeslot_dir)\n                output_path.mkdir(parents=True, exist_ok=True)\n                df.to_parquet(output_path / f\"{uuid}.parquet\")\n</code></pre>"},{"location":"reference/ts_shape/loader/timeseries/timescale_loader/","title":"timescale_loader","text":""},{"location":"reference/ts_shape/loader/timeseries/timescale_loader/#ts_shape.loader.timeseries.timescale_loader","title":"ts_shape.loader.timeseries.timescale_loader","text":"<p>Classes:</p> <ul> <li> <code>TimescaleDBDataAccess</code>           \u2013            </li> </ul>"},{"location":"reference/ts_shape/loader/timeseries/timescale_loader/#ts_shape.loader.timeseries.timescale_loader.TimescaleDBDataAccess","title":"TimescaleDBDataAccess","text":"<pre><code>TimescaleDBDataAccess(start_timestamp: str, end_timestamp: str, uuids: List[str], db_config: Dict[str, str])\n</code></pre> <p>Methods:</p> <ul> <li> <code>fetch_data_as_dataframe</code>             \u2013              <p>Retrieves timeseries data from TimescaleDB and returns it as a single DataFrame.</p> </li> </ul> Source code in <code>src/ts_shape/loader/timeseries/timescale_loader.py</code> <pre><code>def __init__(self, start_timestamp: str, end_timestamp: str, uuids: List[str], db_config: Dict[str, str]):\n    self.start_timestamp = start_timestamp\n    self.end_timestamp = end_timestamp\n    self.uuids = uuids\n    self.db_config = db_config\n    self.engine = create_engine(\n        f'postgresql+psycopg2://{db_config[\"db_user\"]}:{db_config[\"db_pass\"]}@{db_config[\"db_host\"]}/{db_config[\"db_name\"]}'\n    )\n</code></pre>"},{"location":"reference/ts_shape/loader/timeseries/timescale_loader/#ts_shape.loader.timeseries.timescale_loader.TimescaleDBDataAccess.fetch_data_as_dataframe","title":"fetch_data_as_dataframe","text":"<pre><code>fetch_data_as_dataframe() -&gt; DataFrame\n</code></pre> <p>Retrieves timeseries data from TimescaleDB and returns it as a single DataFrame. :return: A combined DataFrame with data for all specified UUIDs within the time range.</p> Source code in <code>src/ts_shape/loader/timeseries/timescale_loader.py</code> <pre><code>def fetch_data_as_dataframe(self) -&gt; pd.DataFrame:\n    \"\"\"\n    Retrieves timeseries data from TimescaleDB and returns it as a single DataFrame.\n    :return: A combined DataFrame with data for all specified UUIDs within the time range.\n    \"\"\"\n    df_list = [chunk for uuid in self.uuids for chunk in self._fetch_data(uuid)]\n    return pd.concat(df_list, ignore_index=True) if df_list else pd.DataFrame()\n</code></pre>"},{"location":"reference/ts_shape/transform/__init__/","title":"init","text":""},{"location":"reference/ts_shape/transform/__init__/#ts_shape.transform","title":"ts_shape.transform","text":"<p>Transform</p> <p>Row-level filtering, column transformations, and timestamp utilities.</p> <ul> <li>IntegerFilter: Equality/inequality/range filters for integer columns.</li> <li>filter_value_integer_match: Select rows equal to a value.</li> <li>filter_value_integer_not_match: Select rows not equal to a value.</li> <li> <p>filter_value_integer_between: Select rows within [min, max].</p> </li> <li> <p>DoubleFilter: NaN removal and numeric ranges for floating-point columns.</p> </li> <li>filter_nan_value_double: Drop rows with NaN in the column.</li> <li> <p>filter_value_double_between: Select rows within [min, max].</p> </li> <li> <p>StringFilter: Equality, contains, regex cleaning, and change detection.</p> </li> <li>filter_na_value_string: Drop rows with NA values.</li> <li>filter_value_string_match: Select rows equal to a string.</li> <li>filter_value_string_not_match: Select rows not equal to a string.</li> <li>filter_string_contains: Select rows containing a substring.</li> <li>regex_clean_value_string: Regex-based cleaning or replacement.</li> <li> <p>detect_changes_in_string: Detect row-to-row changes in a string column.</p> </li> <li> <p>BooleanFilter: Detect raising/falling edges of boolean states.</p> </li> <li>filter_falling_value_bool: True\u2192False transitions.</li> <li> <p>filter_raising_value_bool: False\u2192True transitions.</p> </li> <li> <p>IsDeltaFilter: Select rows by the is_delta flag.</p> </li> <li>filter_is_delta_true: Only True.</li> <li> <p>filter_is_delta_false: Only False.</p> </li> <li> <p>DateTimeFilter: Before/after/between filters for timestamps.</p> </li> <li>filter_after_date: After a given date.</li> <li>filter_before_date: Before a given date.</li> <li>filter_between_dates: Between start and end dates.</li> <li>filter_after_datetime: After a given datetime.</li> <li>filter_before_datetime: Before a given datetime.</li> <li> <p>filter_between_datetimes: Between start and end datetimes.</p> </li> <li> <p>CustomFilter: Free-form DataFrame.query string conditions.</p> </li> <li> <p>filter_custom_conditions: Apply a query string to filter rows.</p> </li> <li> <p>IntegerCalc: Numeric column calculations.</p> </li> <li>scale_column: Multiply by a factor.</li> <li>offset_column: Add a constant.</li> <li>divide_column: Divide by a constant.</li> <li>subtract_column: Subtract a constant.</li> <li>calculate_with_fixed_factors: Multiply then add.</li> <li>mod_column: Modulo operation.</li> <li> <p>power_column: Raise to a power.</p> </li> <li> <p>LambdaProcessor: Apply vectorized callables to columns.</p> </li> <li> <p>apply_function: Apply a Python callable over a column's values.</p> </li> <li> <p>TimestampConverter: Convert integer timestamps to tz-aware datetimes.</p> </li> <li> <p>convert_to_datetime: Convert s/ms/us/ns to datetime in a timezone.</p> </li> <li> <p>TimezoneShift: Timezone localization/conversion helpers.</p> </li> <li>shift_timezone: Convert timezones in-place.</li> <li>add_timezone_column: Add a converted timestamp column.</li> <li>detect_timezone_awareness: Check tz-awareness of a column.</li> <li>revert_to_original_timezone: Convert back to original tz.</li> <li>calculate_time_difference: Difference between two timestamp columns.</li> </ul> <p>Modules:</p> <ul> <li> <code>calculator</code>           \u2013            <p>Calculator</p> </li> <li> <code>filter</code>           \u2013            <p>Filters</p> </li> <li> <code>functions</code>           \u2013            <p>Functions</p> </li> <li> <code>time_functions</code>           \u2013            <p>Time Functions</p> </li> </ul>"},{"location":"reference/ts_shape/transform/calculator/__init__/","title":"init","text":""},{"location":"reference/ts_shape/transform/calculator/__init__/#ts_shape.transform.calculator","title":"ts_shape.transform.calculator","text":"<p>Calculator</p> <p>Numeric column calculations for engineered features.</p> <ul> <li>IntegerCalc: Operations for integer-like numeric columns.</li> <li>scale_column: Multiply by a factor.</li> <li>offset_column: Add a constant.</li> <li>divide_column: Divide by a constant.</li> <li>subtract_column: Subtract a constant.</li> <li>calculate_with_fixed_factors: Multiply then add.</li> <li>mod_column: Modulo operation.</li> <li>power_column: Raise to a power.</li> </ul> <p>Modules:</p> <ul> <li> <code>numeric_calc</code>           \u2013            </li> </ul>"},{"location":"reference/ts_shape/transform/calculator/numeric_calc/","title":"numeric_calc","text":""},{"location":"reference/ts_shape/transform/calculator/numeric_calc/#ts_shape.transform.calculator.numeric_calc","title":"ts_shape.transform.calculator.numeric_calc","text":"<p>Classes:</p> <ul> <li> <code>IntegerCalc</code>           \u2013            <p>Provides class methods for performing calculations on integer columns in a pandas DataFrame.</p> </li> </ul>"},{"location":"reference/ts_shape/transform/calculator/numeric_calc/#ts_shape.transform.calculator.numeric_calc.IntegerCalc","title":"IntegerCalc","text":"<pre><code>IntegerCalc(dataframe: DataFrame, column_name: str = 'systime')\n</code></pre> <p>               Bases: <code>Base</code></p> <p>Provides class methods for performing calculations on integer columns in a pandas DataFrame.</p> <p>Parameters:</p> <p>Methods:</p> <ul> <li> <code>calculate_with_fixed_factors</code>             \u2013              <p>Performs a calculation by multiplying with a factor and then adding an additional factor.</p> </li> <li> <code>divide_column</code>             \u2013              <p>Divides each value in the integer column by the given divisor.</p> </li> <li> <code>get_dataframe</code>             \u2013              <p>Returns the processed DataFrame.</p> </li> <li> <code>mod_column</code>             \u2013              <p>Performs a modulus operation on the integer column with a specified value.</p> </li> <li> <code>offset_column</code>             \u2013              <p>Offsets the integer column by the given value.</p> </li> <li> <code>power_column</code>             \u2013              <p>Raises each value in the integer column to the power of a specified value.</p> </li> <li> <code>scale_column</code>             \u2013              <p>Scales the integer column by the given factor.</p> </li> <li> <code>subtract_column</code>             \u2013              <p>Subtracts a given value from each element in the integer column.</p> </li> </ul> Source code in <code>src/ts_shape/utils/base.py</code> <pre><code>def __init__(self, dataframe: pd.DataFrame, column_name: str = 'systime') -&gt; None:\n    \"\"\"\n    Initializes the Base with a DataFrame, detects time columns, converts them to datetime,\n    and sorts the DataFrame by the specified column (or the detected time column if applicable).\n\n    Args:\n        dataframe (pd.DataFrame): The DataFrame to be processed.\n        column_name (str): The column to sort by. Default is 'systime'. If the column is not found or is not a time column, the class will attempt to detect other time columns.\n    \"\"\"\n    self.dataframe = dataframe.copy()\n\n    # Attempt to convert the specified column_name to datetime if it exists\n    if column_name in self.dataframe.columns:\n        self.dataframe[column_name] = pd.to_datetime(self.dataframe[column_name], errors='coerce')\n    else:\n        # If the column_name is not in the DataFrame, fallback to automatic time detection\n        time_columns = [col for col in self.dataframe.columns if 'time' in col.lower() or 'date' in col.lower()]\n\n        # Convert all detected time columns to datetime, if any\n        for col in time_columns:\n            self.dataframe[col] = pd.to_datetime(self.dataframe[col], errors='coerce')\n\n        # If any time columns are detected, sort by the first one; otherwise, do nothing\n        if time_columns:\n            column_name = time_columns[0]\n\n    # Sort by the datetime column (either specified or detected)\n    if column_name in self.dataframe.columns:\n        self.dataframe = self.dataframe.sort_values(by=column_name)\n</code></pre>"},{"location":"reference/ts_shape/transform/calculator/numeric_calc/#ts_shape.transform.calculator.numeric_calc.IntegerCalc(dataframe)","title":"<code>dataframe</code>","text":"(<code>DataFrame</code>)           \u2013            <p>The DataFrame to be processed.</p>"},{"location":"reference/ts_shape/transform/calculator/numeric_calc/#ts_shape.transform.calculator.numeric_calc.IntegerCalc(column_name)","title":"<code>column_name</code>","text":"(<code>str</code>, default:                   <code>'systime'</code> )           \u2013            <p>The column to sort by. Default is 'systime'. If the column is not found or is not a time column, the class will attempt to detect other time columns.</p>"},{"location":"reference/ts_shape/transform/calculator/numeric_calc/#ts_shape.transform.calculator.numeric_calc.IntegerCalc.calculate_with_fixed_factors","title":"calculate_with_fixed_factors  <code>classmethod</code>","text":"<pre><code>calculate_with_fixed_factors(dataframe: DataFrame, column_name: str = 'value_integer', multiply_factor: float = 1, add_factor: float = 0) -&gt; DataFrame\n</code></pre> <p>Performs a calculation by multiplying with a factor and then adding an additional factor.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>pd.DataFrame: The DataFrame after applying the calculations.</p> </li> </ul> Source code in <code>src/ts_shape/transform/calculator/numeric_calc.py</code> <pre><code>@classmethod\ndef calculate_with_fixed_factors(cls, dataframe: pd.DataFrame, column_name: str = 'value_integer', multiply_factor: float = 1, add_factor: float = 0) -&gt; pd.DataFrame:\n    \"\"\"\n    Performs a calculation by multiplying with a factor and then adding an additional factor.\n\n    Args:\n        dataframe (pd.DataFrame): The DataFrame to perform the operation on.\n        column_name (str): The column to apply the calculations to.\n        multiply_factor (float): The factor to multiply each value by. Defaults to 1 (no scaling).\n        add_factor (float): The value to add after multiplication. Defaults to 0 (no offset).\n\n    Returns:\n        pd.DataFrame: The DataFrame after applying the calculations.\n    \"\"\"\n    dataframe[column_name] = (dataframe[column_name] * multiply_factor) + add_factor\n    return dataframe\n</code></pre>"},{"location":"reference/ts_shape/transform/calculator/numeric_calc/#ts_shape.transform.calculator.numeric_calc.IntegerCalc.calculate_with_fixed_factors(dataframe)","title":"<code>dataframe</code>","text":"(<code>DataFrame</code>)           \u2013            <p>The DataFrame to perform the operation on.</p>"},{"location":"reference/ts_shape/transform/calculator/numeric_calc/#ts_shape.transform.calculator.numeric_calc.IntegerCalc.calculate_with_fixed_factors(column_name)","title":"<code>column_name</code>","text":"(<code>str</code>, default:                   <code>'value_integer'</code> )           \u2013            <p>The column to apply the calculations to.</p>"},{"location":"reference/ts_shape/transform/calculator/numeric_calc/#ts_shape.transform.calculator.numeric_calc.IntegerCalc.calculate_with_fixed_factors(multiply_factor)","title":"<code>multiply_factor</code>","text":"(<code>float</code>, default:                   <code>1</code> )           \u2013            <p>The factor to multiply each value by. Defaults to 1 (no scaling).</p>"},{"location":"reference/ts_shape/transform/calculator/numeric_calc/#ts_shape.transform.calculator.numeric_calc.IntegerCalc.calculate_with_fixed_factors(add_factor)","title":"<code>add_factor</code>","text":"(<code>float</code>, default:                   <code>0</code> )           \u2013            <p>The value to add after multiplication. Defaults to 0 (no offset).</p>"},{"location":"reference/ts_shape/transform/calculator/numeric_calc/#ts_shape.transform.calculator.numeric_calc.IntegerCalc.divide_column","title":"divide_column  <code>classmethod</code>","text":"<pre><code>divide_column(dataframe: DataFrame, column_name: str = 'value_integer', divisor: float = 1) -&gt; DataFrame\n</code></pre> <p>Divides each value in the integer column by the given divisor.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>pd.DataFrame: The DataFrame with the divided column.</p> </li> </ul> Source code in <code>src/ts_shape/transform/calculator/numeric_calc.py</code> <pre><code>@classmethod\ndef divide_column(cls, dataframe: pd.DataFrame, column_name: str = 'value_integer', divisor: float = 1) -&gt; pd.DataFrame:\n    \"\"\"\n    Divides each value in the integer column by the given divisor.\n\n    Args:\n        dataframe (pd.DataFrame): The DataFrame to perform the operation on.\n        column_name (str): The column to apply the division to.\n        divisor (float): The value by which to divide each element.\n\n    Returns:\n        pd.DataFrame: The DataFrame with the divided column.\n    \"\"\"\n    dataframe[column_name] = dataframe[column_name] / divisor\n    return dataframe\n</code></pre>"},{"location":"reference/ts_shape/transform/calculator/numeric_calc/#ts_shape.transform.calculator.numeric_calc.IntegerCalc.divide_column(dataframe)","title":"<code>dataframe</code>","text":"(<code>DataFrame</code>)           \u2013            <p>The DataFrame to perform the operation on.</p>"},{"location":"reference/ts_shape/transform/calculator/numeric_calc/#ts_shape.transform.calculator.numeric_calc.IntegerCalc.divide_column(column_name)","title":"<code>column_name</code>","text":"(<code>str</code>, default:                   <code>'value_integer'</code> )           \u2013            <p>The column to apply the division to.</p>"},{"location":"reference/ts_shape/transform/calculator/numeric_calc/#ts_shape.transform.calculator.numeric_calc.IntegerCalc.divide_column(divisor)","title":"<code>divisor</code>","text":"(<code>float</code>, default:                   <code>1</code> )           \u2013            <p>The value by which to divide each element.</p>"},{"location":"reference/ts_shape/transform/calculator/numeric_calc/#ts_shape.transform.calculator.numeric_calc.IntegerCalc.get_dataframe","title":"get_dataframe","text":"<pre><code>get_dataframe() -&gt; DataFrame\n</code></pre> <p>Returns the processed DataFrame.</p> Source code in <code>src/ts_shape/utils/base.py</code> <pre><code>def get_dataframe(self) -&gt; pd.DataFrame:\n    \"\"\"Returns the processed DataFrame.\"\"\"\n    return self.dataframe\n</code></pre>"},{"location":"reference/ts_shape/transform/calculator/numeric_calc/#ts_shape.transform.calculator.numeric_calc.IntegerCalc.mod_column","title":"mod_column  <code>classmethod</code>","text":"<pre><code>mod_column(dataframe: DataFrame, column_name: str = 'value_integer', mod_value: int = 1) -&gt; DataFrame\n</code></pre> <p>Performs a modulus operation on the integer column with a specified value.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>pd.DataFrame: The DataFrame with the modulus operation applied.</p> </li> </ul> Source code in <code>src/ts_shape/transform/calculator/numeric_calc.py</code> <pre><code>@classmethod\ndef mod_column(cls, dataframe: pd.DataFrame, column_name: str = 'value_integer', mod_value: int = 1) -&gt; pd.DataFrame:\n    \"\"\"\n    Performs a modulus operation on the integer column with a specified value.\n\n    Args:\n        dataframe (pd.DataFrame): The DataFrame to perform the operation on.\n        column_name (str): The column to apply the modulus operation to.\n        mod_value (int): The value to perform the modulus operation with.\n\n    Returns:\n        pd.DataFrame: The DataFrame with the modulus operation applied.\n    \"\"\"\n    dataframe[column_name] = dataframe[column_name] % mod_value\n    return dataframe\n</code></pre>"},{"location":"reference/ts_shape/transform/calculator/numeric_calc/#ts_shape.transform.calculator.numeric_calc.IntegerCalc.mod_column(dataframe)","title":"<code>dataframe</code>","text":"(<code>DataFrame</code>)           \u2013            <p>The DataFrame to perform the operation on.</p>"},{"location":"reference/ts_shape/transform/calculator/numeric_calc/#ts_shape.transform.calculator.numeric_calc.IntegerCalc.mod_column(column_name)","title":"<code>column_name</code>","text":"(<code>str</code>, default:                   <code>'value_integer'</code> )           \u2013            <p>The column to apply the modulus operation to.</p>"},{"location":"reference/ts_shape/transform/calculator/numeric_calc/#ts_shape.transform.calculator.numeric_calc.IntegerCalc.mod_column(mod_value)","title":"<code>mod_value</code>","text":"(<code>int</code>, default:                   <code>1</code> )           \u2013            <p>The value to perform the modulus operation with.</p>"},{"location":"reference/ts_shape/transform/calculator/numeric_calc/#ts_shape.transform.calculator.numeric_calc.IntegerCalc.offset_column","title":"offset_column  <code>classmethod</code>","text":"<pre><code>offset_column(dataframe: DataFrame, column_name: str = 'value_integer', offset_value: float = 0) -&gt; DataFrame\n</code></pre> <p>Offsets the integer column by the given value.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>pd.DataFrame: The DataFrame with the offset column.</p> </li> </ul> Source code in <code>src/ts_shape/transform/calculator/numeric_calc.py</code> <pre><code>@classmethod\ndef offset_column(cls, dataframe: pd.DataFrame, column_name: str = 'value_integer', offset_value: float = 0) -&gt; pd.DataFrame:\n    \"\"\"\n    Offsets the integer column by the given value.\n\n    Args:\n        dataframe (pd.DataFrame): The DataFrame to perform the operation on.\n        column_name (str): The column to apply the offset to.\n        offset_value (float): The value to add (positive) or subtract (negative) from each element in the column.\n\n    Returns:\n        pd.DataFrame: The DataFrame with the offset column.\n    \"\"\"\n    dataframe[column_name] = dataframe[column_name] + offset_value\n    return dataframe\n</code></pre>"},{"location":"reference/ts_shape/transform/calculator/numeric_calc/#ts_shape.transform.calculator.numeric_calc.IntegerCalc.offset_column(dataframe)","title":"<code>dataframe</code>","text":"(<code>DataFrame</code>)           \u2013            <p>The DataFrame to perform the operation on.</p>"},{"location":"reference/ts_shape/transform/calculator/numeric_calc/#ts_shape.transform.calculator.numeric_calc.IntegerCalc.offset_column(column_name)","title":"<code>column_name</code>","text":"(<code>str</code>, default:                   <code>'value_integer'</code> )           \u2013            <p>The column to apply the offset to.</p>"},{"location":"reference/ts_shape/transform/calculator/numeric_calc/#ts_shape.transform.calculator.numeric_calc.IntegerCalc.offset_column(offset_value)","title":"<code>offset_value</code>","text":"(<code>float</code>, default:                   <code>0</code> )           \u2013            <p>The value to add (positive) or subtract (negative) from each element in the column.</p>"},{"location":"reference/ts_shape/transform/calculator/numeric_calc/#ts_shape.transform.calculator.numeric_calc.IntegerCalc.power_column","title":"power_column  <code>classmethod</code>","text":"<pre><code>power_column(dataframe: DataFrame, column_name: str = 'value_integer', power_value: float = 1) -&gt; DataFrame\n</code></pre> <p>Raises each value in the integer column to the power of a specified value.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>pd.DataFrame: The DataFrame with the power operation applied.</p> </li> </ul> Source code in <code>src/ts_shape/transform/calculator/numeric_calc.py</code> <pre><code>@classmethod\ndef power_column(cls, dataframe: pd.DataFrame, column_name: str = 'value_integer', power_value: float = 1) -&gt; pd.DataFrame:\n    \"\"\"\n    Raises each value in the integer column to the power of a specified value.\n\n    Args:\n        dataframe (pd.DataFrame): The DataFrame to perform the operation on.\n        column_name (str): The column to apply the power operation to.\n        power_value (float): The exponent to raise each element to.\n\n    Returns:\n        pd.DataFrame: The DataFrame with the power operation applied.\n    \"\"\"\n    dataframe[column_name] = dataframe[column_name] ** power_value\n    return dataframe\n</code></pre>"},{"location":"reference/ts_shape/transform/calculator/numeric_calc/#ts_shape.transform.calculator.numeric_calc.IntegerCalc.power_column(dataframe)","title":"<code>dataframe</code>","text":"(<code>DataFrame</code>)           \u2013            <p>The DataFrame to perform the operation on.</p>"},{"location":"reference/ts_shape/transform/calculator/numeric_calc/#ts_shape.transform.calculator.numeric_calc.IntegerCalc.power_column(column_name)","title":"<code>column_name</code>","text":"(<code>str</code>, default:                   <code>'value_integer'</code> )           \u2013            <p>The column to apply the power operation to.</p>"},{"location":"reference/ts_shape/transform/calculator/numeric_calc/#ts_shape.transform.calculator.numeric_calc.IntegerCalc.power_column(power_value)","title":"<code>power_value</code>","text":"(<code>float</code>, default:                   <code>1</code> )           \u2013            <p>The exponent to raise each element to.</p>"},{"location":"reference/ts_shape/transform/calculator/numeric_calc/#ts_shape.transform.calculator.numeric_calc.IntegerCalc.scale_column","title":"scale_column  <code>classmethod</code>","text":"<pre><code>scale_column(dataframe: DataFrame, column_name: str = 'value_integer', factor: float = 1) -&gt; DataFrame\n</code></pre> <p>Scales the integer column by the given factor.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>pd.DataFrame: The DataFrame with the scaled column.</p> </li> </ul> Source code in <code>src/ts_shape/transform/calculator/numeric_calc.py</code> <pre><code>@classmethod\ndef scale_column(cls, dataframe: pd.DataFrame, column_name: str = 'value_integer', factor: float = 1) -&gt; pd.DataFrame:\n    \"\"\"\n    Scales the integer column by the given factor.\n\n    Args:\n        dataframe (pd.DataFrame): The DataFrame to perform the operation on.\n        column_name (str): The column to apply the scaling to.\n        factor (float): The scaling factor.\n\n    Returns:\n        pd.DataFrame: The DataFrame with the scaled column.\n    \"\"\"\n    dataframe[column_name] = dataframe[column_name] * factor\n    return dataframe\n</code></pre>"},{"location":"reference/ts_shape/transform/calculator/numeric_calc/#ts_shape.transform.calculator.numeric_calc.IntegerCalc.scale_column(dataframe)","title":"<code>dataframe</code>","text":"(<code>DataFrame</code>)           \u2013            <p>The DataFrame to perform the operation on.</p>"},{"location":"reference/ts_shape/transform/calculator/numeric_calc/#ts_shape.transform.calculator.numeric_calc.IntegerCalc.scale_column(column_name)","title":"<code>column_name</code>","text":"(<code>str</code>, default:                   <code>'value_integer'</code> )           \u2013            <p>The column to apply the scaling to.</p>"},{"location":"reference/ts_shape/transform/calculator/numeric_calc/#ts_shape.transform.calculator.numeric_calc.IntegerCalc.scale_column(factor)","title":"<code>factor</code>","text":"(<code>float</code>, default:                   <code>1</code> )           \u2013            <p>The scaling factor.</p>"},{"location":"reference/ts_shape/transform/calculator/numeric_calc/#ts_shape.transform.calculator.numeric_calc.IntegerCalc.subtract_column","title":"subtract_column  <code>classmethod</code>","text":"<pre><code>subtract_column(dataframe: DataFrame, column_name: str = 'value_integer', subtract_value: float = 0) -&gt; DataFrame\n</code></pre> <p>Subtracts a given value from each element in the integer column.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>pd.DataFrame: The DataFrame with the subtracted column.</p> </li> </ul> Source code in <code>src/ts_shape/transform/calculator/numeric_calc.py</code> <pre><code>@classmethod\ndef subtract_column(cls, dataframe: pd.DataFrame, column_name: str = 'value_integer', subtract_value: float = 0) -&gt; pd.DataFrame:\n    \"\"\"\n    Subtracts a given value from each element in the integer column.\n\n    Args:\n        dataframe (pd.DataFrame): The DataFrame to perform the operation on.\n        column_name (str): The column to apply the subtraction to.\n        subtract_value (float): The value to subtract from each element.\n\n    Returns:\n        pd.DataFrame: The DataFrame with the subtracted column.\n    \"\"\"\n    dataframe[column_name] = dataframe[column_name] - subtract_value\n    return dataframe\n</code></pre>"},{"location":"reference/ts_shape/transform/calculator/numeric_calc/#ts_shape.transform.calculator.numeric_calc.IntegerCalc.subtract_column(dataframe)","title":"<code>dataframe</code>","text":"(<code>DataFrame</code>)           \u2013            <p>The DataFrame to perform the operation on.</p>"},{"location":"reference/ts_shape/transform/calculator/numeric_calc/#ts_shape.transform.calculator.numeric_calc.IntegerCalc.subtract_column(column_name)","title":"<code>column_name</code>","text":"(<code>str</code>, default:                   <code>'value_integer'</code> )           \u2013            <p>The column to apply the subtraction to.</p>"},{"location":"reference/ts_shape/transform/calculator/numeric_calc/#ts_shape.transform.calculator.numeric_calc.IntegerCalc.subtract_column(subtract_value)","title":"<code>subtract_value</code>","text":"(<code>float</code>, default:                   <code>0</code> )           \u2013            <p>The value to subtract from each element.</p>"},{"location":"reference/ts_shape/transform/filter/__init__/","title":"init","text":""},{"location":"reference/ts_shape/transform/filter/__init__/#ts_shape.transform.filter","title":"ts_shape.transform.filter","text":"<p>Filters</p> <p>Row-level predicates for common column types.</p> <ul> <li>IntegerFilter: Equality/inequality/range filters for integer columns.</li> <li>filter_value_integer_match: Select rows equal to a value.</li> <li>filter_value_integer_not_match: Select rows not equal to a value.</li> <li> <p>filter_value_integer_between: Select rows within [min, max].</p> </li> <li> <p>DoubleFilter: NaN removal and numeric ranges for floating-point columns.</p> </li> <li>filter_nan_value_double: Drop rows with NaN in the column.</li> <li> <p>filter_value_double_between: Select rows within [min, max].</p> </li> <li> <p>StringFilter: Equality, contains, regex cleaning, and change detection.</p> </li> <li>filter_na_value_string: Drop rows with NA values.</li> <li>filter_value_string_match: Select rows equal to a string.</li> <li>filter_value_string_not_match: Select rows not equal to a string.</li> <li>filter_string_contains: Select rows containing a substring.</li> <li>regex_clean_value_string: Regex-based cleaning or replacement.</li> <li> <p>detect_changes_in_string: Detect row-to-row changes in a string column.</p> </li> <li> <p>BooleanFilter: Detect raising/falling edges of boolean states.</p> </li> <li>filter_falling_value_bool: True\u2192False transitions.</li> <li> <p>filter_raising_value_bool: False\u2192True transitions.</p> </li> <li> <p>IsDeltaFilter: Select rows by the is_delta flag.</p> </li> <li>filter_is_delta_true: Only True.</li> <li> <p>filter_is_delta_false: Only False.</p> </li> <li> <p>DateTimeFilter: Before/after/between filters for timestamps.</p> </li> <li>filter_after_date: After a given date.</li> <li>filter_before_date: Before a given date.</li> <li>filter_between_dates: Between start and end dates.</li> <li>filter_after_datetime: After a given datetime.</li> <li>filter_before_datetime: Before a given datetime.</li> <li> <p>filter_between_datetimes: Between start and end datetimes.</p> </li> <li> <p>CustomFilter: Free-form DataFrame.query string conditions.</p> </li> <li>filter_custom_conditions: Apply a query string to filter rows.</li> </ul> <p>Modules:</p> <ul> <li> <code>boolean_filter</code>           \u2013            </li> <li> <code>custom_filter</code>           \u2013            </li> <li> <code>datetime_filter</code>           \u2013            </li> <li> <code>numeric_filter</code>           \u2013            </li> <li> <code>string_filter</code>           \u2013            </li> </ul>"},{"location":"reference/ts_shape/transform/filter/boolean_filter/","title":"boolean_filter","text":""},{"location":"reference/ts_shape/transform/filter/boolean_filter/#ts_shape.transform.filter.boolean_filter","title":"ts_shape.transform.filter.boolean_filter","text":"<p>Classes:</p> <ul> <li> <code>BooleanFilter</code>           \u2013            <p>Provides class methods for filtering boolean columns in a pandas DataFrame,</p> </li> <li> <code>IsDeltaFilter</code>           \u2013            <p>Provides class methods for filtering is_delta columns in a pandas DataFrame.</p> </li> </ul>"},{"location":"reference/ts_shape/transform/filter/boolean_filter/#ts_shape.transform.filter.boolean_filter.BooleanFilter","title":"BooleanFilter","text":"<pre><code>BooleanFilter(dataframe: DataFrame, column_name: str = 'systime')\n</code></pre> <p>               Bases: <code>Base</code></p> <p>Provides class methods for filtering boolean columns in a pandas DataFrame, particularly focusing on status changes.</p> <p>Parameters:</p> <p>Methods:</p> <ul> <li> <code>filter_falling_value_bool</code>             \u2013              <p>Filters rows where 'value_bool' changes from True to False.</p> </li> <li> <code>filter_raising_value_bool</code>             \u2013              <p>Filters rows where 'value_bool' changes from False to True.</p> </li> <li> <code>get_dataframe</code>             \u2013              <p>Returns the processed DataFrame.</p> </li> </ul> Source code in <code>src/ts_shape/utils/base.py</code> <pre><code>def __init__(self, dataframe: pd.DataFrame, column_name: str = 'systime') -&gt; None:\n    \"\"\"\n    Initializes the Base with a DataFrame, detects time columns, converts them to datetime,\n    and sorts the DataFrame by the specified column (or the detected time column if applicable).\n\n    Args:\n        dataframe (pd.DataFrame): The DataFrame to be processed.\n        column_name (str): The column to sort by. Default is 'systime'. If the column is not found or is not a time column, the class will attempt to detect other time columns.\n    \"\"\"\n    self.dataframe = dataframe.copy()\n\n    # Attempt to convert the specified column_name to datetime if it exists\n    if column_name in self.dataframe.columns:\n        self.dataframe[column_name] = pd.to_datetime(self.dataframe[column_name], errors='coerce')\n    else:\n        # If the column_name is not in the DataFrame, fallback to automatic time detection\n        time_columns = [col for col in self.dataframe.columns if 'time' in col.lower() or 'date' in col.lower()]\n\n        # Convert all detected time columns to datetime, if any\n        for col in time_columns:\n            self.dataframe[col] = pd.to_datetime(self.dataframe[col], errors='coerce')\n\n        # If any time columns are detected, sort by the first one; otherwise, do nothing\n        if time_columns:\n            column_name = time_columns[0]\n\n    # Sort by the datetime column (either specified or detected)\n    if column_name in self.dataframe.columns:\n        self.dataframe = self.dataframe.sort_values(by=column_name)\n</code></pre>"},{"location":"reference/ts_shape/transform/filter/boolean_filter/#ts_shape.transform.filter.boolean_filter.BooleanFilter(dataframe)","title":"<code>dataframe</code>","text":"(<code>DataFrame</code>)           \u2013            <p>The DataFrame to be processed.</p>"},{"location":"reference/ts_shape/transform/filter/boolean_filter/#ts_shape.transform.filter.boolean_filter.BooleanFilter(column_name)","title":"<code>column_name</code>","text":"(<code>str</code>, default:                   <code>'systime'</code> )           \u2013            <p>The column to sort by. Default is 'systime'. If the column is not found or is not a time column, the class will attempt to detect other time columns.</p>"},{"location":"reference/ts_shape/transform/filter/boolean_filter/#ts_shape.transform.filter.boolean_filter.BooleanFilter.filter_falling_value_bool","title":"filter_falling_value_bool  <code>classmethod</code>","text":"<pre><code>filter_falling_value_bool(dataframe: DataFrame, column_name: str = 'value_bool') -&gt; DataFrame\n</code></pre> <p>Filters rows where 'value_bool' changes from True to False.</p> Source code in <code>src/ts_shape/transform/filter/boolean_filter.py</code> <pre><code>@classmethod\ndef filter_falling_value_bool(cls, dataframe: pd.DataFrame, column_name: str = 'value_bool') -&gt; pd.DataFrame:\n    \"\"\"Filters rows where 'value_bool' changes from True to False.\"\"\"\n    dataframe['previous_value_bool'] = dataframe[column_name].shift(1)\n    return dataframe[(dataframe['previous_value_bool'] == True) &amp; (dataframe[column_name] == False)]\n</code></pre>"},{"location":"reference/ts_shape/transform/filter/boolean_filter/#ts_shape.transform.filter.boolean_filter.BooleanFilter.filter_raising_value_bool","title":"filter_raising_value_bool  <code>classmethod</code>","text":"<pre><code>filter_raising_value_bool(dataframe: DataFrame, column_name: str = 'value_bool') -&gt; DataFrame\n</code></pre> <p>Filters rows where 'value_bool' changes from False to True.</p> Source code in <code>src/ts_shape/transform/filter/boolean_filter.py</code> <pre><code>@classmethod\ndef filter_raising_value_bool(cls, dataframe: pd.DataFrame, column_name: str = 'value_bool') -&gt; pd.DataFrame:\n    \"\"\"Filters rows where 'value_bool' changes from False to True.\"\"\"\n    dataframe['previous_value_bool'] = dataframe[column_name].shift(1)\n    return dataframe[(dataframe['previous_value_bool'] == False) &amp; (dataframe[column_name] == True)]\n</code></pre>"},{"location":"reference/ts_shape/transform/filter/boolean_filter/#ts_shape.transform.filter.boolean_filter.BooleanFilter.get_dataframe","title":"get_dataframe","text":"<pre><code>get_dataframe() -&gt; DataFrame\n</code></pre> <p>Returns the processed DataFrame.</p> Source code in <code>src/ts_shape/utils/base.py</code> <pre><code>def get_dataframe(self) -&gt; pd.DataFrame:\n    \"\"\"Returns the processed DataFrame.\"\"\"\n    return self.dataframe\n</code></pre>"},{"location":"reference/ts_shape/transform/filter/boolean_filter/#ts_shape.transform.filter.boolean_filter.IsDeltaFilter","title":"IsDeltaFilter","text":"<pre><code>IsDeltaFilter(dataframe: DataFrame, column_name: str = 'systime')\n</code></pre> <p>               Bases: <code>Base</code></p> <p>Provides class methods for filtering is_delta columns in a pandas DataFrame.</p> <p>Parameters:</p> <p>Methods:</p> <ul> <li> <code>filter_is_delta_false</code>             \u2013              <p>Filters rows where 'is_delta' is False.</p> </li> <li> <code>filter_is_delta_true</code>             \u2013              <p>Filters rows where 'is_delta' is True.</p> </li> <li> <code>get_dataframe</code>             \u2013              <p>Returns the processed DataFrame.</p> </li> </ul> Source code in <code>src/ts_shape/utils/base.py</code> <pre><code>def __init__(self, dataframe: pd.DataFrame, column_name: str = 'systime') -&gt; None:\n    \"\"\"\n    Initializes the Base with a DataFrame, detects time columns, converts them to datetime,\n    and sorts the DataFrame by the specified column (or the detected time column if applicable).\n\n    Args:\n        dataframe (pd.DataFrame): The DataFrame to be processed.\n        column_name (str): The column to sort by. Default is 'systime'. If the column is not found or is not a time column, the class will attempt to detect other time columns.\n    \"\"\"\n    self.dataframe = dataframe.copy()\n\n    # Attempt to convert the specified column_name to datetime if it exists\n    if column_name in self.dataframe.columns:\n        self.dataframe[column_name] = pd.to_datetime(self.dataframe[column_name], errors='coerce')\n    else:\n        # If the column_name is not in the DataFrame, fallback to automatic time detection\n        time_columns = [col for col in self.dataframe.columns if 'time' in col.lower() or 'date' in col.lower()]\n\n        # Convert all detected time columns to datetime, if any\n        for col in time_columns:\n            self.dataframe[col] = pd.to_datetime(self.dataframe[col], errors='coerce')\n\n        # If any time columns are detected, sort by the first one; otherwise, do nothing\n        if time_columns:\n            column_name = time_columns[0]\n\n    # Sort by the datetime column (either specified or detected)\n    if column_name in self.dataframe.columns:\n        self.dataframe = self.dataframe.sort_values(by=column_name)\n</code></pre>"},{"location":"reference/ts_shape/transform/filter/boolean_filter/#ts_shape.transform.filter.boolean_filter.IsDeltaFilter(dataframe)","title":"<code>dataframe</code>","text":"(<code>DataFrame</code>)           \u2013            <p>The DataFrame to be processed.</p>"},{"location":"reference/ts_shape/transform/filter/boolean_filter/#ts_shape.transform.filter.boolean_filter.IsDeltaFilter(column_name)","title":"<code>column_name</code>","text":"(<code>str</code>, default:                   <code>'systime'</code> )           \u2013            <p>The column to sort by. Default is 'systime'. If the column is not found or is not a time column, the class will attempt to detect other time columns.</p>"},{"location":"reference/ts_shape/transform/filter/boolean_filter/#ts_shape.transform.filter.boolean_filter.IsDeltaFilter.filter_is_delta_false","title":"filter_is_delta_false  <code>classmethod</code>","text":"<pre><code>filter_is_delta_false(dataframe: DataFrame, column_name: str = 'is_delta') -&gt; DataFrame\n</code></pre> <p>Filters rows where 'is_delta' is False.</p> Source code in <code>src/ts_shape/transform/filter/boolean_filter.py</code> <pre><code>@classmethod\ndef filter_is_delta_false(cls, dataframe: pd.DataFrame, column_name: str = 'is_delta') -&gt; pd.DataFrame:\n    \"\"\"Filters rows where 'is_delta' is False.\"\"\"\n    return dataframe[dataframe[column_name] == False]\n</code></pre>"},{"location":"reference/ts_shape/transform/filter/boolean_filter/#ts_shape.transform.filter.boolean_filter.IsDeltaFilter.filter_is_delta_true","title":"filter_is_delta_true  <code>classmethod</code>","text":"<pre><code>filter_is_delta_true(dataframe: DataFrame, column_name: str = 'is_delta') -&gt; DataFrame\n</code></pre> <p>Filters rows where 'is_delta' is True.</p> Source code in <code>src/ts_shape/transform/filter/boolean_filter.py</code> <pre><code>@classmethod\ndef filter_is_delta_true(cls, dataframe: pd.DataFrame, column_name: str = 'is_delta') -&gt; pd.DataFrame:\n    \"\"\"Filters rows where 'is_delta' is True.\"\"\"\n    # No need for instance, working directly on class level\n    return dataframe[dataframe[column_name] == True]\n</code></pre>"},{"location":"reference/ts_shape/transform/filter/boolean_filter/#ts_shape.transform.filter.boolean_filter.IsDeltaFilter.get_dataframe","title":"get_dataframe","text":"<pre><code>get_dataframe() -&gt; DataFrame\n</code></pre> <p>Returns the processed DataFrame.</p> Source code in <code>src/ts_shape/utils/base.py</code> <pre><code>def get_dataframe(self) -&gt; pd.DataFrame:\n    \"\"\"Returns the processed DataFrame.\"\"\"\n    return self.dataframe\n</code></pre>"},{"location":"reference/ts_shape/transform/filter/custom_filter/","title":"custom_filter","text":""},{"location":"reference/ts_shape/transform/filter/custom_filter/#ts_shape.transform.filter.custom_filter","title":"ts_shape.transform.filter.custom_filter","text":"<p>Classes:</p> <ul> <li> <code>CustomFilter</code>           \u2013            </li> </ul>"},{"location":"reference/ts_shape/transform/filter/custom_filter/#ts_shape.transform.filter.custom_filter.CustomFilter","title":"CustomFilter","text":"<pre><code>CustomFilter(dataframe: DataFrame, column_name: str = 'systime')\n</code></pre> <p>               Bases: <code>Base</code></p> <p>Parameters:</p> <p>Methods:</p> <ul> <li> <code>filter_custom_conditions</code>             \u2013              <p>Filters the DataFrame based on a set of user-defined conditions passed as a string.</p> </li> <li> <code>get_dataframe</code>             \u2013              <p>Returns the processed DataFrame.</p> </li> </ul> Source code in <code>src/ts_shape/utils/base.py</code> <pre><code>def __init__(self, dataframe: pd.DataFrame, column_name: str = 'systime') -&gt; None:\n    \"\"\"\n    Initializes the Base with a DataFrame, detects time columns, converts them to datetime,\n    and sorts the DataFrame by the specified column (or the detected time column if applicable).\n\n    Args:\n        dataframe (pd.DataFrame): The DataFrame to be processed.\n        column_name (str): The column to sort by. Default is 'systime'. If the column is not found or is not a time column, the class will attempt to detect other time columns.\n    \"\"\"\n    self.dataframe = dataframe.copy()\n\n    # Attempt to convert the specified column_name to datetime if it exists\n    if column_name in self.dataframe.columns:\n        self.dataframe[column_name] = pd.to_datetime(self.dataframe[column_name], errors='coerce')\n    else:\n        # If the column_name is not in the DataFrame, fallback to automatic time detection\n        time_columns = [col for col in self.dataframe.columns if 'time' in col.lower() or 'date' in col.lower()]\n\n        # Convert all detected time columns to datetime, if any\n        for col in time_columns:\n            self.dataframe[col] = pd.to_datetime(self.dataframe[col], errors='coerce')\n\n        # If any time columns are detected, sort by the first one; otherwise, do nothing\n        if time_columns:\n            column_name = time_columns[0]\n\n    # Sort by the datetime column (either specified or detected)\n    if column_name in self.dataframe.columns:\n        self.dataframe = self.dataframe.sort_values(by=column_name)\n</code></pre>"},{"location":"reference/ts_shape/transform/filter/custom_filter/#ts_shape.transform.filter.custom_filter.CustomFilter(dataframe)","title":"<code>dataframe</code>","text":"(<code>DataFrame</code>)           \u2013            <p>The DataFrame to be processed.</p>"},{"location":"reference/ts_shape/transform/filter/custom_filter/#ts_shape.transform.filter.custom_filter.CustomFilter(column_name)","title":"<code>column_name</code>","text":"(<code>str</code>, default:                   <code>'systime'</code> )           \u2013            <p>The column to sort by. Default is 'systime'. If the column is not found or is not a time column, the class will attempt to detect other time columns.</p>"},{"location":"reference/ts_shape/transform/filter/custom_filter/#ts_shape.transform.filter.custom_filter.CustomFilter.filter_custom_conditions","title":"filter_custom_conditions  <code>classmethod</code>","text":"<pre><code>filter_custom_conditions(dataframe: DataFrame, conditions: str) -&gt; DataFrame\n</code></pre> <p>Filters the DataFrame based on a set of user-defined conditions passed as a string.</p> <p>This method allows for flexible data filtering by evaluating a condition or multiple conditions specified in the 'conditions' parameter. The conditions must be provided as a string that can be interpreted by pandas' DataFrame.query() method.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>pd.DataFrame: A DataFrame containing only the rows that meet the specified conditions.</p> </li> </ul>"},{"location":"reference/ts_shape/transform/filter/custom_filter/#ts_shape.transform.filter.custom_filter.CustomFilter.filter_custom_conditions(dataframe)","title":"<code>dataframe</code>","text":"(<code>DataFrame</code>)           \u2013            <p>The DataFrame to apply the filter on.</p>"},{"location":"reference/ts_shape/transform/filter/custom_filter/#ts_shape.transform.filter.custom_filter.CustomFilter.filter_custom_conditions(conditions)","title":"<code>conditions</code>","text":"(<code>str</code>)           \u2013            <p>A string representing the conditions to filter the DataFrame.             The string should be formatted according to pandas query syntax.</p>"},{"location":"reference/ts_shape/transform/filter/custom_filter/#ts_shape.transform.filter.custom_filter.CustomFilter.filter_custom_conditions--example","title":"Example:","text":""},{"location":"reference/ts_shape/transform/filter/custom_filter/#ts_shape.transform.filter.custom_filter.CustomFilter.filter_custom_conditions--given-a-dataframe-df-containing-columns-age-and-score","title":"Given a DataFrame 'df' containing columns 'age' and 'score':","text":"<p>filtered_data = CustomFilter.filter_custom_conditions(df, \"age &gt; 30 and score &gt; 80\") print(filtered_data)</p> Note <p>Ensure that the column names and values used in conditions match those in the DataFrame. Complex expressions and functions available in pandas query syntax can also be used.</p> Source code in <code>src/ts_shape/transform/filter/custom_filter.py</code> <pre><code>@classmethod\ndef filter_custom_conditions(cls, dataframe: pd.DataFrame, conditions: str) -&gt; pd.DataFrame:\n    \"\"\"\n    Filters the DataFrame based on a set of user-defined conditions passed as a string.\n\n    This method allows for flexible data filtering by evaluating a condition or multiple conditions\n    specified in the 'conditions' parameter. The conditions must be provided as a string\n    that can be interpreted by pandas' DataFrame.query() method.\n\n    Args:\n        dataframe (pd.DataFrame): The DataFrame to apply the filter on.\n        conditions (str): A string representing the conditions to filter the DataFrame.\n                        The string should be formatted according to pandas query syntax.\n\n    Returns:\n        pd.DataFrame: A DataFrame containing only the rows that meet the specified conditions.\n\n    Example:\n    --------\n    # Given a DataFrame 'df' containing columns 'age' and 'score':\n    &gt;&gt;&gt; filtered_data = CustomFilter.filter_custom_conditions(df, \"age &gt; 30 and score &gt; 80\")\n    &gt;&gt;&gt; print(filtered_data)\n\n    Note:\n        Ensure that the column names and values used in conditions match those in the DataFrame.\n        Complex expressions and functions available in pandas query syntax can also be used.\n    \"\"\"\n    return dataframe.query(conditions)\n</code></pre>"},{"location":"reference/ts_shape/transform/filter/custom_filter/#ts_shape.transform.filter.custom_filter.CustomFilter.get_dataframe","title":"get_dataframe","text":"<pre><code>get_dataframe() -&gt; DataFrame\n</code></pre> <p>Returns the processed DataFrame.</p> Source code in <code>src/ts_shape/utils/base.py</code> <pre><code>def get_dataframe(self) -&gt; pd.DataFrame:\n    \"\"\"Returns the processed DataFrame.\"\"\"\n    return self.dataframe\n</code></pre>"},{"location":"reference/ts_shape/transform/filter/datetime_filter/","title":"datetime_filter","text":""},{"location":"reference/ts_shape/transform/filter/datetime_filter/#ts_shape.transform.filter.datetime_filter","title":"ts_shape.transform.filter.datetime_filter","text":"<p>Classes:</p> <ul> <li> <code>DateTimeFilter</code>           \u2013            <p>Provides class methods for filtering time columns in a pandas DataFrame.</p> </li> </ul>"},{"location":"reference/ts_shape/transform/filter/datetime_filter/#ts_shape.transform.filter.datetime_filter.DateTimeFilter","title":"DateTimeFilter","text":"<pre><code>DateTimeFilter(dataframe: DataFrame, column_name: str = 'systime')\n</code></pre> <p>               Bases: <code>Base</code></p> <p>Provides class methods for filtering time columns in a pandas DataFrame. Allows specification of which column to operate on.</p> Inherits from <p>Base (class): Base class with common initializations for DataFrame handling.</p> <p>Parameters:</p> <p>Methods:</p> <ul> <li> <code>filter_after_date</code>             \u2013              <p>Filters the DataFrame to include only rows after the specified date.</p> </li> <li> <code>filter_after_datetime</code>             \u2013              <p>Filters the DataFrame to include only rows after the specified datetime.</p> </li> <li> <code>filter_before_date</code>             \u2013              <p>Filters the DataFrame to include only rows before the specified date.</p> </li> <li> <code>filter_before_datetime</code>             \u2013              <p>Filters the DataFrame to include only rows before the specified datetime.</p> </li> <li> <code>filter_between_dates</code>             \u2013              <p>Filters the DataFrame to include only rows between the specified start and end dates.</p> </li> <li> <code>filter_between_datetimes</code>             \u2013              <p>Filters the DataFrame to include only rows between the specified start and end datetimes.</p> </li> <li> <code>get_dataframe</code>             \u2013              <p>Returns the processed DataFrame.</p> </li> </ul> Source code in <code>src/ts_shape/utils/base.py</code> <pre><code>def __init__(self, dataframe: pd.DataFrame, column_name: str = 'systime') -&gt; None:\n    \"\"\"\n    Initializes the Base with a DataFrame, detects time columns, converts them to datetime,\n    and sorts the DataFrame by the specified column (or the detected time column if applicable).\n\n    Args:\n        dataframe (pd.DataFrame): The DataFrame to be processed.\n        column_name (str): The column to sort by. Default is 'systime'. If the column is not found or is not a time column, the class will attempt to detect other time columns.\n    \"\"\"\n    self.dataframe = dataframe.copy()\n\n    # Attempt to convert the specified column_name to datetime if it exists\n    if column_name in self.dataframe.columns:\n        self.dataframe[column_name] = pd.to_datetime(self.dataframe[column_name], errors='coerce')\n    else:\n        # If the column_name is not in the DataFrame, fallback to automatic time detection\n        time_columns = [col for col in self.dataframe.columns if 'time' in col.lower() or 'date' in col.lower()]\n\n        # Convert all detected time columns to datetime, if any\n        for col in time_columns:\n            self.dataframe[col] = pd.to_datetime(self.dataframe[col], errors='coerce')\n\n        # If any time columns are detected, sort by the first one; otherwise, do nothing\n        if time_columns:\n            column_name = time_columns[0]\n\n    # Sort by the datetime column (either specified or detected)\n    if column_name in self.dataframe.columns:\n        self.dataframe = self.dataframe.sort_values(by=column_name)\n</code></pre>"},{"location":"reference/ts_shape/transform/filter/datetime_filter/#ts_shape.transform.filter.datetime_filter.DateTimeFilter(dataframe)","title":"<code>dataframe</code>","text":"(<code>DataFrame</code>)           \u2013            <p>The DataFrame to be processed.</p>"},{"location":"reference/ts_shape/transform/filter/datetime_filter/#ts_shape.transform.filter.datetime_filter.DateTimeFilter(column_name)","title":"<code>column_name</code>","text":"(<code>str</code>, default:                   <code>'systime'</code> )           \u2013            <p>The column to sort by. Default is 'systime'. If the column is not found or is not a time column, the class will attempt to detect other time columns.</p>"},{"location":"reference/ts_shape/transform/filter/datetime_filter/#ts_shape.transform.filter.datetime_filter.DateTimeFilter.filter_after_date","title":"filter_after_date  <code>classmethod</code>","text":"<pre><code>filter_after_date(dataframe: DataFrame, column_name: str = 'systime', date: str = None) -&gt; DataFrame\n</code></pre> <p>Filters the DataFrame to include only rows after the specified date.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>pd.DataFrame: A DataFrame containing rows where the 'systime' is after the specified date.</p> </li> </ul>"},{"location":"reference/ts_shape/transform/filter/datetime_filter/#ts_shape.transform.filter.datetime_filter.DateTimeFilter.filter_after_date(date)","title":"<code>date</code>","text":"(<code>str</code>, default:                   <code>None</code> )           \u2013            <p>The cutoff date in 'YYYY-MM-DD' format.</p>"},{"location":"reference/ts_shape/transform/filter/datetime_filter/#ts_shape.transform.filter.datetime_filter.DateTimeFilter.filter_after_date--example","title":"Example:","text":"<p>filtered_data = DateTimeFilter.filter_after_date(df, \"systime\", \"2023-01-01\") print(filtered_data)</p> Source code in <code>src/ts_shape/transform/filter/datetime_filter.py</code> <pre><code>@classmethod\ndef filter_after_date(cls, dataframe: pd.DataFrame, column_name: str = 'systime', date: str = None) -&gt; pd.DataFrame:\n    \"\"\"\n    Filters the DataFrame to include only rows after the specified date.\n\n    Args:\n        date (str): The cutoff date in 'YYYY-MM-DD' format.\n\n    Returns:\n        pd.DataFrame: A DataFrame containing rows where the 'systime' is after the specified date.\n\n    Example:\n    --------\n    &gt;&gt;&gt; filtered_data = DateTimeFilter.filter_after_date(df, \"systime\", \"2023-01-01\")\n    &gt;&gt;&gt; print(filtered_data)\n    \"\"\"\n    return dataframe[dataframe[column_name] &gt; pd.to_datetime(date)]\n</code></pre>"},{"location":"reference/ts_shape/transform/filter/datetime_filter/#ts_shape.transform.filter.datetime_filter.DateTimeFilter.filter_after_datetime","title":"filter_after_datetime  <code>classmethod</code>","text":"<pre><code>filter_after_datetime(dataframe: DataFrame, column_name: str = 'systime', datetime: str = None) -&gt; DataFrame\n</code></pre> <p>Filters the DataFrame to include only rows after the specified datetime.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>pd.DataFrame: A DataFrame containing rows where the 'systime' is after the specified datetime.</p> </li> </ul>"},{"location":"reference/ts_shape/transform/filter/datetime_filter/#ts_shape.transform.filter.datetime_filter.DateTimeFilter.filter_after_datetime(datetime)","title":"<code>datetime</code>","text":"(<code>str</code>, default:                   <code>None</code> )           \u2013            <p>The cutoff datetime in 'YYYY-MM-DD HH:MM:SS' format.</p>"},{"location":"reference/ts_shape/transform/filter/datetime_filter/#ts_shape.transform.filter.datetime_filter.DateTimeFilter.filter_after_datetime--example","title":"Example:","text":"<p>filtered_data = DateTimeFilter.filter_after_datetime(df, \"systime\", \"2023-01-01 12:00:00\") print(filtered_data)</p> Source code in <code>src/ts_shape/transform/filter/datetime_filter.py</code> <pre><code>@classmethod\ndef filter_after_datetime(cls, dataframe: pd.DataFrame, column_name: str = 'systime', datetime: str = None) -&gt; pd.DataFrame:\n    \"\"\"\n    Filters the DataFrame to include only rows after the specified datetime.\n\n    Args:\n        datetime (str): The cutoff datetime in 'YYYY-MM-DD HH:MM:SS' format.\n\n    Returns:\n        pd.DataFrame: A DataFrame containing rows where the 'systime' is after the specified datetime.\n\n    Example:\n    --------\n    &gt;&gt;&gt; filtered_data = DateTimeFilter.filter_after_datetime(df, \"systime\", \"2023-01-01 12:00:00\")\n    &gt;&gt;&gt; print(filtered_data)\n    \"\"\"\n    return dataframe[dataframe[column_name] &gt; pd.to_datetime(datetime)]\n</code></pre>"},{"location":"reference/ts_shape/transform/filter/datetime_filter/#ts_shape.transform.filter.datetime_filter.DateTimeFilter.filter_before_date","title":"filter_before_date  <code>classmethod</code>","text":"<pre><code>filter_before_date(dataframe: DataFrame, column_name: str = 'systime', date: str = None) -&gt; DataFrame\n</code></pre> <p>Filters the DataFrame to include only rows before the specified date.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>pd.DataFrame: A DataFrame containing rows where the 'systime' is before the specified date.</p> </li> </ul>"},{"location":"reference/ts_shape/transform/filter/datetime_filter/#ts_shape.transform.filter.datetime_filter.DateTimeFilter.filter_before_date(date)","title":"<code>date</code>","text":"(<code>str</code>, default:                   <code>None</code> )           \u2013            <p>The cutoff date in 'YYYY-MM-DD' format.</p>"},{"location":"reference/ts_shape/transform/filter/datetime_filter/#ts_shape.transform.filter.datetime_filter.DateTimeFilter.filter_before_date--example","title":"Example:","text":"<p>filtered_data = DateTimeFilter.filter_before_date(df, \"systime\", \"2023-01-01\") print(filtered_data)</p> Source code in <code>src/ts_shape/transform/filter/datetime_filter.py</code> <pre><code>@classmethod\ndef filter_before_date(cls, dataframe: pd.DataFrame, column_name: str = 'systime', date: str = None) -&gt; pd.DataFrame:\n    \"\"\"\n    Filters the DataFrame to include only rows before the specified date.\n\n    Args:\n        date (str): The cutoff date in 'YYYY-MM-DD' format.\n\n    Returns:\n        pd.DataFrame: A DataFrame containing rows where the 'systime' is before the specified date.\n\n    Example:\n    --------\n    &gt;&gt;&gt; filtered_data = DateTimeFilter.filter_before_date(df, \"systime\", \"2023-01-01\")\n    &gt;&gt;&gt; print(filtered_data)\n    \"\"\"\n    return dataframe[dataframe[column_name] &lt; pd.to_datetime(date)]\n</code></pre>"},{"location":"reference/ts_shape/transform/filter/datetime_filter/#ts_shape.transform.filter.datetime_filter.DateTimeFilter.filter_before_datetime","title":"filter_before_datetime  <code>classmethod</code>","text":"<pre><code>filter_before_datetime(dataframe: DataFrame, column_name: str = 'systime', datetime: str = None) -&gt; DataFrame\n</code></pre> <p>Filters the DataFrame to include only rows before the specified datetime.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>pd.DataFrame: A DataFrame containing rows where the 'systime' is before the specified datetime.</p> </li> </ul>"},{"location":"reference/ts_shape/transform/filter/datetime_filter/#ts_shape.transform.filter.datetime_filter.DateTimeFilter.filter_before_datetime(datetime)","title":"<code>datetime</code>","text":"(<code>str</code>, default:                   <code>None</code> )           \u2013            <p>The cutoff datetime in 'YYYY-MM-DD HH:MM:SS' format.</p>"},{"location":"reference/ts_shape/transform/filter/datetime_filter/#ts_shape.transform.filter.datetime_filter.DateTimeFilter.filter_before_datetime--example","title":"Example:","text":"<p>filtered_data = DateTimeFilter.filter_before_datetime(df, \"systime\", \"2023-01-01 12:00:00\") print(filtered_data)</p> Source code in <code>src/ts_shape/transform/filter/datetime_filter.py</code> <pre><code>@classmethod\ndef filter_before_datetime(cls, dataframe: pd.DataFrame, column_name: str = 'systime', datetime: str = None) -&gt; pd.DataFrame:\n    \"\"\"\n    Filters the DataFrame to include only rows before the specified datetime.\n\n    Args:\n        datetime (str): The cutoff datetime in 'YYYY-MM-DD HH:MM:SS' format.\n\n    Returns:\n        pd.DataFrame: A DataFrame containing rows where the 'systime' is before the specified datetime.\n\n    Example:\n    --------\n    &gt;&gt;&gt; filtered_data = DateTimeFilter.filter_before_datetime(df, \"systime\", \"2023-01-01 12:00:00\")\n    &gt;&gt;&gt; print(filtered_data)\n    \"\"\"\n    return dataframe[dataframe[column_name] &lt; pd.to_datetime(datetime)]\n</code></pre>"},{"location":"reference/ts_shape/transform/filter/datetime_filter/#ts_shape.transform.filter.datetime_filter.DateTimeFilter.filter_between_dates","title":"filter_between_dates  <code>classmethod</code>","text":"<pre><code>filter_between_dates(dataframe: DataFrame, column_name: str = 'systime', start_date: str = None, end_date: str = None) -&gt; DataFrame\n</code></pre> <p>Filters the DataFrame to include only rows between the specified start and end dates.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>pd.DataFrame: A DataFrame containing rows where the 'systime' is between the specified dates.</p> </li> </ul>"},{"location":"reference/ts_shape/transform/filter/datetime_filter/#ts_shape.transform.filter.datetime_filter.DateTimeFilter.filter_between_dates(start_date)","title":"<code>start_date</code>","text":"(<code>str</code>, default:                   <code>None</code> )           \u2013            <p>The start date of the interval in 'YYYY-MM-DD' format.</p>"},{"location":"reference/ts_shape/transform/filter/datetime_filter/#ts_shape.transform.filter.datetime_filter.DateTimeFilter.filter_between_dates(end_date)","title":"<code>end_date</code>","text":"(<code>str</code>, default:                   <code>None</code> )           \u2013            <p>The end date of the interval in 'YYYY-MM-DD' format.</p>"},{"location":"reference/ts_shape/transform/filter/datetime_filter/#ts_shape.transform.filter.datetime_filter.DateTimeFilter.filter_between_dates--example","title":"Example:","text":"<p>filtered_data = DateTimeFilter.filter_between_dates(df, \"systime\", \"2023-01-01\", \"2023-02-01\") print(filtered_data)</p> Source code in <code>src/ts_shape/transform/filter/datetime_filter.py</code> <pre><code>@classmethod\ndef filter_between_dates(cls, dataframe: pd.DataFrame, column_name: str = 'systime', start_date: str = None, end_date: str = None) -&gt; pd.DataFrame:\n    \"\"\"\n    Filters the DataFrame to include only rows between the specified start and end dates.\n\n    Args:\n        start_date (str): The start date of the interval in 'YYYY-MM-DD' format.\n        end_date (str): The end date of the interval in 'YYYY-MM-DD' format.\n\n    Returns:\n        pd.DataFrame: A DataFrame containing rows where the 'systime' is between the specified dates.\n\n    Example:\n    --------\n    &gt;&gt;&gt; filtered_data = DateTimeFilter.filter_between_dates(df, \"systime\", \"2023-01-01\", \"2023-02-01\")\n    &gt;&gt;&gt; print(filtered_data)\n    \"\"\"\n    mask = (dataframe[column_name] &gt; pd.to_datetime(start_date)) &amp; (dataframe[column_name] &lt; pd.to_datetime(end_date))\n    return dataframe[mask]\n</code></pre>"},{"location":"reference/ts_shape/transform/filter/datetime_filter/#ts_shape.transform.filter.datetime_filter.DateTimeFilter.filter_between_datetimes","title":"filter_between_datetimes  <code>classmethod</code>","text":"<pre><code>filter_between_datetimes(dataframe: DataFrame, column_name: str = 'systime', start_datetime: str = None, end_datetime: str = None) -&gt; DataFrame\n</code></pre> <p>Filters the DataFrame to include only rows between the specified start and end datetimes.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>pd.DataFrame: A DataFrame containing rows where the 'systime' is between the specified datetimes.</p> </li> </ul>"},{"location":"reference/ts_shape/transform/filter/datetime_filter/#ts_shape.transform.filter.datetime_filter.DateTimeFilter.filter_between_datetimes(start_datetime)","title":"<code>start_datetime</code>","text":"(<code>str</code>, default:                   <code>None</code> )           \u2013            <p>The start datetime of the interval in 'YYYY-MM-DD HH:MM:SS' format.</p>"},{"location":"reference/ts_shape/transform/filter/datetime_filter/#ts_shape.transform.filter.datetime_filter.DateTimeFilter.filter_between_datetimes(end_datetime)","title":"<code>end_datetime</code>","text":"(<code>str</code>, default:                   <code>None</code> )           \u2013            <p>The end datetime of the interval in 'YYYY-MM-DD HH:MM:SS' format.</p>"},{"location":"reference/ts_shape/transform/filter/datetime_filter/#ts_shape.transform.filter.datetime_filter.DateTimeFilter.filter_between_datetimes--example","title":"Example:","text":"<p>filtered_data = DateTimeFilter.filter_between_datetimes(df, \"systime\", \"2023-01-01 12:00:00\", \"2023-02-01 12:00:00\") print(filtered_data)</p> Source code in <code>src/ts_shape/transform/filter/datetime_filter.py</code> <pre><code>@classmethod\ndef filter_between_datetimes(cls, dataframe: pd.DataFrame, column_name: str = 'systime', start_datetime: str = None, end_datetime: str = None) -&gt; pd.DataFrame:\n    \"\"\"\n    Filters the DataFrame to include only rows between the specified start and end datetimes.\n\n    Args:\n        start_datetime (str): The start datetime of the interval in 'YYYY-MM-DD HH:MM:SS' format.\n        end_datetime (str): The end datetime of the interval in 'YYYY-MM-DD HH:MM:SS' format.\n\n    Returns:\n        pd.DataFrame: A DataFrame containing rows where the 'systime' is between the specified datetimes.\n\n    Example:\n    --------\n    &gt;&gt;&gt; filtered_data = DateTimeFilter.filter_between_datetimes(df, \"systime\", \"2023-01-01 12:00:00\", \"2023-02-01 12:00:00\")\n    &gt;&gt;&gt; print(filtered_data)\n    \"\"\"\n    mask = (dataframe[column_name] &gt; pd.to_datetime(start_datetime)) &amp; (dataframe[column_name] &lt; pd.to_datetime(end_datetime))\n    return dataframe[mask]\n</code></pre>"},{"location":"reference/ts_shape/transform/filter/datetime_filter/#ts_shape.transform.filter.datetime_filter.DateTimeFilter.get_dataframe","title":"get_dataframe","text":"<pre><code>get_dataframe() -&gt; DataFrame\n</code></pre> <p>Returns the processed DataFrame.</p> Source code in <code>src/ts_shape/utils/base.py</code> <pre><code>def get_dataframe(self) -&gt; pd.DataFrame:\n    \"\"\"Returns the processed DataFrame.\"\"\"\n    return self.dataframe\n</code></pre>"},{"location":"reference/ts_shape/transform/filter/numeric_filter/","title":"numeric_filter","text":""},{"location":"reference/ts_shape/transform/filter/numeric_filter/#ts_shape.transform.filter.numeric_filter","title":"ts_shape.transform.filter.numeric_filter","text":"<p>Classes:</p> <ul> <li> <code>DoubleFilter</code>           \u2013            <p>Provides class methods for filtering double (floating-point) columns in a pandas DataFrame,</p> </li> <li> <code>IntegerFilter</code>           \u2013            <p>Provides class methods for filtering integer columns in a pandas DataFrame.</p> </li> </ul>"},{"location":"reference/ts_shape/transform/filter/numeric_filter/#ts_shape.transform.filter.numeric_filter.DoubleFilter","title":"DoubleFilter","text":"<pre><code>DoubleFilter(dataframe: DataFrame, column_name: str = 'systime')\n</code></pre> <p>               Bases: <code>Base</code></p> <p>Provides class methods for filtering double (floating-point) columns in a pandas DataFrame, particularly focusing on NaN values.</p> <p>Parameters:</p> <p>Methods:</p> <ul> <li> <code>filter_nan_value_double</code>             \u2013              <p>Filters out rows where 'value_double' is NaN.</p> </li> <li> <code>filter_value_double_between</code>             \u2013              <p>Filters rows where 'value_double' is between the specified min and max values (inclusive).</p> </li> <li> <code>get_dataframe</code>             \u2013              <p>Returns the processed DataFrame.</p> </li> </ul> Source code in <code>src/ts_shape/utils/base.py</code> <pre><code>def __init__(self, dataframe: pd.DataFrame, column_name: str = 'systime') -&gt; None:\n    \"\"\"\n    Initializes the Base with a DataFrame, detects time columns, converts them to datetime,\n    and sorts the DataFrame by the specified column (or the detected time column if applicable).\n\n    Args:\n        dataframe (pd.DataFrame): The DataFrame to be processed.\n        column_name (str): The column to sort by. Default is 'systime'. If the column is not found or is not a time column, the class will attempt to detect other time columns.\n    \"\"\"\n    self.dataframe = dataframe.copy()\n\n    # Attempt to convert the specified column_name to datetime if it exists\n    if column_name in self.dataframe.columns:\n        self.dataframe[column_name] = pd.to_datetime(self.dataframe[column_name], errors='coerce')\n    else:\n        # If the column_name is not in the DataFrame, fallback to automatic time detection\n        time_columns = [col for col in self.dataframe.columns if 'time' in col.lower() or 'date' in col.lower()]\n\n        # Convert all detected time columns to datetime, if any\n        for col in time_columns:\n            self.dataframe[col] = pd.to_datetime(self.dataframe[col], errors='coerce')\n\n        # If any time columns are detected, sort by the first one; otherwise, do nothing\n        if time_columns:\n            column_name = time_columns[0]\n\n    # Sort by the datetime column (either specified or detected)\n    if column_name in self.dataframe.columns:\n        self.dataframe = self.dataframe.sort_values(by=column_name)\n</code></pre>"},{"location":"reference/ts_shape/transform/filter/numeric_filter/#ts_shape.transform.filter.numeric_filter.DoubleFilter(dataframe)","title":"<code>dataframe</code>","text":"(<code>DataFrame</code>)           \u2013            <p>The DataFrame to be processed.</p>"},{"location":"reference/ts_shape/transform/filter/numeric_filter/#ts_shape.transform.filter.numeric_filter.DoubleFilter(column_name)","title":"<code>column_name</code>","text":"(<code>str</code>, default:                   <code>'systime'</code> )           \u2013            <p>The column to sort by. Default is 'systime'. If the column is not found or is not a time column, the class will attempt to detect other time columns.</p>"},{"location":"reference/ts_shape/transform/filter/numeric_filter/#ts_shape.transform.filter.numeric_filter.DoubleFilter.filter_nan_value_double","title":"filter_nan_value_double  <code>classmethod</code>","text":"<pre><code>filter_nan_value_double(dataframe: DataFrame, column_name: str = 'value_double') -&gt; DataFrame\n</code></pre> <p>Filters out rows where 'value_double' is NaN.</p> Source code in <code>src/ts_shape/transform/filter/numeric_filter.py</code> <pre><code>@classmethod\ndef filter_nan_value_double(cls, dataframe: pd.DataFrame, column_name: str = 'value_double') -&gt; pd.DataFrame:\n    \"\"\"Filters out rows where 'value_double' is NaN.\"\"\"\n    return dataframe[dataframe[column_name].notna()]\n</code></pre>"},{"location":"reference/ts_shape/transform/filter/numeric_filter/#ts_shape.transform.filter.numeric_filter.DoubleFilter.filter_value_double_between","title":"filter_value_double_between  <code>classmethod</code>","text":"<pre><code>filter_value_double_between(dataframe: DataFrame, column_name: str = 'value_double', min_value: float = 0.0, max_value: float = 100.0) -&gt; DataFrame\n</code></pre> <p>Filters rows where 'value_double' is between the specified min and max values (inclusive).</p> Source code in <code>src/ts_shape/transform/filter/numeric_filter.py</code> <pre><code>@classmethod\ndef filter_value_double_between(cls, dataframe: pd.DataFrame, column_name: str = 'value_double', min_value: float = 0.0, max_value: float = 100.0) -&gt; pd.DataFrame:\n    \"\"\"Filters rows where 'value_double' is between the specified min and max values (inclusive).\"\"\"\n    return dataframe[(dataframe[column_name] &gt;= min_value) &amp; (dataframe[column_name] &lt;= max_value)]\n</code></pre>"},{"location":"reference/ts_shape/transform/filter/numeric_filter/#ts_shape.transform.filter.numeric_filter.DoubleFilter.get_dataframe","title":"get_dataframe","text":"<pre><code>get_dataframe() -&gt; DataFrame\n</code></pre> <p>Returns the processed DataFrame.</p> Source code in <code>src/ts_shape/utils/base.py</code> <pre><code>def get_dataframe(self) -&gt; pd.DataFrame:\n    \"\"\"Returns the processed DataFrame.\"\"\"\n    return self.dataframe\n</code></pre>"},{"location":"reference/ts_shape/transform/filter/numeric_filter/#ts_shape.transform.filter.numeric_filter.IntegerFilter","title":"IntegerFilter","text":"<pre><code>IntegerFilter(dataframe: DataFrame, column_name: str = 'systime')\n</code></pre> <p>               Bases: <code>Base</code></p> <p>Provides class methods for filtering integer columns in a pandas DataFrame.</p> <p>Parameters:</p> <p>Methods:</p> <ul> <li> <code>filter_value_integer_between</code>             \u2013              <p>Filters rows where 'value_integer' is between the specified min and max values (inclusive).</p> </li> <li> <code>filter_value_integer_match</code>             \u2013              <p>Filters rows where 'value_integer' matches the specified integer.</p> </li> <li> <code>filter_value_integer_not_match</code>             \u2013              <p>Filters rows where 'value_integer' does not match the specified integer.</p> </li> <li> <code>get_dataframe</code>             \u2013              <p>Returns the processed DataFrame.</p> </li> </ul> Source code in <code>src/ts_shape/utils/base.py</code> <pre><code>def __init__(self, dataframe: pd.DataFrame, column_name: str = 'systime') -&gt; None:\n    \"\"\"\n    Initializes the Base with a DataFrame, detects time columns, converts them to datetime,\n    and sorts the DataFrame by the specified column (or the detected time column if applicable).\n\n    Args:\n        dataframe (pd.DataFrame): The DataFrame to be processed.\n        column_name (str): The column to sort by. Default is 'systime'. If the column is not found or is not a time column, the class will attempt to detect other time columns.\n    \"\"\"\n    self.dataframe = dataframe.copy()\n\n    # Attempt to convert the specified column_name to datetime if it exists\n    if column_name in self.dataframe.columns:\n        self.dataframe[column_name] = pd.to_datetime(self.dataframe[column_name], errors='coerce')\n    else:\n        # If the column_name is not in the DataFrame, fallback to automatic time detection\n        time_columns = [col for col in self.dataframe.columns if 'time' in col.lower() or 'date' in col.lower()]\n\n        # Convert all detected time columns to datetime, if any\n        for col in time_columns:\n            self.dataframe[col] = pd.to_datetime(self.dataframe[col], errors='coerce')\n\n        # If any time columns are detected, sort by the first one; otherwise, do nothing\n        if time_columns:\n            column_name = time_columns[0]\n\n    # Sort by the datetime column (either specified or detected)\n    if column_name in self.dataframe.columns:\n        self.dataframe = self.dataframe.sort_values(by=column_name)\n</code></pre>"},{"location":"reference/ts_shape/transform/filter/numeric_filter/#ts_shape.transform.filter.numeric_filter.IntegerFilter(dataframe)","title":"<code>dataframe</code>","text":"(<code>DataFrame</code>)           \u2013            <p>The DataFrame to be processed.</p>"},{"location":"reference/ts_shape/transform/filter/numeric_filter/#ts_shape.transform.filter.numeric_filter.IntegerFilter(column_name)","title":"<code>column_name</code>","text":"(<code>str</code>, default:                   <code>'systime'</code> )           \u2013            <p>The column to sort by. Default is 'systime'. If the column is not found or is not a time column, the class will attempt to detect other time columns.</p>"},{"location":"reference/ts_shape/transform/filter/numeric_filter/#ts_shape.transform.filter.numeric_filter.IntegerFilter.filter_value_integer_between","title":"filter_value_integer_between  <code>classmethod</code>","text":"<pre><code>filter_value_integer_between(dataframe: DataFrame, column_name: str = 'value_integer', min_value: int = 0, max_value: int = 100) -&gt; DataFrame\n</code></pre> <p>Filters rows where 'value_integer' is between the specified min and max values (inclusive).</p> Source code in <code>src/ts_shape/transform/filter/numeric_filter.py</code> <pre><code>@classmethod\ndef filter_value_integer_between(cls, dataframe: pd.DataFrame, column_name: str = 'value_integer', min_value: int = 0, max_value: int = 100) -&gt; pd.DataFrame:\n    \"\"\"Filters rows where 'value_integer' is between the specified min and max values (inclusive).\"\"\"\n    return dataframe[(dataframe[column_name] &gt;= min_value) &amp; (dataframe[column_name] &lt;= max_value)]\n</code></pre>"},{"location":"reference/ts_shape/transform/filter/numeric_filter/#ts_shape.transform.filter.numeric_filter.IntegerFilter.filter_value_integer_match","title":"filter_value_integer_match  <code>classmethod</code>","text":"<pre><code>filter_value_integer_match(dataframe: DataFrame, column_name: str = 'value_integer', integer_value: int = 0) -&gt; DataFrame\n</code></pre> <p>Filters rows where 'value_integer' matches the specified integer.</p> Source code in <code>src/ts_shape/transform/filter/numeric_filter.py</code> <pre><code>@classmethod\ndef filter_value_integer_match(cls, dataframe: pd.DataFrame, column_name: str = 'value_integer', integer_value: int = 0) -&gt; pd.DataFrame:\n    \"\"\"Filters rows where 'value_integer' matches the specified integer.\"\"\"\n    return dataframe[dataframe[column_name] == integer_value]\n</code></pre>"},{"location":"reference/ts_shape/transform/filter/numeric_filter/#ts_shape.transform.filter.numeric_filter.IntegerFilter.filter_value_integer_not_match","title":"filter_value_integer_not_match  <code>classmethod</code>","text":"<pre><code>filter_value_integer_not_match(dataframe: DataFrame, column_name: str = 'value_integer', integer_value: int = 0) -&gt; DataFrame\n</code></pre> <p>Filters rows where 'value_integer' does not match the specified integer.</p> Source code in <code>src/ts_shape/transform/filter/numeric_filter.py</code> <pre><code>@classmethod\ndef filter_value_integer_not_match(cls, dataframe: pd.DataFrame, column_name: str = 'value_integer', integer_value: int = 0) -&gt; pd.DataFrame:\n    \"\"\"Filters rows where 'value_integer' does not match the specified integer.\"\"\"\n    return dataframe[dataframe[column_name] != integer_value]\n</code></pre>"},{"location":"reference/ts_shape/transform/filter/numeric_filter/#ts_shape.transform.filter.numeric_filter.IntegerFilter.get_dataframe","title":"get_dataframe","text":"<pre><code>get_dataframe() -&gt; DataFrame\n</code></pre> <p>Returns the processed DataFrame.</p> Source code in <code>src/ts_shape/utils/base.py</code> <pre><code>def get_dataframe(self) -&gt; pd.DataFrame:\n    \"\"\"Returns the processed DataFrame.\"\"\"\n    return self.dataframe\n</code></pre>"},{"location":"reference/ts_shape/transform/filter/string_filter/","title":"string_filter","text":""},{"location":"reference/ts_shape/transform/filter/string_filter/#ts_shape.transform.filter.string_filter","title":"ts_shape.transform.filter.string_filter","text":"<p>Classes:</p> <ul> <li> <code>StringFilter</code>           \u2013            <p>A class for filtering operations on string columns within a pandas DataFrame.</p> </li> </ul>"},{"location":"reference/ts_shape/transform/filter/string_filter/#ts_shape.transform.filter.string_filter.StringFilter","title":"StringFilter","text":"<pre><code>StringFilter(dataframe: DataFrame, column_name: str = 'systime')\n</code></pre> <p>               Bases: <code>Base</code></p> <p>A class for filtering operations on string columns within a pandas DataFrame.</p> <p>Provides class methods for operations on string columns.</p> <p>Parameters:</p> <p>Methods:</p> <ul> <li> <code>detect_changes_in_string</code>             \u2013              <p>Detects changes from row to row in the specified string column.</p> </li> <li> <code>filter_na_value_string</code>             \u2013              <p>Filters out rows where the specified string column is NA.</p> </li> <li> <code>filter_string_contains</code>             \u2013              <p>Filters rows where the specified string column contains the provided substring.</p> </li> <li> <code>filter_value_string_match</code>             \u2013              <p>Filters rows where the specified string column matches the provided string.</p> </li> <li> <code>filter_value_string_not_match</code>             \u2013              <p>Filters rows where the specified string column does not match the provided string.</p> </li> <li> <code>get_dataframe</code>             \u2013              <p>Returns the processed DataFrame.</p> </li> <li> <code>regex_clean_value_string</code>             \u2013              <p>Applies a regex pattern to clean the specified string column.</p> </li> </ul> Source code in <code>src/ts_shape/utils/base.py</code> <pre><code>def __init__(self, dataframe: pd.DataFrame, column_name: str = 'systime') -&gt; None:\n    \"\"\"\n    Initializes the Base with a DataFrame, detects time columns, converts them to datetime,\n    and sorts the DataFrame by the specified column (or the detected time column if applicable).\n\n    Args:\n        dataframe (pd.DataFrame): The DataFrame to be processed.\n        column_name (str): The column to sort by. Default is 'systime'. If the column is not found or is not a time column, the class will attempt to detect other time columns.\n    \"\"\"\n    self.dataframe = dataframe.copy()\n\n    # Attempt to convert the specified column_name to datetime if it exists\n    if column_name in self.dataframe.columns:\n        self.dataframe[column_name] = pd.to_datetime(self.dataframe[column_name], errors='coerce')\n    else:\n        # If the column_name is not in the DataFrame, fallback to automatic time detection\n        time_columns = [col for col in self.dataframe.columns if 'time' in col.lower() or 'date' in col.lower()]\n\n        # Convert all detected time columns to datetime, if any\n        for col in time_columns:\n            self.dataframe[col] = pd.to_datetime(self.dataframe[col], errors='coerce')\n\n        # If any time columns are detected, sort by the first one; otherwise, do nothing\n        if time_columns:\n            column_name = time_columns[0]\n\n    # Sort by the datetime column (either specified or detected)\n    if column_name in self.dataframe.columns:\n        self.dataframe = self.dataframe.sort_values(by=column_name)\n</code></pre>"},{"location":"reference/ts_shape/transform/filter/string_filter/#ts_shape.transform.filter.string_filter.StringFilter(dataframe)","title":"<code>dataframe</code>","text":"(<code>DataFrame</code>)           \u2013            <p>The DataFrame to be processed.</p>"},{"location":"reference/ts_shape/transform/filter/string_filter/#ts_shape.transform.filter.string_filter.StringFilter(column_name)","title":"<code>column_name</code>","text":"(<code>str</code>, default:                   <code>'systime'</code> )           \u2013            <p>The column to sort by. Default is 'systime'. If the column is not found or is not a time column, the class will attempt to detect other time columns.</p>"},{"location":"reference/ts_shape/transform/filter/string_filter/#ts_shape.transform.filter.string_filter.StringFilter.detect_changes_in_string","title":"detect_changes_in_string  <code>classmethod</code>","text":"<pre><code>detect_changes_in_string(dataframe: DataFrame, column_name: str = 'value_string') -&gt; DataFrame\n</code></pre> <p>Detects changes from row to row in the specified string column.</p> Source code in <code>src/ts_shape/transform/filter/string_filter.py</code> <pre><code>@classmethod\ndef detect_changes_in_string(cls, dataframe: pd.DataFrame, column_name: str = 'value_string') -&gt; pd.DataFrame:\n    \"\"\"Detects changes from row to row in the specified string column.\"\"\"\n    changes_detected = dataframe[column_name].ne(dataframe[column_name].shift())\n    result = dataframe[changes_detected]\n    if result.empty:\n        print(f\"No changes detected in the '{column_name}' column between consecutive rows.\")\n    return result\n</code></pre>"},{"location":"reference/ts_shape/transform/filter/string_filter/#ts_shape.transform.filter.string_filter.StringFilter.filter_na_value_string","title":"filter_na_value_string  <code>classmethod</code>","text":"<pre><code>filter_na_value_string(dataframe: DataFrame, column_name: str = 'value_string') -&gt; DataFrame\n</code></pre> <p>Filters out rows where the specified string column is NA.</p> Source code in <code>src/ts_shape/transform/filter/string_filter.py</code> <pre><code>@classmethod\ndef filter_na_value_string(cls, dataframe: pd.DataFrame, column_name: str = 'value_string') -&gt; pd.DataFrame:\n    \"\"\"Filters out rows where the specified string column is NA.\"\"\"\n    return dataframe[dataframe[column_name].notna()]\n</code></pre>"},{"location":"reference/ts_shape/transform/filter/string_filter/#ts_shape.transform.filter.string_filter.StringFilter.filter_string_contains","title":"filter_string_contains  <code>classmethod</code>","text":"<pre><code>filter_string_contains(dataframe: DataFrame, substring: str, column_name: str = 'value_string') -&gt; DataFrame\n</code></pre> <p>Filters rows where the specified string column contains the provided substring.</p> Source code in <code>src/ts_shape/transform/filter/string_filter.py</code> <pre><code>@classmethod\ndef filter_string_contains(cls, dataframe: pd.DataFrame, substring: str, column_name: str = 'value_string') -&gt; pd.DataFrame:\n    \"\"\"Filters rows where the specified string column contains the provided substring.\"\"\"\n    return dataframe[dataframe[column_name].str.contains(substring, na=False)]\n</code></pre>"},{"location":"reference/ts_shape/transform/filter/string_filter/#ts_shape.transform.filter.string_filter.StringFilter.filter_value_string_match","title":"filter_value_string_match  <code>classmethod</code>","text":"<pre><code>filter_value_string_match(dataframe: DataFrame, string_value: str, column_name: str = 'value_string') -&gt; DataFrame\n</code></pre> <p>Filters rows where the specified string column matches the provided string.</p> Source code in <code>src/ts_shape/transform/filter/string_filter.py</code> <pre><code>@classmethod\ndef filter_value_string_match(cls, dataframe: pd.DataFrame, string_value: str, column_name: str = 'value_string') -&gt; pd.DataFrame:\n    \"\"\"Filters rows where the specified string column matches the provided string.\"\"\"\n    return dataframe[dataframe[column_name] == string_value]\n</code></pre>"},{"location":"reference/ts_shape/transform/filter/string_filter/#ts_shape.transform.filter.string_filter.StringFilter.filter_value_string_not_match","title":"filter_value_string_not_match  <code>classmethod</code>","text":"<pre><code>filter_value_string_not_match(dataframe: DataFrame, string_value: str, column_name: str = 'value_string') -&gt; DataFrame\n</code></pre> <p>Filters rows where the specified string column does not match the provided string.</p> Source code in <code>src/ts_shape/transform/filter/string_filter.py</code> <pre><code>@classmethod\ndef filter_value_string_not_match(cls, dataframe: pd.DataFrame, string_value: str, column_name: str = 'value_string') -&gt; pd.DataFrame:\n    \"\"\"Filters rows where the specified string column does not match the provided string.\"\"\"\n    return dataframe[dataframe[column_name] != string_value]\n</code></pre>"},{"location":"reference/ts_shape/transform/filter/string_filter/#ts_shape.transform.filter.string_filter.StringFilter.get_dataframe","title":"get_dataframe","text":"<pre><code>get_dataframe() -&gt; DataFrame\n</code></pre> <p>Returns the processed DataFrame.</p> Source code in <code>src/ts_shape/utils/base.py</code> <pre><code>def get_dataframe(self) -&gt; pd.DataFrame:\n    \"\"\"Returns the processed DataFrame.\"\"\"\n    return self.dataframe\n</code></pre>"},{"location":"reference/ts_shape/transform/filter/string_filter/#ts_shape.transform.filter.string_filter.StringFilter.regex_clean_value_string","title":"regex_clean_value_string  <code>classmethod</code>","text":"<pre><code>regex_clean_value_string(dataframe: DataFrame, column_name: str = 'value_string', regex_pattern: str = '(\\\\d+)\\\\s*([a-zA-Z]*)', replacement: str = '', regex: bool = True) -&gt; DataFrame\n</code></pre> <p>Applies a regex pattern to clean the specified string column.</p> Source code in <code>src/ts_shape/transform/filter/string_filter.py</code> <pre><code>@classmethod\ndef regex_clean_value_string(cls, dataframe: pd.DataFrame, column_name: str = 'value_string', regex_pattern: str = r'(\\d+)\\s*([a-zA-Z]*)', replacement: str = '', regex: bool = True) -&gt; pd.DataFrame:\n    \"\"\"Applies a regex pattern to clean the specified string column.\"\"\"\n    dataframe[column_name] = dataframe[column_name].str.replace(regex_pattern, replacement, regex=regex)\n    return dataframe\n</code></pre>"},{"location":"reference/ts_shape/transform/functions/__init__/","title":"init","text":""},{"location":"reference/ts_shape/transform/functions/__init__/#ts_shape.transform.functions","title":"ts_shape.transform.functions","text":"<p>Functions</p> <p>Column-wise function application helpers.</p> <ul> <li>LambdaProcessor: Apply vectorized callables to columns.</li> <li>apply_function: Apply a Python callable over a column's values.</li> </ul> <p>Modules:</p> <ul> <li> <code>lambda_func</code>           \u2013            </li> </ul>"},{"location":"reference/ts_shape/transform/functions/lambda_func/","title":"lambda_func","text":""},{"location":"reference/ts_shape/transform/functions/lambda_func/#ts_shape.transform.functions.lambda_func","title":"ts_shape.transform.functions.lambda_func","text":"<p>Classes:</p> <ul> <li> <code>LambdaProcessor</code>           \u2013            <p>Provides class methods for applying lambda or callable functions to columns in a pandas DataFrame.</p> </li> </ul>"},{"location":"reference/ts_shape/transform/functions/lambda_func/#ts_shape.transform.functions.lambda_func.LambdaProcessor","title":"LambdaProcessor","text":"<pre><code>LambdaProcessor(dataframe: DataFrame, column_name: str = 'systime')\n</code></pre> <p>               Bases: <code>Base</code></p> <p>Provides class methods for applying lambda or callable functions to columns in a pandas DataFrame. This class inherits from Base, ensuring consistency with other processors.</p> <p>Parameters:</p> <p>Methods:</p> <ul> <li> <code>apply_function</code>             \u2013              <p>Applies a lambda or callable function to a specified column in the DataFrame.</p> </li> <li> <code>get_dataframe</code>             \u2013              <p>Returns the processed DataFrame.</p> </li> </ul> Source code in <code>src/ts_shape/utils/base.py</code> <pre><code>def __init__(self, dataframe: pd.DataFrame, column_name: str = 'systime') -&gt; None:\n    \"\"\"\n    Initializes the Base with a DataFrame, detects time columns, converts them to datetime,\n    and sorts the DataFrame by the specified column (or the detected time column if applicable).\n\n    Args:\n        dataframe (pd.DataFrame): The DataFrame to be processed.\n        column_name (str): The column to sort by. Default is 'systime'. If the column is not found or is not a time column, the class will attempt to detect other time columns.\n    \"\"\"\n    self.dataframe = dataframe.copy()\n\n    # Attempt to convert the specified column_name to datetime if it exists\n    if column_name in self.dataframe.columns:\n        self.dataframe[column_name] = pd.to_datetime(self.dataframe[column_name], errors='coerce')\n    else:\n        # If the column_name is not in the DataFrame, fallback to automatic time detection\n        time_columns = [col for col in self.dataframe.columns if 'time' in col.lower() or 'date' in col.lower()]\n\n        # Convert all detected time columns to datetime, if any\n        for col in time_columns:\n            self.dataframe[col] = pd.to_datetime(self.dataframe[col], errors='coerce')\n\n        # If any time columns are detected, sort by the first one; otherwise, do nothing\n        if time_columns:\n            column_name = time_columns[0]\n\n    # Sort by the datetime column (either specified or detected)\n    if column_name in self.dataframe.columns:\n        self.dataframe = self.dataframe.sort_values(by=column_name)\n</code></pre>"},{"location":"reference/ts_shape/transform/functions/lambda_func/#ts_shape.transform.functions.lambda_func.LambdaProcessor(dataframe)","title":"<code>dataframe</code>","text":"(<code>DataFrame</code>)           \u2013            <p>The DataFrame to be processed.</p>"},{"location":"reference/ts_shape/transform/functions/lambda_func/#ts_shape.transform.functions.lambda_func.LambdaProcessor(column_name)","title":"<code>column_name</code>","text":"(<code>str</code>, default:                   <code>'systime'</code> )           \u2013            <p>The column to sort by. Default is 'systime'. If the column is not found or is not a time column, the class will attempt to detect other time columns.</p>"},{"location":"reference/ts_shape/transform/functions/lambda_func/#ts_shape.transform.functions.lambda_func.LambdaProcessor.apply_function","title":"apply_function  <code>classmethod</code>","text":"<pre><code>apply_function(dataframe: DataFrame, column_name: str, func: Callable[[Any], Any]) -&gt; DataFrame\n</code></pre> <p>Applies a lambda or callable function to a specified column in the DataFrame.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>pd.DataFrame: The DataFrame with the transformed column.</p> </li> </ul> Source code in <code>src/ts_shape/transform/functions/lambda_func.py</code> <pre><code>@classmethod\ndef apply_function(cls, dataframe: pd.DataFrame, column_name: str, func: Callable[[Any], Any]) -&gt; pd.DataFrame:\n    \"\"\"\n    Applies a lambda or callable function to a specified column in the DataFrame.\n\n    Args:\n        dataframe (pd.DataFrame): The DataFrame containing the data.\n        column_name (str): The name of the column to apply the function to.\n        func (Callable): The lambda function or callable to apply to the column.\n\n    Returns:\n        pd.DataFrame: The DataFrame with the transformed column.\n    \"\"\"\n    if column_name not in dataframe.columns:\n        raise ValueError(f\"Column '{column_name}' not found in DataFrame.\")\n\n    dataframe[column_name] = func(dataframe[column_name].values)\n    return dataframe\n</code></pre>"},{"location":"reference/ts_shape/transform/functions/lambda_func/#ts_shape.transform.functions.lambda_func.LambdaProcessor.apply_function(dataframe)","title":"<code>dataframe</code>","text":"(<code>DataFrame</code>)           \u2013            <p>The DataFrame containing the data.</p>"},{"location":"reference/ts_shape/transform/functions/lambda_func/#ts_shape.transform.functions.lambda_func.LambdaProcessor.apply_function(column_name)","title":"<code>column_name</code>","text":"(<code>str</code>)           \u2013            <p>The name of the column to apply the function to.</p>"},{"location":"reference/ts_shape/transform/functions/lambda_func/#ts_shape.transform.functions.lambda_func.LambdaProcessor.apply_function(func)","title":"<code>func</code>","text":"(<code>Callable</code>)           \u2013            <p>The lambda function or callable to apply to the column.</p>"},{"location":"reference/ts_shape/transform/functions/lambda_func/#ts_shape.transform.functions.lambda_func.LambdaProcessor.get_dataframe","title":"get_dataframe","text":"<pre><code>get_dataframe() -&gt; DataFrame\n</code></pre> <p>Returns the processed DataFrame.</p> Source code in <code>src/ts_shape/utils/base.py</code> <pre><code>def get_dataframe(self) -&gt; pd.DataFrame:\n    \"\"\"Returns the processed DataFrame.\"\"\"\n    return self.dataframe\n</code></pre>"},{"location":"reference/ts_shape/transform/time_functions/__init__/","title":"init","text":""},{"location":"reference/ts_shape/transform/time_functions/__init__/#ts_shape.transform.time_functions","title":"ts_shape.transform.time_functions","text":"<p>Time Functions</p> <p>Timestamp conversion and timezone operations.</p> <ul> <li>TimestampConverter: Convert integer timestamps to tz-aware datetimes.</li> <li> <p>convert_to_datetime: Convert s/ms/us/ns to datetime in a timezone.</p> </li> <li> <p>TimezoneShift: Timezone localization/conversion helpers.</p> </li> <li>shift_timezone: Convert timezones in-place.</li> <li>add_timezone_column: Add a converted timestamp column.</li> <li>detect_timezone_awareness: Check tz-awareness of a column.</li> <li>revert_to_original_timezone: Convert back to original tz.</li> <li>calculate_time_difference: Difference between two timestamp columns.</li> </ul> <p>Modules:</p> <ul> <li> <code>timestamp_converter</code>           \u2013            </li> <li> <code>timezone_shift</code>           \u2013            </li> </ul>"},{"location":"reference/ts_shape/transform/time_functions/timestamp_converter/","title":"timestamp_converter","text":""},{"location":"reference/ts_shape/transform/time_functions/timestamp_converter/#ts_shape.transform.time_functions.timestamp_converter","title":"ts_shape.transform.time_functions.timestamp_converter","text":"<p>Classes:</p> <ul> <li> <code>TimestampConverter</code>           \u2013            <p>A class dedicated to converting high-precision timestamp data (e.g., in seconds, milliseconds, microseconds, or nanoseconds)</p> </li> </ul>"},{"location":"reference/ts_shape/transform/time_functions/timestamp_converter/#ts_shape.transform.time_functions.timestamp_converter.TimestampConverter","title":"TimestampConverter","text":"<pre><code>TimestampConverter(dataframe: DataFrame, column_name: str = 'systime')\n</code></pre> <p>               Bases: <code>Base</code></p> <p>A class dedicated to converting high-precision timestamp data (e.g., in seconds, milliseconds, microseconds, or nanoseconds) to standard datetime formats with optional timezone adjustment.</p> <p>Parameters:</p> <p>Methods:</p> <ul> <li> <code>convert_to_datetime</code>             \u2013              <p>Converts specified columns from a given timestamp unit to datetime format in a target timezone.</p> </li> <li> <code>get_dataframe</code>             \u2013              <p>Returns the processed DataFrame.</p> </li> </ul> Source code in <code>src/ts_shape/utils/base.py</code> <pre><code>def __init__(self, dataframe: pd.DataFrame, column_name: str = 'systime') -&gt; None:\n    \"\"\"\n    Initializes the Base with a DataFrame, detects time columns, converts them to datetime,\n    and sorts the DataFrame by the specified column (or the detected time column if applicable).\n\n    Args:\n        dataframe (pd.DataFrame): The DataFrame to be processed.\n        column_name (str): The column to sort by. Default is 'systime'. If the column is not found or is not a time column, the class will attempt to detect other time columns.\n    \"\"\"\n    self.dataframe = dataframe.copy()\n\n    # Attempt to convert the specified column_name to datetime if it exists\n    if column_name in self.dataframe.columns:\n        self.dataframe[column_name] = pd.to_datetime(self.dataframe[column_name], errors='coerce')\n    else:\n        # If the column_name is not in the DataFrame, fallback to automatic time detection\n        time_columns = [col for col in self.dataframe.columns if 'time' in col.lower() or 'date' in col.lower()]\n\n        # Convert all detected time columns to datetime, if any\n        for col in time_columns:\n            self.dataframe[col] = pd.to_datetime(self.dataframe[col], errors='coerce')\n\n        # If any time columns are detected, sort by the first one; otherwise, do nothing\n        if time_columns:\n            column_name = time_columns[0]\n\n    # Sort by the datetime column (either specified or detected)\n    if column_name in self.dataframe.columns:\n        self.dataframe = self.dataframe.sort_values(by=column_name)\n</code></pre>"},{"location":"reference/ts_shape/transform/time_functions/timestamp_converter/#ts_shape.transform.time_functions.timestamp_converter.TimestampConverter(dataframe)","title":"<code>dataframe</code>","text":"(<code>DataFrame</code>)           \u2013            <p>The DataFrame to be processed.</p>"},{"location":"reference/ts_shape/transform/time_functions/timestamp_converter/#ts_shape.transform.time_functions.timestamp_converter.TimestampConverter(column_name)","title":"<code>column_name</code>","text":"(<code>str</code>, default:                   <code>'systime'</code> )           \u2013            <p>The column to sort by. Default is 'systime'. If the column is not found or is not a time column, the class will attempt to detect other time columns.</p>"},{"location":"reference/ts_shape/transform/time_functions/timestamp_converter/#ts_shape.transform.time_functions.timestamp_converter.TimestampConverter.convert_to_datetime","title":"convert_to_datetime  <code>classmethod</code>","text":"<pre><code>convert_to_datetime(dataframe: DataFrame, columns: list, unit: str = 'ns', timezone: str = 'UTC') -&gt; DataFrame\n</code></pre> <p>Converts specified columns from a given timestamp unit to datetime format in a target timezone.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>pd.DataFrame: A DataFrame with the converted datetime columns in the specified timezone.</p> </li> </ul> Source code in <code>src/ts_shape/transform/time_functions/timestamp_converter.py</code> <pre><code>@classmethod\ndef convert_to_datetime(cls, dataframe: pd.DataFrame, columns: list, unit: str = 'ns', timezone: str = 'UTC') -&gt; pd.DataFrame:\n    \"\"\"\n    Converts specified columns from a given timestamp unit to datetime format in a target timezone.\n\n    Args:\n        dataframe (pd.DataFrame): The DataFrame containing the data.\n        columns (list): A list of column names with timestamp data to convert.\n        unit (str): The unit of the timestamps ('s', 'ms', 'us', or 'ns').\n        timezone (str): The target timezone for the converted datetime (default is 'UTC').\n\n    Returns:\n        pd.DataFrame: A DataFrame with the converted datetime columns in the specified timezone.\n    \"\"\"\n    # Validate unit\n    valid_units = ['s', 'ms', 'us', 'ns']\n    if unit not in valid_units:\n        raise ValueError(f\"Invalid unit '{unit}'. Must be one of {valid_units}.\")\n\n    # Validate timezone\n    if timezone not in pytz.all_timezones:\n        raise ValueError(f\"Invalid timezone '{timezone}'. Use a valid timezone name from pytz.all_timezones.\")\n\n    df = dataframe.copy()\n    for col in columns:\n        # Convert timestamps to datetime in UTC first\n        df[col] = pd.to_datetime(df[col], unit=unit, utc=True)\n        # Adjust to the target timezone\n        df[col] = df[col].dt.tz_convert(timezone)\n\n    return df\n</code></pre>"},{"location":"reference/ts_shape/transform/time_functions/timestamp_converter/#ts_shape.transform.time_functions.timestamp_converter.TimestampConverter.convert_to_datetime(dataframe)","title":"<code>dataframe</code>","text":"(<code>DataFrame</code>)           \u2013            <p>The DataFrame containing the data.</p>"},{"location":"reference/ts_shape/transform/time_functions/timestamp_converter/#ts_shape.transform.time_functions.timestamp_converter.TimestampConverter.convert_to_datetime(columns)","title":"<code>columns</code>","text":"(<code>list</code>)           \u2013            <p>A list of column names with timestamp data to convert.</p>"},{"location":"reference/ts_shape/transform/time_functions/timestamp_converter/#ts_shape.transform.time_functions.timestamp_converter.TimestampConverter.convert_to_datetime(unit)","title":"<code>unit</code>","text":"(<code>str</code>, default:                   <code>'ns'</code> )           \u2013            <p>The unit of the timestamps ('s', 'ms', 'us', or 'ns').</p>"},{"location":"reference/ts_shape/transform/time_functions/timestamp_converter/#ts_shape.transform.time_functions.timestamp_converter.TimestampConverter.convert_to_datetime(timezone)","title":"<code>timezone</code>","text":"(<code>str</code>, default:                   <code>'UTC'</code> )           \u2013            <p>The target timezone for the converted datetime (default is 'UTC').</p>"},{"location":"reference/ts_shape/transform/time_functions/timestamp_converter/#ts_shape.transform.time_functions.timestamp_converter.TimestampConverter.get_dataframe","title":"get_dataframe","text":"<pre><code>get_dataframe() -&gt; DataFrame\n</code></pre> <p>Returns the processed DataFrame.</p> Source code in <code>src/ts_shape/utils/base.py</code> <pre><code>def get_dataframe(self) -&gt; pd.DataFrame:\n    \"\"\"Returns the processed DataFrame.\"\"\"\n    return self.dataframe\n</code></pre>"},{"location":"reference/ts_shape/transform/time_functions/timezone_shift/","title":"timezone_shift","text":""},{"location":"reference/ts_shape/transform/time_functions/timezone_shift/#ts_shape.transform.time_functions.timezone_shift","title":"ts_shape.transform.time_functions.timezone_shift","text":"<p>Classes:</p> <ul> <li> <code>TimezoneShift</code>           \u2013            <p>A class for shifting timestamps in a DataFrame to a different timezone, with methods to handle timezone localization and conversion.</p> </li> </ul>"},{"location":"reference/ts_shape/transform/time_functions/timezone_shift/#ts_shape.transform.time_functions.timezone_shift.TimezoneShift","title":"TimezoneShift","text":"<pre><code>TimezoneShift(dataframe: DataFrame, column_name: str = 'systime')\n</code></pre> <p>               Bases: <code>Base</code></p> <p>A class for shifting timestamps in a DataFrame to a different timezone, with methods to handle timezone localization and conversion.</p> <p>Parameters:</p> <p>Methods:</p> <ul> <li> <code>add_timezone_column</code>             \u2013              <p>Creates a new column with timestamps converted from an input timezone to a target timezone, without altering the original column.</p> </li> <li> <code>calculate_time_difference</code>             \u2013              <p>Calculates the time difference between two timestamp columns.</p> </li> <li> <code>detect_timezone_awareness</code>             \u2013              <p>Detects if a time column in a DataFrame is timezone-aware.</p> </li> <li> <code>get_dataframe</code>             \u2013              <p>Returns the processed DataFrame.</p> </li> <li> <code>list_available_timezones</code>             \u2013              <p>Returns a list of all available timezones.</p> </li> <li> <code>revert_to_original_timezone</code>             \u2013              <p>Reverts a timezone-shifted time column back to the original timezone.</p> </li> <li> <code>shift_timezone</code>             \u2013              <p>Shifts timestamps in the specified column of a DataFrame from a given timezone to a target timezone.</p> </li> </ul> Source code in <code>src/ts_shape/utils/base.py</code> <pre><code>def __init__(self, dataframe: pd.DataFrame, column_name: str = 'systime') -&gt; None:\n    \"\"\"\n    Initializes the Base with a DataFrame, detects time columns, converts them to datetime,\n    and sorts the DataFrame by the specified column (or the detected time column if applicable).\n\n    Args:\n        dataframe (pd.DataFrame): The DataFrame to be processed.\n        column_name (str): The column to sort by. Default is 'systime'. If the column is not found or is not a time column, the class will attempt to detect other time columns.\n    \"\"\"\n    self.dataframe = dataframe.copy()\n\n    # Attempt to convert the specified column_name to datetime if it exists\n    if column_name in self.dataframe.columns:\n        self.dataframe[column_name] = pd.to_datetime(self.dataframe[column_name], errors='coerce')\n    else:\n        # If the column_name is not in the DataFrame, fallback to automatic time detection\n        time_columns = [col for col in self.dataframe.columns if 'time' in col.lower() or 'date' in col.lower()]\n\n        # Convert all detected time columns to datetime, if any\n        for col in time_columns:\n            self.dataframe[col] = pd.to_datetime(self.dataframe[col], errors='coerce')\n\n        # If any time columns are detected, sort by the first one; otherwise, do nothing\n        if time_columns:\n            column_name = time_columns[0]\n\n    # Sort by the datetime column (either specified or detected)\n    if column_name in self.dataframe.columns:\n        self.dataframe = self.dataframe.sort_values(by=column_name)\n</code></pre>"},{"location":"reference/ts_shape/transform/time_functions/timezone_shift/#ts_shape.transform.time_functions.timezone_shift.TimezoneShift(dataframe)","title":"<code>dataframe</code>","text":"(<code>DataFrame</code>)           \u2013            <p>The DataFrame to be processed.</p>"},{"location":"reference/ts_shape/transform/time_functions/timezone_shift/#ts_shape.transform.time_functions.timezone_shift.TimezoneShift(column_name)","title":"<code>column_name</code>","text":"(<code>str</code>, default:                   <code>'systime'</code> )           \u2013            <p>The column to sort by. Default is 'systime'. If the column is not found or is not a time column, the class will attempt to detect other time columns.</p>"},{"location":"reference/ts_shape/transform/time_functions/timezone_shift/#ts_shape.transform.time_functions.timezone_shift.TimezoneShift.add_timezone_column","title":"add_timezone_column  <code>classmethod</code>","text":"<pre><code>add_timezone_column(dataframe: DataFrame, time_column: str, input_timezone: str, target_timezone: str) -&gt; DataFrame\n</code></pre> <p>Creates a new column with timestamps converted from an input timezone to a target timezone, without altering the original column.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>pd.DataFrame: A DataFrame with an additional column for the shifted timezone.</p> </li> </ul> Source code in <code>src/ts_shape/transform/time_functions/timezone_shift.py</code> <pre><code>@classmethod\ndef add_timezone_column(cls, dataframe: pd.DataFrame, time_column: str, input_timezone: str, target_timezone: str) -&gt; pd.DataFrame:\n    \"\"\"\n    Creates a new column with timestamps converted from an input timezone to a target timezone, without altering the original column.\n\n    Args:\n        dataframe (pd.DataFrame): The DataFrame containing the data.\n        time_column (str): The name of the time column to convert.\n        input_timezone (str): The timezone of the input timestamps.\n        target_timezone (str): The target timezone.\n\n    Returns:\n        pd.DataFrame: A DataFrame with an additional column for the shifted timezone.\n    \"\"\"\n    # Duplicate the DataFrame to prevent modifying the original column\n    df_copy = dataframe.copy()\n\n    # Create the new timezone-shifted column\n    new_column = f\"{time_column}_{target_timezone.replace('/', '_')}\"\n    df_copy[new_column] = df_copy[time_column]\n\n    # Apply the timezone shift to the new column\n    df_copy = cls.shift_timezone(df_copy, new_column, input_timezone, target_timezone)\n\n    return df_copy\n</code></pre>"},{"location":"reference/ts_shape/transform/time_functions/timezone_shift/#ts_shape.transform.time_functions.timezone_shift.TimezoneShift.add_timezone_column(dataframe)","title":"<code>dataframe</code>","text":"(<code>DataFrame</code>)           \u2013            <p>The DataFrame containing the data.</p>"},{"location":"reference/ts_shape/transform/time_functions/timezone_shift/#ts_shape.transform.time_functions.timezone_shift.TimezoneShift.add_timezone_column(time_column)","title":"<code>time_column</code>","text":"(<code>str</code>)           \u2013            <p>The name of the time column to convert.</p>"},{"location":"reference/ts_shape/transform/time_functions/timezone_shift/#ts_shape.transform.time_functions.timezone_shift.TimezoneShift.add_timezone_column(input_timezone)","title":"<code>input_timezone</code>","text":"(<code>str</code>)           \u2013            <p>The timezone of the input timestamps.</p>"},{"location":"reference/ts_shape/transform/time_functions/timezone_shift/#ts_shape.transform.time_functions.timezone_shift.TimezoneShift.add_timezone_column(target_timezone)","title":"<code>target_timezone</code>","text":"(<code>str</code>)           \u2013            <p>The target timezone.</p>"},{"location":"reference/ts_shape/transform/time_functions/timezone_shift/#ts_shape.transform.time_functions.timezone_shift.TimezoneShift.calculate_time_difference","title":"calculate_time_difference  <code>classmethod</code>","text":"<pre><code>calculate_time_difference(dataframe: DataFrame, start_column: str, end_column: str) -&gt; Series\n</code></pre> <p>Calculates the time difference between two timestamp columns.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>Series</code>           \u2013            <p>pd.Series: A Series with the time differences in seconds.</p> </li> </ul> Source code in <code>src/ts_shape/transform/time_functions/timezone_shift.py</code> <pre><code>@classmethod\ndef calculate_time_difference(cls, dataframe: pd.DataFrame, start_column: str, end_column: str) -&gt; pd.Series:\n    \"\"\"\n    Calculates the time difference between two timestamp columns.\n\n    Args:\n        dataframe (pd.DataFrame): The DataFrame containing the data.\n        start_column (str): The name of the start time column.\n        end_column (str): The name of the end time column.\n\n    Returns:\n        pd.Series: A Series with the time differences in seconds.\n    \"\"\"\n    # Check if both columns are timezone-aware or both are timezone-naive\n    start_is_aware = dataframe[start_column].dt.tz is not None\n    end_is_aware = dataframe[end_column].dt.tz is not None\n\n    if start_is_aware != end_is_aware:\n        raise ValueError(\"Both columns must be either timezone-aware or timezone-naive.\")\n\n    # If timezone-aware, convert both columns to UTC for comparison\n    if start_is_aware:\n        start_times = dataframe[start_column].dt.tz_convert('UTC')\n        end_times = dataframe[end_column].dt.tz_convert('UTC')\n    else:\n        start_times = dataframe[start_column]\n        end_times = dataframe[end_column]\n\n    # Calculate the difference in seconds\n    time_difference = (end_times - start_times).dt.total_seconds()\n\n    return time_difference\n</code></pre>"},{"location":"reference/ts_shape/transform/time_functions/timezone_shift/#ts_shape.transform.time_functions.timezone_shift.TimezoneShift.calculate_time_difference(dataframe)","title":"<code>dataframe</code>","text":"(<code>DataFrame</code>)           \u2013            <p>The DataFrame containing the data.</p>"},{"location":"reference/ts_shape/transform/time_functions/timezone_shift/#ts_shape.transform.time_functions.timezone_shift.TimezoneShift.calculate_time_difference(start_column)","title":"<code>start_column</code>","text":"(<code>str</code>)           \u2013            <p>The name of the start time column.</p>"},{"location":"reference/ts_shape/transform/time_functions/timezone_shift/#ts_shape.transform.time_functions.timezone_shift.TimezoneShift.calculate_time_difference(end_column)","title":"<code>end_column</code>","text":"(<code>str</code>)           \u2013            <p>The name of the end time column.</p>"},{"location":"reference/ts_shape/transform/time_functions/timezone_shift/#ts_shape.transform.time_functions.timezone_shift.TimezoneShift.detect_timezone_awareness","title":"detect_timezone_awareness  <code>classmethod</code>","text":"<pre><code>detect_timezone_awareness(dataframe: DataFrame, time_column: str) -&gt; bool\n</code></pre> <p>Detects if a time column in a DataFrame is timezone-aware.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>bool</code> (              <code>bool</code> )          \u2013            <p>True if the column is timezone-aware, False otherwise.</p> </li> </ul> Source code in <code>src/ts_shape/transform/time_functions/timezone_shift.py</code> <pre><code>@classmethod\ndef detect_timezone_awareness(cls, dataframe: pd.DataFrame, time_column: str) -&gt; bool:\n    \"\"\"\n    Detects if a time column in a DataFrame is timezone-aware.\n\n    Args:\n        dataframe (pd.DataFrame): The DataFrame containing the data.\n        time_column (str): The name of the time column to check.\n\n    Returns:\n        bool: True if the column is timezone-aware, False otherwise.\n    \"\"\"\n    return dataframe[time_column].dt.tz is not None\n</code></pre>"},{"location":"reference/ts_shape/transform/time_functions/timezone_shift/#ts_shape.transform.time_functions.timezone_shift.TimezoneShift.detect_timezone_awareness(dataframe)","title":"<code>dataframe</code>","text":"(<code>DataFrame</code>)           \u2013            <p>The DataFrame containing the data.</p>"},{"location":"reference/ts_shape/transform/time_functions/timezone_shift/#ts_shape.transform.time_functions.timezone_shift.TimezoneShift.detect_timezone_awareness(time_column)","title":"<code>time_column</code>","text":"(<code>str</code>)           \u2013            <p>The name of the time column to check.</p>"},{"location":"reference/ts_shape/transform/time_functions/timezone_shift/#ts_shape.transform.time_functions.timezone_shift.TimezoneShift.get_dataframe","title":"get_dataframe","text":"<pre><code>get_dataframe() -&gt; DataFrame\n</code></pre> <p>Returns the processed DataFrame.</p> Source code in <code>src/ts_shape/utils/base.py</code> <pre><code>def get_dataframe(self) -&gt; pd.DataFrame:\n    \"\"\"Returns the processed DataFrame.\"\"\"\n    return self.dataframe\n</code></pre>"},{"location":"reference/ts_shape/transform/time_functions/timezone_shift/#ts_shape.transform.time_functions.timezone_shift.TimezoneShift.list_available_timezones","title":"list_available_timezones  <code>classmethod</code>","text":"<pre><code>list_available_timezones() -&gt; list\n</code></pre> <p>Returns a list of all available timezones.</p> <p>Returns:</p> <ul> <li> <code>list</code> (              <code>list</code> )          \u2013            <p>A list of strings representing all available timezones.</p> </li> </ul> Source code in <code>src/ts_shape/transform/time_functions/timezone_shift.py</code> <pre><code>@classmethod\ndef list_available_timezones(cls) -&gt; list:\n    \"\"\"\n    Returns a list of all available timezones.\n\n    Returns:\n        list: A list of strings representing all available timezones.\n    \"\"\"\n    return pytz.all_timezones\n</code></pre>"},{"location":"reference/ts_shape/transform/time_functions/timezone_shift/#ts_shape.transform.time_functions.timezone_shift.TimezoneShift.revert_to_original_timezone","title":"revert_to_original_timezone  <code>classmethod</code>","text":"<pre><code>revert_to_original_timezone(dataframe: DataFrame, time_column: str, original_timezone: str) -&gt; DataFrame\n</code></pre> <p>Reverts a timezone-shifted time column back to the original timezone.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>pd.DataFrame: A DataFrame with timestamps reverted to the original timezone.</p> </li> </ul> Source code in <code>src/ts_shape/transform/time_functions/timezone_shift.py</code> <pre><code>@classmethod\ndef revert_to_original_timezone(cls, dataframe: pd.DataFrame, time_column: str, original_timezone: str) -&gt; pd.DataFrame:\n    \"\"\"\n    Reverts a timezone-shifted time column back to the original timezone.\n\n    Args:\n        dataframe (pd.DataFrame): The DataFrame containing the data.\n        time_column (str): The name of the time column to revert.\n        original_timezone (str): The original timezone to revert to.\n\n    Returns:\n        pd.DataFrame: A DataFrame with timestamps reverted to the original timezone.\n    \"\"\"\n    # Validate the original timezone\n    if original_timezone not in pytz.all_timezones:\n        raise ValueError(f\"Invalid original timezone: {original_timezone}\")\n\n    # Convert to the original timezone\n    dataframe[time_column] = dataframe[time_column].dt.tz_convert(original_timezone)\n\n    return dataframe\n</code></pre>"},{"location":"reference/ts_shape/transform/time_functions/timezone_shift/#ts_shape.transform.time_functions.timezone_shift.TimezoneShift.revert_to_original_timezone(dataframe)","title":"<code>dataframe</code>","text":"(<code>DataFrame</code>)           \u2013            <p>The DataFrame containing the data.</p>"},{"location":"reference/ts_shape/transform/time_functions/timezone_shift/#ts_shape.transform.time_functions.timezone_shift.TimezoneShift.revert_to_original_timezone(time_column)","title":"<code>time_column</code>","text":"(<code>str</code>)           \u2013            <p>The name of the time column to revert.</p>"},{"location":"reference/ts_shape/transform/time_functions/timezone_shift/#ts_shape.transform.time_functions.timezone_shift.TimezoneShift.revert_to_original_timezone(original_timezone)","title":"<code>original_timezone</code>","text":"(<code>str</code>)           \u2013            <p>The original timezone to revert to.</p>"},{"location":"reference/ts_shape/transform/time_functions/timezone_shift/#ts_shape.transform.time_functions.timezone_shift.TimezoneShift.shift_timezone","title":"shift_timezone  <code>classmethod</code>","text":"<pre><code>shift_timezone(dataframe: DataFrame, time_column: str, input_timezone: str, target_timezone: str) -&gt; DataFrame\n</code></pre> <p>Shifts timestamps in the specified column of a DataFrame from a given timezone to a target timezone.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>pd.DataFrame: A DataFrame with timestamps converted to the target timezone.</p> </li> </ul> Source code in <code>src/ts_shape/transform/time_functions/timezone_shift.py</code> <pre><code>@classmethod\ndef shift_timezone(cls, dataframe: pd.DataFrame, time_column: str, input_timezone: str, target_timezone: str) -&gt; pd.DataFrame:\n    \"\"\"\n    Shifts timestamps in the specified column of a DataFrame from a given timezone to a target timezone.\n\n    Args:\n        dataframe (pd.DataFrame): The DataFrame containing the data.\n        time_column (str): The name of the time column to convert.\n        input_timezone (str): The timezone of the input timestamps (e.g., 'UTC' or 'America/New_York').\n        target_timezone (str): The target timezone to shift to (e.g., 'America/New_York').\n\n    Returns:\n        pd.DataFrame: A DataFrame with timestamps converted to the target timezone.\n    \"\"\"\n    # Validate timezones\n    if input_timezone not in pytz.all_timezones:\n        raise ValueError(f\"Invalid input timezone: {input_timezone}\")\n    if target_timezone not in pytz.all_timezones:\n        raise ValueError(f\"Invalid target timezone: {target_timezone}\")\n\n    # Ensure the time column is in datetime format\n    if not pd.api.types.is_datetime64_any_dtype(dataframe[time_column]):\n        raise ValueError(f\"Column '{time_column}' must contain datetime values.\")\n\n    # Localize to the specified input timezone if timestamps are naive\n    dataframe[time_column] = pd.to_datetime(dataframe[time_column])\n    if dataframe[time_column].dt.tz is None:\n        dataframe[time_column] = dataframe[time_column].dt.tz_localize(input_timezone)\n    else:\n        # Convert from the existing timezone to the specified input timezone, if they differ\n        dataframe[time_column] = dataframe[time_column].dt.tz_convert(input_timezone)\n\n    # Convert to the target timezone\n    dataframe[time_column] = dataframe[time_column].dt.tz_convert(target_timezone)\n\n    return dataframe\n</code></pre>"},{"location":"reference/ts_shape/transform/time_functions/timezone_shift/#ts_shape.transform.time_functions.timezone_shift.TimezoneShift.shift_timezone(dataframe)","title":"<code>dataframe</code>","text":"(<code>DataFrame</code>)           \u2013            <p>The DataFrame containing the data.</p>"},{"location":"reference/ts_shape/transform/time_functions/timezone_shift/#ts_shape.transform.time_functions.timezone_shift.TimezoneShift.shift_timezone(time_column)","title":"<code>time_column</code>","text":"(<code>str</code>)           \u2013            <p>The name of the time column to convert.</p>"},{"location":"reference/ts_shape/transform/time_functions/timezone_shift/#ts_shape.transform.time_functions.timezone_shift.TimezoneShift.shift_timezone(input_timezone)","title":"<code>input_timezone</code>","text":"(<code>str</code>)           \u2013            <p>The timezone of the input timestamps (e.g., 'UTC' or 'America/New_York').</p>"},{"location":"reference/ts_shape/transform/time_functions/timezone_shift/#ts_shape.transform.time_functions.timezone_shift.TimezoneShift.shift_timezone(target_timezone)","title":"<code>target_timezone</code>","text":"(<code>str</code>)           \u2013            <p>The target timezone to shift to (e.g., 'America/New_York').</p>"},{"location":"reference/ts_shape/utils/__init__/","title":"init","text":""},{"location":"reference/ts_shape/utils/__init__/#ts_shape.utils","title":"ts_shape.utils","text":"<p>Utils</p> <p>Shared utilities used across modules.</p> <ul> <li>Base: Normalize and sort DataFrames by time columns.</li> <li>get_dataframe: Return the processed DataFrame copy.</li> </ul> <p>Modules:</p> <ul> <li> <code>base</code>           \u2013            </li> </ul>"},{"location":"reference/ts_shape/utils/base/","title":"base","text":""},{"location":"reference/ts_shape/utils/base/#ts_shape.utils.base","title":"ts_shape.utils.base","text":"<p>Classes:</p> <ul> <li> <code>Base</code>           \u2013            </li> </ul>"},{"location":"reference/ts_shape/utils/base/#ts_shape.utils.base.Base","title":"Base","text":"<pre><code>Base(dataframe: DataFrame, column_name: str = 'systime')\n</code></pre> <p>Parameters:</p> <p>Methods:</p> <ul> <li> <code>get_dataframe</code>             \u2013              <p>Returns the processed DataFrame.</p> </li> </ul> Source code in <code>src/ts_shape/utils/base.py</code> <pre><code>def __init__(self, dataframe: pd.DataFrame, column_name: str = 'systime') -&gt; None:\n    \"\"\"\n    Initializes the Base with a DataFrame, detects time columns, converts them to datetime,\n    and sorts the DataFrame by the specified column (or the detected time column if applicable).\n\n    Args:\n        dataframe (pd.DataFrame): The DataFrame to be processed.\n        column_name (str): The column to sort by. Default is 'systime'. If the column is not found or is not a time column, the class will attempt to detect other time columns.\n    \"\"\"\n    self.dataframe = dataframe.copy()\n\n    # Attempt to convert the specified column_name to datetime if it exists\n    if column_name in self.dataframe.columns:\n        self.dataframe[column_name] = pd.to_datetime(self.dataframe[column_name], errors='coerce')\n    else:\n        # If the column_name is not in the DataFrame, fallback to automatic time detection\n        time_columns = [col for col in self.dataframe.columns if 'time' in col.lower() or 'date' in col.lower()]\n\n        # Convert all detected time columns to datetime, if any\n        for col in time_columns:\n            self.dataframe[col] = pd.to_datetime(self.dataframe[col], errors='coerce')\n\n        # If any time columns are detected, sort by the first one; otherwise, do nothing\n        if time_columns:\n            column_name = time_columns[0]\n\n    # Sort by the datetime column (either specified or detected)\n    if column_name in self.dataframe.columns:\n        self.dataframe = self.dataframe.sort_values(by=column_name)\n</code></pre>"},{"location":"reference/ts_shape/utils/base/#ts_shape.utils.base.Base(dataframe)","title":"<code>dataframe</code>","text":"(<code>DataFrame</code>)           \u2013            <p>The DataFrame to be processed.</p>"},{"location":"reference/ts_shape/utils/base/#ts_shape.utils.base.Base(column_name)","title":"<code>column_name</code>","text":"(<code>str</code>, default:                   <code>'systime'</code> )           \u2013            <p>The column to sort by. Default is 'systime'. If the column is not found or is not a time column, the class will attempt to detect other time columns.</p>"},{"location":"reference/ts_shape/utils/base/#ts_shape.utils.base.Base.get_dataframe","title":"get_dataframe","text":"<pre><code>get_dataframe() -&gt; DataFrame\n</code></pre> <p>Returns the processed DataFrame.</p> Source code in <code>src/ts_shape/utils/base.py</code> <pre><code>def get_dataframe(self) -&gt; pd.DataFrame:\n    \"\"\"Returns the processed DataFrame.\"\"\"\n    return self.dataframe\n</code></pre>"},{"location":"usage/","title":"Quick Start Guide","text":"<p>This guide walks you through a simple usage example of the <code>ts-shape</code> library to load, transform, and analyze time series data.</p>"},{"location":"usage/#installation","title":"Installation","text":"<p>You can install the library via pip:</p> <pre><code>pip install ts-shape\n</code></pre>"},{"location":"usage/#example-workflow","title":"Example Workflow","text":""},{"location":"usage/#1-import-modules","title":"1. Import Modules","text":"<pre><code>from ts_shape.loader.timeseries import parquet_loader\nfrom ts_shape.transform.filter import boolean_filter\nfrom ts_shape.transform.time_functions import timestamp_converter, timezone_shift\nfrom ts_shape.features.stats import string_stats\n</code></pre>"},{"location":"usage/#2-load-time-series-data","title":"2. Load Time Series Data","text":"<p>Load all parquet files from a given directory:</p> <pre><code>base_path = 'path/to/your/parquet/files'\ndf_all = parquet_loader.ParquetLoader.load_all_files(base_path)\n</code></pre>"},{"location":"usage/#3-filter-data","title":"3. Filter Data","text":"<p>Filter rows where the column <code>is_delta</code> is <code>True</code>:</p> <pre><code>df_is_delta = boolean_filter.IsDeltaFilter.filter_is_delta_true(df_all)\n</code></pre>"},{"location":"usage/#4-convert-timestamps","title":"4. Convert Timestamps","text":"<p>Convert Unix nanosecond timestamps to timezone-aware datetime objects:</p> <pre><code>df_timestamp = timestamp_converter.TimestampConverter.convert_to_datetime(\n    dataframe=df_is_delta,\n    columns=['systime', 'plctime'],\n    unit='ns',\n    timezone='UTC'\n)\n</code></pre>"},{"location":"usage/#5-shift-timezone","title":"5. Shift Timezone","text":"<p>Convert timestamps from UTC to local time (e.g., Europe/Berlin):</p> <pre><code>df_timestamp_shift = timezone_shift.TimezoneShift.shift_timezone(\n    dataframe=df_timestamp,\n    time_column='systime',\n    input_timezone='UTC',\n    target_timezone='Europe/Berlin'\n)\n</code></pre>"},{"location":"usage/#next-steps","title":"Next Steps","text":"<ul> <li>Explore feature extraction with <code>ts_shape.features</code>.</li> <li>Chain multiple transformations into a pipeline.</li> </ul>"}]}